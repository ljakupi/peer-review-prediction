review_file_id,comments,score
338.json,this paper investigates the cold start problem in review spam detection the authors first qualitatively and quantitatively analyze the cold start problem they observe that there is no enough prior data from a new user in this realistic scenario the traditional features fail to help to identify review spam instead they turn to rely on the abundant textual and behavioral information of the existing reviewer to augment the information of a new user in specific they propose a neural network to represent the review of the new reviewer with the learnt word embedding and jointly encoded behavioral information in the experiments the authors make comparisons with traditional methods and show the effectiveness of their model strengths the paper is well organized and clearly written the idea of jointly encoding texts and behaviors is interesting the cold start problem is actually an urgent problem to several online review analysis applications in my knowledge the previous work has not yet attempted to tackle this problem this paper is meaningful and presents a reasonable analysis and the results of the proposed model can also be available for downstream detection models weaknesses in experiments the author set the window width of the filters in the cnn module to did the author try other window widths for example width to extract unigram features to trigram or use them together the authors may add more details about the previous work in the related work section more specifically description would help the readers to understand the task clearly there are also some typos to be corrected sec making purchase decision should be making a the purchase decision sec are devoted to explore should be are devoted to exploring sec there is on sufficient behaviors should be there are no sufficient behaviors sec on business trip should be on a business trip sec there are abundant behavior information should be there is abundant behavior sec the new reviewer only provide us should be the new reviewer only provides us sec features need not to take much should be features need not take much sec there is not any historical reviews should be there are not any historical reviews sec utilizing a embedding learning model should be utilizing an embedding learning model sec the experiment results proves should be the experiment results prove general discussion it is a good paper and should be accepted by acl,5.0
553.json,strengths a nice solid piece of work that builds on previous studies in a productive way well written and clear weaknesses very few possibly avoid some relatively empty statements for example if our task is to identify words used similarly across contexts our scoring function can be specified to give high scores to terms whose usage is similar across the contexts it is educational to study how annotations drawn from the same data are similar or different general discussion in the first sections i was not sure that much was being done that was new or interesting as the methods seemed very reminiscent of previous methods used over the past years to measure similarity albeit with a few new statistical twists but conceptually in the same vein section however describes an interesting and valuable piece of work that will be useful for future studies on the topic in retrospect the background provided in sections is useful if not necessary to support the experiments in section in short the work and results described will be useful to others working in this area and the paper is worthy of presentation at acl minor comments word punctuation missing for word annotations we used ppmi svd and sgns skipgram with negative sampling from mikolov et al b word vectors released by hamilton et al unclear what multiple methods refers to some words were detected by multiple methods with ccla,4.0
553.json,this paper propose a general framework for analyzing similarities and differences in term meaning and representation in different contexts strengths the framework proposed in this paper is generalizable and can be applied to different applications and accommodate difference notation of context different similarity functions different type of word annotations the paper is well written very easy to follow weaknesses i have concerns in terms of experiment evaluation the paper uses qualitative evaluation metrics which makes it harder to evaluate the effectiveness or even the validity of proposed method for example table compares the result with hamilton et al using different embedding vector by listing top words that changed from to it hard to tell quantitatively the performances of ccla the same issue also applies to experiment comparative lexical analysis over context the top words may be meaningful but what about top what about the words that practitioner actually cares without addressing the evaluation issue i find it difficult to claim that ccla will benefit downstream applications,3.0
251.json,this paper delves into the mathematical properties of the skip gram model explaining the reason for its success on the analogy task and for the general superiority of additive composition models it also establishes a link between skip gram and sufficient dimensionality reduction i liked the focus of this paper on explaining the properties of skip gram and generally found it inspiring to read i very much appreciate the effort to understand the assumptions of the model and the way it affects or is affected by the composition operations that it is used to perform in that respect i think it is a very worthwhile read for the community my main criticism is however that the paper is linguistically rather naive the authors use of compositionality as an operation that takes a set of words and returns another with the same meaning is extremely strange two words can of course be composed and produce a vector that is a far away from both b does not correspond to any other concept in the space c still has meaning productivity would not exist otherwise compositionality in linguistic terms simply refers to the process of combining linguistic constituents to produce higher level constructs it does not assume any further constraint apart from some vague and debatable notion of semantic transparency the paper implication l that composition takes place over sets is also wrong ordering matters hugely e g ugar cane is not cane sugar this is a well known shortcoming of additive composition another important aspect is that there are pragmatic factors that make humans prefer certain phrases to single words in particular contexts and the opposite naturally changing the underlying distribution of words in a large corpus for instance talking of a male royalty rather than a king or prince usually has implications with regard to the intent of the speaker here perhaps highlighting a gender difference this means that the equation in l or for that matter the kl divergence modification does not hold not because of noise in the data but because of fundamental linguistic processes this point may be addressed by the section on sdr but i am not completely sure see my comments below in a nutshell i think the way that the authors present composition is flawed but the paper convinces me that this is indeed what happens in skip gram and i think this is an interesting contribution the part about sufficient dimensionality reduction seems a little disconnected from the previous argument as it stands i am afraid i was not able to fully follow the argument and i would be grateful for some clarification in the authors response if i understand it well the argument is that skip gram produces a model where a word neighbours follow some exponential parametrisation of a categorical distribution but it is unclear whether this actually reflects the distribution of the corpus as opposed to what happens in say a pure count based model the fact that skip gram performs well despite not reflecting the data is that it implements some form of sdr which does not need to make any assumption about the underlying form of the data but then is it fair to say that the resulting representations are optimised for tasks where geometrical regularities are important regardless of the actual pattern of the data i e there some kind of denoising going on minor comments the abstract is unusually long and could i think be shortened para starting l i think it would be misconstrued to see circularity here firth observed that co occurrence effects were correlated with similarity judgements but those judgements are the very cognitive processes that we are trying to model with statistical methods co occurrence effects and vector space word representations are in some sense the same thing modelling an underlying linguistic process we do not have direct observations for so pair wise similarity is not there to break any circularity it is there because it better models the kind of judgements humans known to make l i think paraphrase would be a better word than ynonym here given that we are comparing a set of words with a unique lexical item para starting l this is interesting and actually a lot of the zipfian distribution the long tail is fairly uniform l it is probably worth pointing out that the analogy relation does not hold so well in practice and requires to ignore the first returned neighbour of the analogy computation which is usually one of the observed terms para starting l i do not find it so intuitive to say that man would be a synonym paraphrase of anything involving woman the subtraction involved in the analogy computation is precisely not a straightforward composition operation as it involves an implicit negation a last tiny general comment it is usual to write p w c to mean the probability of a word given a context but in the paper w is actually the context and c the target word it makes reading a little bit harder perhaps change the notation literature the claim that arora is the only work to try and understand vector composition is a bit strong for instance see the work by paperno baroni on explaining the success of addition as a composition method over pmi weighted vectors d paperno and m baroni when the whole is less than the sum of its parts how composition affects pmi values in distributional semantic vectors computational linguistics i thank the authors for their response and hope to see this paper accepted,4.0
178.json,the paper describes an extension of word embedding methods to also provide representations for phrases and concepts that correspond to words the method works by fixing an identifier for groups of phrases words and the concept that all denote this concept replace the occurrences of the phrases and words by this identifier in the training corpus creating a tagged corpus and then appending the tagged corpus to the original corpus for training the concept phrase word sets are taken from an ontology since the domain of application is biomedical the related corpora and ontologies are used the researchers also report on the generation of a new test dataset for word similarity and relatedness for real world entities which is novel in general the paper is nicely written the technique is pretty natural though not a very substantial contribution the scope of the contribution is limited because of focused evaluation within the biomedical domain more discussion of the generated test resource could be useful the resource could be the true interesting contribution of the paper there is one small technical problem but that is probably just a matter of mathematical expression than implementation technical problem eq the authors want to define the map calculation this is a good idea thought i think that a natural cut off could be defined rather than ranking the entire vocabulary equation does not define a probability it is quite easy to show this even if the size of the vocabulary is infinite so you need to change the explanation take out talk of a probability small corrections line most concepts has most concepts have,2.0
178.json,summary this paper presents a model for embedding words phrases and concepts into vector spaces to do so it uses an ontology of concepts each of which is mapped to phrases these phrases are found in text corpora and treated as atomic symbols using this the paper uses what is essentially the skip gram method to train embeddings for words the now atomic phrases and also the concepts associated with them the proposed work is evaluated on the task of concept similarity and relatedness using umls and yago to act as the backing ontologies strengths the key question addressed by the paper is that phrases that are not lexically similar can be semantically close and furthermore not all phrases are compositional in nature to this end the paper proposes a plausible model to train phrase embeddings the trained embeddings are shown to be competitive or better at identifying similarity between concepts the software released with the paper could be useful for biomedical nlp researchers weaknesses the primary weakness of the paper is that the model is not too novel it is essentially a tweak to skip gram furthermore the full model presented by the paper does not seem to be the best one in the results in table on the two mayo datasets the choi baseline is substantially better a similar trend seems to dominate table too on the larger umnsrs data the proposed model is at best competitive with previous simpler models chiu general discussion the paper says that it is uses known phrases as distant supervision to train embeddings however it is not clear what the supervision here is if i understand the paper correctly every occurrence of a phrase associated with a concept provides the context to train word embeddings but this is not supervision in the traditional sense say for identifying the concept in the text or other such predictive tasks so the terminology is a bit confusing the notation introduced in section ew etc is never used in the rest of the paper the use of beta to control for compositionality of phrases by words is quite surprising essentially this is equivalent to saying that there is a single global constant that decides how compositional any phrase should be the surprising part here is that the actual values of beta chosen by cross validation from table are odd for pm cl and wikinyt it is zero which basically argues against compositionality the experimental setup for table needs some explanation the paper says that the data labels similarity relatedness of concepts or entities however if the concepts phrases mapping is really many to many then how are the phrase word vectors used to compute the similarities it seems that we can only use the concept vectors in table the approximate phr method which approximate concepts with the average of the phrases in them is best performing so it is not clear why we need the concept ontology instead we could have just started with a seed set of phrases to get the same results,2.0
178.json,the authors presents a method to jointly embed words phrases and concepts based on plain text corpora and a manually constructed ontology in which concepts are represented by one or more phrases they apply their method in the medical domain using the umls ontology and in the general domain using the yago ontology to evaluate their approach the authors compare it to simpler baselines and prior work mostly on intrinsic similarity and relatedness benchmarks they use existing benchmarks in the medical domain and use mechanical turkers to generate a new general domain concept similarity and relatedness dataset which they also intend to release they report results that are comparable to prior work strengths the proposed joint embedding model is straightforward and makes reasonable sense to me its main value in my mind is in reaching a configurable middle ground between treating phrases as atomic units on one hand to considering their compositionallity on the other the same approach is applied to concepts being composed of several representative phrases the paper describes a decent volume of work including model development an additional contribution in the form of a new evaluation dataset and several evaluations and analyses performed weaknesses the evaluation reported in this paper includes only intrinsic tasks mainly on similarity relatedness datasets as the authors note such evaluations are known to have very limited power in predicting the utility of embeddings in extrinsic tasks accordingly it has become recently much more common to include at least one or two extrinsic tasks as part of the evaluation of embedding models the similarity relatedness evaluation datasets used in the paper are presented as datasets recording human judgements of similarity between concepts however if i understand correctly the actual judgements were made based on presenting phrases to the human annotators and therefore they should be considered as phrase similarity datasets and analyzed as such the medical concept evaluation dataset mini mayosrs is extremely small pairs and its larger superset mayosrs is only a little larger pairs and was reported to have a relatively low human annotator agreement the other medical concept evaluation dataset umnsrs is more reasonable in size but is based only on concepts that can be represented as single words and were represented as such to the human annotators this should be mentioned in the paper and makes the relevance of this dataset questionable with respect to representations of phrases and general concepts as the authors themselves note they quite extensively fine tune their hyperparameters on the very same datasets for which they report their results and compare them with prior work this makes all the reported results and analyses questionable the authors suggest that their method is superb to prior work as it achieved comparable results while prior work required much more manual annotation i do not think this argument is very strong because the authors also use large manually constructed ontologies and also because the manually annotated dataset used in prior work comes from existing clinical records that did not require dedicated annotations in general i was missing more useful insights into what is going on behind the reported numbers the authors try to treat the relation between a phrase and its component words on one hand and a concept and its alternative phrases on the other as similar types of a compositional relation however they are different in nature and in my mind each deserves a dedicated analysis for example around line i would expect an nlp analysis specific to the relation between phrases and their component words perhaps the reason for the reported behavior is dominant phrase headwords etc another aspect that was absent but could strengthen the work is an investigation of the effect of the hyperparameters that control the tradeoff between the atomic and compositional views of phrases and concepts general discussion due to the above mentioned weaknesses i recommend to reject this submission i encourage the authors to consider improving their evaluation datasets and methodology before re submitting this paper minor comments line contexts concepts line how are phrase overlaps handled line i believe the dimensions should be w x d also the terminology negative sampling matrix is confusing as the model uses these embeddings to represent contexts in positive instances as well line regarding the observed phrase just completed it not clear to me how words are trained in the joint model the text may imply that only the last words of a phrase are considered as target words but that doesn t make sense notation in equation is confusing using c instead of o line pedersen et al is missing in the reference section line i find it odd to use such a fine grained similarity scale for human annotations line the newly introduced term strings here is confusing i suggest to keep using phrases instead line which task exactly was used for the hyper parameter tuning that s important i couldn t find that even in the appendix table it s hard to see trends here for instance pm cl behaves rather differently than either pm or cl alone it would be interesting to see development set trends with respect to these hyper parameters line missing reference to table,2.0
288.json,the paper analyzes the story endings last sentence of a sentence story in the corpus built for the story cloze task mostafazadeh et al and proposes a model based on character and word n grams to classify story endings the paper also shows better performance on the story cloze task proper distinguishing between right and wrong endings than prior work whereas style analysis is an interesting area and you show better results than prior work on the story cloze task there are several issues with the paper first how do you define style also the paper needs to be restructured for instance your section results actually mixes some results and new experiments and clarified see below for questions comments right now it is quite difficult for the reader to follow what data is used for the different experiments and what data the discussion refers to more details about the data used is necessary in order to assess the claim that subtle writing task imposes different styles on the author lines how many stories are you looking at written by how many different persons and how many stories are there per person from your description of the post analysis of coherence only pairs of stories written by the same person in which one was judged as coherent and the other one as neutral are chosen can you confirm that this is the case so perhaps your claim is justified for your experiment however my understanding is that in experiment where you compare original vs right or original vs wrong we do not have the same writers so i am not convinced lines are correct a lot in the paper is simply stated without any justifications for instance how are the five frequent pos and words chosen are they the most frequent words pos also theses tables are puzzling why two bars in the legend for each category why character grams did you tune that on the development set if these were not the most frequent features but some that you chose among frequent pos and words you need to justify this choice and especially link the choice to style how are these features reflecting style i do not understand how the section design of nlp tasks connects to the rest of the paper and to your results but perhaps this is because i am lost in what training and test sets refer to here it is difficult to understand how your model differs from previous work how do we reconcile lines these results suggest that real understanding of text is required in order to solve the task with your approach the terminology of right and wrong endings is coming from mostafazadeh et al but this is a very bad choice of terms what exactly does a right or wrong ending mean right as in coherent or right as in morally good i took a quick look but could not find the exact prompts given to the turkers i think this needs to be clarified as it is the first paragraph of your section story cloze task lines is not understandable other questions comments table why does the original story differ from the coherent and incoherent one from your description of the corpus it seems that one turker saw the first sentences of the original story and was then ask to write one sentence ending the story in a right way or did they ask to provide a coherent ending and one sentence ending the story in a wrong way or did they ask to provide an incoherent ending i do not find the last sentence of the incoherent story that incoherent if the only shoes that kathy finds great are i can see how kathy does not like buying shoes this led me to wonder how many turkers judged the coherence of the story ending and how variable the judgements were what criterion was used to judge a story coherent or incoherent also does one turker judge the coherence of both the right and wrong endings making it a relative judgement or was this an absolute judgement this would have huge implications on the ratings lines what does we randomly sample original sets mean line virtually all sentences can you quantify this table could we see the weights of the features line compared to ending an existing task the turkers are not ending a task line made sure each pair of endings was written by the same author this is true for the right wrong pairs but not for the original new pairs according to your description line shorter text spans text about what this is unclear lines where is this published,2.0
288.json,strengths the paper has a promising topic different writing styles in finishing a story that could appeal to discourse and pragmatics area participants weaknesses the paper suffers from a convincing and thorough discussion on writing style and implications of the experiments on discourse or pragmatics for example regarding style the authors could have sought answers to the following questions what is the implication of starting an incoherent end of story sentence with a proper noun l is this a sign of topic shift what is the implication of ending a story coherently with a past tense verb etc it is not clear to me why studies on deceptive language are similar to short or long answers in the current study i would have liked to see a more complete comparison here the use of terms such as cognitive load l and mental states l appears somewhat vague there is insufficient discussion on the use of coordinators line onwards the paper would benefit from a more thorough discussion of this issue e g what is the role of coordinators in these short stories and in discourse in general does the use of coordinators differ in terms of the genre of the story how about the use of no coordinators the authors do not seem to make it sufficiently clear who the target readers of this research would be e g language teachers crowd sourcing experiment designers etc the paper needs revision in terms of organization there are repetitions throughout the text also the abbreviations in table and are not clear to me general discussion all in all the paper would have to be revised particularly in terms of its theoretical standpoint and implications to discourse and pragmatics in their response to the reviewers comments the authors indicate their willingness to update the paper and clarify the issues related to what they have experimented with however i would have liked to see a stronger commitment to incorporating the implications of this study to the discourse and pragmatics area,2.0
699.json,this paper divides the keyphrases into two types absent key phrases such phrases do not match any contiguous subsequences of the source document and present key phrases such key phrases fully match a part of the text the authors used rnn based generative models discussed as rnn and copy rnn for keyphrase prediction and copy mechanism in rnn to predict the already occurred phrases strengths the formation and extraction of key phrases which are absent in the current document is an interesting idea of significant research interests the paper is easily understandable the use of rnn and copy rnn in the current context is a new idea as deep recurrent neural networks are already used in keyphrase extraction shows very good performance also so it will be interesting to have a proper motivation to justify the use of rnn and copy rnn over deep recurrent neural networks weaknesses some discussions are required on the convergence of the proposed joint learning process for rnn and copyrnn so that readers can understand how the stable points in probabilistic metric space are obtained otherwise it may be tough to repeat the results the evaluation process shows that the current system which extracts present and absent both kinds of keyphrases is evaluated against baselines which contains only present type of keyphrases here there is no direct comparison of the performance of the current system w r t other state of the arts benchmark systems on only present type of key phrases it is important to note that local phrases keyphrases are also important for the document the experiment does not discuss it explicitly it will be interesting to see the impact of the rnn and copy rnn based model on automatic extraction of local or present type of key phrases the impact of document size in keyphrase extraction is also an important point it is found that the published results of see reference below performs better than with a sufficiently high difference the current system on inspec hulth abstracts dataset it is reported that current system uses documents for training while publications are held out for training baselines why are all publications not used in training the baselines additionally the topical details of the dataset scientific documents used in training rnn and copy rnn are also missing this may affect the chances of repeating results as the current system captures the semantics through rnn based models so it would be better to compare this system which also captures semantics even ref can be a strong baseline to compare the performance of the current system suggestions to improve as per the example given in the figure it seems that all the absent type of key phrases are actually topical phrases for example video search video retrieval video indexing and relevance ranking etc these all define the domain sub domain topics of the document so in this case it will be interesting to see the results or will be helpful in evaluating absent type keyphrases if we identify all the topical phrases of the entire corpus by using tf idf and relate the document to the high ranked extracted topical phrases by using normalized google distance pmi etc as similar efforts are already applied in several query expansion techniques with the aim to relate the document with the query if matching terms are absent in document reference liu zhiyuan peng li yabin zheng and maosong sun b clustering to find exemplar terms for keyphrase extraction in proceedings of the conference on empirical methods in natural language processing pages zhang q wang y gong y huang x keyphrase extraction using deep recurrent neural networks on twitter in proceedings of the conference on empirical methods in natural language processing pp,4.0
699.json,strengths novel model i particularly like the ability to generate keyphrases not present in the source text weaknesses needs to be explicit whether all evaluated models are trained and tested on the same data sets exposition of the copy mechanism not quite clear convincing general discussion this paper presents a supervised neural network approach for keyphrase generation the model uses an encoder decoder architecture that first encodes input text with a rnn then uses an attention mechanism to generate keyphrases from the hidden states there is also a more advanced variant of the decoder which has an attention mechanism that conditions on the keyphrase generated in the previous time step the model is interesting and novel and i think the ability to generate keyphrases not in the source text is particularly appealing my main concern is with the evaluation are all evaluated models trained with the same amount of data and evaluated on the same test sets it not very clear for example on the nus data set section line says that the supervised baselines are evaluated with cross validation other comments the paper is mostly clearly written and easy to follow however some parts are unclear absent keyphrases vs oov i think there is a need to distinguish between the two and the usage meaning of oov should be consistent the rnn models use the most frequent words as the vocabulary section line section line so i suppose oov are words not in this k vocabulary in line do you mean oov or absent words keyphrases speaking of this i am wondering how many keyphrases fall outside of this k the use of unknown words in line is also ambiguous i think it probably clearer to say that the rnn models can generate words not present in the source text as long as they appear somewhere else in the corpus and the k vocabulary exposition of the copy mechanism section this mechanism has a more specific locality than the attention model in basic rnn model however i find the explanation of the intuition misleading if i understand correctly the copy mechanism is conditioned on the source text locations that matches the keyphrase in the previous time step y t so maybe it has a higher tendency to generate n grams seen source text figure i buy the argument that the more sophisticated attention model probably makes copyrnn better than the rnn overall but why is the former model particularly better for absent keyphrases it is as if both models perform equally well on present keyphrases how are the word embeddings initialized,4.0
676.json,strengths this paper is well written and with clear well designed figures the reader can easily understand the methodology even only with those figures predicting the binary code directly is a clever way to reduce the parameter space and the error correction code just works surprisingly well i am really surprised by how bits can achieve out of bleu the parameter reducing technique described in this work is orthogonal to current existing methods weight pruning and sequence level knowledge distilling the method here is not restricted by neural machine translation and can be used in other tasks as long as there is a big output vocabulary weaknesses the most annoying point to me is that in the relatively large dataset aspec the best proposed model is still bleu point lower than the softmax model what about some even larger dataset like the french english there are at most million sentences there will the gap be even larger similarly what is the performance on some other language pairs maybe you should mention this paper https arxiv org abs it speeds up the decoding speed by x and the bleu loss is less than general discussion the paper describes a parameter reducing method for large vocabulary softmax by applying the error corrected code and hybrid with softmax its bleu approaches that of the orignal full vocab softmax model one quick question what is the hidden dimension size of the models i could not find this in the experiment setup the bits can achieve out of bleu on ej that was surprisingly good however how could you increase the number of bits to increase the classification power is too small there plenty of room to use more bits and the computation time on gpu wo not even change another thing that is counter intuitive is that by predicting the binary code the model is actually predicting the rank of the words so how should we interpret these bit embeddings there seems no semantic relations of all the words that have odd rank is it because the model is so powerful that it just remembers the data,4.0
318.json,strengths the proposed models are shown to lead to rather substantial and consistent improvements over reasonable baselines on two different tasks word similarity and word analogy which not only serves to demonstrate the effectiveness of the models but also highlights the potential utility of incorporating sememe information from available knowledge resources for improving word representation learning the paper contributes to ongoing efforts in the community to account for polysemy in word representation learning it builds nicely on previous work and proposes some new ideas and improvements that could be of interest to the community such as applying an attention scheme to incorporate a form of soft word sense disambiguation into the learning procedure weaknesses presentation and clarity important details with respect to the proposed models are left out or poorly described more details below otherwise the paper generally reads fairly well however the manuscript would need to be improved if accepted the evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes as the authors themselves point out more details below general discussion the authors stress the importance of accounting for polysemy and learning sense specific representations while polysemy is taken into account by calculating sense distributions for words in particular contexts in the learning procedure the evaluation tasks are entirely context independent which means that ultimately there is only one vector per word or at least this is what is evaluated instead word sense disambiguation and sememe information are used for improving the learning of word representations this needs to be clarified in the paper it is not clear how the sememe embeddings are learned and the description of the ssa model seems to assume the pre existence of sememe embeddings this is important for understanding the subsequent models do the sac and sat models require pre training of sememe embeddings it is unclear how the proposed models compare to models that only consider different senses but not sememes perhaps the mst baseline is an example of such a model if so this is not sufficiently described emphasis is instead put on soft vs hard word sense disambiguation the paper would be stronger with the inclusion of more baselines based on related work a reasonable argument is made that the proposed models are particularly useful for learning representations for low frequency words by mapping words to a smaller set of sememes that are shared by sets of words unfortunately no empirical evidence is provided to test the hypothesis it would have been interesting for the authors to look deeper into this this aspect also does not seem to explain the improvements much since e g the word similarity data sets contain frequent word pairs related to the above point the improvement gains seem more attributable to the incorporation of sememe information than word sense disambiguation in the learning procedure as mentioned earlier the evaluation involves only the use of context independent word representations even if the method allows for learning sememe and sense specific representations they would have to be aggregated to carry out the evaluation task the example illustrating hownet figure is not entirely clear especially the modifiers of computer it says that the models are trained using their best parameters how exactly are these determined it is also unclear how k is set is it optimized for each model or is it randomly chosen for each target word observation finally what is the motivation for setting k to,4.0
477.json,tldr the authors compare a wide variety of approaches towards sub word modelling in language modelling and show that modelling morphology gives the best results over modelling pure characters further the authors do some precision experiments to show that the biggest benefit towards sub word modelling is gained after words typically exhibiting rich morphology nouns and verbs the paper is comprehensive and the experiments justify the core claims of the paper strengths a comprehensive overview of different approaches and architectures towards sub word level modelling with numerous experiments designed to support the core claim that the best results come from modelling morphemes the authors introduce a novel form of sub word modelling based on character tri grams and show it outperforms traditional approaches on a wide variety of languages splitting the languages examined by typology and examining the effects of the models on various typologies is a welcome introduction of linguistics into the world of language modelling the analysis of perplexity reduction after various classes of words in russian and czech is particularly illuminating showing how character level and morpheme level models handle rare words much more gracefully in light of these results could the authors say something about how much language modelling requires understanding of semantics and how much it requires just knowing various morphosyntactic effects weaknesses the character tri gram lstm seems a little unmotivated did the authors try other character n grams as well as a reviewer i can guess that character tri grams roughly correspond to morphemes especially in semitic languages but what made the authors report results for grams as opposed to or in addition there are roughly possible distinct trigrams in the latin lower case alphabet which is enough to almost constitute a word embedding table did the authors only consider observed trigrams how many distinct observed trigrams were there i do not think you can meaningfully claim to be examining the effectiveness of character level models on root and pattern morphology if your dataset is unvocalised and thus does not have the pattern bit of root and pattern i appreciate that finding transcribed arabic and hebrew with vowels may be challenging but it half of the typology reduplication seems to be a different kind of phenomenon to the other three which are more strictly morphological typologies indonesian and malay also exhibit various word affixes which can be used on top of reduplication which is a more lexical process i am not sure splitting it out from the other linguistic typologies is justified general discussion the paper was structured very clearly and was very easy to read i am a bit puzzled about why the authors chose to use dimensional character embeddings once the dimensionality of the embedding is greater than the size of the vocabulary here the number of characters in the alphabet surely you are not getting anything extra having read the author response my opinions have altered little i still think the same strengths and weakness that i have already discussed hold,4.0
564.json,this paper describes a straightforward extension to left to right beam search in order to allow it to incorporate lexical constraints in the form of word sequences that must appear in mt output this algorithm is shown to be effective for interactive translation and domain adaptation although the proposed extension is very simple i think the paper makes a useful contribution by formalizing it it is also interesting to know that nmt copes well with a set of unordered constraints having no associated alignment information there seem to be potential applications for this technique beyond the ones investigated here for example improving nmt s ability to handle non compositional constructions which is one of the few areas where it still might lag traditional smt the main weakness of the paper is that the experiments are somewhat limited the interactive mt simulation shows that the method basically works but it is difficult to get a sense of how well for instance in how many cases the constraint was incorporated in an acceptable manner the large bleu score increases are only indirect evidence similarly adaptation should have been compared to the standard fine tuning baseline which would be relatively inexpensive to run on the k autodesk corpus despite this weakness i think this is a decent contribution that deserves to be published further details given its common usage in pbmt coverage vector is a potentially misleading term the appropriate data structure seems more likely to be a coverage set table should also give some indication of the number of constraints per source sentence in the test corpora to allow for calibration of the bleu gains,4.0
335.json,this work describes a gated attention based recurrent neural network method for reading comprehension and question answering this method employs a self matching attention technique to counterbalance the limited context knowledge of gated attention based recurrent neural networks when processing passages finally authors use pointer networks with signals from the question attention based vector to predict the beginning and ending of the answer experimental results with the squad dataset offer state of the art performance compared with several recent approaches the paper is well written structured and explained as far as i know the mathematics look also good in my opinion this is a very interesting work which may be useful for the question answering community i was wondering if the authors have plans to release the code of this approach from that perspective i miss a bit of information about the technology used for the implementation theano cuda cudnn which may be useful for readers i would appreciate if authors could perform a test of statistical significance of the results that would highlight even more the quality of your results finally i know that the space may be a constraint but an evaluation including some additional dataset would validate more your work,4.0
270.json,this paper presents a purpose built neural network architecture for textual entailment nli based on a three step process of encoding attention based matching and aggregation the model has two variants one based on treernns and the other based on sequential bilstms the sequential model outperforms all published results and an ensemble with the tree model does better still the paper is clear the model is well motivated and the results are impressive everything in the paper is solidly incremental but i nonetheless recommend acceptance major issues that i would like discussed in the response you suggest several times that your system can serve as a new baseline for future work on nli this is not an especially helpful or meaningful claim it could be said of just about any model for any task you could argue that your model is unusually simple or elegant but i do not think that really a major selling point of the model your model architecture is symmetric in some ways that seem like overkill you compute attention across sentences in both directions and run a separate inference composition aggregation network for each direction this presumably nearly doubles the run time of your model is this really necessary for the very asymmetric task of nli have you done ablation studies on this you present results for the full sequential model esim and the ensemble of that model and the tree based model him why do not you present results for the tree based model on its own minor issues i do not think the barker and jacobson quote means quite what you want it to mean in context it making a specific and not settled point about direct compositionality in formal grammar you would probably be better off with a more general claim about the widely accepted principle of compositionality the vector difference feature that you use which has also appeared in prior work is a bit odd since it gives the model redundant parameters any model that takes vectors a b and a b as input to some matrix multiplication is exactly equivalent to some other model that takes in just a and b and has a different matrix parameter there may be learning related reasons why using this feature still makes sense but it worth commenting on how do you implement the tree structured components of your model are there major issues with speed or scalability there typo klein and d manning figure standard tree drawing packages like tikz qtree produce much more readable parse trees without crossing lines i would suggest using them thanks for the response i still solidly support publication this work is not groundbreaking but it novel in places and the results are surprising enough to bring some value to the conference,4.0
270.json,the paper proposes a model for the stanford natural language inference snli dataset that builds on top of sentence encoding models and the decomposable word level alignment model by parikh et al the proposed improvements include performing decomposable attention on the output of a bilstm and feeding the attention output to another bilstm and augmenting this network with a parallel tree variant strengths this approach outperforms several strong models previously proposed for the task the authors have tried a large number of experiments and clearly report the ones that did not work and the hyperparameter settings of the ones that did this paper serves as a useful empirical study for a popular problem weaknesses unfortunately there are not many new ideas in this work that seem useful beyond the scope the particular dataset used while the authors claim that the proposed network architecture is simpler than many previous models it is worth noting that the model complexity in terms of the number of parameters is fairly high due to this reason it would help to see if the empirical gains extend to other datasets as well in terms of ablation studies it would help to see how well the tree variant of the model does on its own and the effect of removing inference composition from the model other minor issues the method used to enhance local inference equations and seem very similar to the heuristic matching function used by mou et al natural language inference by tree based convolution and heuristic matching you may want to cite them the first sentence in section is an unsupported claim this either needs a citation or needs to be stated as a hypothesis while the work is not very novel the the empirical study is rigorous for the most part and could be useful for researchers working on similar problems given these strengths i am changing my recommendation score to i have read the authors responses,3.0
266.json,this paper compares different ways of inducing embeddings for the task of polarity classification the authors focus on different types of corpora and find that not necessarily the largest corpus provides the most appropriate embeddings for their particular task but it is more effective to consider a corpus or subcorpus in which a higher concentration of subjective content can be found the latter type of data are also referred to as task specific data moreover the authors compare different embeddings that combine information from task specific corpora and generic corpora a combination outperforms embeddings just drawn from a single corpus this combination is not only evaluated on english but also on a less resourced language i e catalan strengths the paper addresses an important aspect of sentiment analysis namely how to appropriately induce embeddings for training supervised classifers for polarity classification the paper is well structured and well written the major claims made by the authors are sufficiently supported by their experiments weaknesses the outcome of the experiments is very predictable the methods that are employed are very simple and ad hoc i found hardly any new idea in that paper neither are there any significant lessons that the reader learns about embeddings or sentiment analysis the main idea i e focusing on more task specific data for training more accurate embeddings was already published in the context of named entity recognition by joshi et al the additions made in this paper are very incremental in nature i find some of the experiments inconclusive as apparently no statistical signficance testing between different classifiers has been carried out in tables and various classifier configurations produce very similar scores in such cases only statistical signficance testing can really give a proper indication whether these difference are meaningful for instance in table on the left half reporting results on rt one may wonder whether there is a significant difference between wikipedia baseline and any of the combinations furthermore one doubts whether there is any signficant difference between the different combinations i e either using subj wiki subj multiun or subj europarl in that table the improvement by focusing on subjective subsets is plausible in general however i wonder whether in real life in particular a situation in which resources are sparse this is very helpful doing a pre selection with opinionfinder is some pre processing step which will not be possible in most languages other than english there are no equivalent tools or fine grained datasets on which such functionality could be learnt the fact that in the experiments for catalan this information is not considered proves that minor details lines the discussion of this dataset is confusing i thought the task is plain polarity classification but the authors here also refer to opinion holder and opinion targets if these information are not relevant to the experiments carried out in this paper then they should not be mentioned here lines the variation of splicing that the authors explain is not very well motivated first why do we need this in how far should this be more effective than simple appending lines how is the subjective information isolated for these configurations i assume the authors here again employ opinionfinder however there is no explicit mention of this here lines the definitions of variables do not properly match the formula i e equation i do not find nk in equation lines similar to lines it is unclear what precise task is carried out do the authors take opinion holders and targets in consideration after authors response thank you very much for these clarifying remarks i do not follow your explanations regarding the incorporation of opinion holders and targets though overall i will not change my scores since i think that this work lacks sufficient novelty the things the authors raised in their response are just insufficient to me this submission is too incremental in nature,2.0
657.json,strengths the paper is thoroughly written and discusses its approach compared to other approaches the authors are aware that their findings are somewhat limited regarding the mean f values weaknesses some minor orthographical mistakes and some repetive clauses in general the paper would benefit if the sections and would be shortened to allow the extension of sections and the main goal is not laid out clearly enough which may be a result of the ambivalence of the paper goals general discussion table should only be one column wide while the figures especially and would greatly benefit from a two column width the paper was not very easy to understand during first read major improvements could be achieved by straightening up the content,3.0
483.json,strengths this is the first neural network based approach to argumentation mining the proposed method used a pointer network pn model with multi task learning and outperformed previous methods in the experiments on two datasets weaknesses this is basically an application of pn to argumentation mining although the combination of pn and multi task learning for this task is novel its novelty is not enough for acl long publication the lack of qualitative analysis and error analysis is also a major concern general discussion besides the weaknesses mentioned above the use of pn is not well motivated although three characteristics of pn were described in l these are not a strong motivation against the use of bi directional lstms and the attention mechanism the authors should describe what problems are solved by pn and discuss in the experiments how much these problems are solved figures and are difficult to understand what are the self link to d and the links from d to e and d d to e these are just the outputs from the decoder and not links the decoder lstm does not have an input from ej in these figures but it does in equation also in figure the abbreviation fc is not defined equation is strange to calculate the probability of each component type the probability of ei is calculated in the experiments i did not understand why only pn which is not a joint model was performed for the microtext corpus it is not clear whether the blstm model is trained with the joint task objective there are some studies on discourse parsing using the attention mechanism the authors should describe the differences from these studies minor issues l should related should be related l is floating l it able it is able i raised my recommendation score after reading the convincing author responses i strongly recommend that the authors should discuss improved examples by pn as well as the details of feature ablation,3.0
769.json,this paper proposes a method for building dialogue agents involved in a symmetric collaborative task in which the agents need to strategically communicate to achieve a common goal i do like this paper i am very interested in how much data driven techniques can be used for dialogue management however i am concerned that the approach that this paper proposes is actually not specific to symmetric collaborative tasks but to tasks that can be represented as graph operations such as finding an intersection between objects that the two people know about in section the authors introduce symmetric collaborative dialogue setting however such dialogs have been studied before such as clark and wilkes gibbs explored cognition and walker furniture layout task journal of artificial research on line the authors say that this domain is too rich for slot value semantics however their domain is based on attribute value pairs so their domain could use a semantics represenation based on attribute value pairs such as first order logic section is hard to follow the authors often refer to figure but i did not find this example that helpful for example for section at what point of the dialogue does this represent is this the same after anyone went to columbia,3.0
723.json,this is a nice paper on morphological segmentation utilizing word embeddings the paper presents a system which uses word embeddings to both measure local semantic similarity of word pairs with a potential morphological relation and global information about the semantic validity of potential morphological segment types the paper is well written and represents a nice extension to earlier approaches on semantically driven morphological segmentation the authors present experiments on morpho challenge data for three languages english turkish and finnish these languages exhibit varying degrees of morphological complexity all systems are trained on wikipedia text the authors show that the proposed morse system delivers clear improvements w r t f score for english and turkish compared to the well known morfessor system which was used as baseline the system fails to reach the performance of morfessor for finnish as the authors note this is probably a result of the richness of finnish morphology which leads to data sparsity and therefore reduced quality of word embeddings to improve the performance for finnish and other languages with a similar degree of morphological complexity the authors could consider word embeddings which take into account sub word information for example article dblp journals corr caor author kris cao and marek rei title a joint model for word embedding and word morphology journal corr volume abs year url http arxiv org abs timestamp fri jul biburl http dblp uni trier de rec bib journals corr caor bibsource dblp computer science bibliography http dblp org article dblp journals corr bojanowskigjm author piotr bojanowski and edouard grave and armand joulin and tomas mikolov title enriching word vectors with subword information journal corr volume abs year url http arxiv org abs timestamp tue aug biburl http dblp uni trier de rec bib journals corr bojanowskigjm bibsource dblp computer science bibliography http dblp org the authors critique the existing morpho challenge data sets for example there are many instances of incorrectly segmented words in the material moreover the authors note that while some segmentations in the the data set may be historically valid for example the segmentation of business into busi ness these segmentations are no longer semantically motivated the authors provide a new data set consisting of semantically motivated segmentation of english word forms from the english wikipedia they show that morse deliver highly substantial improvements compared to morfessor on this data set in conclusion i think this is a well written paper which presents competitive results on the interesting task of semantically driven morphological segmentation the authors accompany the submission with code and a new data set which definitely add to the value of the submission,4.0
723.json,this paper continues the line of work for applying word embeddings for the problem of unsupervised morphological segmentation e g soricut och üstün can the proposed method morse applies a local optimization for segmentation of each word based on a set of orthographic and semantic rules and a few heuristic threshold values associated with them strengths the paper presents multiple ways to evaluate segmentation hypothesis on word embeddings and these may be useful also in other type of methods the results on english and turkish data sets are convincing the paper is clearly written and organized and the biliography is extensive the submission includes software for testing the english morse model and three small data sets used in the expriments weaknesses the ideas in the paper are quite incremental based mostly on the work by soricut och however the main problems of the paper concern meaningful comparison to prior work and analysis of the method limitations first the proposed method does not provide any sensible way for segmenting compounds based on section the method does segment some of the compounds but using the terminology of the method it considers either of the constituents as an affix unsuprisingly the limitation shows up especially in the results of a highly compounding language finnish while the limitation is indicated in the end of the discussion section the introduction and experiments seem to assume otherwise in particular the limitation on modeling compounds makes the evaluation of section quite unfair morfessor is especially good at segmenting compounds ruokolainen et al while morse seems to segment them only by accident thus it is no wonder that morfessor segments much larger proportion of the semantically non compositional compounds a fair experiment would include an equal number of compounds that should be segmented to their constituents another problem in the evaluations in and concerns hyperparameter tuning the hyperparameters of morse are optimized on a tuning data but apparently the hyperparameters of morfessor are not the recent versions of morfessor kohonen et al grönroos et al have a single hyperparameter that can be used to balance precision and recall of the segmentation given that the morse outperforms morfessor both in precision and recall in many cases this does not affect the conclusions but should at least be mentioned some important details of the evaluations and results are missing the morpheme level evaluation method in should be described or referred to moreover table seems to compare results from different evaluation sets the morfessor and base inference methods seem to be from official morpho challenge evaluations llsm is from narasimhan et al who uses aggregated data from morpho challenges probably including both development and training sets and morse is evaluated morpho challenges development set this might not affect the conclusions as the differences in the scores are rather large but it should definitely be mentioned the software package does not seem to support training only testing an included model for english general discussion the paper puts a quite lot of focus on the issue of segmenting semantically non compositional compounds this is problematic in two ways first as mentioned above the proposed method does not seem to provide sensible way of segmenting any compound second finding the level of lexicalized base forms e g freshman and the morphemes as smallest meaning bearing units fresh man are two different tasks with different use cases for example the former would be more sensible for phrase based smt and the latter for asr the unsupervised segmentation methods such as morfessor typically target at the latter and critizing the method for a different goal is confusing finally there is certainly a continuum on the semantic compositionality of the compound and the decision is always somewhat arbitrary unfortunately many gold standards including the morpho challenge data sets tend to be also inconsistent with their decisions sections and mention the computational efficiency and limitation to one million input word forms but does not provide any details what is the bottleneck here collecting the transformations support sets and clusters or the actual optimization problem what were the computation times and how do these scale up the discussion mentions a few benefits of the morse approach adaptability as a stemmer ability to control precision and recall and need for only a small number of gold standard segmentations for tuning as far as i can see all or some of these are true also for many of the morfessor variants creutz and lagus kohonen et al grönroos et al so this is a bit misleading it is true that morfessor works usually fine as a completely unsupervised method but the extensions provide at least as much flexibility as morse has ref mathias creutz and krista lagus inducing the morphological lexicon of a natural language from unannotated text in proceedings of the international and interdisciplinary conference on adaptive knowledge representation and reasoning akrr espoo finland june miscellaneous abstract should maybe mention that this is a minimally supervised method unsupervised to the typical extent i e excluding hyperparameter tuning in section it should be mentioned somewhere that phi is an empty string in section it should be mentioned what specific variant and implementation of morfessor is applied in the experiments in the end of section i doubt that increasing the size of the input vocabulary would alone improve the performance of the method for finnish for a language that is morphologically as complex you never encounter even all the possible inflections of the word forms in the data not to mention derivations and compounds i would encourage improving the format of the data sets e g using something similar to the mc data sets for example using aa as a separator for multiple analyses is confusing and makes it impossible to use the format for other languages in the references many proper nouns and abbreviations in titles are written in lowercase letters narasimhan et al is missing all the publication details,4.0
723.json,strengths i find the idea of using morphological compositionality to make decisions on segmentation quite fruitful motivation is quite clear the paper is well structured weaknesses several points are still unclear how the cases of rule ambiguity are treated see null er examples in general discussion inference stage seems to be suboptimal the approach is limited to known words only general discussion the paper presents semantic aware method for morphological segmentation the method considers sets of simple morphological composition rules mostly appearing as tem plus suffix or prefix the approach seems to be quite plausible and the motivation behind is clear and well argumented the method utilizes the idea of vector difference to evaluate semantic confidence score for a proposed transformational rule it been previously shown by various studies that morpho syntactic relations are captured quite well by doing word analogies vector differences but on the other hand it has also been shown that in case of derivational morphology which has much less regularity than inflectional the performance substantially drops see gladkova vylomova the search space in the inference stage although being tractable still seems to be far from optimized to get a rule matching sky skies the system first needs to searhc though the whole radd set and probably quite huge set of other possible substitutions and limited to known words only for which we can there exist rules it is not clear how the rules for the transformations which are orthographically the same but semantically completely different are treated for instance consider er suffix on one hand if used with verbs it transforms them into agentive nouns such as play player on the other hand it could also be used with adjectives for producing comparative form for instance old older or consider big bigger versus dig digger more over as mentioned before there is quite a lot of irregularity in derivational morphology the same suffix might play various roles for instance er might also represent patiental meanings like in looker are they merged into a single rule cluster no exploration of how the similarity threshold and measure may affect the performance is presented,4.0
467.json,the paper presents a self learning framework for learning of bilingual word embeddings the method uses two embeddings in source and target languages and a seed lexicon on each step of the mapping learning a new bilingual lexicon is induced then the learning step is repeated using the new lexicon for learning of new mapping the process stops when a convergence criterion is met one of the strengths is that the seed lexicon is directly encoded in the learning process as a binary matrix then the self learning framework solves a global optimization problem in which the seed lexicon is not explicitly involved its role is to establish the initial mapping between the two embeddings this guarantees the convergence the initial seed lexicon could be quite small correspondences the small size of the seed lexicon is appealing for mappings between languages for which there are not large bilingual lexicons it will be good to evaluate the framework with respect to the quality of the two word embeddings if we have languages or at least one of the languages with scarce language resources then the word embeddings for both languages could differ in their structure and coverage i think it could be simulated on the basis of the available data via training the corresponding word embeddings on different subcorpora for each language,4.0
543.json,update after rebuttal i appreciate the authors taking the time to clarify their implementation of the baseline and to provide some evidence of the significance of the improvements they report these clarifications should definitely be included in the camera ready version i very much like the idea of using visual features for these languages and i am looking forward to seeing how they help more difficult tasks in future work strengths thinking about chinese japanese korean characters visually is a great idea weaknesses experimental results show only incremental improvement over baseline and the choice of evaluation makes it hard to verify one of the central arguments that visual features improve performance when processing rare unseen words some details about the baseline are missing which makes it difficult to interpret the results and would make it hard to reproduce the work general discussion the paper proposes the use of computer vision techniques cnns applied to images of text to improve language processing for chinese japanese and korean languages in which characters themselves might be compositional the authors evaluate their model on a simple text classification task assigning wikipedia page titles to categories they show that a simple one hot representation of the characters outperforms the cnn based representations but that the combination of the visual representations with standard one hot encodings performs better than the visual or the one hot alone they also present some evidence that the visual features outperform the one hot encoding on rare words and present some intuitive qualitative results suggesting the cnn learns good semantic embeddings of the characters i think the idea of processing languages like chinese and japanese visually is a great one and the motivation for this paper makes a lot of sense however i am not entirely convinced by the experimental results the evaluations are quite weak and it is hard to say whether these results are robust or simply coincidental i would prefer to see some more rigorous evaluation to make the paper publication ready if the results are statistically significant if the authors can indicate this in the author response i would support accepting the paper but ideally i would prefer to see a different evaluation entirely more specific comments below in section paragraph lookup model you never explicitly say which embeddings you use or whether they are tuned via backprop the way the visual embeddings are you should be more clear about how the baseline was implemented if the baseline was not tuned in a task specific way but the visual embeddings were this is even more concerning since it makes the performances substantially less comparable i do not entirely understand why you chose to evaluate on classifying wikipedia page titles it seems that the only real argument for using the visual model is its ability to generalize to rare unseen characters why not focus on this task directly e g what about evaluating on machine translation of oov words i agree with you that some languages should be conceptualized visually and sub character composition is important but the evaluation you use does not highlight weaknesses of the standard approach and so it does not make a good case for why we need the visual features in table are these improvements statistically significant it might be my fault but i found figure very difficult to understand since this is one of your main results you probably want to present it more clearly so that the contribution of your model is very obvious as i understand it rank on the x axis is a measure of how rare the word is i think log frequency with the rarest word furthest to the left and since the visual model intersects the x axis to the left of the lookup model this means the visual model was better at ranking rare words why do not both models intersect at the same point on the x axis are not they being evaluated on the same set of titles and trained with the same data in the author response it would be helpful if you could summarize the information this figure is supposed to show in a more concise way on the fallback fusion why not show performance for for different thresholds seems to be an edge case threshold that might not be representative of the technique more generally the simple traditional experiment for unseen characters is a nice idea but is presented as an afterthought i would have liked to see more eval in this direction i e on classifying unseen words maybe add translations to figure for people who do not speak chinese,4.0
447.json,strengths the main strength of this paper is the incorporation of discourse structure in the dnn attention model which allows the model to learn the weights given to different edus also the paper is very clear and provides a good explanation of both rst and how it is used in the model finally the evaluation experiments are conducted thoroughly with strong state of the art baselines weaknesses the main weakness of the paper is that the results do not strongly support the main claim that discourse structure can help text classification even the unlabeled variant which performs best and does outperform the state of the art only provides minimal gains and hurts in the legal bills domain the approach particularly the full variant seems to be too data greedy but no real solution is provided to address this beyond the simpler unlabeled and root variants general discussion in general this paper feels like a good first shot at incorporating discourse structure into dnn based classification but does not fully convince that rst style structure will significantly boost performance on most tasks given that it is also very costly to build a rst parser for a new domain as would be needed in the legal bill domains described in this paper i wish the authors had explored or at least mentioned next steps in making this approach work in particular in the face of data sparsity for example how about defining task independent discourse embeddings would it be possible to use a dnn for discourse parsing that could be incorporated in the main task dnn and optimized jointly end to end again this is good work i just wish the authors had pushed it a little further given the mixed results,3.0
369.json,the paper describes a method for improving two step translation using deep learning results are presented for chinese spanish translation but the approach seems to be largely language independent the setting is fairly typical for two step mt the first step translates into a morphologically underspecified version of the target language the second step then uses machine learning to fill in the missing morphological categories and produces the final system output by inflecting the underspecified forms using a morphological generator the main novelty of this work is the choice of deep nns as classifiers in the second step the authors also propose a rescoring step which uses a lm to select the best variant overall this is solid work with good empirical results the classifier models reach a high accuracy clearly outperforming baselines such as svms and the improvement is apparent even in the final translation quality my main problem with the paper is the lack of a comparison with some straightforward deep learning baselines specifically you have a structured prediction problem and you address it with independent local decisions followed by a rescoring step unless i misunderstood the approach but this is a sequence labeling task which rnns are well suited for how would e g a bidirectional lstm network do when trained and used in the standard sequence labeling setting after reading the author response i still think that baselines including the standard lstm are run in the same framework i e independently for each local label if that not the case it should have been clarified better in the response this is a problem because you are not using the rnns in the standard way and yet you do not justify why your way is better or compare the two approaches the final re scoring step is not entirely clear to me do you rescore n best sentences what features do you use or are you searching a weighted graph for the single optimal path this needs to be explained more clearly in the paper my current impression is that you produce a graph then look for k best paths in it generate the inflected sentences from these k paths and then use a lm and nothing else to select the best variant but i am not sure from reading the paper this was not addressed in the response you report that larger word embeddings lead to a longer training time do they also influence the final results can you attempt to explain why adding information from the source sentence hurts this seems a bit counter intuitive does e g the number information not get entirely lost sometimes because of this i would appreciate a more thorough discussion on this in the final version perhaps with a couple of convincing examples the paper contains a number of typos and the general level of english may not be sufficient for presentation at acl minor corrections context of the application of mt context of application for mt in this cases mt is faced in two steps in this case mt is divided into two steps markov markov cfr crf task was based on a direct translation task was based on direct translation task provided corpus task provided corpora the phrase based system has dramatically the phrase based approach investigated different set of features sets of features words as source of information words as the source correspondant corresponding classes for gender classifier classes for the for number classifier for the this layer input consists in consists of to extract most relevant the most sigmoid does not output results in but rather a tanh layer would produce information of a word consists in itself of itself this a set the set a empty sentences and longer than words empty sentences and sentences longer than classifier is trained on classifier is trained in aproximately approximately coverage raises the coverage exceeds unless i misunderstand in descendant order in descending order cuadratic quadratic in multiple places but best results but the best results rescoring step improves the rescoring step are not be comparable are not comparable,2.0
369.json,this paper presents a method for generating morphology focusing on gender and number using deep learning techniques from a morphologically simplified spanish text the proposed approach uses a classifier to reassign the gender and number for each token when necessary the authors compared their approach with other learning algorithms and evaluated it in machine translation on the chinese to spanish zh es translation direction recently the task of generating gender and number has been rarely tackled morphology generation methods usually target and are evaluated on morphologically rich languages like german or finnish however calling the work presented in this paper morphology generation is a bit overselling as the proposed method clearly deals only with gender and number and given the fact that some rules are handcrafted for this specific task i do not think this method can be straightforwardly applied to do more complex morphology generation for morphologically rich languages this paper is relatively clear in the sections presenting the proposed method a lot of work has been done to design the method and i think it can have some interesting impact on various nlp tasks however the evaluation part of this work is barely understandable as many details of what is done or why it is done are missing from this evaluation we cannot know if the proposed method brings improvements over state of the art methods while the experiments cannot be replicated furthermore no analysis of the results obtained is provided since half a page is still available there was the possibility to provide more information to make more clear the evaluation this work lacks of motivation why do you think deep learning can especially improve gender and number generation over state of the art methods in your paper the word contribution should be used more wisely as it is now in the paper it is not obvious what are the real contributions more details below abstract what do you mean by unbalanced languages section you claim that your main contribution is the use of deep learning just the use of deep learning in some nlp task is not a contribution section you claim that neural machine translation nmt mentioned as neural approximations does not achieve state of the art results for zh es i recommend to remove this claim from the paper or to discuss it more since junczys dowmunt et al during the last iwslt presented some results for zh es with the un corpus showing that nmt outperforms smt by around bleu points section you wrote that using the zh es language pair is one of your main contributions just using a language pair is not a contribution nonetheless i think it is nice to see a paper on machine translation that does not focus of improving machine translation for english the numbers provided in table were computed before or after preprocessing why did you remove the sentences longer than tokens precise how did you obtain development and test sets or provide them your experiments are currently no replicable especially because of that section you wrote that you used moses and its default parameters but the default parameters of moses are not the same depending on the version so you should provide the number of the version used section what do you mean by hardware cost table more details should be provided regarding how did you obtain these values you chose these values given the classifier accuracy but how precisely and on what data did you train and test the classifiers on the same data used in section if i understood the experiments properly you used simplified spanish but i cannot find in the text how do you simplify spanish and how do you use it to train the classifier and the smt system section your method is better than other classification algorithms but it says nothing about how it performs compared to the state of the art methods you should at least precise why you chose these classifications algorithms for comparison furthermore how your rules impact these results and more generally how do you explain such a high accuracy for you method did you implement all these classification algorithms by yourselves if not you must provide the url or cite the framework you used for the smt experiments i guess you trained your phrase table on simplified spanish you must precise it you chose meteor over other metrics like bleu to evaluate your results you must provide some explanation for this choice i particularly appreciate when i see a mt paper that does not use bleu for evaluation but if you use meteor you must mention which version you used meteor has largely changed since you cited the paper of did you use the version or did you use the last one with paraphrases are your meteor scores statistically significant section as future work you mentioned further simplify morphology in this paper you do not present any simplification of morphology so i think that choosing the word further is misleading some typos femenine ensambling cuadratic style plain text citations should be rewritten like this toutanova et al built should be toutanova et al built place the caption of your tables below the table and not above and with more space between the table and its caption you used the acl template you must use the new one prepared for acl more generally i suggest that you read again the faq and the submission instructions provided on the acl website it will greatly help you to improve the paper there are also important information regarding references you must provide doi or url of all acl papers in your references after authors response thank you for your response you wrote that rules are added just as post processing but does it mean that you do not apply them to compute your classification results or if you do apply them before computing these results i am still wondering about their impact on these results you wrote that spanish is simplified as shown in table but it does not answer my question how did you obtain these simplifications exactly rules software etc the reader need to now that to reproduce your approach the classification algorithms presented in table are not state of the art or if they are you need to cite some paper furthermore this table only tells that deep learning gives the best results for classification but it does not tell at all if your approach is better than state of the art approach for machine translation you need to compare your approach with other state of the art morphology generation approaches described in related work designed for machine translation if you do that your paper will be much more convincing in my opinion,2.0
484.json,the paper considers a synergistic combination of two non hmm based speech recognition techniques ctc and attention based seqseq networks the combination is two fold first similarly to kim et al multitask learning is used to train a model with a joint ctc and seqseq cost second novel contribution the scores of the ctc model and seqseq model are ensembled during decoding results of beam search over the seqseq model are rescored with the ctc model the main novelty of the paper is in using the ctc model not only as an auxiliary training objective originally proposed by kim et al but also during decoding strengths the paper identifies several problems stemming from the flexibility offered by the attention mechanism and shows that by combining the seqseq network with ctc the problems are mitigated weaknesses the paper is an incremental improvement over kim et al since two models are trained their outputs can just as well be ensembled however it is nice to see that such a simple change offers important performance improvements of asr systems general discussion a lot of the paper is spent on explaining the well known classical asr systems a description of the core improvement of the paper better decoding algorithm starts to appear only on p the description of ctc is nonstandard and maybe should either be presented in a more standard way or the explanation should be expanded typically the relation p c z eq is deterministic there is one and only one character sequence that corresponds to the blank expanded form z i am also unsure about the last transformation of the eq,3.0
715.json,strengths task simple model yet the best results on squad single model evaluation and comparison weaknesses analysis of errors results see detailed comments below general discussion in this paper the authors present a method for directly querying wikipedia to answer open domain questions the system consist of two components a module to query fetch wikipedia articles and a module to answer the question given the fetched set of wikipedia articles the document retrieval system is a traditional ir system relying on term frequency models and ngram counts the answering system uses a feature representation for paragraphs that consists of word embeddings indicator features to determine whether a paragraph word occurs in a question token level features including pos ner etc and a soft feature for capturing similarity between question and paragraph tokens in embedding space a combined feature representation is used as an input to a bi direction lstm rnn for encoding for questions an rnn that works on the word embeddings is used these are then used to train an overall classifier independently for start and end spans of sentences within a paragraph to answer questions the system has been trained using different open domain qa datasets such as squad and webquestions by modifying the training data to include articles fetched by the ir engine instead of just the actual correct document passage overall an easy to follow interesting paper but i had a few questions the ir system has a accuracy of over and individually the document reader performs well and can beat the best single models on squad what explains the significant drop in table the authors mention that instead of the fetched results if they test using the best paragraph the accuracy reaches just from but that is still significantly below the in the squad task so presumably the error is this large because the neural network for matching isnt doing as good a job in learning the answers when using the modified training set which includes fetched articles instead of the case when training and testing is done for the document understanding task some analysis of whats going on here should be provided what was the training accuracy in the both cases what can be done to improve it to be fair the authors to allude to this in the conclusion but i think it still needs to be part of the paper to provide some meaningful insights i understand the authors were interested in treating this as a pure machine comprehension task and therefore did not want to rely on external sources such as freebase which could have helped with entity typing but that would have been interesting to use tying back to my first question if the error is due to highly relevant topical sentences as the authors mention could entity typing have helped the authors should also refer to quase sun et al at www and similar systems in their related work quase is also an open domain qa system that answers using fetched passages but it relies on the web instead of just wikipedia,3.0
579.json,strengths well motivated tackles an interesting problem clearly written and structured accompanied by documented code and dataset encouraging results weaknesses limited to completely deterministic hand engineered minimization rules some relevant literature on oie neglected sound but not thorough experimental evaluation general discussion this paper tackles a practical issue of most oie systems i e redundant uninformative and inaccurate extractions the proposed approach dubbed minoie is designed to actually minimize extractions by removing overly specific portions and turning them into structured annotations of various types similarly to ollie the authors put minie on top of a state of the art oie system clausie and test it on two publicly available datasets showing that it effectively leads to more concise extractions compared to standard oie approaches while at the same time retaining accuracy overall this work focuses on an interesting and perhaps underinvestigated aspect of oie in a sound and principled way the paper is clearly written sufficiently detailed and accompanied by supplementary material and a neat java implementation my main concern is however with the entirely static deterministic and rule based structure of minie even though i understand that a handful of manually engineered rules is technically the best strategy when precision is key these approaches are typically very hard to scale e g in terms of languages a recent trend of oie see faruqui and kumar falke et al in other words i think that this contribution somehow falls short of novelty and substance in proposing a pipeline of engineered rules that are mostly inspired by other oie systems such as clausie or reverb for instance i would have really appreciated an attempt to learn these minimization rules instead of hard coding them furthermore the authors completely ignore a recent research thread on semantically informed oie nakashole et al moro and navigli delli bovi et al where traditional extractions are augmented with links to underlying knowledge bases and sense inventories wikipedia wikidata yago babelnet these contributions are not only relevant in terms of related literature in fact having text fragments or constituents explicitly linked to a knowledge base would reduce the need for ad hoc minimization rules such as those in sections and in the example with bill of rights provided by the authors line an oie pipeline with a proper entity linking module would recognize automatically the phrase as mention of a registered entity regardless of the shape of its subconstituents also an underlying sense inventory would seamlessly incorporate the external information about collocations and multi word expressions used in section not by chance the authors rely on wordnet and wiktionary to compile their dictionary of collocations finally some remarks on the experimental evaluation despite the claim of generality of minie the authors choose to experiment only with clausie as underlying oie system most likely the optimal match it would have been very interesting to see if the improvement brought by minie is consistent also with other oie systems in order to actually assess its flexibility as a post processing tool among the test datasets used in section i would have included the recent oie benchmark of stanovsky and dagan where results are reported also for comparison systems not included in this paper textrunner woie kraken references manaal faruqui and shankar kumar multilingual open relation extraction using cross lingual projection naacl hlt tobias falke gabriel stanovsky iryna gurevych and ido dagan porting an open information extraction system from english to german emnlp ndapandula nakashole gerhard weikum and fabian suchanek patty a taxonomy of relational patterns with semantic types emnlp andrea moro roberto navigli wisenet building a wikipedia based semantic network with ontologized relations cikm andrea moro roberto navigli integrating syntactic and semantic analysis into the open information extraction paradigm ijcai claudio delli bovi luca telesca and roberto navigli large scale information extraction from textual definitions through deep syntactic and semantic analysis tacl vol gabriel stanovsky and ido dagan creating a large benchmark for open information extraction emnlp,3.0
71.json,strengths the paper states clearly the contributions from the beginning authors provide system and dataset figures help in illustrating the approach detailed description of the approach the authors test their approach performance on other datasets and compare to other published work weaknesses the explanation of methods in some paragraphs is too detailed and there is no mention of other work and it is repeated in the corresponding method sections the authors committed to address this issue in the final version readme file for the dataset authors committed to add readme file general discussion section mentions examples of dbpedia properties that were used as features do the authors mean that all the properties have been used or there is a subset if the latter please list them in the authors response the authors explain in more details this point and i strongly believe that it is crucial to list all the features in details in the final version for clarity and replicability of the paper in section the authors use lample et al bi lstm crf model it might be beneficial to add that the input is word embeddings similarly to lample et al figure kns in source language or in english since the mentions have been translated to english in the authors response the authors stated that they will correct the figure based on section it seems that topical relatedness implies that some features are domain dependent it would be helpful to see how much domain dependent features affect the performance in the final version the authors will add the performance results for the above mentioned features as mentioned in their response in related work the authors make a strong connection to sil and florian work where they emphasize the supervised vs unsupervised difference the proposed approach is still supervised in the sense of training however the generation of training data doesn t involve human interference,4.0
71.json,strengths very impressive resource fully automatic system particularly suitable for cross lingual learning across many languages good evaluation both within and outside wikipedia good comparison to works that employed manual resources weaknesses the clarity of the paper can be improved general discussion this paper presents a simple yet effective framework that can extract names from languages and link them to an english kb importantly the system is fully automatic which is particularly important when aiming to learn across such a large number of languages although this is far from trivial the authors are able to put their results in context and provide evaluation both within and outside of wikipedia i particularly like the way the put their work in the context of previous work that uses manual resources it is a good scientific practice and i am glad they do not refrain from doing that in worry that this would not look good the clarity of the paper can improve this is not an easy paper to write due to the quite complex process and the very large scale resource it generates however the paper is not very well organized and at many points i felt that i am reading a long list of details i encourage the authors to try and give the paper a better structure as one example i would be happy to see a better problem definition and high level motivations from the very beginning other examples has to do with better exposition of the motivations decisions and contributions in each part of the paper i admire the efforts the authors have already made but i think this can done even better this is an important paper and it deserves a clearer presentation all in all i like the paper and think it provides an important resource i would like to see this paper presented in acl,4.0
67.json,strengths knowledge lean language independent approach weaknesses peculiar task setting marginal improvement over wemb fu et al waste of space language not always that clear general discussion it seems to me that this paper is quite similar to fu et al and only adds marginal improvements it contains quite a lot of redundancy e g related work in sec and sec uninformative figures e g figure vs figure not so useful descriptions of mlp and rnn etc a short paper might have been a better fit the task looks somewhat idiosyncratic to me it is only useful if you already have a method that gives you all and only the hypernyms of a given word this seems to presuppose fu et al figure why are the first two stars connected by conjunction and the last two starts by disjunction why is the output dark star if the the three inputs are white stars sec lines appears to suggest that thresholds were tuned on the test data wemb is poorly explained lines some parts of the text are puzzling i can not make sense of the section titled combined with manually built hierarchies same for sec what do the red and dashed lines mean,2.0
31.json,update after author response my major concern about the optimization of model hyperparameter which are numerous has not been addressed this is very important considering that you report results from folded cross validation the explanation that benefits of their method are experimentally confirmed with difference while evaluating via fold cv on examples is quite unconvincing summary in this paper authors present a complex neural model for detecting factuality of event mentions in text the authors combine the following in their complex model a set of traditional classifiers for detecting event mentions factuality sources and source introducing predicates sips a bidirectional attention based lstm model that learns latent representations for elements on different dependency paths used as input a cnn that uses representations from the lstm and performs two output predictions one to detect specific from underspecified cases and another to predict the actual factuality class from the methodological point of view the authors are combining a reasonably familiar methods att bilstm and cnn into a fairly complex model however this model does not take raw text sequence of word embeddings as input but rather hand crafted features e g different dependency paths combining factuality concepts e g sources sips and clues the usage of hand crafted features is somewhat surprising if coupled with complex deep model the evaluation seems a bit tainted as the authors report the results from folded cross validation but do not report how they optimized the hyperparameters of the model finally the results are not too convincing considering the complexity of the model and the amount of preprocessing required extraction of event mentions sips and clues a macro average gain over the rule based baseline and overall performance seems modest at best looking at micro average the proposed model does not outperform simple maxent classifier the paper is generally well written and fairly easy to understand altogether i find this paper to be informative to an extent but in it current form not a great read for a top tier conference remarks you keep mentioning that the lstm and cnn in your model are combined properly what does that actually mean how does this properness manifest what would be the improper way to combine the models i find the motivation justification for the two output design rather weak the first argument that it allows for later addition of cues i e manually designed features kind of beats the learning representations advantage of using deep models the second argument about this design tackling the imbalance in the training set is kind of hand wavy as there is no experimental support for this claim you first motivate the usage of your complex dl architecture with learning latent representations and avoiding manual design and feature computation and then you define a set of manually designed features several dependency paths and lexical features as input for the model do you notice the discrepancy the lstms bidirectional and also with attention have by now already become a standard model for various nlp tasks thus i find the detailed description of the attention based bidirectional lstm unnecessary what you present as a baseline in section is also part of your model as it generates input to your model thus i think that calling it a baseline undermines the understandability of the paper the results reported originate from a fold cv however the model contains numerous hyperparameters that need to be optimized e g number of filters and filter sizes for cnns how do you optimize these values reporting results from a folded cross validation does not allow for a fair optimization of the hypeparameters either you are not optimizing the model hyperparameters at all or you are optimizing their values on the test set which is unfair notice that some values are non application na grammatically e g pru psu u why is underspecification in ony one dimension polarity or certainty not an option i can easily think of a case where it is clear the event is negative but it is not specified whether the absence of an event is certain probable or possible language style to a great degree great degree is an unusual construct use either great extent or large degree events that can not cannot or do not describes out networks in details shown in figure shown in figure in details,3.0
31.json,comments after author response thank you for clarifying that the unclear two step framework reference was not about the two facets i still do not find this use of a pipeline to be a particularly interesting contribution you state that de marneffe used additional annotated features in their system for fair comparison we re implement their system with annotated information in factbank but the de marneffe et al feature cited in the paper predicate classes requires only a dependency parser and vocabulary lists from roser saurí phd thesis general classes of event might be referring to factml event classes and while i admit it is not particularly clear in their work i am sure they could clarify i continue to find the use of combined properly to be obscure i agree that using lstm and cnn where respectively appropriate is valuable but you seem to imply that some prior work has been improper and that it is their combination which must be proper thank you for reporting on separate lstms for each of the paths i am curious as to why this combination may less effective in any case experiments with this kind of alternative structure deserve to be reported this paper introduces deep neural net technologies to the task of factuality classification as defined by factbank with performance exceeding alternative neural net models and baselines reimplemented from the literature strengths this paper is very clear in its presentation of a sophisticated model for factuality classification and of its evaluation it shows that the use of attentional features and bilstm clearly provide benefit over alternative pooling strategies and that the model also exceeds the performance of a more traditional feature based log linear model given the small amount of training data in factbank this kind of highly engineered model seems appropriate it is interesting to see that the bilstm cnn model is able to provide benefit despite little training data weaknesses my main concerns with this work regard its a apparent departure from the evaluation procedure in the prior literature b failure to present prior work as a strong baseline and c novelty while i feel that the work is original in engineering deep neural nets for the factuality classification task and that such work is valuable its approach is not particularly novel and the proposal of a two step supervised framework line is not particularly interesting given that factbank was always described in terms of two facets assuming i am correct to interpret two step as referring to these facets which i may not be the work cites saurí and pustejovsky but presents their much earlier and weaker system as a baseline nor does it consider qian et al ialp work which compares to the former both these works are developed on the timebank portion of factbank and evaluated on a held out acquaint timebank section while the present work does not report results on a held out set de marneffe et al system is also chosen as a baseline but not all their features are implemented nor is the present system evaluated on their pragbank corpus or other alternative representations of factuality proposed in prabhakaran et al sem and lee et al emnlp the evaluation is therefore somewhat lacking in comparability to prior work there were also important questions left unanswered in evaluation such as the effect of using gold standard events or sips given the famed success of bilstms with little feature engineering it is somewhat disappointing that this work does not attempt to consider a more minimal system employing deep neural nets on this task with for instance only the dependency path from a candidate event to its sip plus a bag of modifiers to that path the inclusion of heterogeneous information in one bilstm was an interesting feature which deserved more experimentation what if the order of inputs were permuted what if delimiters were used in concatenating the dependency paths in rs instead of the strange second nsubj in the rs chain of line what if each of sippath rspath cuepath were input to a separate lstm and combined the attentional features were evaluated together for the cnn and bilstm components but it might be worth reporting whether it was beneficial for each of these components could you benefit from providing path information for the aux words could you benefit from character level embeddings to account for morphology impact on factuality via tense aspect proposed future work is lacking in specificity seeing as there are many questions raised by this model and a number of related tasks to consider applying it to general discussion into what classes are you classifying events please state which are parameters of the model what do you mean by properly you use the same term in and it not clear which work you consider improper nor why is the chain form defined anywhere citation the repetition of nsubj in the example of line seems an unusual feature for the lstm to learn it may be worth footnoting here that each cue is classified separately distance surface distance how many sips cues perhaps add to table table would be good if augmented by the counts for embedded and author events percentages can be removed if necessary why fold given the small amount of training data surely fold would be more useful and not substantially increase training costs it not clear that this benefit comes from psen nor that the increase is significant or substantial does it affect overall results substantially is this significance across all metrics is the drop of f due to precision recall or both not clear what this sentence is trying to say table from the corpus sizes it seems you should only report significant figures for most columns except ct uu and micro a it seems unsurprising that rspath is insufficient given that the task is with respect to a sip and other inputs do not encode that information it would be more interesting to see performance of sippath alone this claim is not precise to my understanding de marneffe et al evaluates on pragbank not factbank minor issues in english usage non application not applicable i think you mean relevant relative can be displayed by a simple source is unclear not sure what you mean by basline do you mean pipeline,3.0
31.json,this paper proposes a supervised deep learning model for event factuality identification the empirical results show that the model outperforms state of the art systems on the factbank corpus particularly in three classes ct pr and ps the main contribution of the paper is the proposal of an attention based two step deep neural model for event factuality identification using bidirectional long short term memory bilstm and convolutional neural network cnn strengths the structure of the paper is not perfectly but well organized the empirical results show convincing statistically significant performance gains of the proposed model over strong baseline weaknesses see below for details of the following weaknesses novelties of the paper are relatively unclear no detailed error analysis is provided a feature comparison with prior work is shallow missing two relevant papers the paper has several obscure descriptions including typos general discussion the paper would be more impactful if it states novelties more explicitly is the paper presenting the first neural network based approach for event factuality identification if this is the case please state that the paper would crystallize remaining challenges in event factuality identification and facilitate future research better if it provides detailed error analysis regarding the results of table and what are dominant sources of errors made by the best system bilstm cnn att what impacts do errors in basic factor extraction table have on the overall performance of factuality identification table the analysis presented in section is more like a feature ablation study to show how useful some additional features are the paper would be stronger if it compares with prior work in terms of features does the paper use any new features which have not been explored before in other words it is unclear whether main advantages of the proposed system come purely from deep learning or from a combination of neural networks and some new unexplored features as for feature comparison the paper is missing two relevant papers kenton lee yoav artzi yejin choi and luke zettlemoyer event detection and factuality assessment with non expert supervision in proceedings of the conference on empirical methods in natural language processing pages sandeep soni tanushree mitra eric gilbert and jacob eisenstein modeling factuality judgments in social media text in proceedings of the nd annual meeting of the association for computational linguistics pages the paper would be more understandable if more examples are given to illustrate the underspecified modality u and the underspecified polarity u there are two reasons for that first the definition of underspecified is relatively unintuitive as compared to other classes such as probable or positive second the examples would be more helpful to understand the difficulties of uu detection reported in line among the seven examples s s only s corresponds to uu and its explanation is quite limited to illustrate the difficulties a minor comment is that the paper has several obscure descriptions including typos as shown below the explanations for features in section are somewhat intertwined and thus confusing the section would be more coherently organized with more separate paragraphs dedicated to each of lexical features and sentence level features by stating that the sip feature comprises two features i e lexical level and sentence level and introduce their corresponding variables l and c at the beginning moving the description of embeddings of the lexical feature in line to the first paragraph and presenting the last paragraph about relevant source identification in a separate subsection because it is not about sip detection the title of section baseline is misleading a more understandable title would be basic factor extraction or basic feature extraction because the section is about how to extract basic factors features not about a baseline end to end system for event factuality identification the presented neural network architectures would be more convincing if it describes how beneficial the attention mechanism is to the task table seems to show factuality statistics only for all sources the table would be more informative along with table if it also shows factuality statistics for author and embed table would be more effective if the highest system performance with respect to each combination of the source and the factuality value is shown in boldface section says auxwords can describe the syntactic structures of sentences whereas section says they auxiliary words can reflect the pragmatic structures of sentences these two claims do not consort with each other well and neither of them seems adequate to summarize how useful the dependency relations aux and mark are for the task s seems to be another example to support the effectiveness of auxiliary words but the explanation for s is thin as compared to the one for s what is the auxiliary word for ensure in s line event go in s should be event go in s line in details should be in detail line in section should be in section to make it more specific line are cent researches should be are cent research or are cent studies are search is an uncountable noun line factbank should be factbank,2.0
66.json,strengths tackles a not very explored task with obvious practical application well written and motivated weaknesses the only method of validation is a user study which has several weaknesses discussion the paper investigates various methods to generate memorable mnemonic encodings of numbers based on the major system as opposed to other methods that rely on this system to encode sequences the methods proposed in this work return a single sequence instead of a set of candidates which is selected to improve memorability since memorability is an ambiguous criterion to optimize for the authors explore various syntactic approaches that aim for short and likely sentences their final model uses a pos template sampled form a set of nice structures and a tri gram language model to fill in the slots of the template the proposed approach is well motivated the section on existing tools places this approach in the context of previous work on security and memorability the authors point to results showing that passwords based on mnemonic phrases offer the best of both worlds in terms of security vs random passwords and memorability vs naive passwords this solid motivation will appease those readers initially skeptical about the importance feasibility of such techniques in terms of the proposed methods the baselines and n gram models unsurprisingly generate bad encodings the results in table show that indeed chunk and sentence produce shorter sentences but for short digits such as this one how relevant are the additional characteristics of these methods eg pos replacements templates etc it seems that a simple n gram model with the number of digits per trigram reweighing could perform well here the evaluation is weaker than the rest of the paper my main concern is that a one time memorization setting seems inadequate to test this framework mnemonic techniques are meant to aid recall after repeated memorization exercises not just a single priming event thus a more informative setting would have had the users be reminded of the number and encoding daily over a period of time and after a buffer period test their recall this would also more closely resemble the real life conditions in which such a technique would be used e g for password memorization in terms of the results the difference between long term recall and recognition is interesting do the authors have some explanation for why in the former most methods performed similarly but in the latter sentence performs better could it be that the use of not very likely words e g officiate in the example provided make the encodings hard to remember but easy to spot if this were the case it would somewhat defeat the purpose of the approach also it would be useful for the reader if the paper provided e g in an appendix some examples of the digits encodings that the users were presented during the study to get a better sense of the difficulty of recall and the quality of the encodings suggestions it would be nice to provide some background on the major system for those not familiar with it which i suspect might be many in the acl audience myself included where does it come from what s the logic behind those digit phoneme maps,3.0
128.json,this paper proposes a neural network architecture that represent structural linguistic knowledge in a memory network for sequence tagging tasks in particular slot filling of the natural language understanding unit in conversation systems substructures e g a node in the parse tree is encoded as a vector a memory slot and a weighted sum of the substructure embeddings are fed in a rnn at each time step as additional context for labeling strengths i think the main contribution of this paper is a simple way to flatten structured information to an array of vectors the memory which is then connected to the tagger as additional knowledge the idea is similar to structured syntax based attention i e attention over nodes from treelstm related work includes zhao et al on textual entailment liu et al on natural language inference and eriguchi et al for machine translation the proposed substructure encoder is similar to dcnn ma et al each node is embedded from a sequence of ancestor words the architecture does not look entirely novel but i kind of like the simple and practical approach compared to prior work weaknesses i am not very convinced by the empirical results mostly due to the lack of details of the baselines comments below are ranked by decreasing importance the proposed model has two main parts sentence embedding and substructure embedding in table the baseline models are treernn and dcnn they are originally used for sentence embedding but one can easily take the node substructure embedding from them too it not clear how they are used to compute the two parts the model uses two rnns a chain based one and a knowledge guided one the only difference in the knowledge guided rnn is the addition of a knowledge vector from the memory in the rnn input eqn and it seems completely unnecessary to me to have separate weights for the two rnns the only advantage of using two is an increase of model capacity i e more parameters furthermore what are the hyper parameters size of the baseline neural networks they should have comparable numbers of parameters i also think it is reasonable to include a baseline that just input additional knowledge as features to the rnn e g the head of each word ner results etc any comments results on the model sensitivity to parser errors comments on the model after computing the substructure embeddings it seems very natural to compute an attention over them at each word is there any reason to use a static attention for all words i guess as it is the knowledge is acting more like a filter to mark important words then it is reasonable to include the baseline suggest above i e input additional features since the weight on a word is computed by inner product of the sentence embedding and the substructure embedding and the two embeddings are computed by the same rnn cnn does not it means nodes phrases similar to the whole sentence gets higher weights i e all leaf nodes the paper claims the model generalizes to different knowledge but i think the substructure has to be represented as a sequence of words e g it does not seem straightforward for me to use constituent parse as knowledge here finally i am hesitating to call it knowledge this is misleading as usually it is used to refer to world external knowledge such as a knowledge base of entities whereas here it is really just syntax or arguably semantics if amr parsing is used general discussion this paper proposes a practical model which seems working well on one dataset but the main ideas are not very novel see comments in strengths i think as an acl paper there should be more takeaways more importantly the experiments are not convincing as it is presented now will need some clarification to better judge the results post rebuttal the authors did not address my main concern which is whether the baselines e g treernn are used to compute substructure embeddings independent of the sentence embedding and the joint tagger another major concern is the use of two separate rnns which gives the proposed model more parameters than the baselines therefore i am not changing my scores,2.0
578.json,strengths the paper proposes an end to end neural model for semantic graph parsing based on a well designed transition system the work is interesting learning semantic representations of dmrs which is capable of resolving semantics such as scope underspecification this work shows a new scheme for computational semantics benefiting from an end to end transition based incremental framework which resolves the parsing with low cost weaknesses my major concern is that the paper only gives a very common introduction for the definition of dmrs and ep and the example even makes me a little confused because i cannot see anything special for dmrs the description can be a little more detailed i think however upon the space limitation it is understandable the same problem exists for the transition system of the parsing model if i do not have any background of mrs and ep i can hardly learn something from the paper just seeing that this paper is very good general discussion overall this paper is very interesting to me i like the dmrs for semantic parsing very much and like the paper very much hope that the open source codes and datasets can make this line of research being a hot topic,5.0
201.json,strengths evaluating bag of words and bound contexts from either dependencies or sentence ordering is important and will be a useful reference to the community the experiments were relatively thorough though some choices could use further justification and the authors used downstream tasks instead of just intrinsic evaluations weaknesses the authors change the objective function of gbow from p c sum wi to p w sum ci this is somewhat justified as dependency based context with a bound representation only has one word available for predicting the context but it unclear exactly why that is the case and deserves more discussion presumably the non dependency context with a bound representation would also suffer from this drawback if so how did ling et al do it unfortunately the authors do not compare any results against the original objective which is a definite weakness in addition the authors change gsg to match gbow again without comparing to the original objective adding results from word vectors trained using the original gbow and gsg objective functions would justify these changes assuming the results do not show large changes the hyperparameter settings should be discussed further this played a large role in levy et al so you should consider trying different hyperparameter values these depend pretty heavily on the task so simply taking good values from another task may not work well in addition the authors are unclear on exactly what model is trained in section they say only that it is a simple linear classifier in section they use logistic regression with the average of the word vectors as input but call it a neural bag of words model technically previous work also used this name but i find it misleading since it just logistic regression and hence a linear model which is not something i would call neural it is important to know if the model trained in section is the same as the model trained in so we know if the different conclusions are the result of the task or the model changing general discussion this paper evaluates context taken from dependency parses vs context taken from word position in a given sentence and bag of words vs tokens with relative position indicators this paper is useful to the community as they show when and where researchers should use word vectors trained using these different decisions emphasis to improve the main takeaway from this paper that future researchers will use is given at the end of and but really should be summarized at the start of the paper specifically the authors should put in the abstract that for pos chunking and ner bound representations outperform bag of words representations and that dependency contexts work better than linear contexts in most cases in addition for a simple text classification model bound representations perform worse than bag of words representations and there seemed to be no major difference between the different models or context types small points of improvement should call unbounded context bag of words this may lead to some confusion as one of the techniques you use is generalized bag of words but this can be clarified easily it the distributional hypothesis not the distributed hypothesis citations should have a comma instead of semicolon separating them deps should be capitalized consistently throughout the paper usually it appears as deps also should be introduced as something like dependency parse tree context deps typo how different contexts affect model performances should have the word do,4.0
554.json,strengths a the paper presents a bayesian learning approach for recurrent neural network language model the method outperforms standard sgd with dropout on three tasks b the idea of using bayesian learning with rnns appears to be novel c the computationally efficient bayesian algorithm for rnn would be of interest to the nlp community for various applications weaknesses primary concern is about evaluation sec the paper reports the performance of difference types of architectures lstm gru vanilla rnn on character lm task while comparing the learning algorithms on the penn treebank task furthermore rmsprop and psgld are compared for the character lm while sgd dropout is compared with sgld dropout on word language model task this is inconsistent i would suggest reporting both these dimensions i e architectures and the exact same learning algorithms on both character and word lm tasks it would be useful to know if the results from the proposed bayesian learning approaches are portable across both these tasks and data sets l the paper states that the performance gain mainly comes from adding gradient noise and model averaging this statement is not justified empirically to arrive at this conclusion an a b experiment with without adding gradient noise and or model averaging needs to be done l gal dropout is run on the sentence classification task but not on language model captions task since gal dropout is not specific to sentence classification i would suggest reporting the performance of this method on all three tasks this would allow the readers to fully assess the utility of the proposed algorithms relative to all existing dropout approaches l is there any sort order for the samples theta thetak e g are samples with higher posterior probabilities likely to be at higher indices why not report the result of randomly selecting k out of s samples as an additional alternative regular rnn lms are known to be expensive to train and evaluate it would be very useful to compare the training evaluation times for the proposed bayesian learning algorithms with sgd dropout that would allow the readers to trade off improvements versus increase in training run times clarifications l what does thetas refer to is this a map estimate of parameters based on only the sample s l clarify what theta means in the context of dropout dropconnect typos l output l rmsprop,4.0
104.json,strengths outperforms align in supervised entity linking task which suggests that the proposed framework improves representations of text and knowledge that are learned jointly direct comparison with closely related approach using very similar input data analysis of the smoothing parameter provides useful analysis since impact of popularity is a persistent issue in entity linking weaknesses comparison with align could be better align used content window size vs this paper vector dimension of vs this paper also its not clear to me whether n ej includes only entities that link to ej the graph is directed and consists of wikipedia outlinks but is adjacency defined as it would be for an undirected graph for align the context of an entity is the set of entities that link to that entity if n ej is different we cannot tell how much impact this change has on the learned vectors and this could contribute to the difference in scores on the entity similarity task it is sometimes difficult to follow whether mention means a string type or a particular mention in a particular document the phrase mention embedding is used but it appears that embeddings are only learned for mention senses it is difficult to determine the impact of sense disambiguation order without comparison to other unsupervised entity linking methods general discussion,3.0
104.json,strengths good ideas simple neural learning interesting performance altough not striking and finally large set of applications weaknesses amount of novel content clarity in some sections the paper presents a neural learning method for entity disambiguation and linking it introduces a good idea to integrate entity mention and sense modeling within the smame neural language modeling technique the simple training procedure connected with the modeling allows to support a large set of application the paper is clear formally but the discussion is not always at the same level of the technical ideas the empirical evaluation is good although not striking improvements of the performance are reported although it seems an extension of yamada et al conll it adds novel ideas and it is of a releant interest the weaker points of the paper are the prose is not always clear i found section not as clear some details of figure are not explained and the terminology is somehow redundant for example why do you refer to the dictionary of mentions or the dictionary of entity mention pairs are these different from text anchors and types for annotated text anchors tha paper is quite close in nature to yamada et al and the authors should at least outline the differences one general observation on the current version is the paper tests the multiple embedding model against entity linking disambiguation tasks however word embeddings are not only used to model such tasks but also some processes not directly depending on entities of the kb e g parsing coreference or semantic role labeling the authors should show that the word embeddings provided by the proposed mpme method are not weaker wrt to simpler wordspaces in such other semantic tasks i e those involving directly entity mentions i did read the author response,4.0
387.json,strengths this paper tackles an interesting problem and provides a to my knowledge novel and reasonable way of learning and combining cognitive features with textual features for sentiment analysis and irony detection the paper is clearly written and organized and the authors provided a lot of useful detail and informative example and plots most of the results are convincing and the authors did a good job comparing their approach and results with previous work weaknesses just from the reading abstract i expected that the authors approach would significantly outperform previous methods and that using both the eye gaze and textual features consistently yields the best results upon reading the actual results section however it seems like the findings were more mixed i think it would be helpful to update the abstract and introduction to reflect this when evaluating the model on dataset for sentiment analysis were the sarcastic utterances included did the model do better on classifying the non sarcastic utterances than the sarcastic ones i understand why the eye movement data would be useful for sarcasm detection but it was not as obvious to me why it would be helpful for non sarcastic sentiment classification beyond the textual features general discussion this paper contains a lot of interesting content and the approach seems solid and novel to me the results were a little weaker than i had anticipated from the abstract but i believe would still be interesting to the larger community and merits publication,4.0
779.json,strengths this is a well written paper the paper is very clear for the most part the experimental comparisons are very well done the experiments are well designed and executed the idea of using kd for zero resource nmt is impressive weaknesses there were many sentences in the abstract and in other places in the paper where the authors stuff too much information into a single sentence this could be avoided one can always use an extra sentence to be more clear there could have been a section where the actual method used could be explained in a more detailed this explanation is glossed over in the paper it non trivial to guess the idea from reading the sections alone during test time you need the source pivot corpus as well this is a major disadvantage of this approach this is played down in fact it not mentioned at all i could strongly encourage the authors to mention this and comment on it general discussion this paper uses knowledge distillation to improve zero resource translation the techniques used in this paper are very similar to the one proposed in yoon kim et al the innovative part is that they use it for doing zero resource translation they compare against other prominent works in the field their approach also eliminates the need to do double decoding detailed comments line the authors could have avoided this complicated structure for two simple sentences line johnson et al has sota on english french and german english line there is no evidence provided as to why combination of multiple languages increases complexity please retract this statement or provide more evidence evidence in literature seems to suggest the opposite line the two lines here are repeated again they were first mentioned in the previous paragraph line figure not,4.0
759.json,this paper proposes a joint model of salient phrase selection and discourse relation prediction in spoken meeting experiments using meeting corpora show that the proposed model has higher performance than the svm based classifier strengths the paper is written to be easy to read technical details are described fully and high performance is also shown in experimental evaluation it also shows useful comparisons with related research in the field of discourse structure analysis and key phrase identification it is interesting to note that not only the performance evaluation of phrase selection from discourse discourse relation labeling and summary generation as their applications but also application to the prediction of the consistency of understanding by team members is also verified weaknesses jointly modeling salient phrase extraction and discourse relationship labeling between speaker turns has been proposed if intuitive explanation about their interactivity and the usefulness of considering it is fully presented general discussion svm based classifier is set as a comparative method in the experiment it would be useful to mention the validity of the setting,3.0
562.json,the paper presents a method for relation extraction based on converting the task into a question answering task the main hypothesis of the paper is that questions are a more generic vehicle for carrying content than particular examples of relations and are easier to create the results seem to show good performance though a direct comparison on a standard relation extraction task is not performed strengths the technique seems to be adept at identifying relations a bit under f measure it works well both on unseen questions for seen relations and relatively well on unseen relations the authors describe a method for obtaining a large training dataset weaknesses i wish performance was also shown on standard relation extraction datasets it is impossible to determine what types of biases the data itself has here relations are generated from wikidata via wikireading extracted from wikipedia not regular newswire newsgroups etc it seems to me that the nist tac kbp slot filling dataset is good and appropriate to run a comparison one comparison that the authors did not do here but should is to train a relation detection model on the generated data and see how well it compares with the qa approach general discussion i found the paper to be well written and argued and the idea is interesting and it seems to work decently i also found it interesting that the zero shot nl method behaved indistinguishably from the single question baseline and not very far from the multiple questions system,4.0
237.json,summary this paper presents an empirical study to identify a latent dimension of sentiment in word embeddings strengths s tackles a challenging problem of unsupervised sentiment analysis s figure in particular is a nice visualisation weaknesses w the experiments in particular are very thin i would recommend also measuring f performance and expanding the number of techniques compared w the methodology description needs more organisation and elaboration the ideas tested are itemised but insufficiently justified w the results are quite weak in terms of the reported accuracy and depth of analysis perhaps this work needs more development particularly with validating the central assumption that the distributional hypothesis implies that opposite words although semantically similar are separated well in the vector space,1.0
86.json,this paper presents a method for translating natural language descriptions into source code via a model constrained by the grammar of the programming language of the source code i liked this paper it well written addresses a hard and interesting problem by taking advantage of inherent constraints and shows significant performance improvements strengths addresses an interesting and important problem space constraints inherent to the output space are incorporated well into the model good evaluation and comparisons also showing how the different aspects of the model impact performance clearly written paper weaknesses my primary and only major issue with the paper is the evaluation metrics while accuracy and bleu are easy to compute i do not think they give a sufficiently complete picture accuracy can easily miss correctly generated code because of trivial and for program functionality inconsequential changes you could get accuracy with functional correctness as for bleu i am not sure how well it evaluates code where you can perform significant changes e g tree transformations of the ast without changing functionality i understand why bleu is being used but it seems to me to be particularly problematic given its token level n gram evaluation perhaps bleu can be applied to the asts of both reference code and generated code after some level of normalization of the asts what i would really like to see is an evaluation testing for functional equivalence of reference and generated code understandably this is difficult since test code will have to be written for each reference however even if this is done for a random reasonably small subsample of the datasets i think it would give a much more meaningful picture minor issues page para structural information helps to model information flow within the network by network do you mean the ast section action embedding are the action embedding vectors in wr and wg simply one hot vectors or do you actually have a non trivial embedding for the actions if so how is it computed if not what is the difference between the vectors of wr and e r in equation section preprocessing if you replace quoted strings in the descriptions for the django dataset how are cases where those strings need to be copied into the generated code handled it is also mentioned in the supplementary material that infrequent words are filtered out if so how do you handles cases where those words describe the variable name or a literal that needs to be in the code i have read the author response,4.0
388.json,this paper describes interesting and ambitious work the automated conversion of universal dependency grammar structures into what the paper calls semantic logical form representations in essence each ud construct is assigned a target construction in logical form and a procedure is defined to effect the conversion working inside out using an intermediate form to ensure proper nesting of substructures into encapsulating ones two evaluations are carried out comparing the results to gold standard lambda structures and measuring the effectiveness of the resulting lambda expressions in actually delivering the answers to questions from two qa sets it is impossible to describe all this adequately in the space provided the authors have taken some care to cover all principal parts but there are still many missing details i would love to see a longer version of the paper particularly the qa results are short changed it would have been nice to learn which types of question are not handled and which are not answered correctly and why not this information would have been useful to gaining better insight into the limitations of the logical form representations that leads to my main concern objection this logical form representation is not in fact a real semantic one it is essentially a rather close rewrite of the dependency structure of the input with some good steps toward semanticization including the insertion of lambda operators the explicit inclusion of dropped arguments via the enhancement operation and the introduction of appropriate types units for such constructions as eventive adjectives and nouns like running horse and president in but many even simple aspects of semantic are either not present at least not in the paper and or simply wrong missing quantification as in every or all numbers as in or just over various forms of reference as in he that man what i said before negation and modals which change the semantics in interesting ways inter event relationships as in the subevent relationship between the events in the vacation was nice but traveling was a pain etc etc to add them one can easily cheat by treating these items as if they were just unusual words and defining obvious and simple lambda formulas for them but they in fact require specific treatment for example a number requires the creation of a separate set object in the representation with its own canonical variable allowing later text to refer to one of them and bind the variable properly for another example person a s model of an event may differ from person b s so one needs two representation symbols for the event plus a coupling and mapping between them for another example one has to be able to handle time even if simply by temporally indexing events and states none of this is here and it is not immediately obvious how this would be added in some cases as drt shows quantifier and referential scoping is not trivial it is easy to point to missing things and unfair to the paper in some sense you can t be expected to do it all but you cannot be allowed to make obvious errors very disturbing is the assignment of event relations strictly in parallel with the verb s or noun s syntactic roles no one can claim seriously that he broke the window and the window broke has he and the window filling the same semantic role for break that s simply not correct and one cannot dismiss the problem as the paper does to some nebulous subsequent semantic processing this really needs adequate treatment even in this paper this is to my mind the principal shortcoming of this work for me this is the make or break point as to whether i would fight to have the paper accepted in the conference i would have been far happier if the authors had simply acknowledged that this aspect is wrong and will be worked on in future with a sketch saying how perhaps by reference to framenet and semantic filler requirements independent of the representation the notation conversion procedure is reasonably clear i like the facts that it is rather cleaner and simpler than its predecessor based on stanford dependencies and also that the authors have the courage of submitting non neural work to the acl in these days of unbridled and giddy enthusiasm for anything neural,3.0
12.json,this paper describes a rule based approach to time expression extraction its key insights are time expressions typically are short and contain at least time token it first recognizes the time token through a combination of dictionary lookup regular expression match with pos tagging information it then expands the time segment from either direction of the time token until it reaches based on a set of heuristic rules finally it merges the time segments into a single time expression based on another set of rules evaluation of this approach with both rule based ml based systems on data sets show significant improvements strengths it well written and clearly presented the rules are motivated by empirical observations of the data and seems to be well justified as evidenced by the evaluation weaknesses there are some underspecification in the paper that makes it difficult to reproduce the results see below for details general discussion section what are there seasons what about things such as ramadan month or holiday season section two benchmark datasets three datasets section an example without time token will be helpful section given this approach is close to the ceiling of performance since expressions contain time token and the system has achieved recall how do you plan to improve further is there any plan to release the full set of rules software used,4.0
214.json,strengths the macro discourse structure is a useful complement to micro structures like rst the release of the dataset would be helpful to a range of nlp applications weaknesses providing more comparisons with the existed cdtb will be better the primary secondary relationship is mentioned a lot in this paper however its difference with the nuclearity is unclear and not precisely defined the experiment method is not clearly described in the paper general discussion,3.0
193.json,this paper presents the first parser to ucca a recently proposed meaning representation the parser is transition based and uses a new transition set designed to recover challenging discontinuous structures with reentrancies experiments demonstrate that the parser works well and that it is not easy to build these representation on top of existing parsing approaches this is a well written and interesting paper on an important problem the transition system is well motivated and seems to work well for the problem the authors also did a very thorough experimental evaluation including both varying the classifier for the base parser neural linear model etc and also comparing to the best output you could get from other existing but less expressive parsing formulations this paper sets a strong standard to ucca parsing and should also be interesting to researchers working with other expressive meaning representations or complex transition systems my only open question is the extent to which this new parser subsumes all of the other transition based parsers for amr sdp etc could the ucca transition scheme be used in these cases which heuristic alignments if necessary and would it just learn to not use the extra transitions for non terminals etc would it reduce to an existing algorithm or perhaps work better answering this question isn t crucial the paper is very strong as is but it would add to the overall understanding and could point to interesting areas for future work i read the author response and agree with everything they say,4.0
193.json,the authors response answer most of the clarification questions of my review summary the paper describes a transition based system for ucca graphs featuring non terminal nodes reentrancy and discontinuities the transition set is a mix of already proposed transitions the key aspects are the swap transition to cope with discontinuities and transitions not popping the stack to allow multiple parents for a node the best performance is obtained using as transition classifier a mlp with features based on bidirectional lstms the authors compare the obtained performance with other state of the art parsers using conversion schemes to bilexical graphs and to tree approximations the parsers are trained on converted data used to predict graphs or trees and the predicted structures are converted ack to ucca and confronted with gold ucca representations strengths the paper presents quite solid work with state of the art transition based techniques and machine learning for parsing techniques it is very well written formal and experimental aspects are described in a very precise way and the authors demonstrate a very good knowledge of the related work both for parsing techniques and for shallow semantic representations weaknesses maybe the weakness of the paper is that the originality lies mainly in the targeted representations ucca not really in the proposed parser more detailed comments and clarification questions introduction lines note that discontinuous nodes could be linked to non projectivity in the dependency framework so maybe rather state that the difference is with phrase structure syntax not dependency syntax section in the ucca scheme description the alternative a node or unit corresponds to a terminal or to several sub units is not very clear do you mean something else than a node is either a terminal or a non terminal can not a non terminal node have one child only and thus neither be a terminal nor have several sub units note that movement action or state is not totally appropriate since there are processes which are neither movements nor actions e g agentless transformations the ucca guidelines use these three terms but later state the state process dichotomy with processes being an action movement or some other relation that evolves in time lines note that the contrast between john and mary trip and john and mary children is not very felicitous the relational noun children evokes an underlying relation between two participants the children and john mary that has to be accounted for in ucca too section concerning the conversion procedures while it is very complete to provide the precise description of the conversion procedure in the supplementary material it would ease reading to describe it informally in the paper as a variant of the constituent to dependency conversion procedure à la manning also it would be interesting to motivate the priority order used to define the head of an edge how l u is chosen in case of several children with same label should be made explicit leftmost in the converted graphs in figure some edges seem inverted e g the direction between john and moved and between john and gave should be the same further i am confused as to why the upper bound for remote edges in bilexical approximations is so low the current description of the conversions do not allow to get an quick idea of which kind of remote edges cannot be handled concerning the comparison to other parsers it does not seem completely fair to tune the proposed parser but to use default settings for the other parsers section line please better motivate the claim using better input encoding section i am not convinced by the alledged superiority of representations with non terminal nodes although it can be considered more elegant not to choose a head for some constructions it can be noted that formally co head labels can be used in bilexical dependencies to recover the same information,4.0
107.json,this paper describes a model for cross lingual named entity recognition ner the authors employ conditional random fields maximum entropy markov and neural network based ner methods in addition authors propose two methods to combine the output of those methods probability based and ranking based and a method to select the best training instances from cross lingual comparable corpora the cross lingual projection is done using a variant of mikolov s proposal in general the paper is easy to follow well structured and the english quality is also correct the results of the combined annotations are interesting detailed comments i was wondering which is the motivation behind proposing a continuous bag of word cbow model variation you don t give much details about this or the parameters employed was the original model or the continuous skip gram model offering low results i suggest to include also the results with the cbow model so readers can analyse the improvements of your approach since you use a decay factor for the surrounding embeddings i suggest to take a look to the exponential decay used in similarly to the previous comment i would like to look at the differences between the original mikolov s cross lingual projections and your frequency weighted projections these contributions are more valuable if readers can see that your method is really superior the proposed data selection scheme is very effective in selecting good quality projection labeled data and the improvement is significant have you conducted a test of statistical significance i would like to know if the differences between result in this work are significant i suggest to integrate the text of section at the beginning of section it would look cleaner i also recommend to move the evaluation of table to the evaluation section i miss a related work section your introduction includes part of that information i suggest to divide the introduction in two sections the evaluation is quite short pages with conclusion section there you obtain state of the art results and i would appreciate more discussion and analysis of the results suggested references iacobacci i pilehvar m t navigli r embeddings for word sense disambiguation an evaluation study in proceedings of the th annual meeting of the association for computational linguistics vol pp,4.0
384.json,strengths this paper presents an approach for fine grained isa extraction by learning modifier interpretations the motivation of the paper is easy to understand and this is an interesting task in addition the approach seems solid in general and the experimental results show that the approach increases in the number of fine grained classes that can be populated weaknesses some parts of the paper are hard to follow it is unclear to me why d e p o is multiplied by w in eq and why the weight for e in eq is explained as the product of how often e has been observed with some property and the weight of that property for the class mh in addition it also seems unclear how effective introducing compositional models itself is in increasing the coverage i think one of the major factors of the increase of the coverage is the modifier expansion which seems to also be applicable to the baseline hearst it would be interesting to see the scores hearst with modifier expansion general discussion overall the task is interesting and the approach is generally solid however since this paper has weaknesses described above i am ambivalent about this paper minor comment i am confused with some notations for example it is unclear for me what h stands for it seems that h sometimes represents a class such as in e h o but sometimes represents a noun phrase such as in h p n w d is my understanding correct in paragraph precision recall analysis why the authors use area under the roc curve instead of area under the precision recall curve despite the paragraph title precision recall analysis after reading the response thank you for the response i am not fully satisfied with the response as to the modifier expansion i do not think the modifier expansion can be applied to hearst as to the proposed method however i am wondering whether there is no way to take into account the similar modifiers to improve the coverage of hearst i am actually between and but since it seems still unclear how effective introducing compositional models itself is i keep my recommendation as it is,3.0
239.json,this paper proposes a framework for evaluation of word embeddings based on data efficiency and simple supervised tasks the main motivation is that word embeddings are generally used in a transfer learning setting where evaluation is done based on how faster is to train a target model the approach uses a set of simple tasks evaluated in a supervised fashion including common benchmarks such as word similarity and word analogy experiments on a broad set of embeddings show that ranks tend to be task specific and change according to the amount of training data used strengths the transfer learning data efficiency motivation is an interesting one as it directly relates to the idea of using embeddings as a simple semi supervised approach weaknesses a good evaluation approach would be one that propagates to end tasks specifically if the approach gives some rank r for a set of embeddings i would like it to follow the same rank for an end task like text classification parsing or machine translation however the approach is not assessed in this way so it is difficult to trust the technique is actually more useful than what is traditionally done the discussion about injective embeddings seems completely out of topic and does not seem to add to the paper understanding the experimental section is very confusing section points out that the analysis results in answers to questions as is it worth fitting syntax specific embeddings even when supervised datset is large but i fail to understand where in the evaluation the conclusion was made still in section the manuscript says this hints that purely unsupervised large scale pretraining might not be suitable for nlp applications this is a very bold assumption and i again fail to understand how this can be concluded from the proposed evaluation approach all embeddings were obtained as off the shelf pretrained ones so there is no control over which corpora they were trained on this limits the validity of the evaluation shown in the paper the manuscript needs proofreading especially in terms of citing figures in the right places why figure which is on page is only cited in page general discussion i think the paper starts with a very interesting motivation but it does not properly evaluate if their approach is good or not as mentioned above for any intrinsic evaluation approach i expect to see some study if the conclusions propagate to end tasks and this is not done in the paper the lack of clarity and proofreading in the manuscript also hinders the understanding in the future i think the paper would vastly benefit from some extrinsic studies and a more controlled experimental setting using the same corpora to train all embeddings for instance but in the current state i do not think it is a good addition to the conference,2.0
444.json,this paper presents evaluation metrics for lyrics generation exploring the need for the lyrics to be original but in a similar style to an artist whilst being fluent and co herent the paper is well written and the motivation for the metrics are well explained the authors describe both hand annotated metrics fluency co herence and match and an automatic metric for similarity whilst the metric for similarity is unique and interesting the paper does not give any evidence of this as an effective automatic metric as correlations between this metric and the others are low which they say that they should be used separately the authors claim it can be used to meaningfully analyse system performance but we have to take their word for it as again there is no correlation with any hand annotated performance metric getting worse scores than a baseline system isn t evidence that the metric captures quality e g you could have a very strong baseline some missing references e g recent work looking at automating co herence e g using mutual information density e g li et al in addition some reference to style matching from the nlg community are missing e g dethlefs et al and the style matching work by pennebaker,3.0
444.json,this paper proposes to present a more comprehensive evaluation methodology for the assessment of automatically generated rap lyrics as being similar to a target artist while the assessment of the generation of creative work is very challenging and of great interest to the community this effort falls short of its claims of a comprehensive solution to this problem all assessment of this nature ultimately falls to a subjective measure can the generated sample convince an expert that the generated sample was produced by the true artist rather than an automated preocess this is essentially a more specific version of a turing test the effort to automate some parts of the evaluation to aid in optimization and to understand how humans assess artistic similarity is valuable however the specific findings reported in this work do not encourage a belief that these have been reliably identified specifically consider the central question was a sample generated by a target artist the human annotators who were asked this were not able to consistently respond to this question this means either the annotators did not have sufficient expertise to perform the task or the task was too challenging or some combination of the two the proposed automatic measures also failed to show a reliable agreement to human raters performing the same task this dramatically limits their efficacy in providing a proxy for human assessment the low interannotator agreement may be expected because the task is subjective but the idea of decomposing the evaluation into fluency and coherence components is meant to make it more tractable and thereby improve the consistency of rater scores a low iaa for an evaluation metric is a cause for concern and limits its viability as a general purpose tool specific questions comments why is a line by line level evaluation prefered to a verse level analysis specifically for coherence a line by line analysis limits the scope of coherence to consequtive lines style matching this term assumes that these artists each have a distinct style and always operate in that style i would argue that some of these artists kanye west eminem jay z drake tupac and notorious big have produced work in multiple styles a more accurate term for this might be artist matching in section the central automated component of the evaluation is low tf idf with existing verses and similar rhyme density given the limitations of rhyme density how well does this work even with the manual intervention described in section this description should include how many judges were used in this study in how many cases did the judges already know the verse they were judging in this case the test will not assess how easy it is to match style but rather the judges recall and rap knowledge,2.0
501.json,strengths authors generate a dataset of rephrased captions and are planning to make this dataset publicly available the way authors approached dmc task has an advantage over vqa or caption generation in terms of metrics it is easier and more straightforward to evaluate problem of choosing the best caption authors use accuracy metric while for instance caption generation requires metrics like blue or meteor which are limited in handling semantic similarity authors propose an interesting approach to rephrasing e g selecting decoys they draw decoys form image caption dataset e g decoys for a single image come from captions for other images these decoys however are similar to each other both in terms of surface bleu score and semantics pv similarity authors use lambda factor to decide on the balance between these two components of the similarity score i think it would be interesting to employ these for paraphrasing authors support their motivation for the task with evaluation results they show that a system trained with the focus on differentiating between similar captions performs better than a system that is trained to generate captions only these are however showing that system that is tuned for a particular task performs better on this task weaknesses it is not clear why image caption task is not suitable for comprehension task and why author s system is better for this in order to argue that system can comprehend image and sentence semantics better one should apply learned representation e g embeddings e g apply representations learned by different systems on the same task for comparison my main worry about the paper is that essentially authors converge to using existing caption generation techniques e g bahdanau et al chen et al they way formula is presented is a bit confusing from formula it seems that both decoy and true captions are employed for both loss terms however as it makes sense authors mention that they do not use decoy for the second term that would hurt mode performance as model would learn to generate decoys as well the way it is written in the text is ambiguous so i would make it more clear either in the formula itself or in the text otherwise it makes sense for the model to learn to generate only true captions while learning to distinguish between true caption and a decoy general discussion authors formulate a task of dual machine comprehension they aim to accomplish the task by challenging computer system to solve a problem of choosing between two very similar captions for a given image authors argue that a system that is able to solve this problem has to understand the image and captions beyond just keywords but also capture semantics of captions and their alignment with image semantics i think paper need to make more focus on why chosen approach is better than just caption generation and why in their opinion caption generation is less challenging for learning image and text representation and their alignment for formula i wonder if in the future it is possible to make model to learn not to generate decoys by adjusting second loss term to include decoys but with a negative sign did authors try something similar,3.0
33.json,strengths this paper proposes a nice way to combine the neural model lstm with linguistic knowledge sentiment lexicon negation and intensity the method is simple yet effective it achieves the state of the art performance on movie review dataset and is competitive against the best models on sst dataset weaknesses similar idea has also been used in teng et al though this work is more elegant in the framework design and mathematical representation the experimental comparison with teng et al is not as convincing as the comparisons with the rest methods the authors only reported the re implementation results on the sentence level experiment of sst and did not report their own phrase level results some details are not well explained see discussions below general discussion the reviewer has the following questions suggestions about this work since the sst dataset has phrase level annotations it is better to show the statistics of the times that negation or intensity words actually take effect for example how many times the word nothing appears and how many times it changes the polarity of the context in section the bi lstm is used for the regularizers is bi lstm used to predict the sentiment label the authors claimed that we only use the sentence level annotation since one of our goals is to avoid expensive phrase level annotation however the reviewer still suggest to add the results please report them in the rebuttal phase if possible sc is a parameter to be optimized but could also be set fixed with prior knowledge the reviewer did not find the specific definition of sc in the experiment section is it learned or set fixed what is the learned or fixed value in section and it is suggested to conduct an additional experiment with part of the sst dataset where only phrases with negation intensity words are included report the results on this sub dataset with and without the corresponding regularizer can be more convincing,4.0
331.json,strengths detailed guidelines and explicit illustrations weaknesses the document independent crowdsourcing annotation is unreliable general discussion this work creates a new benchmark corpus for concept map based mds it is well organized and written clearly the supplement materials are sufficient i have two questions here is it necessary to treat concept map extraction as a separate task on the one hand many generic summarization systems build a similar knowledge graph and then generate summaries accordingly on the other hand with the increase of the node number the concept map becomes growing hard to distinguish thus the general summaries should be more readable how can you determine the importance of a concept independent of the documents the definition of summarization is to reserve the main concepts of documents therefore the importance of a concept highly depends on the documents for example in the given topic of coal mining accidents assume there are two concepts a an instance of coal mining accidents and b a cause of coal mining accidents then if the document describes a series of coal mining accidents a is more important than b in comparison if the document explores why coal mining accidents happen b is more significant than a therefore just given the topic and two concepts a b it is impossible to judge their relative importance i appreciate the great effort spent by authors to build this dataset however this dataset is more like a knowledge graph based on common sense rather than summary,3.0
433.json,the paper describes a deep learning based model for parsing the creole singaporean english to universal dependencies they implement a parser based on the model by dozat and manning and add neural stacking chen et al to it they train an english model and then use some of the hidden representations of the english model as input to their singlish parser this allows them to make use of the much larger english training set along with a small singlish treebank which they annotate they show that their approach las works better than just using an english parser las or training a parser on their small singlish data set las they also analyze for which common constructions their approach improves parsing quality they also describe and evaluate a stacked pos model based on chen et al they discuss how common constructions should be analyzed in the ud framework and they provide an annotated treebank of sentences of them were annotated by two people and their inter annotator agreement was uas and las strengths they obtain good results and their experimental setup appears to be solid they perform many careful analyses and explore the influence on many parameters of their model they provide a small singlish treebank annotated according to the universal dependencies v guidelines they propose very sound guidelines on how to analyze common singlish constructions in ud their method is linguistically informed and they nicely exploit similarity between standard english and the creole singaporean english the paper presents methods for a low resource language they are not just applying an existing english method to another language but instead present a method that can be potentially used for other closely related language pairs they use a well motivated method for selecting the sentences to include in their treebank the paper is very well written and easy to read weaknesses the annotation quality seems to be rather poor they performed double annotation of sentences and their inter annotator agreement is just in terms of las this makes it hard to assess how reliable the estimate of the las of their model is and the las of their model is in fact slightly higher than the inter annotator agreement update their rebuttal convincingly argued that the second annotator who just annotated the examples to compute the iaa did not follow the annotation guidelines for several common constructions once the second annotator fixed these issues the iaa was reasonable so i no longer consider this a real issue general discussion i am a bit concerned about the apparently rather poor annotation quality of the data and how this might influence the results but overall i liked the paper a lot and i think this would be a good contribution to the conference questions for the authors who annotated the sentences you just mention that sentences were annotated by one of the authors to compute inter annotator agreement but you do not mention who annotated all the sentences why was the inter annotator agreement so low in which cases was there disagreement did you subsequently discuss and fix the sentences for which there was disagreement table a there seem to be a lot of discourse relations almost as many as dobj relations in your treebank is this just an artifact of the colloquial language or did you use discourse for things that are not considered discourse in other languages in ud table a are all of these discourse particles or discourse imported vocab if the latter perhaps put them in separate tables and glosses would be helpful low level comments it would have been interesting if you had compared your approach to the one by martinez et al https arxiv org pdf pdf perhaps you should mention this paper in the reference section you use the word grammar in a slightly strange way i think replacing grammar with syntactic constructions would make it clearer what you try to convey e g line line i do not think this can be regarded as a variant of it extraposition but i agree with the analysis in figure so perhaps just get rid of this sentence line i think the model by dozat and manning is no longer state of the art so perhaps just replace it with very high performing model or something like that it would be helpful if you provided glosses in figure,4.0
18.json,strengths a well motivated approach with a clear description and solid results weaknesses nothing substantial other than the comments below general discussion the paper describes a new method called attention over attention for reading comprehension first layers of the network compute a vector for each query word and document word resulting in a q xk matrix for the query and a d xk for the document since the answer is a document word an attention mechanism is used for assigning weights to each word depending on their interaction with query words in this work the authors deepen a traditional attention mechanism by computing a weight for each query word through a separate attention and then using that to weight the main attention over document words evaluation is properly conducted on benchmark datasets and various insights are presented through an analysis of the results as well as a comparison to prior work i think this is a solid piece of work on an important problem and the method is well motivated and clearly described so that researchers can easily reproduce results and apply the same techniques to other similar tasks other remarks p equation i am assuming i is iterating over training set and p w is referring to p w d q in the previous equation please clarify to avoid confusion i am wondering whether you explored discussed initializing word embeddings with existing vectors such as google news or glove is there a reason to believe the general purpose word semantics would not be useful in this task p l it is not clear what the authors are referring to when they say letting the model explicitly learn weights between individual attentions is this referring to their own architecture more specifically the gru output indirectly affecting how much attention will be applied to each query and document word clarifying that would be useful also i think the improvement on validation is not rather p table why do you think the weight for local lm is relatively higher for the cn task while the benefit of adding it is less since you included the table i think it will be nice to provide some insights to the reader i would have liked to see the software released as part of this submission typo p l right column is not that effective than expected is not as effective as expected typo p l right column appear much frequent appears more frequently typo p l left column the model is hard to it is hard for the model to hard to made hard to make,5.0
619.json,this paper presents a corpus of annotated essay revisions it includes two examples of application for the corpus student revision behavior analysis and automatic revision identification the latter is essentially a text classification task using an svm classifier and a variety of features the authors state that the corpus will be freely available for research purposes the paper is well written and clear a detailed annotation scheme was used by two annotators to annotate the corpus which added value to it i believe the resource might be interesting to researcher working on writing process research and related topics i also liked that you provided two very clear usage scenarios for the corpus i have two major criticisms the first could be easily corrected in case the paper is accepted but the second requires more work there are no statistics about the corpus in this paper this is absolutely paramount when you describe a corpus there are some information that should be there i am talking about number of documents i assume the corpus has documents essays x drafts is that correct number of tokens around words each essay number of sentences etc i assume we are talking about unique essays x words so about words in total is that correct if we take the drafts we end up with about words but probably with substantial overlap between drafts a table with this information should be included in the paper if the aforementioned figures are correct we are talking about a very small corpus i understand the difficulty of producing hand annotated data and i think this is one of the strengths of your work but i am not sure about how helpful this resource is for the nlp community as a whole perhaps such a resource would be better presented in a specialised workshop such as bea or a specialised conference on language resources like lrec instead of a general nlp conference like acl you mentioned in the last paragraph that you would like to augment the corpus with more annotation are you also willing to include more essays comments minor as you have essays by native and non native speakers one further potential application of this corpus is native language identification nli p where the unigram feature was used as the baseline word unigram be more specific p and the svm classifier was used as the classifier redundant,2.0
588.json,strengths the paper empirically verifies that using external knowledge is a benefit weaknesses real world nlp applications should utilize external knowledge for making better predictions the authors propose rare entity prediction task to demonstrate this is the case however the motivation of the task is not fully justified why is this task important how would real world nlp applications benefit from this task the paper lacks a convincing argument for proposing a new task for current reading comprehension task the evidence for a correct answer can be found in a given text thus we are interested in learning a model of the world i e causality for example or a basic reasoning model comparing to reading comprehension rare entity prediction is rather unrealistic as humans are terrible with remembering name the authors mentioned that the task is difficult due to the large number of rare entities however challenging tasks with the same or even more difficult level exist such as predicting correct morphological form of a word in morphologically rich languages such tasks have obvious applications in machine translation for example general discussion it would be helpful if the authors characterize the dataset in more details from figure and table it seems to me that overlapping entities is an important feature there is noway i can predict the blank in figure if i do not see the word london in peter ackoyd description that being said before brutalizing neural networks it is essential to understand the characteristic of the data and the cognitive process that searches for the right answer given the lack of characteristic of the dataset i find that the baselines are inappropriate first of all the contenc is a natural choice at the first sigh however as the authors mentioned that candidate entities are rare the embeddings of those entities are unrealizable as a consequence it is expected that contenc does not work well would it is fairer if the embeddings are initialized from pre trained vectors on massive dataset one would expect some sort of similarity between larnaca and cyprus in the embedding space and contenc would make a correct prediction in table what would be the performance of tf idf cos and avgemb cos if only entities are used to compute those vectors from modeling perspective i appreciate that the authors chose a sigmoid predictor that output a numerical score between this would help avoiding normalization over the list of candidates which are rare and is difficult to learn reliable weights for those however a sidestep technique does exist such as pointer network a representation hi for ci blank included can be computed by an lstm or bilstm then pointer network would give a probabilistic interpretation p ek ci propto exp dot d ek hi in my opinion pointer network would be an appropriate baseline another related note does the unbalanced set of negative positive labels affect the training during training the models make positive prediction while number of negative predictions is at least times higher while i find the task of rare entity prediction is unrealistic having the dataset it would be more interesting to learn about the reasoning process that leads to the right answer such as which set of words the model attends to when making prediction,2.0
96.json,summary the paper introduces a new dataset for a sarcasm interpretation task and a system called sarcasm sign based on machine translation framework moses the new dataset was collected from sarcastic tweets with hashtag sarcasm and interpretations for each from humans the sarcasm sign is built based on moses by replacing sentimental words by their corresponding clusters on the source side sarcasm and then de cluster their translations on the target side non sarcasm sarcasm sign performs on par with moses on the mt evaluation metrics but outperforms moses in terms of fluency and adequacy strengths the paper is well written the dataset is collected in a proper manner the experiments are carefully done and the analysis is sound weaknesses lack statistics of the datsets e g average length vocabulary size the baseline moses is not proper because of the small size of the dataset the assumption sarcastic tweets often differ from their non sarcastic interpretations in as little as one sentiment word is not supported by the data general discussion this discussion gives more details about the weaknesses of the paper half of the paper is about the new dataset for sarcasm interpretation however the paper does not show important information about the dataset such as average length vocabulary size more importantly the paper does not show any statistical evidence to support their method of focusing on sentimental words because the dataset is small only tweets i guess that many words are rare therefore moses alone is not a proper baseline a proper baseline should be a mt system that can handle rare words very well in fact using clustering and declustering as in sarcasm sign is a way to handle rare words sarcasm sign is built based on the assumption that sarcastic tweets often differ from their non sarcastic interpretations in as little as one sentiment word table however strongly disagrees with this assumption the human interpretations are often different from the tweets at not only sentimental words i thus strongly suggest the authors to give statistical evidence from the dataset that supports their assumption otherwise the whole idea of sarcasm sign is just a hack i have read the authors response i do not change my decision because of the following reasons the authors wrote that the fiverr workers might not take this strategy to me it is not the spirit of corpus based nlp a model must be built to fit given data not that the data must follow some assumption that the model is built on the authors wrote that the bleu scores of moses and sign are above which is generally considered decent in the mt literature to me the number does not show anything at all because the sentences in the dataset are very short and that if we look at table changed of moses is only meaning that even more than half of the time translation is simply copying the blue score is more than while higher scores might be achieved with mt systems that explicitly address rare words these systems do not focus on sentiment words it true but i was wondering whether sentiment words are rare in the corpus if they are those mt systems should obviously handle them in addition to other rare words,3.0
818.json,thank you for the author response it addresses some my concerns though much of it are promises we will necessarily so given space constraints but then this is precisely the problem i would like to see the revision to the paper to be able to check that the drawbacks have been fixed the changes needed are quite substantial and the new experimental results that they promise to include will not have undergone review if the paper is accepted at this stage i am still not sure that we can simply leave it to the authors to make the necessary changes without a further reviewing round i upgrade my score to a to express this ambivalence i do like the research in the paper but it extremely messy in its presentation strengths the topic of the paper is very creative and the purpose of the research really worthwhile the paper aims at extracting common knowledge from text overcoming the well known problem of reporting bias the fact that people will not state the obvious such as the fact that a person is usually bigger than a ball by doing joint inference on information that is possible to extract from text weaknesses many aspects of the approach need to be clarified see detailed comments below what worries me the most is that i did not understand how the approach makes knowledge about objects interact with knowledge about verbs such that it allows us to overcome reporting bias the paper gets very quickly into highly technical details without clearly explaining the overall approach and why it is a good idea the experiments and the discussion need to be finished in particular there is no discussion of the results of one of the two tasks tackled lower half of table and there is one obvious experiment missing variant b of the authors model gives much better results on the first task than variant a but for the second task only variant a is tested and indeed it does not improve over the baseline general discussion the paper needs quite a bit of work before it is ready for publication detailed comments five dimensions not six figure caption implies physical relations how do you know which physical relations it implies figure and what you are trying to do it looks to me is essentially to extract lexical entailments as defined in formal semantics see e g dowty for verbs could you please explicit link to that literature dowty david thematic proto roles and argument selection language around here you should explain the key insight of your approach why and how does doing joint inference over these two pieces of information help overcome reporting bias values value please also consider work on multimodal distributional semantics here and or in the related work section the following two papers are particularly related to your goals bruni elia et al distributional semantics in technicolor proceedings of the th annual meeting of the association for computational linguistics long papers volume association for computational linguistics silberer carina vittorio ferrari and mirella lapata models of semantic representation with visual attributes acl please clarify that your contribution is the specific task and approach commonsense knowledge extraction from language is long standing task it is not clear what grounded means at this point section why these dimensions and how did you choose them explain terms pre condition and post condition and how they are relevant here an example of the full distribution for an item obtained by the model or crowd sourced or ideal would help figure i do not really see the x is slower than y part it seems to me like this is related to the distinction in formal semantics between stage level vs individual level predicates when a person throws a ball the ball is faster than the person stage level but it not true in general that balls are faster than people individual level i guess this is related to the pre condition vs post condition issue please spell out the type of information that you want to extract above definition determiner missing section action verbs which classes do you pick and you do you choose them are the verbs that you pick all explicitly tagged as action verbs by levin ff what are action frames how do you pick them how do you know whether the frame is under or over generating table are the partitions made by frame by verb or how that is do you reuse verbs or frames across partitions also proportions are given for cases and agreement whereas counts are only given for one case which with pmi something missing threshold did you do this partitions randomly rate the general relationship knowledge dimension we choose how do you choose which dimensions you will annotate for each frame section what is a factor graph please give enough background on factor graphs for a cl audience to be able to follow your approach what are substrates and what is the role of factors how is the factor graph different from a standard graph more generally at the beginning of section you should give a higher level description of how your model works and why it is a good idea both classes of knowledge antecedent missing object first type so far you have been only talking about object pairs and verbs and suddenly selectional preference factors pop in they seem to be a crucial part of your model introduce earlier in any case i did not understand their role also where do you get verb level similarities from figure i find the figure totally unintelligible maybe if the text was clearer it would be interpretable but maybe you can think whether you can find a way to convey your model a bit more intuitively also make sure that it is readable in black and white as per acl submission instructions define term message and its role in the factor graph why do you need a soft instead of a hard ff you need to provide more details about the emb maxent classifier how did you train it what was the input data how was it encoded and also explain why it is an appropriate baseline more skimp seed knowledge here and in problem with table reference should be table ff i like the thought but i am not sure the example is the right one in what sense is the entity larger than the revolution also larger is not the same as stronger as mentioned above you should discuss the results for the task of inferring knowledge on objects and also include results for model b incidentally it would be better if you used the same terminology for the model in tables and latent in verbs why do not you mention objects here both tasks antecedent missing the references should be checked for format e g grice sorower et al for capitalization the verbnet reference for bibliographic details,3.0
726.json,the paper presents a neural model for predicting sql queries directly from natural language utterances without going through an intermediate formalism in addition an interactive online feedback loop is proposed and tested on a small scale strengths the paper is very clearly written properly positioned and i enjoyed reading it the proposed model is tested and shown to perform well on different domains academic geographic queries and flight booking the online feedback loop is interesting and seems promising despite of the small scale of the experiment a new semantic corpus is published as part of this work and additionally two existing corpora are converted to sql format which i believe would be beneficial for future work in this area weaknesses clarifications section entity anonymization i am not sure i understand the choice of the length of span for querying the search engine why and how is it progressively reduced line section benchmark experiments if i understand correctly the feedback loop algorithm is not used for these experiments if this is indeed the case i am not sure when does data augmentation occur is all the annotated training data augmented with paraphrases when is the initial data from templates added is it also added to the gold training set if so i think it not surprising that it does not help much as the gold queries may be more diverse in any case i think this should be stated more clearly in addition i think it interesting to see what is the performance of the vanilla model without any augmentation i think that this is not reported in the paper tables and i find the evaluation metric used here somewhat unclear does the accuracy measure the correctness of the execution of the query i e the retrieved answer as the text seem to indicate line mentions executing the query alternatively are the queries themselves compared as seems to be the case for dong and lapata in table if this is done differently for different systems i e dong and lapata how are these numbers comparable in addition the text mentions the sql model has slightly lower accuracy than the best non sql results line yet in table the difference is almost points in accuracy what is the observation based upon was some significance test performed if not i think the results are still impressive for direct to sql parsing but that the wording should be changed as the difference in performance does seem significant line regarding the data recombination technique used in jia and liang since this technique is applicable in this scenario why not try it as well currently it an open question whether this will actually improve performance is this left as future work or is there something prohibiting the use of this technique section three stage online experiment several details are missing unclear what was the technical background of the recruited users who were the crowd workers how were they recruited and trained the text says we recruited new users and asked them to issue at least utterances does this mean queries each e g overall or in total for each what was the size of the initial synthesized training set report statistics of the queries some measure of their lexical variability length complexity of the generated sql this seems especially important for the first phase which is doing surprisingly well furthermore since scholar uses sql and nl it would have been nice if it were attached to this submission to allow its review during this period section scholar dataset the dataset seems pretty small in modern standards utterances in total while one of the main advantages of this process is its scalability what hindered the creation of a much larger dataset comparing performance is it possible to run another baseline on this newly created dataset to compare against the reported accuracy obtained in this paper line evaluation of interactive learning experiments section i find the experiments to be somewhat hard to replicate as they involve manual queries of specific annotators for example who to say if the annotators in the last phase just asked simpler questions i realise that this is always problematic for online learning scenarios but i think that an effort should be made towards an objective comparison for starters the statistics of the queries as i mentioned earlier is a readily available means to assess whether this happens second maybe there can be some objective held out test set this is problematic as the model relies on the seen queries but scaling up the experiment as i suggested above might mitigate this risk third is it possible to assess a different baseline using this online technique i am not sure whether this is applicable given that previous methods were not devised as online learning methods minor comments line requires require footnote seems too long to me consider moving some of its content to the body of the text algorithm i am not sure what new utterances refers to i guess it new queries from users i think that an accompanying caption to the algorithm would make the reading easier line is is it is line mentions an anonymized utterance this confused me at the first reading and if i understand correctly it refers to the anonymization described later in i think it would be better to forward reference this general discussion overall i like the paper and given answers to the questions i raised above would like to see it appear in the conference author response i appreciate the detailed response made by the authors please include these details in a final version of the paper,4.0
726.json,this paper proposes a simple attention based rnn model for generating sql queries from natural language without any intermediate representation towards this end they employ a data augmentation approach where more data is iteratively collected from crowd annotation based on user feedback on how well the sql queries produced by the model do results on both the benchmark and interactive datasets show that data augmentation is a promising approach strengths no intermediate representations were used release of a potentially valuable dataset on google scholar weaknesses claims of being comparable to state of the art when the results on geoquery and atis do not support it general discussion this is a sound work of research and could have future potential in the way semantic parsing for downstream applications is done i was a little disappointed with the claims of near state of the art accuracies on atis and geoquery which doesn t seem to be the case points difference from liang et al and i do not necessarily think that getting sota numbers should be the focus of the paper it has its own significant contribution i would like to see this paper at acl provided the authors tone down their claims in addition i have some questions for the authors what do the authors mean by minimal intervention does it mean minimal human intervention because that does not seem to be the case does it mean no intermediate representation if so the latter term should be used being less ambiguous table what is the breakdown of the score by correctness and incompleteness what of incompleteness do these queries exhibit what is expertise required from crowd workers who produce the correct sql queries it would be helpful to see some analysis of the of user questions which could not be generated figure is a little confusing i could not follow the sharp dips in performance without paraphrasing around the th th stages table needs a little more clarification what splits are used for obtaining the atis numbers i thank the authors for their response,4.0
97.json,this paper describes a system to assist written test scoring strengths the paper represents an application of an interesting nlp problem recognizing textual entailment to an important task written test scoring weaknesses there is not anything novel in the paper it consist of an application of an existing technology to a known problem the approach described in the paper is not autonomous it still needs a human to do the actual scoring the paper lacks any quantitative or qualitative evaluation of how useful such system is that is is it making the job of the scorer easier is the scorer more effective as compared to not having automatic score the system contains multiple components and it is unclear how the quality of each one of them contributes to the overall experience the paper needs more work with the writing language and style is rough in several places the paper also contains several detailed examples which do not necessarily add a lot of value to the discussion for the evaluation of classification what is the baseline of predicting the most frequent class general discussion i find this paper not very inspiring i do not see the message in the paper apart from announcing having build such a system,1.0
395.json,tmp strength the paper propose drl sense model that shows a marginal improvement on scws dataset and a significant improvement on esl and rd datasets weakness the technical aspects of the paper raise several concerns could the authors clarify two drawbacks in the first drawback states that optimizing equation leads to the underestimation of the probability of sense as i understand eq is the expected reward of sense selection z ik and z jl are independent actions and there are only two actions to optimize this should be relatively easy in nlp setting optimizing the expected rewards over a sequence of actions for episodic task has been proven doable sequence level training with recurrent neural networks ranzato even in a more challenging setting of machine translation where the number of actions and the average sequence length words the drl sense model has maximum actions and it does not have sequential nature of rl this makes it hard to accept the claim about the first drawback the second drawback accompanied with the detail math in appendix a states that the update formula is to minimize the likelihood due to the log likelihood is negative note that most out of box optimizers adam sgd adadelta minimize a function f however a common practice when we want to maximize f we just minimize f since the reward defined in the paper is negative any standard optimizer can be use on the expected of the negative reward which is always greater than this is often done in many modeling tasks such as language model we minimize negative log likelihood instead of maximizing the likelihood the authors also claim that when the log likelihood reaches it also indicates that the likelihood reaches infinity and computational flow on u and v line why likelihood infinity should it be likelihood could the authors also explain how drl sense is based on q learning the horizon in the model is length of there is no transition between state actions and there is not markov property as i see it k and l are draw independently i am having trouble to see the relation between q learning and drl sense in mnih et al the reward is given from the environment whereas in the paper the rewards is computed by the model what s the reward in drl sense is it for all the state action pairs or the cross entropy in eq cross entropy is defined as h p q sum x q x log q x which variable do the authors sum over in i see that q ct z i k is a scalar computed in eq while co z ik z jl is a distribution over total number of senses eq these two categorial variables do not have the same dimension how is cross entropy h in eq is computed then could the authors justify the dropout exploration why not epsilon greedy exploration dropout is often used for model regularization preventing overfitting how do the authors know the gain in using dropout is because of exploration but regularization the authors states that q value is a probabilistic estimation line can you elaborate what is the set of variables the distribution is defined when you sum over that set of variable do you get i interpret that q is a distribution over senses per word however definition of q in eq does not contain a normalizing constant so i do not see q is a valid distribution this also related to the value in section as a threshold for exploration why is chosen here where q is just an arbitrary number between and the constrain sumz q z does not held does the authors allow the creation of a new sense in the very beginning or after a few training epochs i would image that at the beginning of training the model is unstable and creating new senses might introduce noises to the model could the authors comment on that general discussion what s the justification for omitting negative samples in line negative sampling has been use successfully in wordvec due to the nature of the task learning representation negative sampling however does not work well when the main interest is modeling a distribution p over senses words noise contrastive estimation is often preferred when it comes to modeling a distribution the drl sense uses collocation likelihood to compute the reward i wonder how the approximation presented in the paper affects the learning of the embeddings would the authors consider task specific evaluation for sense embeddings as suggested in recent research evaluation methods for unsupervised word embeddings tobias schnabel igor labutov david mimno and thorsten joachims problems with evaluation of word embeddings using word similarity tasks manaal faruqui yulia tsvetkov pushpendre rastogi chris dyer i have read the response,2.0
19.json,strengths this paper introduced a novel method to improve zero pronoun resolution performance the main contributions of this papers are proposed a simple method to automatically generate a large training set for zero pronoun resolution task adapted a two step learning process to transfer knowledge from large data set to the specific domain data differentiate unknown words using different tags in general the paper is well written experiments are thoroughly designed weaknesses but i have a few questions regarding finding the antecedent of a zero pronoun how will an antecedent be identified when the prediction is a pronoun the authors proposed a method by matching the head of noun phrases it s not clear how to handle the situation when the head word is not a pronoun what if the prediction is a noun that could not be found in the previous contents the system achieves great results on standard data set i m curious is it possible to evaluate the system in two steps the first step is to evaluate the performance of the model prediction i e to recover the dropped zero pronoun into a word the second step is to evaluate how well the systems works on finding an antecedent i m also curious why the authors decided to use attention based neural network a few sentences to provide the reasons would be helpful for other researchers a minor comment in figure should it be s s instead of d d general discussion overall it is a great paper with innovative ideas and solid experiment setup,4.0
706.json,strengths the ideas and the task addressed in this paper are beautiful and original combining indirect supervision accepting the resulting parse with direct supervision giving a definition makes it a particularly powerful way of interactively building a natural language interface to a programming language the proposed has a wide range of potential applications weaknesses the paper has several typos and language errors and some text seems to be missing from the end of section it could benefit from careful proofreading by a native english speaker general discussion the paper presents a method for collaborative naturalization of a core programming language by a community of users through incremental expansion of the syntax of the language this expansion is performed interactively whereby a user just types a command in the naturalized language and then either selects through a list of candidate parses or provides a definition also in the natural language the users give intuitive definitions using literals instead of variables e g select orange which makes this method applicable to non programmers a grammar is induced incrementally which is used to provide the candidate parses i have read the authors response,4.0
173.json,strengths introduces a new document clustering approach and compares it to several established methods showing that it improves results in most cases the analysis is very detailed and thorough quite dense in many places and requires careful reading the presentation is organized and clear and i am impressed by the range of comparisons and influential factors that were considered argument is convincing and the work should influence future approaches weaknesses the paper does not provide any information on the availability of the software described general discussion needs some minor editing for english and typos here are just a few line regardless the size regardless of the size line resources because resources because line consist ing mk consisting of mk line versionand version and,4.0
768.json,this paper addresses the task of lexical entailment detection in context e g is chess a kind of game given a sentence containing each of the words relevant for qa the major contributions are a new dataset derived from wordnet using synset exemplar sentences and a context relevance mask for a word vector accomplished by elementwise multiplication with feature vectors derived from the context sentence fed to a logistic regression classifier the masked word vectors just beat state of the art on entailment prediction on a ppdb derived dataset from previous literature combined with other existing features they beat state of the art by a few points they also beats the baseline on the new wn derived dataset although the best scoring method on that dataset does not use the masked representations the paper also introduces some simple word similarity features cosine euclidean distance which accompany other cross context similarity features from previous literature all of the similarity features together improve the classification results by a large amount but the features in the present paper are a relatively small contribution the task is interesting and the work seems to be correct as far as it goes but incremental the method of producing the mask vectors is taken from existing literature on encoding variable length sequences into min max mean vectors but i do not think they have been used as masks before so this is novel however excluding the ppdb features it looks like the best result does not use the representation introduced in the paper a few more specific points in the creation of the new context wn dataset are there a lot of false negatives resulting from similar synsets in the permuted examples if you take word w with synsets i and j is it guaranteed that the exemplar context for a hypernym synset of j is a bad entailment context for i what if i and j are semantically close why does the masked representation hurt classification with the context agnostic word vectors rows in table when row does so well would not the classifier learn to ignore the context agnostic features the paper should make clearer which similarity measures are new and which are from previous literature it currently says that previous lit used the most salient similarity features but that not informative to the reader the paper should be clearer about the contribution of the masked vectors vs the similarity features it seems like similarity is doing most of the work i do not understand the intuition behind the macro f measure or how it relates to how sensitive are our models to changes in context what changes how do we expect macro f to compare with f the cross language task is not well motivated missing a relevant citation learning to distinguish hypernyms and co hyponyms julie weeds daoud clarke jeremy reffin david weir and bill keller coling i have read the author response as noted in the original reviews a quick examination of the tables shows that the similarity features make the largest contribution to the improvement in f score on the two datasets aside from ppdb features the author response makes the point that similarities include contextualized representations however the similarity features are a mixed bag including both contextualized and non contextualized representations this would need to be teased out more as acknowledged in the response neither table nor gives results using only the masked representations without the similarity features this makes the contribution of the masked representations difficult to isolate,2.0
355.json,strengths this paper presents a sophisticated application of grid type recurrent neural nets to the task of determining predicate argument structures pas in japanese the approach does not use any explicit syntactic structure and outperforms the current soa systems that do include syntactic structure the authors give a clear and detailed description of the implementation and of the results in particular they pay close attention to the performance on dropped arguments zero pronouns which are prevalent in japanese and especially challenging with respect to pas their multi sequence model which takes all of the predicates in the sentence into account achieves the best performance for these examples the paper is detailed and clearly written weaknesses i really only have minor comments there are some typos listed below the correction of which would improve english fluency i think it would be worth illustrating the point about the pred including context around the predicate with the example from fig where the accusative marker is included with the verb in the pred string i did not understand the use of boldface in table p general discussion typos p error propagation does not need a the nor does multi predicate interactions p as an solution as a solution single sequence model a single sequence model multi sequence model a multi sequence model p example in fig she ate a bread she ate bread p assumes the independence assumed independence the multi predicate interactions multi predicate interactions the multi sequence model a multi sequence model p the residual connections residual connections the multi predicate interactions multi predicate interactions twice p naist text corpus the naist text corpus the state of the art result state of the art results i have read the author response and am satisfied with it,4.0
355.json,this paper proposes new prediction models for japanese srl task by adopting the english state of the art model of zhou and xu the authors also extend the model by applying the framework of grid rnns in order to handle the interactions between the arguments of multiple predicates the evaluation is performed on the well known benchmark dataset in japanese srl and obtained a significantly better performance than the current state of the art system strengths the paper is well structured and well motivated the proposed model obtains an improvement in accuracy compared with the current state of the art system also the model using grid rnns achieves a slightly better performance than that of proposed single sequential model mainly due to the improvement on the detection of zero arguments that is the focus of this paper weakness to the best of my understanding the main contribution of this paper is an extension of the single sequential model to the multi sequential model the impact of predicate interactions is a bit smaller than that of ouchi et al there is a previous work shibata et al that extends the ouchi et al model with neural network modeling i am curious about the comparison between them,4.0
435.json,this paper develops an lstm based model for classifying connective uses for whether they indicate that a causal relation was intended the guiding idea is that the expression of causal relations is extremely diverse and thus not amenable to syntactic treatment and that the more abstract representations delivered by neural models are therefore more suitable as the basis for making these decisions the experiments are on the altlex corpus developed by hidley and mckeown the results offer modest but consistent support for the general idea and they provide some initial insights into how best to translate this idea into a model the paper distribution includes the tensorflow based models used for the experiments some critical comments and questions the introduction is unusual in that it is more like a literature review than a full overview of what the paper contains this leads to some redundancy with the related work section that follows it i guess i am open to a non standard sort of intro but this one really does not work despite reviewing a lot of ideas it does not take a stand on what causation is or how it is expressed but rather only makes a negative point it not reducible to syntax we are not really told what the positive contribution will be except for the very general final paragraph of the section extending the above i found it disappointing that the paper is not really clear about the theory of causation being assumed the authors seem to default to a counterfactual view that is broadly like that of david lewis where causation is a modal sufficiency claim with some other counterfactual conditions added to it see line and following that arrow needs to be a very special kind of implication for this to work at all and there are well known problems with lewis theory see http bcopley com wp content uploads copleywolff pdf there are comments elsewhere in the paper that the authors do not endorse the counterfactual view but then what is the theory being assumed it can not just be the temporal constraint mentioned on page i do not understand the comments regarding the example on line the authors seem to be saying that they regard the sentence as false if it true then there should be some causal link between the argument and the breakage there are remaining issues about how to divide events into sub events and these impact causal theories but those are not being discussed here leaving me confused the caption for figure is misleading since the diagram is supposed to depict only the pairlstm variant of the model my bigger complaint is that this diagram is needlessly imprecise i suppose it okay to leave parts of the standard model definition out of the prose but then these diagrams should have a clear and consistent semantics what are all the empty circles between input and the lstm boxes the prose seems to say that the model has a look up layer a glove layer and then what how many layers of representation are there the diagram is precise about the pooling tanh layers pre softmax but not about this i am also not clear on what the lstm boxes represent it seems like it just the leftmost final representation that is directly connected to the layers above i suggest depicting that connection clearly i do not understand the sentence beginning on line the models under discussion do not intrinsically require any padding i am guessing this is a requirement of tensorflow and or efficient training that fine if that correct please say that i do not understand the final clause though how is this issue even related to the question of what is the most convenient way to encode the causal meaning i do not see how convenience is an issue or how this relates directly to causal meaning the authors find that having two independent lstms statedlstm is somewhat better than one where the first feeds into the second this issue is reminiscent of discussions in the literature on natural language entailment where the question is whether to represent premise and hypothesis independently or have the first feed into the second i regard this as an open question for entailment and i bet it needs further investigation for causal relations too so i can not really endorse the sentence beginning on line this behaviour means that our assumption about the relation between the meanings of the two input events does not hold so it is better to encode each argument independently and then to measure the relation between the arguments by using dense layers this is very surprising since we are talking about subparts of a sentence that might share a lot of information it hard to make sense of the hyperparameters that led to the best performance across tasks compare line with line for example should we interpret this or just attribute it to the unpredictability of how these models interact with data section concludes by saying of the connective which then that the system can correctly disambiguate its causal meaning whereas that of hidey and mckeown does not that might be correct but one example does not suffice to show it to substantiate this point i suggest making up a wide range of examples that manifest the ambiguity and seeing how often the system delivers the right verdict this will help address the question of whether it got lucky with the example from table,2.0
142.json,the paper explores the use of probabilistic models gaussian processes to regress on the target variable of post editing time rates for quality estimation of mt output the paper is well structured with a clear introduction that highlights the problem of qe point estimates in real world applications i especially liked the description of the different asymmetric risk scenarios and how they entail different estimators for readers familiar with gps the paper spends quite some space to reflect them but i think it is worth the effort to introduce these concepts to the reader the gp approach and the choices for kernels and using warping are explained very clearly and are easy to follow in general the research questions that are to be answered by this paper are interesting and well phrased however i do have some questions suggestions about the results and discussion sections for intrinsic uncertainty evaluation why were post editing rates chosen over prediction h ter ter is a common value to predict in qe research and it would have been nice to justify the choice made in the paper section i do not understand the first paragraph at all what exactly is the trend you see for fr en en de that you do not see for en es nll and nlpd would rastically decrease with warped gps for all three datasets the paper indeed states that it does not want to advance state of the art given that they use only the standard baseline features but it would have been nice to show another point estimate model from existing work in the result tables to get a sense of the overall quality of the models related to this it is hard to interpret nll and nlpd values so one is always tempted to look at mae in the tables to get a sense of how different the predictions are since the whole point of the paper is to say that this is not the right thing to do it would be great provide some notion of what is a drastic reduce in nll nlpd worth a qualitative analysis with actual examples section is very nicely written and explains results very intuitively overall i like the paper since it points out the problematic use of point estimates in qe a difficult task in general where additional information such as confidence arguably are very important the submission does not advance state of the art and does not provide a lot of novelty in terms of modeling since gps have been used before but its research questions and goals are clearly stated and nicely executed minor problems section over and underestimates over and underestimates figure caption lines are actually blue and green not blue and red as stated in the caption if a certain toolkit was used for gp modeling it would be great to refer to this in the final paper,4.0
129.json,the paper describes an mt training data selection approach that scores and ranks general domain sentences using a cnn classifier comparison to prior work using continuous or n gram based language models is well done even though it is not clear of the paper also compared against bilingual data selection e g sum of difference of cross entropies the motivation to use a cnn instead of an rnn lstm was first unclear to me but it is a strength of the paper to argue that certain sections of a text sentence are more important than others and this is achieved by a cnn however the paper does not experimentally show whether a bow or seq or the combination of both representation is more important and why the textual description of the cnn one hot or semi supervised using pre trained embeddings is clear detailed and points out the important aspects however a picture of the layers showing how inputs are combined would be worth a thousand words the paper is overall well written but some parentheses for citations are not necessary citet vs citep e g line experiments and evaluation support the claims of the paper but i am a little bit concerned about the method of determining the number of selected in domain sentences line based on a separate validation set what validation data is used here it is also not clear on what data hyperparameters of the cnn models are chosen how sensitive are the models to this table should really compare scores of different approaches with the same number of sentences selected as figure shows the approach of the paper still seems to outperform the baselines in this case other comments i would be interested in an experiment that compares the technique of the paper against baselines when more in domain data is available not just the development set the results or discussion section could feature some example sentences selected by the different methods to support the claims made in section in regards to the argument of abstracting away from surface forms in another baseline to compare against could have been the work of axelrod who replace some words with pos tags to reduce lm data sparsity to see whether the wordvec embeddings provide an additional advantage over this using the sum of source and target classification scores is very similar to source target lewis moore lm data selection sum of difference of cross entropies a reference to this work around line would be reasonable finally i wonder if you could learn weights for the sum of both source target classification scores by extending the cnn model to the bilingual parallel setting,4.0
124.json,this paper proposed a very interesting idea of using cognitive features for sentiment analysis and sarcasm detection more specifically the eye movement patterns of human annotators are recorded to derive a new set of features the authors claim that this is the first work to include cognitive features into the nlp community strength the paper is generally well written and easy to follow very interesting idea which may inspire research in other nlp tasks weakness the motivation of using cognitive features for sentiment analysis is not very well justified i can imagine these features may help reflect the reading ease but i do not see why they are helpful in detecting sentiment polarities the improvement is marginal after considering cognitive features by comparing sn sr gz with sn sr although the authors discussed about the feasibility of the approach in section but i am not convinced especially about the example given in section i do not see why this technique is helpful in such a scenario,4.0
124.json,this paper is about introducing eye tracking features for sentiment analysis as a type of cognitive feature i think that the idea of introducing eye tracking features as a proxy for cognitive load for sentiment analysis is an interesting one i think the discussion on the features and comparison of feature sets is clear and very helpful i also like that the feasibility of the approach is addressed in section i wonder if it would help the evaluation if the datasets did not conflate different domains e g the movie review corpus and the tweet corpus for one it might improve the prediction of movie review resp tweets if the tweets resp movie reviews were not in the training it would also make the results easier to interpret the results in table would seem rather low compared to state of the art results for the pang and lee data but look much better if compared to results for twitter data in section there are no overlapping snippets in the training data and testing data of datasets and right even if they come from the same sources e g pang lee and sentiment minor some of the extra use of bold is distracting or maybe it just me,4.0
166.json,this paper proposes an approach for multi lingual named entity recognition using features from wikipedia by relying on a cross lingual wikifier it identifies english wikipedia articles for phrases in a target language and uses features based on the wikipedia entry experiments show that this new feature helps not only in the monolingual case but also in the more interesting direct transfer setting where the english model is tested on a target language i liked this paper it proposes a new feature for named entity recognition and conducts a fairly thorough set of experiments to show the utility of the feature the analysis on low resource and the non latin languages are particularly interesting but what about named entities that are not on wikipedia in addition to the results in the paper it would be interesting to see results on how these entities are affected by the proposed method the proposed method is strongly dependent on the success of the cross lingual wikifier with this additional step in the pipeline how often do we get errors in the prediction because of errors in the wikifier given the poor performance of direct transfer on tamil and bengali when lexical features are added i wonder if it is possible to regularize the various feature classes differently so that the model does not become over reliant on the lexical features,4.0
166.json,this paper is concerned with cross lingual direct transfer of ner models using a very recent cross lingual wikification model in general the key idea is not highly innovative and creative as it does not really propose any core new technology the contribution is mostly incremental and marries the two research paths direct transfer for downstream nlp tasks such as ner parsing or pos tagging and very recent developments in the cross lingual wikification technology however i pretty much liked the paper as it is built on a coherent and clear story with enough experiments and empirical evidence to support its claims with convincing results i still have several comments concerning the presentation of the work related work a more detailed description in related work on how this paper relates to work of kazama and torisawa is needed it is also required to state a clear difference with other related ner system that in one way or another relied on the encyclopaedic wikipedia knowledge the differences are indeed given in the text but they have to be further stressed to facilitate reading and placing the work in context although the authors argue why they decided to leave out pos tags as features it would still be interesting to report experiments with pos tags features similar to tackstrom et al the reader might get an overview supported by empirical evidence regarding the usefulness or its lack of such features for different languages i e for the languages for which universal pos are available at least section could contribute from a running example as i am still not exactly sure how the edited model from tsai and roth works now i e the given description is not entirely clear since the authors mention several times that the approaches from tackstrom et al and nothman et al are orthogonal to theirs and that they can be combined with the proposed approach it would be beneficial if they simply reported some preliminary results on a selection of languages using the combination of the models it will add more flavour to the discussion along the same line although i do acknowledge that this is also orthogonal approach why not comparing with a strong projection baseline again to put the results into more even more context and show the usefulness or limitations of wikification based approaches why is dutch the best training language for spanish and spanish the best language for yoruba only a statistical coincidence or something more interesting is going on there a paragraph or two discussing these results in more depth would be quite interesting although the idea is sound the results from table are not that convincing with only small improvements detected and not in all scenarios a statistical significance test reported for the results from table could help support the claims minor comments sect projection can also be performed via methods that do not require parallel data which makes such models more widely applicable even for languages that do not have any parallel resources e g see the work of peirsman and pado naacl or vulic and moens emnlp which exploit bilingual semantic spaces instead of direct alignment links to perform the transfer several typos detected in the text so the paper should gain quite a bit from a more careful proofreading e g first sentence of section as a the base model this sentence is not parsable page they avoid the traditional pipeline of ner then el by to disambiguate every n grams on page,4.0
12.json,the paper describes a modification to the output layer of recurrent neural network models which enables learning the model parameters from both gold and projected annotations in a low resource language the traditional softmax output layer which defines a distribution over possible labels is further multiplied by a fully connected layer which models the noise generation process resulting in another output layer representing the distribution over noisy labels overall this is a strong submission the proposed method is apt simple and elegant the paper reports good results on pos tagging for eight simulated low resource languages and two truly low resource languages making use of a small set of gold annotations and a large set of cross lingually projected annotations for training the method is modular enough that researchers working on different nlp problems in low resource scenarios are likely to use it from a practical standpoint the experimental setup is unusual while i can think of some circumstances where one needs to build a pos tagger with as little as token annotations e g evaluations in some darpa sponsored research projects it is fairly rare a better empirical validation of the proposed method would have been to plot the tagging accuracy of the proposed method and baselines while varying the size of gold annotations this plot would help answer questions such as does it hurt the performance on a target language if we use this method while having plenty of gold annotations what is the amount of gold annotations approximately below which this method is beneficial does the answer depend on the target language beyond cross lingual projections noisy labels could potentially be obtained from other sources e g crowd sourcing and in different tag sets than gold annotations although the additional potential impact is exciting the paper only shows results with cross lingual projections with the same tag set it is surprising that the proposed training objective gives equal weights to gold vs noisy labels since the setup assumes the availability of a small gold annotated corpus it would have been informative to report whether it is beneficial to tune the contribution of the two terms in the objective function in line the paper describes the projected data as pairs of word tokens xt and their vector representations tilde y but does not explicitly mention what the vector representation looks like e g a distribution over cross lingually projected pos tags for this word type a natural question to ask here is whether the approach still works if we construct tilde y using the projected pos tags at the token level rather than aggregating all predictions for the same word type also since only one to one word alignments are preserved it is not clear how to construct tilde y for words which are never aligned line replace one of the two closing brackets with an opening bracket,4.0
91.json,this paper investigates three simple weight pruning techniques for nmt and shows that pruning weights based on magnitude works best and that retraining after pruning can recover original performance even with fairly severe pruning the main strength of paper is that the technique is very straightforward and the results are good itâ s also clearly written and does a nice job covering previous work a weakness is that the work isnâ t very novel being just an application of a known technique to a new kind of neural net and application namely nmt with results that arenâ t very surprising itâ s not clear to me what practical significance these results have since to take advantage of them you would need sparse matrix representations which are trickier to get working fast on a gpu and after all speed is the main problem with nmt not space there may be new work that changes this picture since the field is evolving fast but if so you need to describe it and generally do a better job explaining why we should care about pruning a suggestion for dealing with the above weakness would be to use the pruning results to inform architecture changes for instance figure suggests that you might be able to reduce the number of hidden layers to two and also potentially reduce the dimension of source and target embeddings another suggestion is that you try to make a link between pruning retraining and dropout eg â a theoretically grounded application of dropout in recurrent neural networksâ gal arxiv detailed comments line â softmax weightsâ â output embeddingsâ may be a preferable term s itâ s misleading to call n the â dimensionâ of the network and specify all parameter sizes as integer multiples of this number as if this were a logical constraint line you should cite bahdanau et al here for the attention idea rather than luong for their use of it s class uniform and class distribution seem very similar and naturally get very similar results consider dropping one or the other figure suggestion that you could hybridize pruning use class blind for most classes but class uniform for the embeddings figure should show perplexity too what pruning is used in section figure figure does loss pertain to training or test corpora figure this seems to be missing softmax weights i found this diagram somewhat hard to interpret it might be better to give relevant statistics such as the proportion of each class that is removed by class blind pruning at various levels line you might want to cite le et al â a simple way to initialize recurrent networks of rectified linear unitsâ arxiv,3.0
18.json,this paper presents a transition based graph parser able to cope with the rich representations of a semantico cognitive annotation scheme instantiated in the ucca corpora the authors start first by exposing what according to them should cover a semantic based annotation scheme i being graph based possibility for a token node of having multiple governors having non terminal nodes representing complex structures â syntactic coordinate phrases lexical multiword expression and allowing discontinuous elements eg verbs particules interestingly none of these principles is tied to a semantic framework they could also work for syntax or other representation layers the authors quickly position their work by first introducing the larger context of broad coverage semantic parsing then their annotation scheme of choice ucca they then present sets of parsing experiments i one devoted to phrase based parsing using the stanford parser and an ucca to constituency conversion ii one devoted to dependency parsing using an ucca to dependency conversion and finally iii the core of their proposal a set of experiments showing that their transition based graph parser is suitable for direct parsing of ucca graphs i found this work interesting but before considering a publication i have several concerns with regards to the methodology and the empirical justifications the authors claimed that there are the first to propose a parser for a semantically oriented scheme such as theirs of course they are but with all due respect to the work behind this scheme it is made of graphs with a various level of under specified structural arguments and semantically oriented label process state and nothing in their transition sets treats the specificities of such a graph even the transitions related to the remote edges could have been handled by the other ones assuming a difference in the label set itself like adding an affix for example if we restrict the problem to graph parsing many works post the semeval shared tasks almeda and martins ribeyre et al proposed an extension to transition based graph parser or an adaptation of a higher model one and nothing precludes their use on this data set itâ s mostly the use of a specific feature template that anchors this model to this scheme even though itâ s less influencial than the count features and the unigram one anyway because the above mentioned graph parsers are available i donâ t understand why they couldnâ t be used as a baseline or source of comparisons regarding the phrase based experiments using uparse it could have been also validated by another parser from fernandez gonzales and martins which can produce lcfrs like parsing as good as uparse ref missing when you first introduced uparse because this scheme supports a more abstract view of syntaxico semantic structures than most of the sdp treebanks it would have been important to use the same metrics as in the related shared task at this point in the field many systems models and data set are competing and i think that the lack of comparison points with other models and parsers is detrimental to this work as whole yet i found it interesting and because weâ re at crossing time in term of where to go next i think that this paper should be discussed at a conference such as conll note in random order please introduce the â grounded semanticâ before page you use that phrase before why havenâ t you try to stick to constituent tree with rich node labels and propagater traces and then train parse with the berkeley parser it could have been a good baseline the conversion to surface dependency trees is in my mind useless you loose too many information here a richer conversion such as the one from â schluter et al semeval sdp should have been used can you expand on â ucca graphs may contains implicit unit that have no correspondent in the textâ or provide a ref or an example you mentioned other representations such as mrs and drt this raises the fact that your scheme doesnâ t seem to allow for a modelling of quantifier scope information itâ s thus fully comparable to other more syntax oriented scheme itâ s indeed more abstract than dm for example and probably more underspecified than the semantic level of the pcedt but how much how really informative is this scheme and how really â parsableâ is it according to your scores it seems â harderâ but an error analysis would have been useful as i said before the principles you devised could apply to a lot of things they look a bit ad hoc to me and would probably need to take place in a much wider and a bit clearer introduction what are you trying to argue for a parser that can parse ucca a model suitable for semantic analysis or a semantic oriented scheme that can actually be parsable you are trying to say all of those in a very dense way and it borderline to be be confusing http www corentinribeyre fr projects view dagparser https github com andre martins turboparser and https github com andre martins turboparser tree master semevaldata,3.0
25.json,this paper presents an approach to tag word senses with temporal information past present future or atemporal they model the problem using a graph based semi supervised classification algorithm that allows to combine item specific information such as the presence of some temporal indicators in the glosses and the structure of wordnet that is semantic relations between synsets â and to take into account unlabeled data they perform a full annotation of wordnet based on a set of training data labeled in a previous work and using the rest of wordnet as unlabeled data specifically they take advantage of the structure of the label set by breaking the task into a binary formulation temporal vs atemporal then using the data labeled as temporal to perform a finer grained tagging past present or future in order to intrinsically evaluate their approach they annotate a subset of synsets in wordnet using crowd sourcing they compare their system to the results obtained by a state of the art time tagger stanford sutime using an heuristic as a backup strategy and to previous works they obtain improvements around in accuracy and show that their approach allows performance higher than previous systems using only labeled data finally they perform an evaluation of their resource on an existing task tempeval and show improvements of about in f on labels this paper is well constructed and generally clear the approach seems sound and well justified this work led to the development of a resource with fine grained temporal information at the word sense level that would be made available and could be used to improve various nlp tasks i have a few remarks especially concerning the settings of the experiments i think that more information should be given on the task performed in the extrinsic evaluation section an example could be useful to understand what the system is trying to predict the features describe â entity pairsâ but it has not been made clear before what are these pairs and what are the features especially what are the entity attributes what is the pos for a pair is it one dimension or two are the lemmas obtained automatically the sentence describing the labels used is confusing i am not sure to understand what â event to document creation timeâ and â event to same sentence eventâ means are they the kind of pairs considered are they relations as they are described as relation at the beginning of p i find unclear the footnote about the relations why the other relations have to be ignored what makes a mapping too â complexâ also are the scores macro or micro averaged finally the ablation study seems to indicate a possible redundancy between lexica and entity with quite close scores any clue about this behavior i have also some questions about the use of the svm for the extrinsic evaluation the authors say that they optimized the parameters of the algorithm what are these parameters and since a svm is also used within the mincut framework is it optimized and how finally if it the libsvm library that is used weka wrapper i think a reference to libsvm should be included other remarks it would be interesting to have the number of examples per label in the gold data the figures are given for coarse grained labels temporal vs atemporal but not for the finer grained it would also be nice to have an idea of the number of words that are ambiguous at the temporal level words like â presentâ it is said in the caption of the table that the results presented are â significantly betterâ but no significancy test is indicated neither any p value minor remarks related work what kind of task was performed in filannino and nenadic related work â requires a post calibration procedureâ needs a reference and p in footnote it would be clearer to explain calibration related work â their model differ from oursâ in what table is really too small maybe remove the parenthesis put the â p r f â in the caption and give only two scores e g prec and f the caption should also be reduced information in table would be better represented using a graph beginning of p â tempeval reference table would be made clearer by ordering the scores for one column p paragraph atemporal â atemporal,4.0
151.json,this paper describes a new deterministic dependency parsing algorithm and analyses its behaviour across a range of languages the core of the algorithm is a set of rules defining permitted dependencies based on pos tags the algorithm starts by ranking words using a slightly biased pagerank over a graph with edges defined by the permitted dependencies stepping through the ranking each word is linked to the closest word that will maintain a tree and is permitted by the head rules and a directionality constraint overall the paper is interesting and clearly presented though seems to differ only slightly from sogaard unsupervised dependency parsing without training i have a few questions and suggestions head rules table it would be good to have some analysis of these rules in relation to the corpus for example in section the fact that they do not always lead to a connected graph is mentioned but not how frequently it occurs or how large the components typically are i was surprised that head direction was chosen using the test data rather than training or development data given how fast the decision converges sentences this is not a major issue but a surprising choice how does tie breaking for words with the same pagerank score work does it impact performance significantly or are ties rare enough that it does not have an impact the various types of constraints head rules directionality distance will lead to upper bounds on possible performance of the system it would be informative to include oracle results for each constraint to show how much they hurt the maximum possible score that would be particularly helpful for guiding future work in terms of where to try to modify this system minor we obtain the rank table and table have columns in different orders i found the table arrangement clearer isolate the contribution of both,3.0
151.json,this paper presents a way to parse trees namely the universal dependency treebanks by relying only on pos and by using a modified version of the pagerank to give more way to some meaningful words as opposed to stop words this idea is interesting though very closed to what was done in sã gaard paper the personalization factor giving more weight to the main predicate is nice but it would have been better to take it to the next level as far as i can tell the personalization is solely used for the main predicate and its weight of seems arbitrary regarding the evaluation and the detailed analyses some charts would have been beneficial because it is sometimes hard to get the gist out of the tables finally it would have been interesting to get the scores of the pos tagging in the prediction mode to be able to see if the degradation in parsing performance is heavily correlated to the degradation in tagging performance which is what we expect all in all the paper is interesting but the increment over the work of sã gaard is small smaller issues l the the main idea the main idea,3.0
13.json,this paper presents a model for the task of event entity linking where they propose to use sentential features from cnns in place of external knowledge sources which earlier methods have used they train a two part model the first part learns an event mention representation and the second part learns to calculate a coreference score given two event entity mentions the paper is well written well presented and is easy to follow i rather like the analysis done on the ace corpus regarding the argument sharing between event coreferences furthermore the analysis on the size impact of the dataset is a great motivation for creating their ace dataset however there are a few major issues that need to be addressed the authors fail to motivate and analyze the pros and cons of using cnn for generating mention representations it is not discussed why they chose cnn and there are no comparisons to the other models e g straightforwardly an rnn given that the improvement their model makes according various metrics against the state of the art is only or points on f score there needs to be more evidence that this architecture is indeed superior it is not clear what is novel about the idea of tackling event linking with sentential features given that using cnn in this fashion for a classification task is not new the authors could explicitly point out and mainly compare to any existing continuous space methods for event linking the choice of methods in table is not thorough enough there is no information regarding how the ace dataset is collected a major issue with the ace dataset is its limited number of event types making it too constrained and biased it is important to know what event types ace covers this can also help support the claim in section that other approaches are strongly tied to the domain where these semantic features are availableâ our approach does not depend on resources with restrictedâ you need to show that those earlier methods fail on some dataset that you succeed on also for enabling any meaningful comparison in future the authors should think about making this dataset publicly available some minor issues i would have liked to see the performance of your model without gold references in table as well it would be nice to explore how this model can or cannot be augmented with a vanilla coreference resolution system for the specific example in line the off the shelf corenlp system readily links it to bombing which can be somehow leveraged in an event entity linking baseline given the relatively small size of the ace dataset i think having a compelling model requires testing on the other available resources as well this further motivates working on entity and event coreference simultaneously i also believe that testing on eventcorefbank in parallel with ace is essential table shows that the pairwise features have been quite effective which signals that feature engineering may still be crucial for having a competitive model at least on the scale of the ace dataset one would wonder which features were the most effective and why not report how the current set was chosen and what else was tried,3.0
683.json,this paper proposes a boosting based ensemble procedure for residual networks by adopting the deep incremental boosting method that was used for cnn mosca magoulas a at each step t a new block of layers are added to the network at a position pt and the weights of all layers are copied to the current network to speed up training the method is not sufficiently novel since the steps of deep incremental boosting are slightly adopted instead of adding a layer to the end of the network this version adds a block of layers to a position pt starts at a selected position p and merges layer accordingly hence slightly adopts dib the empirical analysis does not use any data augmentation it is not clear whether the improvements if there is of the ensemble disappear after data augmentation also one of the main baselines dib has no skip connections therefore this can negatively affect the fair comparison the authors argue that they did not involve state of art res nets since their analysis focuses on the ensemble approach however any potential improvement of the ensemble can be compensated with an inherent feature of res net variant the boosting procedure can be computationally restrictive in case of imagenet training and res net variants may perform much better in that case too therefore the baselines should include the state of art res nets and dense convolutional networks hence current results are preliminary in addition it is not clear how sensitive the boosting to the selection of injection point this paper adopts dib to res nets and provides some empirical analysis however the contribution is not sufficiently novel and the empirical results are not satisfactory for demonstrating that the method is significant pros provides some preliminary results for boosting of res nets cons not sufficiently novel an incremental approach empirical analysis is not satisfactory,3.0
379.json,the paper describes a novel technique to improve the efficiency of computation graphs in deep learning frameworks an impressive speedup can be observed in their implementation within tensorflow the content is presented with sufficient clarity although some more graphical illustrations could be useful this work is relevant in order to achieve highest performance in neural network training pros significant speed improvements through dynamic batching source code provided cons the effect on a large real world asr smt would allow the reader to put the improvements better into context presentation vizualisation can be improved,8.0
396.json,the paper proposes a model for image generation where the back ground is generated first and then the foreground is pasted in by generating first a foregound mask and corresponding appearance curving the appearance image using the mask and transforming the mask using predicted affine transform to paste it on top of the image using amturkers the authors verify their generated images are selected of the time as being more naturally looking than corresponding images from a dc gan model that does not use a figure ground aware image generator the segmentations masks learn to depict objects in very constrained datasets birds only thus the method appears limited for general shape datasets as the authors also argue in the paper yet the architectural contributions have potential merit it would be nice to see if multiple layers of foreground occluding foregrounds are ever generated with this layered model or it is just figure ground aware,6.0
545.json,this paper proposes a new learning model compositional kernel machines ckms that extends the classic kernel machines by constructing compositional kernel functions using sum product networks this paper considers the convnets as nicely learned nonlinear decision functions and resort their success in classification to their compositional nature this perspective motivates the design of compositional kernel functions and the sum product implementation is indeed interesting i agree the composition is important for convnets but it is not the whole story of convnets success one essential difference between convnets and ckms is that all the kernels in convnets are learned directly from data while ckms still build on top of feature descriptors this i believe limits the representation power of ckms a recent paper deep convolutional networks are hierarchical kernel machines by anselmi f et al seems to be interesting to the authors experiments seem to be preliminary in this paper it good to see promising results of ckms on small norb but it is quite important to show competitive results on recent classification standard benchmarks such as mnist cifar and even imagenet in order to establish a novel learning model in norb compositions ckms seem to be better than convnets at classifying images by their dominant objects i suspect it is because the use of sparse orb features it will be great if this paper could show the accuracy of orb features with matching kernel svms some details about this experiment need further clarification such as what are the high and low probabilities of sampling from each collections and how many images are generated in norb symmetries ckms show better performance than convnets with small data but the convnets seem not converged yet could it be possible to show results with larger dataset,5.0
545.json,the authors propose a method to efficiently augment an svm variant with many virtual instances and show promising preliminary results the paper was an interesting read with thoughtful methodology but has partially unsupported and potentially misleading claims pros thoughtful methodology with sensible design choices potentially useful for smaller n datasets with a lot of statistical structure nice connections with sum product literature cons claims about scalability are very unclear generally the paper does not succeed in telling a complete story about the properties and applicability of the proposed method experiments are very preliminary the scalability claims are particularly unclear the paper repeatedly mentions lack of scalability as a drawback for convnets but it appears the proposed ckm is less scalable than a standard svm yet svms often handle much fewer training instances than deep neural networks it appears the scalability advantages are mostly for training sets with roughly fewer than instances and even if the method could scale to training instances it unclear whether the predictive accuracy would be competitive with convnets in that domain moreover the idea of doing operations simply for creating virtual instances on training points and test points is still somewhat daunting what if we had training instances and testing instances because scalability in the number of training instances is one of the biggest drawbacks of using svms e g with gaussian kernels on modern datasets the scalability claims in this paper need to be significantly expanded and clarified on a related note the suggestion that convnets grow quadratically in computation with additional training instances in the introduction needs to be augmented with more detail and is potentially misleading convnets typically scale linearly with additional training data in general the paper suffers greatly from a lack of clarity and issues of presentation as above the full story is not presented with critical details often missing moreover it would strengthen the paper to remove broad claims such as just as support vector machines svms eclipsed multilayer perceptrons in the s ckms could become a compelling alternative to convnets with reduced training time and sample complexity suggesting that ckms could eclipse convolutional neural networks and instead provide more helpful and precise information convnets are multilayer perceptrons used in the s as well as now and they are not eclipsed by svms they have different relative advantages and based on the information presented broadly advertising scalability over convnets is misleading can ckms scale to datasets with millions of training and test instances it seems as if the scalability advantages are limited to smaller datasets and asymptotic scalability could be much worse in general and even if ckms could scale to such datasets would they have as good predictive accuracy as convnets on those applications being specific and with full disclosure about the precise strengths and limitations of the work would greatly improve this paper ckms may be more robust to adversarial examples than standard convnets due to the virtual instances but there are many approaches to make deep nets more robust to adversarial examples it would be useful to consider and compare to these the ideas behind ckms also are not inherently specific to kernel methods have you considered looking at using virtual instances in a similar way with deep networks a full exploration might be its own paper but the idea is worth at least brief discussion in the text a big advantage of svms with gaussian kernels over deep neural nets is that one can achieve quite good performance with very little human intervention design choices however ckms seem to require extensive intervention in terms of architecture as with a neural network and in insuring that the virtual instances are created in a plausible manner for the particular application at hand it very unclear in general how one would want to create sensible virtual instances and this topic deserves further consideration moreover unlike svms with for example gaussian or linear kernels or standard convolutional networks which are quite general models ckms as applied in this paper seem more like svms or kernel methods which have been highly tailored to a particular application in this case the norb dataset there is certainly nothing wrong with the tailored approach but it would help to be clear and detailed about where the presented ideas can be applied out of the box or how one would go about making the relevant design choices for a range of different problems and indeed it would be good to avoid the potentially misleading suggestions early in the paper that the proposed method is a general alternative to convnets the experiments give some insights into the advantages of the proposed approach but are very limited to get a sense of the properties the strengths and limitations of the proposed method one needs a greater range of datasets with a much larger range of training and test sizes the comparisons are also quite limited why not an svm with a gaussian kernel what about an svm using convnet features from the dataset at hand light blue curve in figure it should do at least as well as the light blue curve there are also other works that could be considered which combine some of the advantages of kernel methods with deep networks also the claim that the approach helps with the curse of dimensionality is sensible but not particularly explored it also seems the curse of dimensionality could affect the scalability of creating a useful set of virtual instances and it unclear how ckm would work without any orb features even if the method can be adapted to scale to n it unclear whether it will be more useful than convnets in that domain indeed in the experiments here convnets essentially match ckms in performance after examples and would probably perform better than ckms on larger datasets we can only speculate because the experiments do not consider larger problems the methodology largely takes inspiration from sum product networks but its application in the context of a kernel approach is reasonably original and worthy of exploration it reasonable to expect the approach to be significant but its significance is not demonstrated the quality is high in the sense that the methods and insights are thoughtful but suffers from broad claims and a lack of full and precise detail in short i like the paper but it needs more specific details and a full disclosure of where the method should be most applicable and its precise advantages and limitations code would be helpful for reproducibility,5.0
545.json,this paper proposes a new learning framework called compositional kernel machines ckm it combines two ideas kernel methods and sum product network spn ckm first defines leaf kernels on elements of the query and training examples then it defines kernel recursively similar to sum product network this paper has shown that the evaluation ckm can be done efficiently using the same tricks in spn positive i think the idea in this paper is interesting instance based learning methods such as svm with kernels have been successful in the past but have been replaced by deep learning methods e g convnet in the past few years this paper investigate an unexplored area of how to combine the ideas from kernel methods and deep networks spn in this case negative although the idea of this paper is interesting this paper is clearly very preliminary in its current form i simply do not see any advantage of the proposed framework over convnet i will elaborate below one of the most important claims of this paper is that ckm is faster to learn than convnet i am not clear why that is the case both ckm and convnet use gradient descent during learning why would ckm be faster also during inference the running time of convnet only depends on its network structure but for ckm in addition to the network structure it also depends on the size of training set from this perspective it does not seem ckm is very scalable when the training size is big that is probably why this paper has to use all kinds of specialized data structures and tricks even on a fairly simple dataset like norb i am having a hard time understanding what the leaf kernel is capturing for example if the elements correspond to raw pixel intensities a leaf kernel essentially compares the intensity value of a pixel in the query image with that in a training image but in this case would not you end up comparing a lot of background pixels across these two images which does not help with recognition i think it probably helps to explain sec a bit better in its current form this part is very dense and hard to understand it is also not entirely clear to me how you would design the architecture of the sum product function the example is sec seems to be fairly arbitrary the experiment section is probably the weakest part norb is a very small and toy ish dataset by today standard even on this small dataset the proposed method is only slighly better than svm it is not clear whether svm in table is linear svm or kernel svm if it is linear svm i suspect the performance of svm will be even higher when you use kernel svm and far worse than convnet the proposed method only shows improvement over convnet on synthetic datasets norb compositions norm symmetries overall i think this paper has some interesting ideas but in its current form it is a bit too preliminary and more work is needed to show its advantage having said that i acknowledge that in the machine learning history many important ideas seem pre mature when they were first proposed and it took time for these ideas to develop,5.0
768.json,this paper proposes a modification to convnet training so that the feature activations before the linear classifier are divided into groups such that all pairs of features across all pairs of groups are encouraged to have low statistical correlation instead of discovering the groups automatically the work proposes to use supervision which they call privileged information to assign features to groups in a hand coded fashion the developed method is applied to image classification pros the paper is clear and easy to follow the experimental results seem to show some benefit from the proposed approach cons the paper proposes one core idea group orthogonality w privileged information but then introduces background feature suppression without much motivation and without careful experimentation no comparison with an ensemble full experiments on imagenet under the partial privileged information setting would be more impactful this paper is promising and i would be willing to accept an improved version however the current version lacks focus and clean experiments first the abstract and intro focus on the need to replace ensembles with a single model that has diverse ensemble like features the hope is that such a model will have the same boost in accuracy while requiring fewer flops and less memory based on this introduction i expect the rest of the paper to focus on this point but it does not there are no experimental results on ensembles and no experimental evidence that the proposed approach in able to avoid the speed and memory cost of ensembles while also retaining the accuracy benefit second the technical contribution of the paper is presented as group orthogonality go however in sec the idea of background feature suppression is introduced while some motivation for it is given the motivation does not tie into go go does not require bg suppression and the introduction of it seems ad hoc moreover the experiments never decouple go and bg suppression so we are unable to understand how go works on its own this is a critical experimental flaw in my reading minor suggestions comments the equation in definition has an incorrect normalizing factor c k figure seems to have incorrect mask placements the top mask is one that will mask out the background and only allow the fg to pass,5.0
768.json,the starting point of this work is the understanding that by having decorrelated neurons e g neurons that only fire on background or only on foreground regions one provides independent pieces of information to the subsequent decisions as such one gives complementary viewpoints of the input to the subsequent layers which can be thought of as performing ensembling expert combination within the model rather than using an ensemble of networks for this the authors propose a sensible method to decorrelate the activations of intermediate neurons with the aim of delivering complementary inputs to the final classification layers they split intermediate neurons to a foreground and a background subset and append side losses that force them to be zero on background and foreground pixels respectively they demonstrate that this can improve classification on a mid scale classification example a fraction of imagenet and a resnet with rather than layers when compared to a vanilla baseline that does not use these losses i enjoyed reading the paper because the idea is simple smart and seems to be effective but there are a few concerns firstly the way of doing this seems very particular to vision in vision one knows that masking the features during both training and testing helps e g,6.0
787.json,the paper proposes a semantic embedding based approach to multilabel classification conversely to previous proposals sem considers the underlying parameters determining the observed labels are low rank rather than that the observed label matrix is itself low rank however it is not clear to what extent the difference between the two assumptions is significant sem models the labels for an instance as draws from a multinomial distribution parametrized by nonlinear functions of the instance features as such it is a neural network the proposed training algorithm is slightly more complicated than vanilla backprop the significance of the results compared to nnml in particular on large datasets delicious and eurlex is not very clear the paper is well written and the main idea is clearly presented however the experimental results are not significant enough to compensate the lack of conceptual novelty,4.0
787.json,the paper presents the semantic embedding model for multi label prediction in my questions i pointed that the proposed approach assumes the number of labels to predict is known and the authors said this was an orthogonal question although i do not think it is i was trying to understand how different is sem from a basic mlp with softmax output which would be trained with a two step approach instead of stochastic gradient descent it seems reasonable given their similarity to compare to this very basic baseline regarding the sampling strategy to estimate the posterior distribution and the difference with jean et al i agree it is slightly different but i think you should definitely refer to it and point to the differences one last question why is it called semantic embeddings usually this term is used to show some semantic meaning between trained embeddings but this does not seem to appear in this paper,4.0
380.json,this paper addresses one of the major shortcomings of generative adversarial networks their lack of mechanism for evaluating held out data while other work such as bigans ali address this by learning a separate inference network here the authors propose to change the gan objective function such that the optimal discriminator is also an energy function rather than becoming uninformative at the optimal solution training this new objective requires gradients of the entropy of the generated data which are difficult to approximate and the authors propose two methods to do so one based on nearest neighbors and one based on a variational lower bound the results presented show that on toy data the learned discriminator energy function closely approximates the log probability of the data and on more complex data the discriminator give a good measure of quality for held out data i would say the largest shortcomings of the paper are practical issues around the scalability of the nearest neighbors approximation and accuracy of the variational approximation which the authors acknowledge also since entropy estimation and density estimation are such closely linked problems i wonder if any practical method for egans will end up being equivalent to some form of approximate density estimation exactly the problem gans were designed to circumvent nonetheless the elegant mathematical exposition alone makes the paper a worthwhile contribution to the literature also some quibbles about the writing it seems that something is missing in the sentence at the top of pg finally let whose discriminative power i am not sure what the authors mean to say here and the title undersells the paper it makes it sound like they are making a small improvement to training an existing model rather than deriving an alternative training framework,8.0
553.json,combining storage and processing capabilities is an interesting research topic because data transfer is a major issue for many machine learning tasks the paper itself is well written but unfortunately addresses a lot of things only to medium depth probably due length constraints my opinion is that a journal with an in depth discussion of the technical details would be a better target for this paper even though the researchers took an interesting approach to evaluate the performance of the system it difficult for me to grasp the expected practical improvements of this approach with such a big focus on gpu and more specialized hardware such as tpus the one question that comes to mind by how much does this or do you expect it to beat the latest and greatest gpu on a real task i do not consider myself an expert on this topic even though i have some experience with systemc,6.0
553.json,while the idea of moving the processing for machine learning into silicon contained within the ssd data storage devices is intriguing and offers the potential for low power efficient computation it is a rather specialized topic so i do not feel it will be of especially wide interest to the iclr audience the paper describes simulation results rather than actual hardware implementation and describes implementations of existing algorithms the comparisons of algorithms train test performance does not seem relevant since there is no novelty in the algorithms and the use of a single layer perceptron on mnist calls into question the practicality of the system since this is a tiny neural network by today standards i did not understand from the paper how it was thought that this could scale to contemporary scaled networks in terms of numbers of parameters for both storage and bandwidth i am not an expert in this area so have not evaluated in depth,5.0
601.json,it would seem that the shelf life of a dataset has decreased rapidly in recent literature squad dataset has been heavily pursued as soon as it hit online couple months ago the best performance on their leaderboard now reaching to this is rather surprising when taking into account the fact that the formal conference presentation of the dataset took place only a month ago at emnlp and that the reported machine performance at the time of paper submission was only at one reasonable speculation is that the dataset may have not been hard enough newsqa the paper in submission aims to address this concern by presenting a dataset of a comparable scale created through different qa collection strategies most notably the authors solicit questions without requiring answers from the same turkers in order to promote more diverse and hard to answer questions another notable difference is that the questions are gathered without showing the content of the news articles and the dataset makes use of a bigger subset of cnn daily corpus k k as opposed to a much smaller subset k used by squad in sum i think newsqa dataset presents an effort to construct a harder large scale reading comprehension challenge a recently hot research topic for which we don t yet have satisfying datasets while not without its own weaknesses i think this dataset presents potential values compared to what are available out there today that said the paper does read like it was prepared in a hurry as there are numerous small things that the authors could have done better as a result i do wonder about the quality of the dataset for one human performance of squad measured by the authors is lower than that reported by squad i think this sort of difference can easily happen depending on the level of carefulness the annotators can maintain after all not all humans have the same level of carefulness or even the same level of reading comprehension i think it d be the best if the authors can try to explain the reason behind these differences and if possible perform a more careful measurement of human performance if anything i don t think it looks favorable for newsqa if the human performance is only at the level of as it looks as if the difficulty of the dataset comes mainly from the potential noise from the qa collection process which implies that the low model performance could result from not necessarily because of the difficulty of the comprehension and reasoning but because of incorrect answers given by human annotators i m also not sure whether the design choice of not presenting the news article when soliciting the questions was a good one i can imagine that people might end up asking similar generic questions when not enough context has been presented perhaps taking a hybrid what i would like to suggest is to present news articles where some sentences or phrases are randomly redacted so that the question generators can have a bit more context while not having the full material in front of them yet another way of encouraging the turkers from asking too trivial questions is to engage an automatic qa system on the fly turkers must construct a qa pair for which an existing state of the art system cannot answer correctly,6.0
314.json,deep rl using deep neural networks for function approximators in rl algorithms have had a number of successes solving rl in large state spaces this empirically driven work builds on these approaches it introduces a new algorithm which performs better in novel d environments from raw sensory data and allows better generalization across goals and environments notably this algorithm was the winner of the visual doom ai competition the key idea of their algorithm is to use additional low dimensional observations such as ammo or health which is provided by the game engine as a supervised target for prediction importantly this prediction is conditioned on a goal vector which is given not learned and the current action once trained the optimal action for the current state can be chosen as the action that maximises the predicted outcome according the goal unlike in successor feature representations learning is supervised and there is no td relationship between the predictions of the current state and the next state there have been a number of prior works both in predicting future states as part of rl and goal driven function approximators which the authors review in section the key contributions of this work are the focus on monte carlo estimation rather than td the use of low dimensional measurements for prediction the parametrized goals and perhaps most importantly the empirical comparison to relevant prior work in addition to the comparison with visual doom ai the authors show that their algorithm is able to learn generalizable policies which can respond without further training to limited changes in the goal the paper is well communicated and the empirical results compelling and will be of significant interest some minor potential improvements there is an approximation in the supervised training as it is making an on policy assumption but it learns from a replay buffer with the monte carlo regression the expectation of the remainder of the trajectory is assumed to follow the current policy but is being sampled from episodes generated by prior versions of the policy this should be discussed the algorithm uses additional metadata the information about which parts of the sensory input are worth predicting that the compared algorithms do not i think this and the limitations of this approach e g it may not work well in a sensory environment if such measurements are not provided should be mentioned more clearly,7.0
314.json,this paper presents an on policy deep rl method with additional auxiliary intrinsic variables the method is a special case of an universal value function based approach and the authors do cite the correct references maybe the biggest claimed technical contribution of this paper is to distill many of the existing ideas to solve d navigation problems i think the contributions should be more clearly stated in the abstract intro i would have liked to see failure modes of this approach under what circumstances does the model have problems generalizing to changing goals there are other conceptual problems since this is an on policy method there will be catastrophic forgetting if the agent dose not repeatedly train on goals from the distant past since the main contribution of this paper is to integrate several key ideas and show empirical advantage i would have liked to see results on other domains like atari maybe using the rom as intrinsic variables overall i think this paper does show clear empirical advantage of using the proposed underlying formulations and experimental insights from this paper might be valuable for future agents,8.0
528.json,this paper presents design decisions of terpret and experiments about learning simple loop programs and list manipulation tasks the terpret line of work is one of those which bridges the gap between the programming languages pl and machine learning ml communities contrasted to the recent interest of the ml community for program induction the focus here is on using the design of the programming language to reduce the search space namely here they used the structure of the control flow if then else foreach zipwithi and foldli templates immutable data no reuse of a neural memory and types they tried penalizing ill typedness and restricting the search only to well typed programs which works better my bird eye view would be that this stands in between make everything continuous and perform gradient descent ml and discretize all the things and perform structured and heuristics guided combinatorial search pl i liked that they have a relevant baseline lambda but i wished that they also included a fully neural network program synthesis baseline admittedly it would not succeed except on the simplest tasks but i think some of their experimental tasks are simple enough for non generating code nns to succeed on i wished that terpret was available and the code to reproduce these experiments too i wonder if how the otherwise very interesting recommendations for the design of programming languages to perform gradient descent based inductive programming would hold perform on harder task than these loops even though these tasks are already interesting and challenging i wonder how much of these tasks biased the search for good subset of constraints e g those for structuring the control flow overall i think that the paper is good enough to appear at iclr but i am no expert in program induction synthesis writing the paper is at times hard to follow for instance the naming scheme of the model variants could be summarized in a table with boolean information about the features it embeds introduction basis modern computing of page training objective minimize the cross entropy between the distribution in the output register rr t and a point distribution with all probability mass on the correct output value if you want to cater to the ml community at large i think that it is better to say that you treat the output of rr t as a classification problem with the correct output value you can give details and say exactly which type of criterion loss cross entropy you use terpret a probabilistic programming language for program induction gaunt et al,7.0
656.json,in this paper the authors explicitly design geometrical structure into a cnn by combining it with a scattering network this aids stability and limited data performance the paper is well written the contribution of combining scattering and cnns is novel and the results seem promising i feel that such work was a missing piece in the scattering literature to make it useful for practical applications i wish the authors would have investigated the effect of the stable bottom layers with respect to adversarial examples this can be done in a relatively straightforward way with software like cleverhans or deep fool it would be very interesting if the first layer stability in the hybrid architectures increases robustness significantly as this would tell us that these fooling images are related to low level geometry finding that this is not the case would be very interesting as well further the proposed architecture is not evaluated on real limited data problems this would further strengthen the improved generalization claim however i admit that the cifar cifar difference already seems like a promising indicator in this regard if one of the two points above will be addressed in an additional experiment i would be happy to raise my score from to summary an interesting approach is presented that might be useful for real world limited data scenarios limited data results look promising adversarial examples are not investigated in the experimental section no realistic small data problem is addressed minor the authors should add a sota resnet to table as nin is indeed out of fashion these days some typos tacke developping learni,7.0
713.json,the paper deals with a very important issue of vanishing gradients and the quest for a perfect activation function proposed is an approach of learning the activation functions during the training process i find this research very interesting but i am concerned that the paper is a bit premature there is a long experimental section but i am not sure what the conclusion is the authors appear to be somewhat confused themselves the amount of maybe could mean perhaps etc statements in the paper is exceptionally high for this paper to be accepted it needs a bold statement about the performance with a solid evidence in my opinion that is lacking as of now this approach is either a breakthrough or a dud and after reading the paper i am not convinced which case it is the theoretical section could be made a little clearer finally how is the performance affected the huge advantage if relu is in the fact that the formula is so simple and thus not costly to evaluate how do pelu s compare,5.0
713.json,this paper proposes a modification of the elu activation function for neural networks by parameterizing it with trainable parameters per layer this parameter is proposed to more effectively counter vanishing gradients my main concern regarding this paper is related to the authors claims about the effectiveness of pelu the analysis in sections and discusses how pelu might improve training by combating gradient propagation issues this by itself does not imply that improved generalization will result only that models may be easier to train however the experiments all seek to demonstrate improved generalization performance but this could in principle be due to a better inductive bias and have nothing to do with the optimization analysis none of the experiments are designed to directly support the stated theoretical advantage of pelu compared to elu in optimizing models in the response to the pre review question the authors state that the claims in section and are meant to apply to generalization performance i fail to see how this is true for most claims except the flexibility claim as the authors agree better training may or may not lead to better out of sample performance i can only agree that having flexibility can sometimes help the network adapt its inductive bias to the problem instead of overfitting but this is a much weaker claim compared to the mathematical justifications for improved optimization on selection of learning hyperparameters the authors state in the discussion on openreview that the learning rates selected were favorable to relu and not pelu however this does not guarantee that they were not unfavorable to elu it raises the question can a regime be constructed where elu has better performance than pelu if so how can we draw the conclusion that pelu is better overall i am not yet convinced by the experimental setup and the match between theory and experiments in this paper,4.0
586.json,overall the paper has the feel of a status update by some of the best researchers in the field the paper is very clear the observations are interesting but the remarks are scattered and do not add up to a quantum of progress in the study of what can be done with the neural gpu model minor remark on the use of the term rnn in table i found table confusing because several of the columns are for models that are technically rnns and use of rnns for e g translation and wordvec highlight that rnns can be characterized in terms of the length of their input sequence the length of their input and the sizes per step of their input output and working memories basic model question how are inputs presented each character hot and outputs retrieved when there are e g filters in the model if inputs and outputs are hot encoded and treated with the same filters as intermediate layers then the intermediate activation functions should be interpretable as digits and we should be able to interpret the filters as implementing a reliable e g multiplication with carry algorithm looking at the intermediate values may shed some light on why the usually working models fail on e g the pathological cases identified in table the preliminary experiment on input alignment is interesting in two ways the seeds for effective use of an attentional mechanism are there but also it suggests that the model is not presently dealing with general expression evaluation the way a correct algorithm should the remarks in the abstract about improving the memory efficiency of neural gpu seem overblown the paragraph at the top of page describes the improvements as using tf whileloop instead of unrolling the graph and using swapmemory to use host memory when gpu memory runs short these both seem like good practice but not a remarkable improvement to the efficiency of the model in fact it would likely slow down training and inference when memory does in fact fit in the gpu the point about trying many of random seeds to get convergence makes me wonder if the neural gpu is worth its computational cost at all when evaluated as means of learning algorithms that are already well understood e g parsing and evaluating s exprs consider spending all of the computational cycles that go into training one of these models with the multiple seeds on a traditional search through program space e g sampling lisp programs or something the notes on the curriculum strategies employed to get the presented results were interesting to read as an indication of the lengths to which someone might have to go to train this sort of model but it does leave this reviewer with the impression that despite the stated extensions of the neural gpu model it remains unclear how useful it might be to practical problems,5.0
549.json,this paper proposes sparse coding problem with cosine loss and integrated it as a feed forward layer in a neural network as an energy based learning approach the bi directional extension makes the proximal operator equivalent to a certain non linearity crelu although unnecessary the experiments do not show significant improvement against baselines pros minimizing the cosine distance seems useful in many settings where compute inner product between features are required the findings that the bidirectional sparse coding is corresponding to a feed forward net with crelu non linearity cons unrolling sparse coding inference as a feed foward network is not new the class wise encoding makes the algorithm unpractical in multi class cases due to the requirement of sparse coding net for each class it does not show the proposed method could outperform baseslines in real world tasks,5.0
725.json,update because no revision of the paper has been provided by the authors i am reducing my rating to marginally below acceptance this paper addresses the problem of training stochastic feedforward neural networks it proposes to transfer weights from a deterministic deep neural network trained using standard procedures including techniques such as dropout and batch normalization to a stochastic network having the same topology the initial mechanism described for performing the transfer involves a rescaling of unit inputs and layer weights and appropriate specification of the stochastic latent units if the dnn used for pretraining employs relu nonlinearities initial experiments on mnist classification and a toy generative task with a multimodal target distribution show that the simple transfer process works well if the dnn used for pretraining uses sigmoid nonlinearities but not if the pretraining dnn uses relus to tackle this problem the paper introduces the simplified stochastic feedforward neural network in which every stochastic layer is followed by a layer that takes an expectation over samples from its input thus limiting the propagation of stochasticity in the network a modified process for transferring weights from a pretraining dnn to the simplified sfnn is described and justified the training process then occurs in three steps pretrain a dnn transfer weights from the dnn to a simplified sfnn and continue training and optionally transfer the weights to a full sfnn and continue training or transfer them to a deterministic model called dnn and continue training the third step can be skipped and the simplified sfnn may also be used directly as an inference model experimental results on mnist classification show that the use of simplified sfnn training can improve a deterministic dnn model over a dnn baseline trained with batch normalization and dropout experiments on two generative tasks mnist half and the toronto faces database show that the proposed pretraining process improves test set negative log likelihoods finally experiments on cifar cifar and svhn with the lenet network in network and wide residual network architectures show that use of a stochastic training step can improve performance of a deterministic dnn model it is a bit confusing to refer to multi modal tasks when what is meant is generative tasks with a multimodal target distribution because multi modal task can also refer to a learning task that crosses sensory modalities such as audio visual speech recognition text based image retrieval or image captioning i recommend that you use the more precise term generative tasks with a multimodal target distribution early in the introduction and then say that you will refer to such tasks as multi modal tasks in the rest of the paper for the sake of brevity the paper would be easier to read if sfnn were not used to refer to both the singular stochastic feedforward neural network and plural stochastic feedforward neural networks cases when the plural is meant write sfnns in table why does the hidden layer sfnn initialized from a relu dnn have so much worse of a test nll than the hidden layer sfnn initialized from a relu dnn the notation that uses superscripts to indicate layer indexes is confusing the reader naturally parses n² as n squared and not as the number of units in the second layer when you transfer weights back from the simplified sfnn to the dnn model do you need to perform some sort of rescaling that undoes the operations in equation in the paper what does ncsfnn stand for in the supplementary material pros the proposed model is easy to implement and apply to other tasks the mnist results showing that the stochastic model training can produce a deterministic model called dnn in the paper that generalizes better than a dnn trained with batch normalization and dropout is quite exciting cons for the reasons outlined above the paper is at times a bit hard to follow the results cifar cifar and svhn would be more convincing if the baselines used dropout and batch normalization while this is shown on minst demonstration of a similar result on a more challenging task would strengthen the paper minor issues it has been believed that stochastic it is believed that stochastic underlying these successes is on the efficient training methods underlying these successes is efficient training methods necessary in order to model complex stochastic natures in many real world tasks necessary in to model the complex stochastic nature of many real world tasks structured prediction image generation and memory networks memory networks are models not tasks furthermore it has been believed that sfnn furthermore it is believed that sfnn using backpropagation under the variational techniques and the reparameterization tricks using backpropagation with variational techniques and reparameterization tricks there have been several efforts developing efficient training methods there have been several efforts toward developing efficient training methods however training sfnn is still significantly slower than doing dnn however training a sfnn is still significantly slower than training a dnn e g most prior works on this line have considered a consequently most prior works in this area have considered a instead of training sfnn directly instead of training a sfnn directly whether pre trained parameters of dnn whether pre trained parameters from a dnn with further fine tuning of light cost with further low cost fine tuning recent advances in dnn on its design and training recent advances in dnn design and training it is rather believed that transferring parameters it is believed that transferring parameters but the opposite direction is unlikely possible but the opposite is unlikely to address the issues we propose to address these issues we propose which intermediates between sfnn and dnn which is intermediate between sfnn and dnn in forward pass and computing gradients in backward pass in the forward pass and computing gradients in the backward pass in order to handle the issue in forward pass in order to handle the issue in the forward pass neal proposed a gibbs sampling neal proposed gibbs sampling for making dnn and sfnn are equivalent for making the dnn and sfnn equivalent in the case when dnn uses the unbounded relu in the case when the dnn uses the unbounded relu are of relu dnn type due to the gradient vanishing problem are of the relu dnn type because they mitigate the gradient vanishing problem multiple modes in outupt space y multiple modes in output space y the only first hidden layer of dnn only the first hidden layer of the dnn is replaced by stochastic one is replaced by a stochastic layer the former significantly outperforms for the latter for the the former significantly outperforms the latter for the simple parameter transformations from dnn to sfnn are not clear to work in general simple parameter transformations from dnn to sfnn do not clearly work in general is a special form of stochastic neural networks is a special form of stochastic neural network as like the first layer is as in the first layer is this connection naturally leads an efficient training procedure this connection naturally leads to an efficient training procedure,5.0
660.json,as you noted for figure left sometimes it seems sufficient to tune learning rates i see your argument for figure right but not for all good learning rates make adam fail i guess you selected the one where it did note that adam was several times faster than eve in the beginning i do not buy eve always converges because you show it only for and since eve is not adam of adam is not of eve because of dt to my understanding you define dt over time with hyperparameters similarly one can define dt directly the behaviour of dt that you show is not extraordinary and can be parameterized if eve is better than adam then looking at dt we can directly see whether we underestimated or overestimated learning rates you could argue that eve does it automatically but you do tune learning rates for each problem individually anyway,5.0
322.json,this paper addresses the problem of allowing networks to change the number of units that are used during training this is done in a simple but elegant and well motivated way units with zero input or output weights are added or removed during training while a group sparsity norm for regularization is used to encourage unit weights to go to zero the main theoretical contribution is to show that with proper regularization the loss is minimized by a network with a finite number of units in practice this result does not guarantee that the resulting network will not over or under fit the training data but some initial experiments show that this does not seem to be the case one potential advantage of approaches that learn the number of units to use in a network is to ease the burden of tuning hyperparameters one disadvantage of this approach and maybe any such approach is that it does not really solve this problem the network still has several hyperparameters that implicitly control the number of units that will emerge including parameters that control how often new units are added and how rapidly weights may decay to zero it is not clear whether these hyperparameters will be easier or harder to tune than the ones in standard approaches in fairness the authors do not claim that they have made training easier but it is a little disappointing that this does not seem to be the case the authors do emphasize that they are able to train networks that use fewer units to achieve comparable performance to networks trained parametrically this is potentially important because smaller networks can reduce run time at testing and power consumption and memory footprint which is important on mobile devices in particular however the authors do not compare experimentally to existing approaches that attempt to reduce the size of parametrically trained networks eg by pruning trained networks so it is not clear whether this approach is really competitive with the best current approaches to reducing the size of trained networks another potential disadvantage of the proposed approach is that the same hyperparameters control both the number of units that will appear in the network and the training time therefore training might potentially be much slower for this approach than for a parametric approach with fixed hyperparameters in practice many parametric approaches require methods like grid search to choose hyperparameters which can be very slow but in many other cases experience with similar problems can make the choice of hyperparameters relatively easy this means that the cost of grid search is not always paid but the slowness of the authors approach may be endemic the authors do not discuss how this issue will scale as much larger networks are trained it is a concern that this approach may not be practical for large scale networks because training will be very slow in general the experiments are helpful and encouraging but not comprehensive or totally convincing i would want to see experiments on much larger problems before i was convinced that this approach can really be practical or widely useful overall i found this to be an interesting and clearly written paper that makes a potentially useful point the overall vision of building networks that can grow and adapt through life long learning is inspiring and this type of work might be needed to realize such a vision but the current results remain pretty speculative,5.0
334.json,this paper presents a succinct argument that the principle of optimizing receptive field location and size in a simulated eye that can make saccades with respect to a classification error of images of data whose labels depend on variable size and variable location subimages explains the existence of a foveal area in e g the primate retina the argument could be improved by using more realistic image data and drawing more direct correspondence with the number receptive field sizes and eccentricities of retinal cells in e g the macaque but the authors would then face the challenge of identifying a loss function that is both biologically plausible and supportive of their claim the argument could also be improved by commenting on the timescales involved presumably the density of the foveal center depends on the number of of saccades allowed by the inference process as well as the size of the target sub images and also has an impact on the overall classification accuracy why does the classification error rate of dataset remain stubbornly at this seems so high that the model may not be working the way we d like it to it seems that the overall argument of the paper pre supposes that the model can be trained to be a good classifier if there are other training strategies or other models that work better and differently then it raises the question of why do our eyes and visual cortex not work more like those ones if evolutionary pressures are applying the same pressure as our training objective why does the model with zooming powers out do the translation only model on dataset where all target images are the same size and tie the translation only model dataset where the target images have different sizes for which the zooming model should be tailor made between this strange tie and the high classification rate on dataset i wonder if maybe one or both models isn t being trained to its potential which would undermine the overall claim comparing this model to other attention models e g spatial transformer networks draw would be irrelevant to what i take to be the main point of the paper but it would address the potential concerns above that training just didn t go very well or there was some problem with the model parameterization that could be easily fixed,6.0
524.json,the concept of data augmentation in the embedding space is very interesting the method is well presented and also justified on different tasks such as spoken digits and image recognition etc one comments of the comparison is the use of a simple layer mlp as the baseline model throughout all the tasks it not clear whether the gains maintain when a more complex baseline model is used another comment is that the augmented context vectors are used for classification just wondering how does it compare to using the reconstructed inputs and furthermore as in table both input and feature space extrapolation improves the performance whether these two are complementary or not,7.0
318.json,the paper proposes an extension of the gated graph sequence neural network by including in this model the ability to produce complex graph transformations the underlying idea is to propose a method that will be able build modify a graph structure as an internal representation for solving a problem and particularly for solving question answering problems in this paper the author proposes different possible differentiable transformations that will be learned on a training set typically in a supervised fashion where the state of the graph is given at each timestep a particular occurence of the model is presented that takes a sequence as an input a iteratively update an internal graph state to a final prediction and which can be applied for solving qa tasks e g babi with interesting results the approach in this paper is really interesting since the proposed model is able to maintain a representation of its current state as a complex graph but still keeping the property of being differentiable and thus easily learnable through gradient descent techniques it can be seen as a succesfull attempt to mix continuous and symbolic representations it moreover seems more general that the recent attempts made to add some ymbolic stuffs in differentiable models memory networks ntm etc since the shape of the state is not fixed here and can evolve my main concerns is about the way the model is trained i e by providing the state of the graph at each timestep which can be done for particular tasks e g babi only and cannot be the solution for more complex problems my other concern is about the whole content of the paper that would perhaps best fit a journal format and not a conference format making the article still difficult to read due to its density,9.0
573.json,this paper proposes a setting to learn models that will seek information e g by asking question in order to solve a given task they introduce a set of tasks that were designed for that goal they show that it is possible to train models to solve these tasks with reinforcement learning one key motivation for the tasks proposed in this work are the existence of games like q or battleships where an agent needs to ask questions to solve a given task it is quite surprising that the authors do not actually consider these games as potential tasks to explore beside the hangman it is also not completely clear how the tasks have been selected a significant amount of work has been dedicated in the past to understand the property of games like q e g navarro et al and how humans solve them it would interesting to see how the tasks proposed in this work distinguish themselves from the ones studied in the existing literature and how humans would perform on them in particular cohen lake m have recently studied the questions games in their paper searching large hypothesis spaces by asking questions where they both evaluate the performance of humans and computer i believe that this paper would really benefits from a similar study developing the ability of models to actively seek for information to solve a task is a very interesting but challenging problem in this paper all of the tasks require the agent to select a questions from a finite set of clean and informative possibilities this allows a simpler analysis of how a given agent may perform but at the cost of a reducing the level of noise that would appear in more realistic settings this paper also show that by using a relatively standard mix of deep learning models and reinforcement learning they are able to train agents that can solve these tasks in the way it was intended to this validates their empirical setting but also may exhibit some of the limitation of their approach using relatively toy ish settings with perfect information and a fixed number of questions may be too simple while it is interesting to see that their agent are able to perform well on all of their tasks the absence of baselines limit the conclusions we can draw from these experiments for example in the hangman experiment it seems that the frequency based model obtains promising performance it would interesting to see how good are baselines that may use the co occurrence of letters or the frequency of character n grams overall this paper explores a very interesting direction of research and propose a set of promising tasks to test the capability of a model to learn from asking question however the current analysis of the tasks is a bit limited and it is hard to draw any conclusion from them it would be good if the paper would focus more on how humans perform on these tasks on strong simple baselines and on more tasks related to natural language since it is one of the motivation of this work rather than on solving them with relatively sophisticated models,4.0
420.json,this paper focusses on attention for neural language modeling and has two major contributions authors propose to use separate key value and predict vectors for attention mechanism instead of a single vector doing all the functions this is an interesting extension to standard attention mechanism which can be used in other applications as well authors report that very short attention span is sufficient for language models which is not very surprising and propose an n gram rnn which exploits this fact the paper has novel models for neural language modeling and some interesting messages authors have done a thorough experimental analysis of the proposed ideas on a language modeling task and cbt task i am convinced with authors responses for my pre review questions minor comment ba et al reed de freitas and gulcehre et al should be added to the related work section as well,7.0
532.json,final review the writers were very responsive and i agree the reviewer that their experimental setup is not wrong after all and increased the score by one but i still think there is lack of experiments and the results are not conclusive as a reader i am interested in two things either getting a new insight and understanding something better or learn a method for a better performance this paper falls in the category two but fails to prove it with more throughout and rigorous experiments in summary the paper lacks experiments and results are inconclusive and i do not believe the proposed method would be quite useful and hence not a conference level publication the paper proposes to train a policy network along the main network for selecting subset of data during training for achieving faster convergence with less data pros it well written and straightforward to follow the algorithm has been explained clearly cons section mentions that the validation accuracy is used as one of the feature vectors for training the ndf this invalidates the experiments as the training procedure is using some data from the validation set only one dataset has been tested on papers such as this one that claim faster convergence rate should be tested on multiple datasets and network architectures to show consistency of results especially larger datasets as the proposed methods is going to use less training data at each iteration it has to be shown in much larger scaler datasets such as imagenet as discussed more in detail in the pre reviews question if the paper is claiming faster convergence then it has to compare the learning curves with other baselines such adam plain sgd is very unfair comparison as it is almost never used in practice and this is regardless of what is the black box optimizer they use the case could be that adam alone as black box optimizer works as well or better than adam as black box ndf,4.0
476.json,this paper describes a careful experimental study on the cifar task that uses data augmentation and bayesian hyperparameter optimization to train a large number of high quality deep convolutional network classification models from hard targets an ensemble of the best models is then used as a teacher model in the distillation framework where student models are trained to match the averaged logits from the teacher ensemble data augmentation and bayesian hyperparameter optimization is also applied in the training of the student models both non convolutional mlp and convolutional student models of varying depths and parameter counts are trained convolutional models with the same architecture and parameter count as some of the convolutional students are also trained using hard targets and cross entropy loss the experimental results show that convolutional students with only one or two convolutional layers are unable to match the results of students having more convolutional layers under the constraint that the number of parameters in all students is kept constant pros this is a very thorough and well designed study that make use of the best existing tools to try to answer the question of whether or not deep convolutional models need both depth and convolution it builds nicely on the preliminary results in ba caruana cons it is difficult to prove a negative as the authors admit that said this study is as convincing as possible given current theory and practice in deep learning section should state that the logits are unnormalized log probabilities they do not include the log partition function the paper does not follow the iclr citation style quoting from the template when the authors or the publication are included in the sentence the citation should not be in parenthesis as in see hinton et al for more information otherwise the citation should be in parenthesis as in deep learning shows promise to make progress towards ai bengio lecun there are a few minor issues with english usage and typos that should be cleaned up in the final manuscript necessary when training student models with more than convolutional layers necessary when training student models with more than convolutional layer remaining images as validation set remaining images as the validation set evaluate the ensemble s predictions logits on these samples and save all data evaluated the ensemble s predictions logits on these samples and saved all data more detail about hyperparamter optimization more detail about hyperparameter optimization we trained deep cnn models with spearmint we trained deep cnn models with spearmint the best model obtained an accuracy of the fifth best achieved the best model obtained an accuracy of the fifth best achieved the sizes and architectures of three best models the sizes and architectures of the three best models clearly suggests that convolutional is critical clearly suggests that convolution is critical similarly from the hyperparameter opimizer s point of view similarly from the hyperparameter optimizer s point of view,8.0
499.json,a well known limitation in deep neural networks is that the same parameters are typically used for all examples even though different examples have very different characteristics for example recognizing animals will likely require different features than categorizing flowers using different parameters for different types of examples has the potential to greatly reduce underfitting this can be seen in recent results with generative models where image quality is much better for less diverse datasets however it is difficult to use different parameters for different examples because we typically train using minibatches which relies on using the same parameters for all examples in a minibatch i e doing matrix multiplies in a fully connected network the hypernetworks paper cleverly proposes to get around this problem by adapting different parameters for different time steps in recurrent networks and different the basic insight is that a minibatch will always include many different examples from the same time step or spatial position so there is no computational issue involved with using different parameters in this paper the parameters are modified for different positions based on the output from a hypernetwork which conditions on the time step hypothetically this hypernetwork could also condition on other features that are shared by all sequences in the minibatch i expect this method to become standard for training rnns especially where the length of the sequences is the same during the training and testing phases penn treebank is a highly competitive baseline so the sota result reported here is impressive the experiments on convolutional networks are less experimentally impressive i suspect that the authors were aiming to achieve state of the art results here but settled with achieving a reduction in the number of parameters it might even be worthwhile to consider a synthetic experiment where two completely different types of image are appended i e birds on the left and flowers on the right and show that the hypernetwork helps in this situation it may be the case that for convnets the cases where hypernetworks help are very specific for rnns it seems to be the case that explicitly changing the nature of the computation depending on the position in the sequence greatly improves generalization while a usual rnn could learn to store a counter indicating the position in the sequence the hypernetwork could be a more efficient way to add capacity applications to time series forecasting and modeling could be an interesting area for future work,9.0
421.json,this paper presents a novel model for unsupervised segmentation and classification of time series data a recurrent hidden semi markov model is proposed this extends regular hidden semi markov models to include a recurrent neural network rnn for observations each latent class has its own rnn for modeling observations for that category further an efficient training procedure based on a variational approximation experiments demonstrate the effectiveness of the approach for modeling synthetic and real time series data this is an interesting and novel paper the proposed method is a well motivated combination of duration modeling hmms with state of the art observation models based on rnns the combination alleviates shortcomings of standard hsmm variants in terms of the simplicity of the emission probability the method is technically sound and demonstrated to be effective it would be interesting to see how this method compares quantitatively against crf based methods e g ammar dyer and smith nips crfs can model more complex data likelihoods though as noted in the response phase there are still limitations regardless i think the merits of using rnns for the class specific generative models are clear,7.0
525.json,the authors present an online learning method for learning the structure of sum product networks the algorithm assumes gaussian coordinate wise marginal distributions and learns both parameters and structure online the parameters are updated by a recursive procedure that reweights nodes in the network that most contribute to the likelihood of the current data point the structure learning is done by either merging independent product gaussian nodes into multivariate leaf nodes or creating a mixture over the two nodes when the multivariate would be too large the fact that the dataset is scaled to some larger datasets in terms of the number of datapoints is promising although the number of variables is still quite small current benchmarks for tractable continuous density modeling with neural networks include the nice and real nvp families of models which can be scaled to both large numbers of datapoints and variables intractable methods like gan genmmn vae have the same property the main issue with this work for the iclr audience is the use of mainly a set of spn specific datasets that are not used in the deep learning generative modeling literature the use of genmmn as a baseline is also not a good choice to bridge the gap to the neural community as its parzen window based likelihood evaluation is not really meaningful better ways to evaluate the likelihood through annealed importance sampling are discussed in on the quantitative analysis of decoder based generative models by wu et al i would recommend the use of a simple vae type model to get a lower bound on the likelihood or something like real nvp most neural network density models are scalable to large numbers of observations as well as instances and it is not clear that this method scales well horizontally like this evaluating the feasibility of modeling something like mnist would be interesting spns have the strength that not only marginal but also various type of conditional queries are tractable but performance on this is not evaluated or compared one interesting application could be in imputation of unknown pixels or color channels in images for which there is not currently a high performing tractable model despite the disconnect from other iclr generative modeling literature the algorithm here seems simple and intuitive and convincingly works better than the previous state of the art for online spn structure learning i think vae is a much better baseline for continuous data than genmmn when attempting to compare to neural network approaches further the sum product network could actually be combined with such deep latent variable models as an observation model or posterior which could be a very powerful combination i would like it if these spn models were better known by the iclr probabilistic modeling community but i do not know if this paper does enough to make them relevant as with the other reviewers i am not an expert on spns however this seems to be a simple and effective algorithm for online structure induction and the scalability aspect is something that is important in much recent work in the learning of representations i think it is good enough for publication although i would prefer to see many of the above additions to more clearly bridge the gap with other literature in deep generative modeling,6.0
362.json,this paper proposes an approach to learning a custom optimizer for a given class optimization problems i think in the case of training machine learning algorithms a class would represent a model like logistic regression the authors cleverly cast this as a reinforcement learning problem and use guided policy search to train a neural network to map the current location and history onto a step direction magnitude overall i think this is a great idea and a very nice contribution to the fast growing meta learning literature however i think that there are some aspects that could be touched on to make this a stronger paper my first thought is that the authors claim to train the method to learn the regularities of an entire class of optimization problems rather than learning to exploit regularities in a given set of tasks the distinction here is not terribly clear to me for example in learning an optimizer for logistic regression the authors seem to claim that learning on a randomly sampled set of logistic regression problems will allow the model to learn about logistic regression itself i am not convinced of this because there is bias in the randomly sampled data itself from the paper in this case the instances are drawn randomly from two multivariate gaussians with random means and covariances with half drawn from each it seems the optimizer is then trained to optimize instances of logistic regression given this specific family of training inputs and not logistic regression problems in general a simple experiment to prove the method works more generally would be to repeat the existing experiments but where the test instances are drawn from a completely different distribution it would be even more interesting to see how this changes as the test distribution deviates further from the training distribution can the authors comment on the choice of architecture used here why one layer with hidden units and softplus activations specifically why not e g units layers and relus presumably this is to prevent overfitting but given the limited capacity of the network how do these results look when the dimensionality of the input space increases beyond or i would love to see what kind of policy the network learns on e g a d function using a contour plot what do the steps look like on a random problem instance when compared to other hand engineered optimizers overall i think this a really interesting paper with a great methodological contribution my main concern is that the results may be oversold as the problems are still relatively simple and constrained however if the authors can demonstrate that this approach produces robust policies for a very general set of problems then that would be truly spectacular minor notes below section should you be using pit to denote the optimal policy you use pit and pi currently are the problems here considered noiseless that is is the state transition given an action deterministic it would be very interesting to see this on noisy problems,7.0
335.json,this is an page paper plus appendix which presents a mathematical derivation for infomax for an actual neural population with noise the original bell sejnowski infomax framework only considered the no noise case results are shown for natural image patches and the mnist dataset which qualitatively resemble results obtained with other methods this seems like an interesting and potentially more general approach to unsupervised learning however the paper is quite long and it was difficult for me to follow all the twists and turns for example the introduction of the hierarchical model was confusing and it took several iterations to understand where this was going hierarchical is probably not the right terminology here because it not like a deep net hierarchy it just decomposing the tuning curve function into different parts i would recommend that the authors try to condense the paper so that the central message and important steps are conveyed in short order and then put the more complete mathematical development into a supplementary document also the authors should look at the work of karklin simoncelli which is highly related they also use an infomax framework for a noisy neural population to derive on and off cells in the retina and they show the conditions under which orientation selectivity emerges,7.0
636.json,the paper presents an approach for compensating the input activation variance introduced by dropout in a network additionally a practical inference trick of re estimating the batch normalization parameters with dropout turned off before testing the authors very well show how dropout influences the input activation variance and then scale the initial weights accordingly to achieve unit variance which helps in avoiding activation outputs exploding or vanishing it is shown that the presented approach serves as a good initialization technique for deep networks and results in performances op par or slightly better than the existing approaches the limited experimental validation and only small difference in accuracies compared to existing methods makes it difficult to judge the effectiveness of presented approach perhaps observing the statistics of output activations and gradients over training epochs in multiple experiments can better support the argument of stability of the network using proposed approach authors might consider adding some validation for considering the backpropagation variance on multiple occasions comparison is drawn against batch normalization which i believe does much more than a weight initialization technique the presented approach is a good initialization technique just not sure if its better than existing ones,6.0
374.json,summary the paper proposes a gating mechanism to combine word embeddings with character level word representations the gating mechanism uses features associated to a word to decided which word representation is the most useful the fine grain gating is applied as part of systems which seek to solve the task of cloze style reading comprehension question answering and twitter hashtag prediction for the question answering task a fine grained reformulation of gated attention for combining document words and questions is proposed in both tasks the fine grain gating helps to get better accuracy outperforming state of the art methods on the cbt dataset and performing on par with state of the art approach on the squad dataset overall judgment this paper proposes a clever fine grained extension of a scalar gate for combining word representation it is clear and well written it covers all the necessary prior work and compares the proposed method with previous similar models i liked the ablation study that shows quite clearly the impact of individual contributions and i also liked the fact that some shallow linguistic prior knowledge e g pos tags ner tags frequency etc has been used in a clever way it would be interesting to see if syntactic features can be helpful,7.0
661.json,the authors introduce a semi supervised method for neural networks inspired from label propagation the method appears to be exactly the same than the one proposed in weston et al the authors cite the paper the optimized objective function in eq is exactly the same than eq in weston et al as possible novelty the authors propose to use the adjacency matrix as input to the neural network when there are no other features and show success on the blogcatalog dataset experiments on text classification use neighbors according to wordvec average embedding to build the adjacency matrix top reported accuracies are not convincing compared to zhang et al reported performance last experiment is on semantic intent classification which a custom dataset neighbors are also found according to a wordvec metric in summary the paper propose few applications to the original weston et al paper it rebrands the algorithm under a new name and does not bring any scientific novelty and the experimental section lacks existing baselines to be convincing,3.0
661.json,this paper proposes the neural graph machine that adds in graph regularization on neural network hidden representations to improve network learning and take the graph structure into account the proposed model however is almost identical to that of weston et al as the authors have clarified in the answers to the questions there are a few new things that previous work did not do they showed that graph augmented training for a range of different types of networks including ff cnn rnns etc and works on a range of problems graphs help to train better networks e g layer cnn with graphs does as well as than layer cnns graph augmented training works on a variety of different kinds of graphs however all these points mentioned above seems to simply be different applications of the graph augmented training idea and observations made during the applications i think it is therefore not proper to call the proposed model a novel model with a new name neural graph machine but rather making it clear in the paper that this is an empirical study of the model proposed by weston et al to different problems would be more acceptable,4.0
495.json,the main contribution of this paper is a construction to eps approximate a piecewise smooth function with a multilayer neural network that uses o log eps layers and o poly log eps hidden units where the activation functions can be either relu or binary step or any combination of them the paper is well written and clear the arguments and proofs are easy to follow i only have two questions it would be great to have similar results without binary step units to what extent do you find the binary step unit central to the proof is there an example of piecewise smooth function that requires at least poly eps hidden units with a shallow network,7.0
616.json,in this work the authors propose to use a perhaps deterministic retrieval function to replace uniform sampling over the train data in training the discriminator of a gan although i like the basic idea the experiments are very weak there are essentially no quantitative results no real baselines and only a small amount of not especially convincing qualititative results it is honestly hard to review the paper there is not any semblance of normal experimental validation note what is happening with the curves in fig,3.0
616.json,this paper proposes a model that generates a latent representation of input image s and optimizes a reconstruction loss with an adversarial loss eq over nearest neighbors from a bank of images memory the framework is adapted to three tasks i image in painting ii intrinsic image decomposition iii figure ground layer extraction qualitative results are shown for all three tasks i think the proposed model has potential merits i particularly like the fact that it seems to be reasoning over image composites via matching against a bank of images somewhat similar to segmenting scenes by matching image composites work in nips however i won t champion the paper as the overall clarity and evaluation could be improved more detailed comments i believe the fatal flaw of the paper is there is no quantitative evaluation of the approach at the very least there should be a comparison against prior work on intrinsic image decomposition e g sirfs maybe benchmark on intrinsic images in the wild dataset i found the writing vague and confusing throughout for instance memory database could mean a number of things and in the end it seems that it s simply a set of images imagination is also vague on page r m x has the database and input image as arguments but fig doesn t show the input image as an input to r the contributions listed on page should be tightened e g it s not clear what relevant memory retrieval for informative adversarial priors means fig seems inconsistent with fig as the module for memory database is not present the fully convolutional discriminator could use more details one possibility is to provide a cost function,5.0
641.json,the paper proposes two methods for what is called wild variational inference the goal is to obtain samples from the variational approximate distribution q without requiring to evaluate the density q z by which it becomes possible to consider more flexible family of distributions the authors apply the proposed method to the problem of optimizing the hyperparamter of the sgld sampler the experiments are performed on a d mixture of gaussian distribution and bayesian logistic regression tasks the key contribution seems to connect the previous findings in svgd and ksd to the concept of inference networks and to use them for hyperparameter optimization of sgld this can not only be considered as a rather simple connection extension but also the toyish experiments are not enough to convince readers on the significance of the proposed model particularly i am wondering how the particle based methods can deal with the multimodality not the simple d gaussian mixture case in general also the method seems still to require to evaluate the true gradient of the target distribution e g the posterior distribution for each z q this seems to be a computational problem for large dataset settings in the experiments the authors compare the methods for the same number of update steps but considering the light computation of sgld per update i think sgld can make much more updates per unit time than the proposed methods particularly for large datasets the bayesian logistic regression on dimensions seems also a quite simple experiment considering that its posterior is close to a gaussian distribution also including hamiltonian monte carlo hmc with automatic hyperparameter tuning mechanism like no u turn sampler would be interesting the paper is written very unclearly especially it is not clear what is the exact contributions of the paper compared to the other previous works including the authors works the main message is quite simple but most of the pages are spent to explain previous works overall i would like to suggest to have more significant high dimension large scale experiments and to improve the writing,3.0
641.json,the authors propose two variational methods based on the theme of posterior approximations which may not have a tractable density the first is from another iclr submission on amortized svgd wang and liu where here the innovation is in using sgld as the inference network the second is from a nips paper ranganath et al on minimizing the stein divergence with a parametric approximating family where here the innovation is in defining their test functions to be an rkhs obtaining an analytic solution to the inner optimization problem the methodology is incremental everything up to section is essentially motivation background or related work the notion of a wild variational approximation was already defined in ranganath et al termed a variational program it would be useful for the authors to comment on the difference if any section is at first interesting because it analytically solves the maximum problem that is faced in ranganath et al however this requires use of a kernel which will certainly not scale in high dimensions so it is then equivalent in practice to having chosen a very simple test function family to properly scale to high dimensions would require a deeper kernel and also learning its parameters this is not any easier than parameterizing the test function family as a neural network to begin with which ranganath et al do section introduces a langevin inference network which essentially chooses the variational approximation as an evolving sequence of markov transition operators as in salimans et al i had trouble understanding this for a while because i could not understand what they mean by inference network none of it is amortized in the usual inference network sense which is that the parameters are given by the output of a neural network here the authors simple define global parameters of the sgld chain which are used across all the latent variables which is strictly worse what then makes it an inference network is this not the variational approximation used in salimans et al but using a different objective to train it the experiments are limited on a toy mixture of gaussians posterior and bayesian logistic regression none of this addresses the problems one might suspect on high dimensional and real data such as the lack of scalability for the kernel the comparison to salimans et al for the langevin variational approximation and any note of runtime or difficulty of training minor comments it not clear if the authors understood previous work on expressive variational families or inference networks for example they argue rezende mohamed b tran et al ranganath et al require handcrafted inference networks however all of them assume use of any neural network for amortized inference none of them even require an inference network perhaps the authors mean handcrafted posterior approximations which to some extent is true however the three mentioned are all algorithmic in nature in rezende mohamed the main decision choice is the flow length tran et al the size of the variational data ranganath et al the flow length on the auxiliary variable space each works well on different problems but this is also true of variational objectives which admit intractable q as the latter two consider as does salimans et al the paper motivation could be better explained and perhaps the authors could be clearer on what they mean by inference network i also recommend the authors not term a variational inference method based on the class of approximating family while black box variational inference in ranganath et al assumes a mean field family the term itself has been used in the literature to mean any variational method that imposes few constraints on the model class,3.0
354.json,i do not have much to add to my pre review questions the main thing i would like to see that would strengthen my review further is a larger scale evaluation more discussion of the hyperparameters etc where test error are reported for snapshot ensembles it would be useful to report statistics about the performance of individual ensemble members for comparison mean and standard deviation maybe best single member error rate,7.0
712.json,the authors propose a recurrent neural network approach for constructing a stochastic volatility model for financial time series they introduce an inference network based on a recurrent neural network that computes the approximation to the posterior distribution for the latent variables given the past data this variational approximation is used to maximize the marginal likelihood in order to learn the parameters of the model the proposed method is validated in experiments with synthetic and real world time series showing to outperform parametric garch models and a gaussian process volatility model quality the method proposed seems technically correct with the exception that in equation the inference model is doing filtering and not smoothing in the sense that the posterior for zt only depends on those other zt and xt values with t,5.0
342.json,the work combines variational recurrent neural networks and adversarial neural networks to handle domain adaptation for time series data the proposed method along with several competing algorithms are compared on two healthcare datasets constructed from mimic iii in domain adaptation settings the new contribution of the work is relatively small it extends vrnn with adversarial training for learning domain agnostic representations from the experimental results the proposed method clearly out performs competing algorithms however it is not clear where the advantage is coming from the only difference between the proposed method and r dann is using variational rnn vs rnn little insights were provided on how this could bring such a big difference in terms of performance and the drastic difference in the temporal dependencies captured by these two methods in figure detailed comments please provide more details on what is plotted in figure is b is the t sne projection of representations learned by dann or r dann the text in section suggests it s the later case it is surprising to see such a regular plot for vrada what do you think are the two dominant latent factors encoded in figure c in table the two baselines have quite significant difference in performance testing on the entire target including validation set vs on the test set only vrada on the other hand performs almost identical in these two settings could you please offer some explanation on this please explain figure and in more details how to interpret the x axis of figure and the x and y axes of figure again the right two plots in figure are extremely regular comparing to the ones on the left,5.0
342.json,update i thank the authors for their comments after reading them i still think the paper is not novel enough so i am leaving the rating untouched this paper proposes a domain adaptation technique for time series the core of the approach is a combination of variational recurrent neural networks and adversarial domain adaptation at the last time step pros the authors consider a very important application of domain adaptation the paper is well written and relatively easy to read solid empirical evaluation the authors compare their method against several recent domain adaptation techniques on a number of datasets cons the novelty of the approach is relatively low it s just a straightforward fusion of the existing techniques the paper lacks any motivation for use of the particular combination vrnn and revgrad i still believe comparable results can be obtained by polishing r dann e g carefully penalizing domain discrepancy at every step additional comments i m not convinced by the discussion presented in section i don t think the visualization of firing patterns can be used to support the efficiency of the proposed method figure c looks very suspicious i can hardly believe t sne could produce this very regular structure for non degenerate non synthetic real world data overall it s a solid paper but i m not sure if it is up to the iclr standard,6.0
529.json,this paper uses a combination of likelihood and reward based learning to learn sequence models for music the ability to combine likelihood and reward based learning has been long known as a result of the unification of inference and learning first appearing in the ml literature with the em formalism of attias for fixed horizons extended by toussaint and storkey to general horizon settings toussaint et al to pomdps and generalised further by kappen et al and rawlik et al these papers introduced the basic unification and so any additional probabilistic or data driven objective can be combined with the reinforcement learning signal it is all part of a unified reward likelihood hence the optimal control target under unification is p b tau ep a s prodt pi at st i e the probability of getting reward and probability of the policy actions under the known data derived distribution thereby introducing the log p at st into too the interpretation of the secondary objective as the prior is an alternative approach under a stochastic optimal control setting but not the most natural one given the whole principle of soc of matching control objectives to inference objectives the soc off policy objective still does still contain the kl term so the approach would still differ from the approach of this paper though the discussion of optimal control is good i think some further elaboration of the history and how reward augmentation can work in soc would be valuable this would allow soc off policy methods to be compared with the dqn directly like for like the motivation of the objective is sensible but could be made clearer via the unification argument above then the paper uses dcn to take a different approach from the variational soc for achieving that objective another interesting point of discussion is the choice of epi log p at st this means the policy must cover the model but one problem in generation is that a well trained model is often underfit resulting in actions that over the course of a number of iterations move the state into data unsupported parts of the space as a result the model is no longer confident and quickly tends to be fairly random this approach as opposed to a kl p pi which is not obvious how to implement cannot mitigate against that without a very strong signal to overcome the tails of a distribution in music with a smaller discrete alphabet this is likely to be less of a problem than for real valued policy densities with exponentially decaying tails some further discussion of what you see in light of this issue would be valuable the use of c to balance things seems critical and it seems clear from figure that the reward signal needed to be very high to push the log p signal into the right range altogether in the music setting this paper provides a reasonable demonstration that augmentation of a sequence model with an additional reward constraint is valuable it demonstrates that dqn is one way of learning that signal but afaics it does not compare learning the same signal via other techniques instead for the comparator techniques it reverts to treating the p a s as a prior term rather than a reward term leaving a bit of a question as to whether dqn is particularly appropriate another interesting question for the discussion is whether the music theory reward could be approximated by a differentiable model mitigating the need for an rl approach at all,5.0
600.json,the paper proposes the group sparse autoencoder that enforces sparsity of the hidden representation group wise where the group is formed based on labels i e supervision the p th group hidden representation is used for reconstruction with group sparsity penalty allowing learning more discriminative class specific patterns in the dataset the paper also propose to combine both group level and individual level sparsity as in equation clarity of the paper is a bit low do you use only p th group activation for reconstruction if it is true then for equation do you use all individual hidden representation for reconstruction or still using the subset of representation corresponding to that class only in equation rhs misses the summation over p and wondering it is a simple typo is the algorithm end to end trainable it seems to me that the group sparse cnn is no more than the gsa whose input data is the feature extracted from sequential cnns or any other pretrained cnns other comments are as follows furthermore the group sparse autoencoder is semi supervised method since it uses label information to form a group whereas the standard sparse autoencoder is fully unsupervised that being said it is not surprising that group sparse autoencoder learns more class specific pattern whereas sparse autoencoder does not i think the fair comparison should be to autoencoders that combines classification for their objective function although authors claim that gsa learns more group relevant features figure b is not convincing enough to support this claim for example the first row contains many filters that does not look like e g very last column looks like other than visual inspection do you observe improvement in classification using proposed algorithm on mnist experiments the comparison to the baseline model is missing i believe the baseline model should not be the sequential cnn but the sequential cnn sparse autoencoder in addition more control experiment is required that compares between the equation with different values of alpha and beta missing reference shang et al discriminative training of structured dictionaries via block orthogonal matching pursuit sdm they consider block orthgonal matching pursuit for dictionary learning whose blocks i e projection matrices are constructed based on the class labels for discirminative training,4.0
552.json,my main objection with this work is that it operates under a hypothesis that is becoming more and more popular in the literature that all we need is to have gradients flow in order to solve long term dependency problems the usual approach is then to enforce orthogonal matrices which in absence of the nonlinearity results in unitary jacobians hence the gradients do not vanish and do not explode however this hypothesis is taken for granted and we do not know it is true yet and instead of synthetic data we do not have any empirical evidence that is strong enough to convince us the hypothesis is true my own issues with this way of thinking is a what about representational power restricting to orthogonal matrices it means we can not represent the same family of functions as before e g we can not have complex attractors and so forth if we run the model forward without any inputs you can only get those if you have eigenvalues larger than it also becomes really hard to deal with noise since you attempt to preserve every detail of the input or rather every part of the input affects the output ideally you would want to preserve only what you need for the task given limited capacity but you can not learn to do that my issue is that everyone is focused on solving this preserved issue without worrying of the side effects i would like one of these papers going for jacobians having eigenvalues of show this helps in realistic scenarios on complex datasets,4.0
381.json,this paper presents a novel way of pruning filters from convolutional neural networks with a strong theoretical justification the proposed methods is derived from the first order taylor expansion of the loss change while pruning a particular unit this leads to simple weighting of the unit activation with its gradient w r t loss function and performs better than simply using the activation magnitude as the heuristic for pruning this intuitively makes sense as we would like to remove not only the filters with low activation but also filters where the incorrect activation value would not have small influence on the target loss authors thoroughly investigate multiple baselines including an oracle which sets an upper bound on the target performance even though it is computationally expensive the devised method seems to be quite elegant and authors show that it generalizes well on multiple tasks and is computationally more than feasible as it is easy to combine with traditional fine tuning procedure also the work clearly shows the trade offs of increased speed and decreased performance which is useful for practical applications it would be also useful to compare against different baselines e g however this method seems to be more useful as it does not involve training of a new network and thus is probably much faster suggestion maybe it can be extended in the future towards also removing only parts of the filters e g for the d convolution this may be more complicated as it would need to change the implementation of convolution operator but can lead to further speedup,9.0
440.json,the authors propose a novel way of using bayesian nns for policy search in stochastic dynamical systems specifically the authors minimize alpha divergence with alpha as opposed to standard vb the authors claim that their method is the first model based system to solve a year old benchmark problem i am not very familiar with this literature so it difficult for me to assess this claim the paper seems technically sound i feel the writing could be improved the notation in sections feels a bit dense and there are a lot of terminology approximations introduced which makes it hard to follow the writing could be better structured to distinguish between novel contributions vs review of prior work if i understand section correctly it mostly a review of black box alpha divergence minimization if so it would probably make sense to move this to the appendix there was a paper at nips showing promising results using sghmc for bayesian optimization bayesian optimization with robust bayesian neural networks by springenberg et al could you comment on applicability of stochastic gradient mcmc sgld sghmc for your setup can you comment on the computational complexity of the different approaches section why can not you use the original data in what sense is it fair to simulate data using another neural network can you evaluate pso p on this problem,6.0
401.json,in this paper the authors use a separate introspection neural network to predict the future value of the weights directly from their past history the introspection network is trained on the parameter progressions collected from training separate set of meta learning models using a typical optimizer e g sgd pros the organization is generally very clear novel meta learning approach that is different than the previous learning to learn approach cons the paper will benefit from more thorough experiments on other neural network architectures where the geometry of the parameter space are sufficiently different than cnns such as fully connected and recurrent neural networks neither mnist nor cifar experimental section explained the architectural details mini batch size for the experiments were not included in the paper comparison with different baseline optimizer such as adam would be a strong addition or at least explain how the hyper parameters such as learning rate and momentum are chosen for the baseline sgd method overall due to the omission of the experimental details in the current revision it is hard to draw any conclusive insight about the proposed method,7.0
397.json,this paper proposes a variational autoencoder model that can discard information found irrelevant in order to learn interesting global representations of the data this can be seen as a lossy compression algorithm hence the name variational lossy autoencoder to achieve such model the authors combine vaes with neural autoregressive models resulting in a model that has both a latent variable structure and a powerful recurrence structure the authors first present an insightful bits back interpretation of vae to show when and how the latent code is ignored as it was also mentioned in the literature they say that the autoregressive part of the model ends up explaining all structure in the data while the latent variables are not used then they propose two complementary approaches to force the latent variables to be used by the decoder the first one is to make sure the autoregressive decoder only uses small local receptive field so the model has to use the latent code to learn long range dependency the second is to parametrize the prior distribution over the latent code with an autoregressive model they also report new state of the art results on binarized mnist both dynamical and statically binarization omniglot and caltech silhouettes review the bits back interpretation of vae is a nice contribution to the community having novel interpretations for a model helps to better understand it and sometimes like in this paper highlights how it can be improved having a fine grained control over the kind of information that gets included in the learned representation can be useful for a lot of applications for instance in image retrieval such learned representation could be used to retrieve objects that have similar shape no matter what texture they have however the authors say they propose two complementary classes of improvements to vae that is the lossy code via explicit information placement section and learning the prior with autoregressive flow section however they never actually showed how a vae without af prior but that has a pixelcnn decoder performs what would be the impact on the latent code is no af prior is used also it is not clear if windowaround i represents only a subset of x,7.0
373.json,the authors propose transfer learning variants for neural net based models applied to a bunch of nlp tagging tasks the field of multi tasking is huge and the approaches proposed here do not seem to be very novel in terms of machine learning parts of a general architecture for nlp are shared the amount of shared layers being dependent of the task of interest the novelty lies in the type of architecture which is used in the particular setup of nlp tagging tasks the experimental results show that the approach seems to work well when there is not much labeled data available figure table show some limited improvement at full scale figure results are debatable though it seems the authors fixed the architecture size while varying the amount of labeled data it is very likely that tuning the architecture for each size would have led to better results overall while the paper reads well the novelty seems a bit limited and the experimental section seems a bit disappointing,5.0
373.json,authors response well answered my questions thanks evaluation not changed this paper proposes a hierarchical framework of transfer learning for sequence tagging which is expected to help the target task with the source task by sharing as many levels of representation as possible it is a general framework for various neural models the paper has extensive and solid experiments and the performance is competitive with the state of the art on multiple benchmark datasets the framework is clear by itself except that more details about training procedure i e sec need to be added the experimental results show that for some task pairs s t this framework can help low resource target task t and the improvement increases with more levels of representations can be shared firstly i suggest that the terms source and target should be more precisely defined in the current framework because due to sec the s and t in each pair are sort of interchangeable that is either of them can be the source or target task especially when p x s p x t is used in the task sampling the difference is one is low resourced and the other is not thus it could be thought of as multi tasking between tasks with imbalanced resource so one question is does this framework simultaneously help both tasks in the pair by learning more generalizable representations for different domains applications languages or is it mostly likely to only help the low resourced one does it come with sacrifice on the high resourced side secondly as the paper shows that the low resourced tasks are improved for the selected task pairs it would also be interesting and helpful to know how often this could happen that is when the tasks are randomly paired one chosen from a low resource pool and the other from a high resource pool how often could this framework help the low resourced one moreover the choice of t a t b t c lies intuitively in how many levels of representation could be shared as possible this implicitly assumes share more help more although i tend to believe so it would be interesting to have some empirical comparison for example one could perhaps select some cross domain pair and see if t a t b t c on such pairs as mentioned in the author s answer to the pre review question in general i think this is a solid paper and more exploration could be done in this direction so i tend to accept this paper,7.0
324.json,the idea of pruning where it matters is great the authors do a very good job of thinking it through and taking to the next level by studying pruning across different layers too extra points for clarity of the description and good pictures even more extra points for actually specifying what spaces are which layers are mapping into which mathbb symbol two thumbs up the experiments are well done and the results are encouraging of course more experiments would be even nicer but is it ever not the case my question issue is the proposed pruning criterion proposed yes pruning on the filter level is what in my opinion is the way to go but i would be curious how the min sum of weights criterion compares to other approaches how does it compare to other pruning criteria is it better than pruning at random overall i liked the paper,7.0
324.json,this paper proposes a very simple idea prune low weight filters from convnets in order to reduce flops and memory consumption the proposed method is experimented on with vgg and resnets on cifar and imagenet pros creates structured sparsity which automatically improves performance without changing the underlying convolution implementation very simple to implement cons no evaluation of how pruning impacts transfer learning i am generally positive about this work while the main idea is almost trivial i am not aware of any other papers that propose exactly the same idea and show a good set of experimental results therefore i am inclined to accept it the only major downside is that the paper does not evaluate the impact of filter pruning on transfer learning for example there is not much interest in the tasks of cifar or even imagenet instead the main interest in both academia and industry is the value of the learned representation for transferring to other tasks one might expect filter pruning or any other kind of pruning to harm transfer learning it possible that the while the main task has about the same performance transfer learning is strongly hurt this paper has missed an opportunity to explore that direction nit fig title says vgg in b and vggbn in c are these the same models,7.0
332.json,on one hand this paper is fairly standard in that it uses deep metric learning with a siamese architecture on the other the connections to human perception involving persistence is quite interesting i am not an expert in human vision but the comparison in general and the induced hierarchical groupings in particular seem like something that should interest people in this community the experimental suite is ok but i was disappointed that it is synthetic the authors could have used a minimally viable real dataset such as aloi,7.0
627.json,i have problems understanding the motivation of this paper the authors claimed to have captured a latent representation of text and image during training and can translate better without images at test time but did not demonstrate convincingly that images help not to mention the setup is a bit strange when there are no images at test time what i see are only speculative comments we observed some gains so these should come from our image models the qualitative analysis does not convince me that the models have learned latent representations i am guessing the gains are due to less overfitting because of the participation of images during training the dataset is too small to experiment with nmt i am not sure if it fair to compare their models with nmt and vnmt given the following description in section vnmt is fine tuned by nmt and our models are fine tuned with vnmt there should be more explanation on this besides i have problems with the presentation of this paper a there are many symbols being used unnecessary for example f g are used for x source and y target in section b the symbol is not being used in a consistent manner making it sometimes hard to follow the paper for example in section there are references about h  pi obtained from eq which is about h pi yes i understand what the authors mean but there can be better ways to present that c i am not sure if it correct in section h z is computed from mu and sigma so how mu and sigma are being used d g o avg should be something like g o avg the minus sign makes it looks like there an ablation test there similarly for other symbols other things no explanations for figure there a missing pi symbol in appendix a before the kl derivation,3.0
559.json,the paper is an extension of the matching networks by vinyals et al in nips instead of using all the examples in the support set during test the method represents each class by the mean of its learned embeddings the training procedure and experimental setting are very similar to the original matching networks i am not completely sure about its advantages over the original matching networks it seems to me when dealing with shot case these two methods are identical since there is only one example seen in this class so the mean of the embedding is the embedding itself when dealing with shot case original matching networks compute the weighted average of all examples but it is at most x cost the experimental results reported for prototypical nets are only slightly better than matching networks i think it is a simple straightforward novel extension but i am not fully convinced its advantages,5.0
735.json,this paper considers an alternate formulation of kernel pca with rank constraints incorporated as a regularization term in the objective the writing is not clear the focus keeps shifting from estimating causal factors to nonlinear dimensionality reduction to kernel pca to ill posed inverse problems the problem reformulation of kernel pca uses somewhat standard tricks and it is not clear what are the advantages of the proposed approach over the existing methods as there is no theoretical analysis of the overall approach or empirical comparison with existing state of the art not sure what the authors mean by causal factors there is a reference to it in abstract and in problem formulation on page without any definition discussion in kpca i am not sure why one is interested in step iii outlined on page of finding a pre image for each authors outline two key disadvantages of the existing kpca approach the first one that of low dimensional manifold assumption not holding exactly has received lots of attention in the machine learning literature it is common to assume that the data lies near a low dimensional manifold rather than on a low dimensional manifold second disadvantage is somewhat unclear as finding a data point pre image corresponding to each projection in the input space is not a standard step in kpca on page you never define mathcal x times n mathcal y times n mathcal h times n clearly they cannot be cartesian products i have to assume that notation somehow implies n tuples on page section mathcal x and mathcal y are sets what do you mean by mathcal y ll mathcal x on page mathcal s n is never defined experiments none of the standard algorithms for matrix completion such as optspace or svt were considered experiments there is no comparison with alternate existing approaches for non rigid structure from motion proof of the main result theorem to get from to using the holder inequality as stated one would end up with a term that involves sum of fourth powers of weights w ij why would they equal to one using the orthonormal constraints it would be useful to give more details here as i don t see how the argument goes through at this point,3.0
365.json,the method proposes to compress the weight matrices of deep networks using a new density diversity penalty together with a computing trick sorting weights to make computation affordable and a strategy of tying weights this density diversity penalty consists of an added cost corresponding to the l norm of the weights density and the l norm of all the pairwise differences in a layer regularly the most frequent value in the weight matrix is set to zero to encourage sparsity as weights collapse to the same values with the diversity penalty they are tied together and then updated using the averaged gradient the training process then alternates between training with the density diversity penalty and untied weights and training without this penalty but with tied weights the experiments on two datasets mnist for vision and timit for speech shows that the method achieves very good compression rates without loss of performance the paper is presented very clearly presents very interesting ideas and seems to be state of the art for compression the approach opens many new avenues of research and the strategy of weight tying may be of great interest outside of the compression domain to learn regularities in data the result tables are a bit confusing unfortunately minor issues p english mistake while networks that consist of convolutional layers p p table are confusing compared to the baseline dc your method dp seems to perform worse in table overall table overall fc table overall dp is less sparse and more diverse than the dc baseline this would suggest a worse compression rate for dp and is inconsistent with the text which says they should be similar or better i assume the sparsity value is inverted and that you in fact report the number of non modal values as a fraction of the total,9.0
365.json,this work introduces a number of techniques to compress fully connected neural networks while maintaining similar performance including a density diversity penalty and associated training algorithm the core technique of this paper is to explicitly penalize both the overall magnitude of the weights as well as diversity between weights this approach results in sparse weight matrices comprised of relatively few unique values despite introducing a more efficient means of computing the gradient with respect to the diversity penalty the authors still find it necessary to apply the penalty with some low probability per mini batch the approach achieves impressive compression of fully connected layers with relatively little loss of accuracy i wonder if the cost of having to sort weights even for only or out of mini batches might make this method intractable for larger networks perhaps the sparsity could help remove some of this cost i think the biggest fault this paper has is the number of different things going on in the approach that are not well explored independently sparse initialization weight tying probabilistic application of density diversity penalty and setting the mode to and alternating schedule between weight tied standard training and diversity penalty training the authors do not provide enough discussion of the relative importance of these parts furthermore the only quantitative metric shown is the compression rate which is a function of both sparsity and diversity such that they cannot be compared on their own i would really like to see how each component of the algorithm affects diversity sparsity and overall compression a quick verification section claims the density diversity penalty is applied with a fixed probability per batch while implies structured phases alternating between application of density diversity and weight tied standard cross entropy is this scheme in only applying the density diversity penalty probabilistically when it is in the density diversity phase preliminary rating i think this is an interesting paper but lacks sufficient empirical evaluation of its many components as a result the algorithm has the appearance of a collection of tricks that in the end result in good performance without fully explaining why it is effective minor notes please resize equation to fit within the margins resizebox columnwidth blah works well in latex for this,6.0
467.json,this is a parallel work with ali the idea is using auto encoder to provide extra information for discriminator this approach seems is promising from reported result for feature learning part of bigan there still is a lot of space to improve compare to standard supervised convnet,7.0
575.json,the authors propose to combine a cca objective with a downstream loss this is a really nice and natural idea however both the execution and presentation leave a lot to be desired in the current version of the paper it is not clear what the overall objective is this was asked in a pre review question but the answer did not fully clarify it for me is it the sum of the cca objective and the final top layer objective including the cca constraints is there some interpolation of the two objectives by saying that the top layer objective is cosine distance or squared cosine distance do you really mean you are just minimizing this distance between the matched pairs in the two views if so then of course that does not work out of the box without the intervening cca layer you could minimize it by setting all of the projections to a single point a better comparison would be against a contrastive loss like the hermann blunsom one mentioned in the reviewer question which aims to both minimize the distance for matched pairs and separate mismatched ones where mismatched ones can be uniformly drawn or picked in some cleverer way but other discriminative top layer objectives that are tailored to a downstream task could make sense there is some loose terminology in the paper the authors refer to the correlation and cross correlation between two vectors correlation normally applies to scalars so you need to define what you mean here cross correlation typically refers to time series in eq you are taking the max of a matrix finally i am not too sure in what way this approach is fully differentiable while regular cca is not perhaps it is worth revisiting this term as well also just a small note about the relationship between cosine distance and correlation they are related when we view the dimensions of each of the two vectors as samples of a single random variable in that case the cosine distance of the mean normalized vectors is the same as the correlation between the two corresponding random variables in cca we are viewing each dimension of the vectors as its own random variable so i fear the claim about cosine distance and correlation is a bit of a red herring here a couple of typos prosed proposed allong along,3.0
426.json,this paper discusses aligning word vectors across language when those embeddings have been learned independently in monolingual settings there are reasonable scenarios in which such a strategy could come in helpful so i feel this paper addresses an interesting problem the paper is mostly well executed but somewhat lacks in evaluation it would have been nice if a stronger downstream task had been attempted the inverted softmax idea is very nice a few minor issues that ought to be addressed in a published version of this paper there is no mention of haghighi et al learning bilingual lexicons from monolingual corpora which strikes me as a key piece of prior work regarding the use of cca in learning bilingual alignment this paper and links to the work here ought to be discussed likewise hermann blunsom multilingual distributed representations without word alignment is probably the correct paper to cite for learning multilingual word embeddings from multilingual aligned data it would have been nicer if experiments had been performed with more divergent language pairs rather than just european romance languages a lot of the argumentation around the orthogonality requirements feels related to the idea of using a mahalanobis distance covar matrix to learn such mappings this might be worth including in the discussion i do not have a better suggestion but is there an alternative to using the term translation performance etc when discussing word alignment across languages translation implies something more complex than this in my mind the mikolov citation in the abstract is messed up,7.0
308.json,this is a strong submission regarding one of the most important and recently introduced methods in neural networks generative adversarial networks the authors analyze theoretically the convergence of gans and discuss the stability of gans both are very important to the best of my knowledge this is one of the first theoretical papers about gans and the paper contrary to most of the submissions in the field actually provides deep theoretical insight into this architecture the stability issues regarding gans are extremely important since the first proposed versions of gans architecture were very unstable and did not work well in practice theorems are novel and introduces mathematical techniques are interesting i have some technical questions regarding the proof of theorem but these are pretty minor,10.0
685.json,this paper proposes an extension of neural network language nlm models to better handle large vocabularies the main idea is to obtain word embeddings by combining character level embeddings with a convolutional network the authors compare word embeddings we character embeddings ce as well a combined character and word embeddings cwe it quite obvious how ce or cwe embeddings can be used at the input of an nlm but this is more tricky at the output layer the authors propose to use nce to handle this problem nce allows to speed up training but has no impact on inference during testing the full softmax output layer must be calculated and normalized which can be very costly it was not clear to me how the network is used during testing with an open vocabulary since the nlm is only used during reranking the unnormalized probability of the requested word could be obtained at the output however when reranking n best lists with the nlm feature different sentences are compared and i wonder whether this does work well without proper normalization in addition the authors provide perplexities in table and figures and this needs normalization but it is not clear to me how this was performed the authors mention a k output vocabulary i doubt that the softmax was calculated over k values please explain the model is evaluated by reranking n best lists of an smt systems for the iwslt en cz task in the abstract the authors mention a gain of bleu i do not agree with this claim a vanilla word based nlm i e a well known model achieves already a gain of bleu therefore the new model proposed in this paper brings only an additional improvement of bleu this is not statistically significant i conjecture that a similar variation could be obtained by just training several models with different initializations etc unfortunately the nlm models which use a character representation at the output do not work well there are already several works which use some form of character level representations at the input could you please discuss the computational complexity during training and inference minor comments figure and have the caption figure this is misleading the format of the citations is unusual eg while the use of subword units botha blunsom while the use of subword units botha blunsom,3.0
685.json,in this submission an interesting approach to character based language modeling is pursued that retains word level representations both in the context and optionally also in the output however the approach is not new cf kim et al as cited in the submission as well as jozefowicz et al both kim and jozefowicz already go beyond this submission by applying the approach using rnns lstms also jozefowicz et al provide a comparative discussion of different approaches to character level modeling which i am missing here at least by discussing this existing work the remaining novelty of the approach then would be its application to machine translation although it remains somewhat unclear inhowfar reranking of n best lists can handle the oov problem the translation related part of the ovv problem should be elaborated here that said some of the claims of this submission seems somewhat exaggerated like the statement in sec making the notion of vocabulary obsolete whereas the authors e g express doubts concerning the interpretation of perplexity w o an explicit output vocabulary for example modeling of especially frequent word forms still can be expected to contribute as shown in e g arxiv sec you claim that the objective requires a finite vocabulary this statement only is correct if the units considered are limited to full word forms however using subwords and even individual characters implicitly larger and even infinite vocabularies can be covered with the log likelihood criterion even though this require a model different from the one proposed here the corresponding statement should qualified in this respect the way character embeddings are used for the output should be clarified the description in sec is not explicit enough in my view concerning the configuration of nce it would be desirable to get a better idea of how you arrived at your specific configuration and parameterization described in sec sec you might want to mention that kim et al came to similar conclusions w r t the performance of using character embeddings at the output and discuss the suggestions for possible improvements given therein sec there are ways to calculate and interpret perplexity for unknown words cf shaik et al iwslt sec and table the size of the full training vocabulary should be provided here minor comments p bottom three different input layer three different input layers plural fig fonts within the figure are way too small p first item below fig that we will note we that we will denote we sec the parameters estimation the parameter estimation or the parameters estimation p first paragraph in factored way in a factored way p second paragraph a n best list a nk best list an n best list an nk best list sec last sentence despite adaptive gradient verb and article missing,4.0
390.json,this paper introduces an approach to reinforcement learning and control wherein rather than training a single controller to perform a task a metacontroller with access to a base level controller and a number of accessory experts is utilized the job of the metacontroller is to decide how many times to call the controller and the experts and which expert to invoke at which iteration the controller is a bit special in that in addition to being provided the current state it is given a summary of the history of previous calls to itself and previous experts the sequence of controls and expert advice is embedded into a fixed size vector through an lstm the method is tested on an n body control task where it is shown that there are benefits to multiple iterations pondering even for simple experts and that the metacontroller can deliver accuracy and computational cost benefits over fixed iteration controls the paper is in general well written and reasonably easy to follow as the authors note the topic of metareasoning has been studied to some extent in ai but its use as a differentiable and fully trainable component within an rl system appears new at this stage it is difficult to evaluate the impact of this kind of approach the overall model architecture is intriguing and probably merits publication but whether and how this will scale to other domains remains the subject of future work the experimental validation is interesting and well carried out but remains of limited scope moreover given such a complex architecture there should be a discussion of the training difficulties and convergence issues if any here are a few specific comments questions and suggestions in figure a the meaning of the graphical language should be explained for instance there are arrows of different thickness and line style do these mean different things in figure the caption should better explain the contents of the figure for example what do the colours of the different lines refer to also in the top row there are dots and error bars that are given but this is explained only in the bottom row part this makes understanding this figure difficult in figure the shaded area represents a confidence interval on the regression line in addition it would be helpful to give a standard error on the regression slope to verify that it excludes zero i e the slope is significant as well as a fraction of explained variance r in figure the fraction of samples using the mlp expert does not appear to decrease monotonically with the increasing cost of the mlp expert i e the bottom left part of the right plot with a few red shaded boxes why is that is there lots of variance in these fractions from experiment to experiment the supplementary materials are very helpful thank you for all these details,7.0
543.json,this paper presents a javascript framework including webcl components for training and deploying deep neural networks the authors show that it is possible to reach competitive speeds with this technology even higher speed than a compiled application with viennacl on amd gpus while remaining a little more than factor three slower than compiled high performance software on nvidia gpus it offers compelling possibilities for easily deployable training and application settings for deep learning my main points of criticism are in tab different batch sizes are used even if this is due to technical limits for the javascript library it would only be fair to use the smaller batch sizes for the other frameworks as well on the gpus probably in favor of the presented framework in fig why not include more information in the graphs especially as stated in the question why not include the node js values while i do see the possible application with one server and many low performance clients the setting of having a few dedicated high performance servers is quite likely even if not these are good values to compare with for the sake of consistency please include in both subfigures firefox chrome node js apart from these points well written understandable and conclusive,7.0
406.json,this paper explores ensemble optimisation in the context of policy gradient training ensemble training has been a low hanging fruit for many years in the this space and this paper finally touches on this interesting subject the paper is well written and accessible in particular the questions posed in section are well posed and interesting that said the paper does have some very weak points most obviously that all of its results are for a very particular choice of domain parameters i eagerly look forward to the journal version where these experiments are repeated for all sorts of source domain target domain parameter combinations,8.0
514.json,pros part of the paper addresses an industrially important topic namely how to make deep networks work properly on point clouds i e in many most potential applications they should be invariant to permutations of the points within the cloud as well as rigid transformations of the cloud depends on the application the authors propose a formalism for dealing with compositions of different kinds of invariance cons for me the explanation of the generalization is really hard to follow for me the paper would be stronger if were less broad but went into more depth for the permutation invariance case it is very easy to sit down and come up with network structures that are permutation invariant it seems the author tried a few networks in the family a few different point cloud sizes a couple options for the number of parameters averaging vs max in the set dropout vs no dropout but unless the space is more completely and systematically explored there not much reason for a practitioner to use the proposed structure vs some other random structure they cook up that is also permutation invariant i e what about just using a fc layer that is shared between the points instead of your three set invariant layers seems simpler more general and also permutation invariant it is not clear to me how valuable the author definition of minimally invariant is is a sufficiently large composition of set invariant layers a universal approximator for permutation invariant functions i am concerned that proposed set invariant layer might be strongly variant to spatial transformations as well as vulnerable to large outliers in particular there is a term that subtracts a corner of the clouds bounding box i e the max over set operator inside the first layer before the cloud goes through a learned affine transform and pixelwise nonlinearity seems like that could saturate the whole network i am reviewing with low confidence because there a chance the formalism in the first part of the paper is more valuable than i realize i have not fully understood it,6.0
514.json,this paper discusses ways to enforce invariance in neural networks using weight sharing the authors formalize a way for feature functions to be invariant to a collection of relations and the main invariance studied is a set invariant function which is used in an anomaly detection setting and a point cloud classification problem invariance is at a high level an important issue of course since we don t want to spend parameters to model spurious ordering relationships which may potentially be quite wasteful and i like the formalization of invariance presented in this paper however there are a few weaknesses that i feel prevent this from being a strong submission first the exposition is too abstract and this paper could really use a running and concrete example starting from the very beginning second set invariance which is the main type of invariance studied in the paper is defined via the author s formalization of invariance but is never explicitly related to what i might think of as set invariance e g to permutations of input or output dimensions explicitly defining set invariance in some other way then relating it to the structural invariance formulation may be a better way to explain things it is never made clear for example why figure b is the set data structure i like the discussion of compositionality of structures one question i have here is are the resulting compositional structures are still valid as structures but the authors have ignored the other kind of compositionality that is important to neural networks specifically that relating the proposed notion of invariance to function composition seems important i e under what conditions do compositions of invariant functions remain invariant and it is clear to me that just by having one layer of invariance in a network doesn t make the entire network invariant for example so if we look at the anomaly detection network at the end for example is it clear that the final predictor is set invariant in some sense regarding experiments there are no baselines presented for anomaly detection baselines are presented in the point cloud classification problem but the results of the proposed model are not the best and this should be addressed i should say that i don t know enough about the dataset to say whether these are exactly fair comparisons or not it is also never really made clear why set invariance is a desirable property for a point cloud classification setting as a suggestion try a network that uses a fully connected layer at the end but uses data augmentation to enforce set invariance also what about classical set kernels other random things example shouldn t s in the case of left right and up down symmetry parameters shared within a relation is vague and undefined why is set convolution called set convolution in the appendix what is convolutional about it is there a relationship to symmetric function theory,5.0
514.json,this review is only an informed guess unfortunately i cannot assess the paper due to my lack of understanding of the paper i have spent several hours trying to read this paper but it has not been possible for me to follow partially due to my own limitations but also i think due to an overly abstract level of presentation the paper is clearly written but in the same way that a n bourbaki book is clearly written i would prefer to leave the accept reject decision to the other reviewers who may have a better understanding even if the authors had made a serious mistake i would not be able to tell my proposal is positive because the paper is apparently clearly written and the empirical evaluation is quite promising but some effort will be needed in order to address the broader audience that could potentially be interested in the topic i therefore would like to provide feedback only at the level of presentation my main source of problems is that the authors do not try to ground their abstract formalism with concrete examples when the examples show up it is by revelation rather than by explaining how they connect to the previous concepts the one example that could unlock most people understanding is how convolution or inner product operations connect with the setting described here for what i know convolution is tied with space or time and is understood as an equivariant operation shifting the signal shifts the output it is not explained how the x x pairs used by the authors in order to build relations structures and then to define invariance relate to this setting going from sets to relations to functions to operators and then to shift invariant operators convolutions involves many steps and some hand holding is needed why is the x convolution associated to relations are these relations referring to the input at a given coordinate and its contribution to the output w offset x i offset in that case why is there a backward arrow from the center node to the other nodes and why are there arrows across nodes what is a cardinal and what is a cartesian convolution in signal processing terms clearly these are not standard terms are we talking about separable filters what are the x and square symbols in figure and what are the horizontal and vertical sub graphs standing for what is x and what is x x x and what is the relationship between them i realize that to the authors these questions may seem to be trivial and left as homework for the reader but i think part of publishing a paper is doing a big part of the homework for the readers so that it becomes easy to get the idea clearly the authors target the more general case but spending some time to explain how the particular case is an instance of the the general case would be a good use of space i would propose that the authors explain what are x x i and x s for the simplest possible example e g convolving a x signal with a x filter how the convolution filter parameters show up in the function f as well as how the spatial invariance or equivariance of convolution is reflected here,7.0
451.json,this is an incremental result several related results that the authors of the paper mentioned here were already published the authors claim that they can get rid of the technical assumptions from the previous papers but the results they propose are significantly weaker and also quite technical the main theoretical result theorem is not convincing at all furthermore the paper is badly written no theoretical intuition is given the experimental section is weak and in some places the formatting is wrong,2.0
451.json,this paper studies the energy landscape of the loss function in neural networks it is generally clearly written and nicely provides intuitions for the results one main contribution is to show that the level sets of the loss becomes connected as the network is increasingly overparameterized it also quantifies in a way the degree of disconnectedness possible in terms of the increase in loss that one must allow to find a connected path it would seem that this might have some implications for the likelihood of escaping local minima with stochastic gradient descent the paper also presents a simple algorithm for finding geodesic paths between two networks such that the loss is decreasing along the path using this they show that the loss seems to become more nonconvex when the loss is smaller this is also quite interesting the work does have some significant limitations which is not surprising given the difficulty of fully analyzing the network loss function however the authors are quite clear about these limitations which especially include not yet analyzing deep networks and analyzing only the oracle loss and not the empirical loss i would have also appreciated a little more practical discussion of the bound in theorem it is hard to tell whether this bound is tight enough to be practically relevant,7.0
781.json,the paper addresses the problem of learning compact binary data representations i have a hard time understanding the setting and the writing of the paper is not making it any easier for example i can not find a simple explanation of the problem and i am not familiar with these line of research i read all the responses provided by authors to reviewer questions and re read the paper again and i still do not fully understand the setting and thus can not really evaluate the contributions of these work the related work section does not exist and instead the analysis of the literature is somehow scattered across the paper there are no derivations provided statements often miss references e g the ones in the fourth paragraph of section this makes me conclude that the paper still requires significant work before it can be published,3.0
781.json,this paper proposes a new kind of expert model where a sparse subset of most reliable experts is chosen instead of the usual logarithmic opinion pool of a poe i find the paper very unclear i tried to find a proper definition of the joint model p x z but could not extract this from the text the proposed em like algorithm should then also follow directly from this definition at this point i do not see if such as definition even exists in other words is there is an objective function on which the iterates of the proposed algorithm are guaranteed to improve on the train data we also note that the product of unifac models from hinton tries to do something very similar where only a subset of the experts will get activated to generate the input,3.0
502.json,this paper propose a new evaluation metric for dialogue systems and show it has a higher correlation with human annotation i agree the mt based metrics like bleu are too simple to capture enough semantic information but the metric proposed in this paper seems to be too compliciated to explain on the other hand we could also use equation as a retrieval based dialogue system so what is suggested in this paper is basically to train one dialogue model to evaluate another model then the high level question is why we should trust this model this question is also relevant to the last item of my detail comments detail comments how to justify what is captured evaluated by this metric in terms of bleu we know it actually capture n gram overlap but for this model i guess it is hard to say what is captured if this is true then it is also difficult to answer the question like will the data dependence be a problem why not build model incrementally as shown in equation this metric uses both context and reference to compute a score is it possible to show the score function using only reference it will guarantee this metric use the same information source as bleu or rouge another question about equation is it possible to design the metric to be a nonlinear function since from what i can tell the comparison between bleu or rouge and the new metric in figure is much like a comparison between the exponential scale and the linear scale i found the two reasons in section are not convincing if we put them together based on these two reasons i would like to see the correlation with average score a more reasonable way is to show the results both with and without averaging in table it looks like the metric favors the short responses if that is true this metric basically does the opposite of bleu since bleu will panelize short sentences on the other hand human annotators also tends to give short respones high scores since long sentences will have a higher chance to contain some irrelevant words can we eliminate the length factor during the annotation otherwise it is not surprise that the correlation,4.0
555.json,the topic is very interesting but the paper is not convincing specifically the experiment part is weak the study should include datasets that are familiar to the community as well as the ones that are not often addressed by deep learning the comparison to other approaches is not comprehensive,4.0
607.json,the paper proposes an attention based approach for video description the approach uses three lstms and two attention mechanisms to sequentially predict words from a sequence of frames in the lstm encoder of the frames tem the first attention approach predicts a spatial attention per frame and computes the weighted average the second lstm ham predicts an attention over the hidden states of the encoder lstm the third lstm which run temporally in parallel to the second lstm generates the sentence one word at a time strength the paper works on a relevant and interesting problem using layers of attention in the proposed way have to my knowledge not been used before for video description the exact architecture is thus novel but the work claims much more without sufficient attribution see blow the experiments are evaluated on two datasets msvd and charades showing performance on the level of related work for msvd and improvements for charades weaknesses claims about the contribution novelty of the model seem not to hold one of the main contributions is the hierarchical attention memory ham it is not clear to me how the presented model eq are significantly different from the presented model in xu et al yao et al while xu et al attends over spatial image locations and yao et al attend over frames this model attends over encoded video representations hv i a slight difference might be that xu et al use the same lstm to generate while this model uses an additional lstm for the decoding the paper states in section we propose fm to memorize the previous attention however hm t only consist of the last hidden state furthermore the model fm does not have access to the attention alpha this was also discussed in comments by others but remains unclear in the discussion of comments the authors claim that attention not only is a function a current time step but also a function of all previous attentions and network states while it is true that there is a dependency but that is true also for any lstm however the model does not have access to the previous network states as hg t only consist of the last hidden state as well as hm t at least that is what the formulas say and what figure suggests the authors claim to have multi layer attention in ham however it remains unclear where the multi layer comes from the paper states that in section cnn features tend to discard the low level information useful in modeling the motion in the video ballas et al this suggests that the approach which follows attacks this problem however it cannot model motion as attention rho between frames is not available when predicting the next frame also it is not clear how the model can capture anything low level as it operates on rather high level vgg conv features related work the difference of ham to yao et al and xu et al should be made more clear or these papers should be cited in the ham section conceptual limitation of the model the model has two independent attention mechanisms a spatial one and a temporal one the spatial within a frame is independent of the sentence generation it thus cannot attend to different aspects of the frames for different words which would make sense e g if the sentence is the dog jumps on the trampoline the model should focus on the dog when saying dog and on the trampoline when saying trampoline however as the spatial attention is fixed this is difficult also the encoder model does not have an explicitly way to look at different aspects in the frame during the encoding so might likely get stuck and always predict the same spatial attention for all frames or it might e g always attend to the dog which moves around but never on the scene eq contradicts fig how is the model exactly receiving the previous word as input eq suggests it is the softmax if this is the case the authors should emphasize this in the text as this is unusual more common would be to use the ground truth previous word during training which fig suggests and the hardmax i e the highest predicted previous word encoded as one hot vector at test time clarity it would be helpful if the same notation would be used in eq and why is a different notation required it would be helpful if fig could contain more details or additional figures for the corresponding parts would be added if space is a problem e g the well known equations for lstm softmax eq and log likelihood loss eq could be omitted or inlined evaluation the paper claims that the the proposed architecture outperforms all previously proposed methods and leads to a new state of the art results for the msvd dataset this clearly is wrong even given the same feature representation pan et al a in table achieve higher meteor for this strong claim i would also expect that it outperforms all previous results independent of the features used which is also wrong again yu et al achieve higher performance in all compared metrics for charades dataset this claim is also too bold as hardly any methods have been evaluated on this dataset so at least all the ablations reported in table should also be reported for the charades dataset to make for this dataset any stronger claims missing qualitative results of attention the authors should show qualitative results of the attention for both attention mechanisms to understand if anything sensible is happening there how diverse is the spatial and the temporal attention is it peaky or rather uniform performance improvement is not significant over model ablations the improvements over att no tem is only meteor blue and the performance drops for cider by missing human evaluation i disagree with the authors that a human evaluation is not feasible an evaluation on a subset of the test data is not so difficult even if other authors do not provide their code model and some do they are typically happy to share the predicted sentences which is sufficient and even better for human evaluation if not i would explicitly mention that some authors did not share sentences as this seems clearly wrong for model ablations the sentences are available to the authors several of the comments raised by reviewers others have not yet been incorporated in a revised version of the paper and or are still not clear from the explanations given e g including spice evaluation and making fixes seems trivial hyperparameters are inconsistent why are the hyperparemters inconsistent between the ablation analysis frames are sampled and the performance comparison frames should this not be selected on the validation set what is the performance of all the ablations with frames other minor discussion points equation what happens with hm and hg the lstm formulas provided only handle two inputs are hm and hg concatenated there is a section but no the paper states in section our proposed architecture can alone not only learn a representation for video that can model the temporal structure of a video sequence but also a representation that can effectively map visual space to the language space however this seems to be true also for many most other approaches e g venugopalan et al iccv summary while the paper makes strong claims w r t to the approach and results the approach lacks novelty and the results are not convincing over related work and ablations furthermore improved clarity and visualizations of the model and attention results would benefit the paper,4.0
715.json,this paper proposes two pruning methods to reduce the computation of deep neural network in particular whole feature maps and the kernel connections can be removed with not much decrease of classification accuracy however this paper also has the following problems the method is somehow trivial since the pruning masks are mainly chosen by simple random sampling the novelty and scalability are both limited experiment results are mainly focused on the classification rate and the ideal complexity as a paper on improving computation efficiency it should include results on practical time consumption it is very common that reducing numbers of operations may not lead to reduced computational time on a highly parallel platform e g gpu it is more important to improve the computational efficiency on large scale models e g imagenet classification network than on small models e g mnist cifar network however results on large scale network is missing logical validity of the proposed method for feature map pruning what if just to train reduced size network is trained from scratch without transfer any knowledge from the pretrained large network is it possible to get the same accuracy if so it will simply indicate the hyper parameter is not optimal for the original network experimental results are necessary to clarify the necessity of feature map pruning note that i agree with that a smaller network may be more generalizable than a larger network comments to the authors response thanks for replying to my comments i still believe that the proposed methods are trivial it is nice to show gpu implementation compared to existing toolboxes e g torch caffe tensorflow is the implementation of convolution efficient enough experiments on cifar are helpful better than cifar but it is not really large scale where speed up is not so critical imagenet and places datasets are examples of large scale datasets the author did not reply to the question wrt the validity of the proposed methods this question is critical,4.0
715.json,summary there are many different pruning techniques to reduce memory footprint of cnn models and those techniques have different granularities layer maps kernel or intra kernel pruning ratio and sparsity of representation the work proposes a method to choose the best pruning masks out to many trials tested on cifar svhn and mnist pros proposes a method to choose pruning mask out of n trials analysis on different pruning methods cons questions the proposed strategy selects the best pruned network through n random pruning trials this approach enables one to select pruning mask in one shot and is simpler than the multi step technique how can one get the best pruning mask in one shot if you ran n random pruning trials answered missing tests of the approach with bigger cnn like alexnet vgg googlenet or resnet extended to vgg ok since reducing model size for embedded systems is the final goal then showing how much memory space in mb is saved with the proposed technique compared with other approaches like han et al would be good misc typo in figure a caption featuer corrected,6.0
345.json,this paper proposes to use an empirical bayesian approach to learn the parameters of a neural network and their priors a mixture model prior over the weights leads to a clustering effect in the weight posterior distributions which are approximated with delta peaks this clustering effect can exploited for parameter quantisation and compression of the network parameters the authors show that this leads to compression rates and predictive accuracy comparable to related approaches earlier work han et al is based on a three stage process of pruning small magnitude weights clustering the remaining ones and updating the cluster centres to optimise performance the current work provides a more principled approach that does not have such an ad hoc multi stage structure but a single iterative optimisation process a first experiment described in section shows that an empirical bayes approach without the use of hyper priors already leads to a pronounced clustering effect and to setting many weights to zero in particular a compression rate of is obtained on the lenet model in section the text refers to figure c i suppose this should be figure section describes an experiment where hyper priors are used and the parameters of these distributions as well as other hyper parameters such as the learning rates are being optimised using spearmint snoek et al figure shows the performance of the different points in the hyper parameter space that have been evaluated each trained network gives an accuracy compressionrate point in the graph the text claims that best results lie on a line this seems a little opportunistic interpretation given the limited data moreover it would be useful to add a small discussion on whether such a linear relationship would be expected or not currently the results of this experiment lack interpretation section describes results obtained for both cnn models and compares results to the recent results of han et al and guo et al comparable results are obtained in terms of compression rate and accuracy the authors state that their current algorithm is too slow to be useful for larger models such as vgg but they do briefly report some results obtained for this model but do not compare to related work it would be useful here to explain what slows the training down with respect to standard training without the weight clustering approach and how the proposed algorithm scales in terms of the relevant quantities of the data and the model the contribution of this paper is mostly experimental leveraging fairly standard ideas from empirical bayesian learning to introduce weight clustering effects in cnn training this being said it is an interesting result that such a relatively straightforward approach leads to results that are on par with state of the art but more ad hoc network compression techniques the paper could be improved by clearly describing the algorithm used for training and how it scales to large networks and datasets another point that would deserve further discussion is how the hyper parameter search is performed not using test data i assume and how the compared methods dealt with the search over hyper parameters to determine the accuracy compression tradeoff ideally i think methods should be evaluated across different points on this trade off,7.0
345.json,this paper revives a classic idea involving regularization for purposes of compression for modern cnn models on resource constrained devices model compression is hot and we are in the midst of lots of people rediscovering old ideas in this area so it is nice to have a paper that explicitly draws upon classic approaches from the early s to obtain competitive results on standard benchmarks there not too much to say here this study is an instance of a simple idea applied effectively to an important problem written up in an illuminating manner with appropriate references to classic approaches the addition of the filter visualizations enhances the contribution,7.0
596.json,in this paper the authors proposed an implicit resonet model for knowledge base completion the proposed model performs inference implicitly by a search controller and shared memory the proposed approach demonstrates promising results on fbk benchmark dataset pros the proposed approach demonstrates strong performance on fbk dataset the idea of using shared memory for knowledge base completion is new and interesting the proposed approach is general and can be applied in various tasks cons there is no qualitative analysis on the results and it is hard to see why the proposed approach works on the knowledge base completion task the introduction section can be improved specifically the authors should motivate shared memory more in the introduction and how it different from existing methods that using unshared memory for knowledge base completion similarly the function of search controller is unclear in the introduction section as it is unclear what does search mean in the content of knowledge base completion the concept of shared memory and search controller only make sense to me after reading through section,6.0
596.json,summary this paper proposes a new way for knowledge base completion which highlights adopting an implicit shared memory which makes no assumption about its structure and is completely learned during training modeling a multi step search process that can decide when to terminate the experimental results on wn and fbk seem pretty good the authors also perform an analysis on a shortest path synthetic task and demonstrate that this model is better than standard seqseq the paper is well written and it is easy to follow major comments i actually do like the idea and am also impressed that this model can work well the main concern is that this paper presents too little analysis about how it works and whether it is sensitive to the hyper parameters besides that only reporting a final model on wn and fbk one key hyper parameter i believe is the size of shared memory using for the experiments i don t think that this number should be fixed for all tasks at least it should depend on the kb scale could you verify this in your experiments would it be even possible to make a memory structure with dynamic size the rl setting stochastic search process is also one highlight of the paper but could you demonstrate that how much it does really help i think it is necessary to compare to the following remove the termination gate and fix the number of inference steps and see how well the model does also show how the performance varies on of steps i appreciate your attempts on the shortest path synthetic task however i think it would be much better if you can demonstrate that under a real kb setting you can still perform the shortest path analysis but using kb e g freebase entities and relations minor comments i am afraid that the output gate illustrated in figure is a bit confusing there should be only one output depending on when the search process is terminated,6.0
353.json,the paper combines a hierarchical variational autoencoder with pixelcnns to model the distribution of natural images they report good although not state of the art likelihoods on natural images and briefly start to explore what information is encoded by the latent representations in the hierarchical vae i believe that combining the pixelcnn with a vae as was already suggested in the pixelcnn paper is an important and interesting contribution the encoding of high mid and low level variations at the different latent stages is interesting but seems not terribly surprising since the size of the image regions the latent variables model is also at the corresponding scale showing that the pixelcnn improves the latent representation of the vae with regard to some interesting task would be a much stronger result also while the paper claims that combining the pixelcnn with the vae reduces the number of computationally expensive autoregressive layers it remains unclear how much more efficient their whole model is than an pixelcnn with comparable likelihood in general i find the clarity of the presentation wanting for example i agree with reviewer that the exact structure of their model remains unclear from the paper and would be difficult to reproduce,6.0
754.json,this paper uses a pointer network over a sparse window of identifiers to improve code suggestion for dynamically typed languages code suggestion seems an area where attention and or pointers truly show an advantage in capturing long term dependencies the sparse pointer method does seem to provide better results than attention for similar window sizes specifically comparing a window size of for the attention and sparse pointer method shows the sparse pointer winning fairly definitively across the board given a major advantage of the pointer method is being able to use a large window size well thanks to the supervision the pointer provides it was unfortunate though understandable due to potential memory issues not to see larger window sizes having a different batch size for the sparse pointer and attention models is unfortunate given it complicates an otherwise straight comparison between the two models the construction and filtering of the python corpus sounds promising but as of now it is still inaccessible listed in the paper as todo given that code suggestion seems an interesting area for future long term dependency work it may be promising as an avenue for future task exploration overall this paper and the dataset are likely an interesting contribution even though there are a few potential issues,6.0
493.json,the paper proposes new bounds on the misclassification error the bounds lead to training classifiers with an adaptive loss function and the algorithm operates in successive steps the parameters are trained by minimizing the log loss weighted by the probability of the observed class as given by the parameters of the previous steps the bound improves on standard log likelihood when outliers underfitting prevents the learning algorithm to properly optimize the true classification error experiments are performed to confirm the therotical intuition and motivation they show different cases where the new algorithm leads to improved classification error because underfitting occurs when using standard log loss and other cases where the new bounds do not lead to any improvement because the log loss is sufficient to fit the dataset the paper also discusses the relationship between the proposed idea and reinforcement learning as well as with classifiers that have an uncertain label while the paper is easy to read and well written overall in a second read i found it difficult to fully understand because two problems are somewhat mixed together here considering only binary classification for simplicity a the optimization of the classification error of a randomized classifier which predicts with probability p x theta and b the optimization of the deterministic classifier which predicts sign p x theta in a way that is robust to outliers underfitting the reason why i am confused is that the standard approach to supervised classification as is mentioned in the abstract is to use deterministic classifiers at test time and the log loss up to constants is an upper bound on the classification error of the deterministic classifier however the bounds discussed in the paper only concern the randomized classifier question in the experiments what kind of classifier is used the randomized one as would the sentence in the first page suggest assuming the class is chosen according to p y x θ or the more standard deterministic classifier argmaxy p y x theta as far as i can see there are two cases either i the paper deals with learning randomized classifiers in which case it should compare the performances with the deterministic counterparts that people use in practice or ii the paper makes sense as soon as we accept that the optimization of criterion a is a good surrogate for b in both cases i think the write up should be made clearer because in case ii the algorithm does not minimize an upper bound on the classification error and in case i what is done does not correspond to what is usually done in binary classification comments the section allowing uncertainty in the decision may be improved by adding some references e g bartlett wegkamp classification with a reject option using a hinge loss or sayedi et al trading off mistakes and don t know predictions there seems to be a sign missing in the p x theta in l theta lambda in section the idea presented in the paper is interesting and original while i give a relatively low score for now i am willing to increase this score if the clarifications are made final comments i think the paper is clear enough in its current form even though there should still be improvement in the justification of why and to what extent the error of the randomized classifier is a good surrogate for the error of the true classifier while the smoothed version of the loss is an acceptable explanation in the standard classification setup it is less clear in the section dealing with an additional uncertain label i increase my score from to,6.0
755.json,this paper studies the optimization issue of linear resnet and shows mathematically that for shortcuts and zero initialization the hessian has condition number independent of depth i skimmed through the proof but have not checked them carefully this result is a nice observation for training deep linear networks but i do not think the paper has fully resolved the linear vs nonlinear issue some question though the revision has added some results using relu units it seems it is only added to the mid positions of the network sec is this how it is typically done in resnet moreover relu is not differentiable at zero point which does not satisfy the condition you had in theorem why not use differentiable activations like sigmoid or tanh from equation in the appendix it seems for nonlinear activations the condition number depends on the derivative sigma prime at therefore if we use tanh which has derivative at zero the condition number is the same for linear and tanh activations but this probably is not enough to explain the bit difference in performance or optimization for linear and nonlinear networks or how the situations evolve after learning the point as for the success of resnet or convnets in general in computer vision i believe there are more types of nonlinearity such as pooling can the result here generalizes to pooling as well minor sec last paragraph low approximation error typically means more powerful model class and better training error but not necessarily better test error sec what do you mean by zero initialization with small random perturbations why not exactly zero initialization how large is the random perturbation,5.0
755.json,resnet and other architectures that use shortcuts have shown empirical success in several domains and therefore studying the optimization for such architectures is very valuable this paper is an attempt to address some of the properties of networks that use shortcuts some of the experiments in the paper are interesting however there are two main issues with the current paper linear vs non linear i think studying linear networks is valuable but we should be careful not to extend the results to networks with non linear activations without enough evidence this is especially true for hessian as the hessian of non linear networks have very large condition number see the iclr submission singularity of hessian in deep learning even in cases where the optimization is not challenging therefore i do not agree with the claims in the paper on non linear networks moreover one plot on mnist is not enough to claim that non linear networks behave similar to linear networks hessian at zero initial point the explanation of why we should be interested in hessain at zero initial point is not acceptable the zero initial point is not interesting because it is a very particular point that cannot tell us about the hessian during optimization,4.0
305.json,this is a nice paper that demonstrates an end to end trained image compression and decompression system which achieves better bit rate vs quality trade offs than established image compression algorithms like jpeg in addition to showing the efficacy of would eep learning for a new application a key contribution of the paper is the introduction of a differentiable version of rate function which the authors show can be used for effective training with different rate distortion trade offs i expect this will have impact beyond the compression application itself for other tasks that might benefit from differentiable approximations to similar functions the authors provided a thoughtful response to my pre review question i would still argue that to minimize distortion under a fixed range and quantization a sufficiently complex network would learn automatically produce codes within a fixed range with the highest possible entropy i e it would meet the upper bound but the second argument is convincing doing so forces a specific form on how the compressor output is used which to match the effective compression of the current system would require a more complex network that is able to carry out the computations currently being done by a separate variable rate encoder used to store q,8.0
647.json,this paper proposes the rims that unrolls variational inference procedure the author claims that the novelty lies in the separation of the model and inference procedure making the map inference as an end to end approach the effectiveness is shown in image restoration experiments while unrolling the inference is not new the author does raise an interesting perspective towards the model free configuration where model and inference are not separable and can be learnt jointly however i do not quite agree the authors argument regarding and although both and have pre defined map inference problem it is not necessarily that a separate step is required in fact both do not have either a pre defined prior model or an explicit prior evaluation step as shown in fig a i believe that the implementation of both follows the same procedure as the proposed that could be explained through fig c that is to say the whole inference procedure eventually becomes a learnable neural network and the energy is implicitly defined through learning the parameters moreover the rnn block architecture gru and non linearity tanh restrict the flexibility and implicitly form the inherent family of variational energy and inference algorithm this is also similar with and based on that fact i have the similar feeling with r that the novelty is somewhat limited also some discussions should be added in terms of the architecture and nonlinearity that you have chosen,5.0
651.json,the proposed regularizer seems to be a particular combination of existing methods though the implied connection between nonlinearities and stochastic regularizers is intriguing in my opinion the empirical performance does not exceed the performance achieved by similar methods by a large enough margin to arrive at a meaningful conclusion,4.0
651.json,the method proposed essential trains neural networks without a traditional nonlinearity using multiplicative gating by the cdf of a gaussian evaluated at the preactivation this is motivated as a relaxation of a probit bernoulli stochastic gate experiments are performed with both the work is somewhat novel and interesting little is said about why this is preferable to other similar parameterizations of the same sigmoidal softsign etc it would be stronger with more empirical interrogation of why this works and exploration of the nearby conceptual space the cifar results look okay by today standards but the mnist results are quite bad neural nets were doing better than a decade ago and the soi map results and the relu baseline are above timit results on frame classification also are not that interesting without evaluating word error rate within a speech pipeline but this is a minor point the idea put forth that soi map networks without additional nonlinearities are comparable to linear functions is rather misleading as they are in expectation nonlinear functions of their input varying an input example by multiplying or adding a constant will not be linearly reflected in the expected output of the network in this sense they are more nonlinear than relu networks which are at least locally linear the plots are very difficult to read in grayscale,5.0
485.json,the paper presents an analysis of the ability of deep networks with relu functions to represent particular types of low dimensional manifolds specifically the paper focuses on what the authors call monotonic chains of linear segments which are essentially sets of intersecting tangent planes the paper presents a construction that efficiently models such manifolds in a deep net and presents a basic error analysis of the resulting construction while the presented results are novel to the best of my knowledge they are hardly surprising given what we already know about the representational power of deep networks and given that the study selects a deep network architecture and a data structure that are very compatible in particular i have three main concerns with respect to the results presented in this paper in the last decade there has been quite a bit of work on learning data representations from sets of local tangent planes examples that spring to mind are local tangent space analysis of zhang zha manifold charting by brand and alignment of local models by verbeek roweis and vlassis none of this work is referred to in related work even though it seems highly relevant to the analysis presented here for instance it would be interesting to see how these old techniques compare to the deep network trained to produce the embedding of figure this may provide some insight into the inductive biases the deep net introduces does it learn better representations that non parametric techniques because it has better inductive biases or does it learn worse representations because the loss being optimized is non convex it is difficult to see how the analysis generalizes to more complex data in which local linearity assumptions on the data manifold are vacuous given the sparsity of data in high dimensional space or how it generalizes to deep network architectures that are not pure relu networks for instance most modern networks use a variant of batch normalization this already appears to break the presented analyses the error bound presented in section appears vacuous for any practical setting as the upper bound on the error is exponential in the total curvature a quantity that will be quite large in most practical settings this is underlined by the analysis of the swiss roll dataset of which the authors state that the bound for this case is very loose the fact that the bound is already so loose for this arguably very simple manifold makes that the error analysis may tell us very little about the representational power of deep nets i would encourage the authors to address issue in the revision of the paper issue and may be harder to address but is essential that they are addressed for the line of work pioneered by this paper to have an impact on our understanding of deep learning minor comments in prior work the authors only refer to fully supervised siamese network approaches these approaches differ from that taken by the authors as their approach is unsupervised it should be noted that the authors are not the first to study unsupervised representation learners parametrized by deep networks other important examples are deep autoencoders hinton salakhutdinov and work on denoising autoencoders from bengio group and parametric t sne van der maaten what loss do the authors use in their experiments using the difference between the ground truth distance and the distance computed by the network seems odd because it encourages the network to produce infinitely large distances to get a loss of minus infinity is the difference squared,6.0
485.json,summary this paper discusses how data from a special type of low dimensional structure monotonic chain can be efficiently represented in terms of neural networks with two hidden layers pros interesting easy to follow view on some of the capabilities of neural networks highlighting the dimensionality reduction aspect and pointing at possible directions for further investigation cons the paper presents a construction illustrating certain structures that can be captured by a network but it does not address the learning problem although it presents experiments where such structures do emerge more or less comments it would be interesting to study the ramifications of the presented observations for the case of deep er networks also to study to what extent the proposed picture describes the totality of functions that are representable by the networks minor comments figure could be referenced first in the text color coded where the color codes what thank you for thinking about revising the points from my first questions note isometry on the manifold on page mention how the orthogonal projection on sk is realized in the network on page divided into segments here segments is maybe not the best word on page the mean relative error is what is the baseline here or what does this number mean,7.0
606.json,the paper presents a framework to formulate data structures in a learnable way it is an interesting and novel approach that could generalize well to interesting datastructures and algorithms in its current state revision of dec th there are two strong weaknesses remaining analysis of related work and experimental evidence reviewer detailed some of the related work already and especially deepmind which i am not affiliated with presented some interesting and highly related results with its neural touring machine and following work while it may be of course very hard to make direct comparisons in the experimental section due to complexity of the re implementation it would at least be very important to mention and compare to these works conceptually the experimental section shows mostly qualitative results that do not fully conclusively treat the topic some suggestions for improvements it would be highly interesting to learn about the accuracy of the stack and queue structures for increasing numbers of elements to store can a queue stack be used in arbitrary situations of push pop operations occuring even though it was only trained solely with consecutive pushes consecutive pops does it in this enhanced setting diverge at some point the encoded elements from mnist even though in a x binary space are elements of a ten element set and can hence be encoded a lot more efficiently just by parsing them which cnns can do quite well is the nn just learning to do that if so its performance can be expected to strongly degrade when having to learn to stack more than numbers in case of an optimal parser and loss less encoding to argue more in this direction experiments would be needed with an increasing number of stack queue elements experimenting with an mnist parsing nn in front of the actual stack queue network could help strengthening or falsifying the claim the claims about mental representations have very little support throughout the paper if indication for correspondence to mental models etc could be found it would allow to hold the claim otherwise i would remove it from the paper and focus on the nn aspects and maybe mention mental models as motivation,4.0
411.json,this work builds on top of stoke schkufza et al which is a superoptimization engine for program binaries it works by starting with an existing program and proposing modifications to it according to a proposal distribution proposals are accepted according to the metropolis hastings criteria the acceptance criteria takes into account the correctness of the program and performance of the new program thus the mcmc process is likely to converge to correct programs with high performance typically the proposal distribution is fixed the contribution of this work is to learn the proposal distribution as a function of the features of the program bag of words of all the opcodes in the program the experiments compare with the baselines of uniform proposal distribution and a baseline where one just learns the weights of the proposal distribution but without conditioning on the features of the program the evaluation shows that the proposed method has slightly better performance than the compared baselines the significance of this work at iclr seems to be quite low both because this is not a progress in learning representations but a straightforward application of neural networks and reinforce to yet another task which has non differentiable components the task itself superoptimization is not of significant interest to iclr readers attendees a conference like aaai uai seem a better fit for this work the proposed method is seemingly novel typical mcmc based synthesis methods are lacking due to their being no learning components in them however to make this work compelling the authors should consider demonstrating the proposed method in other synthesis tasks or even more generally other tasks where mh mcmc is used and a learnt proposal distribution can be beneficial superoptimization alone esp with small improvements over baselines is not compelling enough it is also not clear if there is any significant representation learning is going on since a bow feature is used to represent the programs the neural network cannot possibly learn anything more than just correlations between presence of opcodes and good moves such a model cannot possibly understand the program semantics in any way it would have been a more interesting contribution if the authors had used a model such as tree lstm which attempts to learn the semantics the program the quite naive method of learning makes this paper not a favorable candidate for acceptance,6.0
692.json,the paper proposes to enhance the attention mechanism for sentiment classification by using global context computed by a bi lstm the proposed models outperform many existing models in the literature on sentiment analysis datasets the key idea of using bi lstm to compute global context for attention is actually not novel as proposed several times in the literature e g luong et al and shen lee especially luong et al already proposed to combine global context with local context for attention regarding to the experiments of course it would be nice if the model can work well without the need of tricks like dropout or pre trained word embeddings however it would be even better if the model can work well using those tricks the authors should show results of the models using those tricks and compare them to the results in the literature ref luong et al effective approaches to attention based neural machine translation emnlp,3.0
368.json,summary this paper describes how to estimate log likelihoods of currently popular decoder based generative models using annealed importance sampling ais and hmc it validates the method using bidirectional monte carlo on the example of mnist and compares the performance of gans and vaes review although this seems like a fairly straight forward application of ais to me correct me if i missed an important trick to make this work i very much appreciate the educational value and empirical contributions of this paper it should lead to clarity in debates around the density estimation performance of gans and should enable more people to use ais space permitting it might be a good idea to try to expand the description of ais all the components of ais are mentioned and a basic description of the algorithm is given but the paper doesn t explain well why the algorithm does what it does why it works i was initially confused by the widely different numbers in figure on first glance my expectation was that this figure is comparing gan gmmn and iwae because of the labeling at the bottom and because of the leading words in the caption s descriptions perhaps mention in the caption that a and b use continuous mnist and c uses discrete mnist gmmn should probably be gmmn using reconstructions for evaluation of models may be a necessary but is not sufficient condition for a good model depending on the likelihood a posterior sample might have very low density under the prior for example it would be great if the authors could point out and discuss the limitations of this test a bit more minor perhaps add a reference to mackay s density networks mackay for decoder based generative models in section the authors write the prior over z can be drastically different than the true posterior p z x especially in high dimension i think the flow of the paper could be improved here especially for people less familiar with importance sampling ais i don t think the relevance of the posterior for importance sampling is clear at this point in the paper in section the authors claim that is often more meaningful to estimate p x in log space because of underflow problems meaningful seems like the wrong word here perhaps revise to say that it s more practical to estimate log p x because of underflow problems or to say that it s more meaningful to estimate log p x because of its connection to compression surprise entropy,7.0
368.json,the paper describes a method to evaluate generative models such as vae gan and gmmn this is very much needed in our community where we still eyeball generated images to judge the quality of a model however the technical increment over the nips paper measuring the reliability of mcmc inference with bidirectional monte carlo is very small or nonexistent but please correct me if i am wrong grosse et al the relative contribution of this paper is the application of this method to generative models in section the authors seem to make a mistake they write e p x p x but i think they mean e log p x log e p x log p x also for what value of x if p x is normalized it can t be true for all values of x anyways i think there are typos here and there and the equations could be more precise on page top of the page it is said that the ais procedure can be initialized with q z x instead of p z however it is unclear what value of x is then picked is it perhaps ep x q z x i am confused with the use of the term overfitting p bottom does a model a overfit relative to a another model b if the test accuracy of a is higher than that of b even though the gap between train and test accuracy is also higher for b than for a i think not perhaps the last sentence on page should say that vae underfits less than gmmn the experimental results are interesting in that it exposes the fact that gans and gmmns seem to have much lover test accuracy than vae despite the fact that their samples look great,6.0
387.json,this paper proposes a modification of the parametric texture synthesis model of gatys et al to take into account long range correlations of textures to this end the authors add the gram matrices between spatially shifted feature vectors to the synthesis loss some of the synthesised textures are visually superior to the original gatys et al method in particular on textures with very structured long range correlations such as bricks the paper is well written the method and intuitions are clearly exposed and the authors perform quite a wide range of synthesis experiments on different textures my only concern which is true for all methods including gatys et al is the variability of the samples clearly the global minimum of the proposed objective is the original image itself this issue is partially circumvented by performing inpainting experiments by which the synthesised paths needs to stay coherent with the borders as the authors did there are no additional insights into this problem in this paper which would have been a plus all in all this work is a simple and nice modification of gatys at al which is worth publishing but does not constitute a major breakthrough,7.0
503.json,paper proposes gated muiltimodal unit a building block for connectionist models capable of handling multiple modalities figure the bimodal case returns weighted activation by gains of gating units do you do anything special to keep multi modal case weighted as well i e how the equation for h in section would look like for multi modal case also what s the rationale for using tanh nonlinearity over say relu is it somehow experimentally optimised choice i would find interesting a discussion on a possibility of handling missing data in case one or more modalities are unavailable at test time is this possible in the current model to back off to fewer modalities synthetic example may suggest that s in fact possible those numbers perhaps could be added to table in the synthetic experiment you should compare mgu with the fully connected mlp model really with similar complexity that is at least two hidden units as gmu has two such for each modality followed by logistic regression at least in terms of capability of drawing decision boundary those should be comparable i think broader discussion shall be written on the related work associated with mixture of experts models which is fact are very similar conceptually as well as multiplicative rnn models also gating unit in lstm can in principle play very similar role when multiple modalities are spliced in the input overall the paper is interesting so is the associated and to be released dataset minor comments typos sec layers and a mlp see section layers and an mlp apologies for unacceptably late review multiplicative lstm for sequence modelling b krause l lu i murray s renals,7.0
503.json,this paper proposed the gated multimodal unit gmu model for information fusion the gmu learns to decide how modalities influence the activation of the unit using multiplicative gates the paper collected a large genre dataset from imdb and showed that gmu gets good performance the proposed approach seems quite interesting and the audience may expect it can be used in general scenarios beyond movie genre prediction so it is quite straightforward that the paper should test the algorithm in other applications which was not done yet that is the biggest shortcoming of this paper in my opinions another concern lies in how to evaluate the performance of information fusion the abstract claims the model improves the macro f score performance of single modality models by and with respect to visual and textual information respectively however such an improvement is off the key if two modals are complementary to each other the fusion results will always be higher the key fact is how much better than baselines the proposed gmu is there is a long list of techniques for fusions so it is difficult to conduct an impressive comparison on only one real dataset i think gmu did a nice work on movie dataset but i would also expect other techniques including fine tuning dropout distillation may help too it would be nice if the author could compare these techniques i also hope this paper could talk in more details the connection with mixture of expert moe model both models are based on the nonlinear gated functions while both method may suffer from local minimum for optimization on small datasets i would like more in depth discussion in their similarity and difference to gain more attention for gmu i would encourage the author to open source their code and try more datasets,6.0
780.json,this paper analyzes the ring based allreduce approach for multi gpu data parallel training of deep net comments the name linear pipeline is somewhat confusing to the readers as the technique is usually referred as ring based approach in allreduce literature the author should use the standard name to make the connection easier the cost analysis of ring based allreduce is already provided in the existing literature this paper applied the analysis to the case of multi gpu deep net training and concluded that the scaling is invariant of number of gpus the ring based allreduce approach is already supported by nvidia s nccl library although the authors claim that their implementation comes earlier than the nccl implementation the overlap of communication of computation is an already applied technique in systems such as tensorflow and mxnet the schedule proposed by the authors exploits the overlap partially doing backprop of t while doing reduce note that the dependency pattern can be further exploited with the forward of layer t depend on update of parameter of layer t in last iteration this can be done by a dependency scheduler since this paper is about analysis of allreduce it would be nice to include detailed analysis of tree shape reduction ring based approach and all to all approach the discussion of all to all approach is missing in the current paper in summary this is a paper discussed existing allreduce techniques for data parallel multi gpu training of deep net with cost analysis based on existing results while i personally find the claimed result not surprising as it follows from existing analysis of allreduce the analysis might help some other readers i view this as a baseline paper the analysis of allreduce could also been improved see comment,5.0
515.json,this paper proposes to use the tensor train tt decomposition to represent the full polynomial linear model the tt form can reduce the computation complexity in both of inference and model training a stochastic gradient over a riemann manifold has been proposed to solve the tt based formulation the empirical experiments validate the proposed method the proposed approach is very interesting and novel for me i would like to vote acceptance on this paper my only suggestion is to include the computational complexity per iteration,7.0
779.json,this paper conducts a comprehensive series of experiments on vocabulary selection strategies to reduce the computational cost of neural machine translation a range of techniques are investigated ranging from very simple methods such as word co occurences to the relatively complex use of svms the experiments are solid comprehensive and very useful in practical terms it is good to see that the best vocabulary selection method is very effective at achieving a very high proportion of the coverage of the full vocabulary model fig however i feel that the experiments in section vocabulary selection during training was rather limited in their scope i would have liked to see more experiments here a major criticism i have with this paper is that there is little novelty here the techniques are mostly standard methods and rather simple and in particular there it seems that there is not much additional material beyond the work of mi et al so although the work is solid the lack of originality lets it down minor comments in the word co occurence measure was any smoothing used to make this measure more robust to low counts,5.0
779.json,this paper compares several strategies for guessing a short list of vocabulary for the target language in neural machine translation the primary findings are that word alignment dictionaries work better than a variety of other techniques my take on this paper is that to have a significant impact it needs to make the case for why one might want vocabulary rather than characters or sub word units like bpe i think there are likely many very good reasons to do this that could be argued for synthesize morphology deal with transliteration etc but most of these would suggest some particular models and experiments which are of course not in this paper as it is i think this paper is a useful but minor contribution that shows that word alignment is a good way of getting short lists but it does not strongly make the case that we should abandon work in other directions minor comments in addition to the svm approach for modeling vocabulary the discriminative word lexicon of mauser et al and the neural version of ha et al are also worth mentioning it would be useful to know what the coverage rate of the actual full vocabulary would be rather than the k full vocabulary since presumably this technique could be used to work with much larger vocabularies when reducing the vocabulary size for training the mi et al technique of taking the union of all the vocabularies in a mini batch seems like a rather strange objective if the vocabulary of a single sentence is used the probabilistic semantics of the translation model can still be preserved since p e f vocab f p e f if p vocab f f i e is deterministic which it is here whereas the objective is no longer a sensible probability model in the mini batch vocabulary case thus while it may be a bit more difficult to implement it seems like it would at least be a sensible comparison to make,4.0
407.json,in this paper a well known soft mixture of experts model is adapted for and applied to a specific type of transfer learning problem in reinforcement learning rl namely transfer of action policies and value functions between similar tasks although not treated as such the experimental setup is reminiscent of hierarchical rl works an aspect which the paper does not consider at length regrettably one possible implication of this work is that architecture and even learning algorithm choices could simply be stated in terms of the objective of the target task rather than being hand engineered by the experimenter this is clearly an interesting direction of future work which the paper illuminates pros the paper diligently explains how the network architecture fits in with various widely used reinforcement learning setups which does facilitate continuation of this work the experiments are good proofs of concept but do not go beyond that i m h o even so this work provides convincing clues that collections of deep networks which were trained on not entirely different tasks generalize better to related tasks when used together rather than through conventional transfer learning e g fine tuning cons as the paper well recounts in the related work section libraries of fixed policies have long been formally proposed for reuse while learning similar tasks indeed it is well understood in hierarchical rl literature that it can be beneficial to reuse libraries of fixed fernandez veloso or jointly learned policies which may not apply to the entire state space e g options pricop et al what is not well understood is how to build such libraries and this paper does not convincingly shed light in that direction as far as i can tell the transfer tasks have been picked to effectively illustrate the potential of the proposed architecture but the paper does not tackle negative transfer or compositional reuse in well known challenging situations outlined in previous work e g parisotto et al rusu el al since the main contributions are of an empirical nature i am curious how the results shown in figures look plotted against wall clock time since relatively low data efficiency is not a limitation for achieving perfect play in pong see mnih et al it would be more illuminating to consider tasks where final performance is plausibly limited by data availability it would also be interesting if the presented results were achieved with reduced amounts of computation or reduced representation sizes compared to learning from scratch especially when one of the useful source tasks is an actual policy trained on the target task finally it is perhaps underwhelming that it takes a quarter of the data required for learning pong from scratch just to figure out that a perfect pong policy is already in the expert library simply evaluating each expert for episodes and using an average score weighted majority vote to mix action choices would probably achieve the same final performance for a smaller fraction of the data,7.0
309.json,this paper is about improving feature learning in deep reinforcement learning by augmenting the main policy optimization problem with terms corresponding to domain independent auxiliary tasks these tasks are about control learning other policies that attempt to maximally modify the state space i e here the pixels immediate reward prediction and value function replay except for the latter these auxiliary tasks are only used to help shape the features by sharing the cnn lstm feature extraction network experiments show the benefits of this approach on atari and labyrinth problems with in particular much better data efficiency than ac the paper is well written ideas are sound and results pretty convincing so to me this is a clear acceptance at high level i only have few things to say none being of major concern i believe you should say something about the extra computational cost of optimizing these auxiliary tasks how much do you lose in terms of training speed which are the most costly components if possible please try to make it clearer in the abstract intro that the agent is learning different policies for each task when i read in the abstract that the agent also maximises many other pseudo reward functions simultaneously by reinforcement learning my first understanding was that it learned a single policy to optimize all rewards together and i realized my mistake only when reaching eq the feature control idea is not validated empirically the preliminary experiment in fig is far from convincing as it only seems to help slightly initially i like that idea but i am worried by the fact the task is changing during learning since the extracted features are being modified there might be stability convergence issues at play here since as you mentioned the performance of our agents is still steadily improving why not keep them going to see how far they go at least the best ones why are not the auxiliary tasks weight parameters the lambda hyperparameters to optimize were there any experiments to validate that using was a good choice please mention the fact that auxiliary tasks are not trained with true q learning since they are trained off policy with more than one step of empirical rewards as discussed in the openreview comments minor stuff policy gradient algorithms adjust the policy to maximise the expected reward lpi that actually a loss to be minimized in eq lambdac should be within the sum just below eq rt c should be rt k c figure does not seem to be referenced in the text also figure d should be referenced in the features discovered in this manner is shared are shared the text around eq refers to the loss lpc but that term is not defined and is not explicitly in eq please explain what clip means for dueling networks in the legend of figure i would have liked to see more ablated versions on atari to see in particular if the same patterns of individual contribution as on labyrinth were observed in the legend of figure the mentioned are for labyrinth which is not clear from the text in figure right shows it is actually the top left plot of the figure also later this is shown in figure top should be figure top right figure shows the learning curves for the top hyperparameter settings on three labyrinth navigation levels i think it is referring to the left and middle plots of the figure so only on two levels the text above might also need fixing in the left side shows the average performance curves of the top agents for all three methods the right half shows missing a comma or something after methods appendix further details are included in the supplementary materials where are they what is the value of lambdapc i guess edit i know some of my questions were already answered in comments no need to re answer them,8.0
309.json,this work proposes to train rl agents to also perform auxiliary tasks positing that doing so will help models learn stronger features they propose two pseudo control tasks control the change in pixel intensity and control the activation of latent features they also propose a supervised regression task predict immediate reward following a sequence of events the latter is learned offline via a skewed sampling of an experience replay buffer in order to balance seeing reward or not to chance such agents perform significantly well on discrete action continuous space rl tasks and reach baseline performance in x less iterations this work contrasts with traditional passive unsupervised or model based learning instead of forcing the model to learn a potentially useless representation of the input or to learn the possibly impossible due to partial observability task modelling objective learning to control local and internal features of the environment complements learning the optimal control policy to me the approach is novel and proposes a very interesting alternative to unsupervised learning that takes advantage of the possibility of control that an agent has over the environment the proposed tasks are explained at a rather high level which is convenient to understand intuition but i think some lower level of detail might be useful for example lpc should be explicitly mentioned before reaching the appendix otherwise this work is clear and easily understandable by readers familiar with deep rl the methodology is sound on one hand hand the distribution of best hyperparameters might be different for ac and unreal but also measuring top ensures that presuming that the both best hyperparameters for ac and for unreal are within the explored intervals the per method best hyperparameters are found i think one weakness of the paper or rather considering the number of things that can fit in a paper crucially needed future work is that there is very little experimental analysis of the effect of the auxiliary tasks appart from their very strong effect on performance in the same vein pixel feature control seems to have the most impact in labyrinth just ac pc beats anything else except unreal i think it would have been worth looking at this either in isolation or in more depth measuring more than just performance on rl tasks,8.0
535.json,this paper presents a model for video captioning with both soft and hard attention using a cd network for the encoder and a rnn for the decoder experiments are presented on youtubetext m vad and msr vtt while the ideas of image captioning with soft and hard attention and video captioning with soft attention have already been demonstrated in previous work the main contribution here is the specific architecture and attention over different layers of the cnn the work is well presented and the experiments clearly show the benefit of attention over multiple layers however in light of previous work in captioning the contribution and resulting insights is too incremental for a conference paper at iclr further experiments and analysis of the main contribution would strengthen the paper but i would recommend resubmission to a more suitable venue,4.0
427.json,the paper presents a theoretically well motivated for visualizing what parts of the input feature map are responsible for the output decision the key insight is that features that maximally change the output and are simultaneously more unpredictable from other features are the most important ones most previous work has focused on finding features that maximally change the output without accounting for their predictability from other features authors build upon ideas presented in the work of robnik šikonja kononenko the results indicate that the proposed visualization mechanism based on modeling conditional distribution identifies more salient regions as compared to a mechanism based on modeling marginal distribution i like that authors have presented visualization results for a single image across multiple networks and multiple classes there results show that the proposed method indeed picks up on class discriminative features authors have provided a link to visualizations for a random sample of images in a comment i encourage the authors to include this in the appendix of the paper my one concern with the paper is zeiler et al proposed a visualization method by greying small square regions in the image this is similar to computing the visualization using the marginal distribution authors compute the marginal visualization using samples however in the limit of infinite samples the image region would be gray the conditional distribution is computed using a normal distribution that provides some regularization and therefore estimating the conditional and marginal distributions using samples each is not justified i would like to see the comparison when grey image patches akin to zeiler et al are used for visualization against the approach based on the conditional distribution,6.0
427.json,the authors propose a way to visualize which areas of an image provide mostly influence a certain dnn response mostly they apply some very elegant and convincing improvements to the basic method by robnik sikonja and konononko from to dnns thus improving it analysis and making it usable for images and dnns the authors provide a very thorough analysis of their methods and show very convincing examples which they however handpicked it would be very nice to have maybe at least one figure showing the analysis on e g random picks from imagenet one thing i would like to see is how their method compares to some other methods they mention in the introduction like gradient based ones or deconvolution based ones they paper is very clearly written all necessary details are given and the paper is very nice to read alltogether the problem of understanding how dnns function and how they draw their conclusions is discussed a lot the author method provides a clear contribution that can lead to further progress in this field e g i like figure showing how alexnet googlenet and vgg differ in where they collect evidence from i can think of several potential applications of the method and therefore consider it of high significance update the authors did a great job of adopting all of my suggestions therefore i improve the rating from to,9.0
562.json,this paper proposes an extension of the gan framework known as gap whereby multiple generators and discriminators are trained in parallel the generator discriminator pairing is shuffled according to a periodic schedule pros the proposed approach is simple and easy to replicate cons the paper is confusing to read the results are suggestive but do not conclusively show a performance win for gap the main argument of the paper is that gap leads to improved convergence and improved coverage of modes the coverage visualizations are suggestive but there still is not enough evidence to conclude that gap is in fact improving coverage and for convergence it is difficult to assess the effect of gap on the basis of learning curves the proposed gam ii metric is circular in that model performance depends on the collection of baselines the model is being compared with estimating likelihood via ais seems to be a promising way to evaluate as does using the inception score perhaps a more systematic way to determine gap effect would be to set up a grid search of hyperparameters and train an equal number of gans and gap gans for each setting then a histogram over final inception scores or likelihood estimates of the trained models would help to show whether gap tended to produce better models overall the approach seems promising but there are too many open questions regarding the paper in its current form section remark that when seems like a to do section a the proposed metric is not described in adequate detail,4.0
718.json,the multiagent system is proposed as a generalization of neural network the proposed system can be used with less restrictive network structures more efficiently by computing only those necessary computations in the graph unfortunately i do not find the proposed system different from the framework of artificial neural network although for today neural network structures are designed to have a lot of matrix matrix multiplications but it is not limited to have such architecture in other words the proposed multiagent system can be framed in the artificial neural network with more complicated layer connectivity structures while considering each neuron as layer the computation efficiency is argued among different sparsely connected denoising autoencoder in multiagent system framework only but the baseline comparison should be against the fully connected neural network that employs matrix matrix multiplication,3.0
466.json,this paper investigates the identity parametrization also known as shortcuts where the output of each layer has the form h x x instead of h x this has been shown to perform well in practice eg resnet the discussions and experiments in the paper are interesting here a few comments on the paper section studying the linear networks is interesting by itself however it is not clear that how this could translate to any insight about non linear networks for example you have proved that every critical point is global minimum i think it is helpful to add some discussion about the relationship between linear and non linear networks section the construction is interesting but the expressive power of residual network is within a constant factor of general feedforward networks and i do not see why we need a different proof given all the results on finite sample expressivity of feedforward networks i appreciate if you clarify this section i like the experiments the choice of random projection on the top layer is brilliant however since you have combined this choice with all convolutional residual networks it is hard for the reader to separate the affect of each of them therefore i suggest reporting the numbers for all convolutional residual networks with learned top layer and also resnet with random projection on the top layer minor comments i do not agree that batch normalization can be reduced to identity transformation and i do not know if bringing that in the abstract without proper discussion is a good idea page above assumption x i x i,6.0
489.json,this paper analyzes various unsupervised sentence embedding approaches by means of a set of auxiliary prediction tasks by examining how well classifiers can predict word order word content and sentence length the authors aim to assess how much and what type of information is captured by the different embedding models the main focus is on a comparison between and encoder decoder model ed and a permutation invariant model cbow there is also an analysis of skip thought vectors but since it was trained on a different corpus it is hard to compare there are several interesting and perhaps counter intuitive results that emerge from this analysis and the authors do a nice job of examining those results and for the most part explaining them however i found the discussion of the word order experiment rather unsatisfying it seems to me that the appropriate question should have been something like how well does model x do compared to the theoretical upper bound which can be deduced from natural language statistics this is investigated from one angle in section but i would have preferred to the effect of natural language statistics discussed up front rather than presented as the explanation to a urprising observation i had a similar reaction to the word order experiments most of the interesting results in my opinion are about the ed model it is fascinating that the lstm encoder does not seem to rely on natural language ordering statistics it seems like doing so should be a big win in terms of per parameter expressivity i also think that it strange that word content accuracy begins to drop for high dimensional embeddings i suppose this could be investigated by handicapping the decoder overall this is a very nice paper investigating some aspects of the information content stored in various types of sentence embeddings i recommend acceptance,8.0
489.json,this paper presents a set of experiments investigating what kinds of information are captured in common unsupervised approaches to sentence representation learning the results are non trivial and somewhat surprising for example they show that it is possible to reconstruct word order from bag of words representations and they show that lstm sentence autoencoders encode interpretable features even for randomly permuted nonsense sentences effective unsupervised sentence representation learning is an important and largely unsolved problem in nlp and this kind of work seems like it should be straightforwardly helpful towards that end in addition the experimental paradigm presented here is likely more broadly applicable to a range of representation learning systems some of the results seem somewhat strange but i see no major technical concerns and think that that they are informative i recommend acceptance one minor red flag the massive drop in cbow performance in figures b and b are not explained and seem implausible enough to warrant serious further investigation can you be absolutely certain that those results would appear with a different codebase and different random seed implementing the same model fortunately this point is largely orthogonal to the major results of the paper two writing comments i agree that the results with word order and cbow are surprising but i think it slightly misleading to say that cbow is predictive of word order it does not represent word order at all but it possible to probabilistically reconstruct word order from the information that it does encode saying that lstm auto encoders are more effective at encoding word order than word content does not really make sense these two quantities are not comparable,8.0
523.json,the authors introduce an adaptive softmax approximation tailored for faster performance on gpus the key idea which is very sensible is to use a class based hierarchical softmax but where the clusters hierarchy are distributed such that the resulting matrix multiplications are optimally sized for gpu computation based on their empirical tests their results indicate that the system does indeed work very well in terms of presentation i found the paper to have both clear and unclear elements fortunately the underlying concepts and logic seem quite clear unfortunately at various points the writing is not there are various minor typos as mentioned by anonreviewer in addition to some other spots e g the notation describing recurrent network in section mentions an xt which is surely different from the xt used in the previous paragraph on regular feedforward nn i think it belonged in the equation for ht the use of the two matrices a and p in eq is strange etc also while section intuition for cluster case was a good idea to include and helpful and while the concepts underlying the complexity analysis were straightforward it could be made a lot clearer by a adding an additional figure such as figure along with b a few well placed additional sentences unpacking the logic of the argument into easier to follow steps for example it was only when i saw eq and combined with fig that the analysis on the previous page made more sense in terms of arriving at the eq for the complexity of putting the head of the distribution in the root of the tree perhaps an appendix might be the most appropriate place to add such an explanation,7.0
671.json,the authors present a general framework for defining a wide variety of recurrent neural network architectures including seqseq models tree structured models attention and a new family of dynamically connected architectures the framework defines a new general purpose recurrent unit called the tbru which takes a transition system defining and constraining its inputs and outputs and input function which defines the mapping between raw inputs and fixed width vector representations and recurrence function that defines the inputs to each recurrent step as a function of the current state and an rnn cell that computes the output from the input fixed and recurrent many example instantiations of this framework are provided including sequential tagging rnns google s parsey mcparseface parser encoder decoder networks tree lstms and less familiar examples that demonstrate the power this framework the most interesting contribution of this work is the ease by which it can be used to incorporate dynamic recurrent connections through the definition of the transition system in particular this paper explores the application of these dynamic connections to syntactic dependency parsing both as a standalone task and by multitasking parsing with extractive summarization using the same compositional phrase representations as features for the parser and summarization previous work used discrete parse features which is particularly simple elegant in this framework in experimental results the authors demonstrate that such multitasking leads to more accurate summarization models and using the framework to incorporate more structure into existing parsing models also leads to increased accuracy with no big oh efficiency loss compared with e g attention the raison d etre in particular the example perhaps described even more thoroughly explicitly should be made as clear as possible as soon as possible this is the most important contribution but it gets lost in the description and presentation as a framework emphasizing that attention seqseq etc can be represented in the framework is distracting and makes it seem less novel than it is anonreviewer clearly missed this point as did i in my first pass over the paper to get this idea across and to emphasize the benefits of this representation i d love to see more detailed analysis of these representations and their importance to achieving your experimental results i think it would also be helpful to emphasize the difference between a stack lstm and example overall i think this paper presents a valuable contribution though the exposition could be improved and analysis of experimental results expanded,7.0
630.json,summary this paper proposes a read again attention based representation of the document with the copy mechanism for the summarization task the model reads each sentence in the input document twice and creates a hierarchical representation of it instead of a bidirectional rnn during the decoding it uses the representation of the document obtained via the read again mechanism and points the words that are oov in the source document the model does abstractive summarization the authors show improvements on duc dataset and provide an analysis of their model with different configurations contributions the main contribution of this paper is the read again attention mechanism where the model reads the same sentence twice and obtains a better representation of the document writing the text of this paper needs more work there are several typos and the explanations of the model architecture are not really clear some parts of the paper feel somewhat bloated pros the proposed model is a simple extension to the model to the model proposed in for summarization the results are better than the baselines cons the improvements are not that large justifications are not strong enough the paper needs a better writeup several parts of the text are not using a clear precise language and the paper needs a better reorganization some parts of the text is somewhat informal the paper is very application oriented question how does the training speed when compared to the regular lstm some criticisms a similar approach to the read again mechanism which is proposed in this paper has already been explored in in the context of algorithmic learning and i wouldn t consider the application of that on the summarization task a significant contribution the justification behind the read again mechanism proposed in this paper is very weak it is not really clear why additional gating alphai is needed for the read again stage as authors also suggest pointer mechanism for the unknown rare words and it is adopted for the read again attention mechanism however in the paper it is not clear where the real is the gain coming from whether from read again mechanism or the use of pointing the paper is very application focused the contributions of the paper in terms of ml point of view is very weak it is possible to try this read again mechanism on more tasks other than summarization such as nmt in order to see whether if those improvements are the writing of this paper needs more work in general it is not very well written minor comments some of the corrections that i would recommend fixing on page better than a single value scalar gating on page single value lacks the ability to model the variances among these dimensions scalar gating couldn t capture the on page where h and h are initial zero vectors h and h are initialized to a zero vector in the beginning of each sequence there are some inconsistencies for example parts of the paper refer to tab and some parts of the paper refer to table better naming of the models in table is needed the location of table is a bit off zaremba wojciech and ilya sutskever reinforcement learning neural turing machines arxiv preprint arxiv gulcehre caglar et al pointing the unknown words arxiv preprint arxiv,5.0
630.json,this work explores the neural models for sentence summarisation by using a read again attention model and a copy mechanism which grants the ability of direct copying word representations from the source sentences the experiments demonstrate the model achieved better results on duc dataset overall this paper is not well written there are confusing points some of the claims are lack of evidence and the experimental results are incomplete detailed comments read again attention how does it work better than a vanilla attention what would happen if you read the same sentences multiple times have you compared it with staked lstm with same number of parameters there is no model ablation in the experiment section why do you need reading two sentences the gigaword dataset is a source to compression dataset which does not need multiple input sentences how do you compare your model with single sent input and two sent input copy mechanism what if there are multiple same words appeared in the source sentences to be copied according to equation you only copy one vector to the decoder however there is no this kind of issue for a hard copy mechanism besides there is no comparison between the hard copy mechanism and this vector copy mechanism in the experiment section vocabulary size this part is a bit off the main track of this paper if there is no evidence showing this is the special property of vector copy mechanism it would be trivial in this paper experiments on the duc dataset it compares the model with other up to date models while on the gigaword dataset paper only compares the model with the abs rush et al and the gru which are quite weak baseline models it is irresponsible to claim this model achieved the state of the art performance in the context of summarization typos tab table fig,5.0
519.json,the authors show how the hidden states of an lstm can be normalised in order to preserve means and variances the method s gradient behaviour is analysed experimental results seem to indicate that the method compares well with similar approaches points the writing is sloppy in parts see at the end of the review for a non exhaustive list the experimental results show marginal improvements of which the the statistical significance is impossible to asses not completely the author s fault for ptb as they partially rely on results published by others weight normalisation seems to be a viable alternative in the the performance and runtime are similar the implementation complexity of weight norm is however arguably much lower more effort could have been put in by the authors to clear that up in the current state practitioners as well as researchers will have to put in more effort to judge whether the proposed method is really worth it for them to replicate section is nice and i applaud the authors for doing such an analysis list of typos etc maintain maintain requisits requisites a lstm an lstm the gradients of ot and ft are equivalent to equation gradients cannot be equivalent to an equation beacause because one of the γx γh at the end of page is wrong,4.0
372.json,this paper proposes a new memory module for large scale life long and one shot learning the module is general enough that the authors apply the module to several neural network architectures and show improvements in performance using k nearest neighbors for memory access is not completely new this has been recently explored in rae et al and chandar et al k nearest neighbors based memory for one shot learning has also been explored in r this paper provides experimental evidence that such an approach can be applied to a variety of architectures authors have addressed all my pre review questions and i am ok with their response are the authors willing to release the source code to reproduce the results at least for omniglot experiments and synthetic task experiments references r charles blundell benigno uria alexander pritzel yazhe li avraham ruderman joel z leibo jack rae daan wierstra demis hassabis model free episodic control corr abs,6.0
667.json,this paper addresses the problem of data sparsity in the healthcare domain by leveraging hierarchies of medical concepts organized in ontologies the paper focuses on sequential prediction given a patient s medical record a sequence of medical codes some of which might occur very rarely instead of simply assigning each medical code an independent embedding before feeding it to an rnn the proposed approach assigns each node in the medical ontology a basic embedding and composes a final embedding for each medical code by taking a learned weighted average via an attention mechanism of the medical code s ancestors in the ontology notably the paper is well written and the approach is quite intuitive i have the following comments why is the patient s visit taken as just the sum of medical codes found in the visit and not say the average or a learned weighted average wouldn t this bias for against the number of codes in the visit i don t see why basic embeddings are not fine tuned as well did you find that to hurt performance do you have an explanation for that looking at figure the results seem very close and the figures are not very clear figure b top is missing also i am wondering how significant the differences are so it would be nice to comment on that finally i think this is an interesting application paper applying well established deep learning techniques the paper deals with an important issue that arises when applying deep learning models in domains with scarce data resources however i would like the authors to comment on what there paper offers as new insights to the iclr community and why they think iclr is a good avenue for their work,6.0
536.json,the approximation capabilities of neural networks have been studied before for approximating different classes of functions the goal of this paper is to provide an analog of the approximation theorem for the class of noise stable functions the class of functions that are noise stable and their output does not significantly depend on an individual input seems an interesting class and therefore i find the problem definition interesting the paper is well written and it is easy to follow the proofs and arguments i have two major comments presentation the way i understand this arguments is that the noise stability measures the true dimensionality of the data based on the dependence of the function on different dimensions therefore it is possible to restate and prove an analog to the approximation theorems based on true dimensionality of data it is also unclear when the stability based bounds are tighter than dimension based bounds as both of them grow exponentially i find these discussions interesting but unfortunately the authors present the result as some bound that does not depend on the dimension and a constant that grows exponentially with eps this is not entirely the right picture because the epsilon in the stability could itself depend on the dimension i believe in most problems epsilon grows with the dimension contribution even though the connection is new and interesting the contribution of the paper is not significant enough the presented results are direct applications of previous works and most of the lemmas in the paper are restating the known results i believe more discussions and results need to be added to make this a complete work,5.0
473.json,this work offers a theoretical justification for reusing the input word embedding in the output projection layer it does by proposing an additional loss that is designed to minimize the distance between the predictive distribution and an estimate of the true data distribution this is a nice setup since it can effectively smooth over the labels given as input however the construction of the estimate of the true data distribution seems engineered to provide the weight tying justification in eqs and it is not obvious why the projection matrix l in eq let rename it to l should be the same as that in eq for example l could be obtained through wordvec embeddings trained on a large dataset or it could be learned as an additional set of parameters in the case that l is a new learned matrix it seems the result in eq is to use an independent matrix for the output projection layer as is usually done the experimental results are good and provide support for the approximate derivation done in section particularly the distance plots in figure minor comments third line in abstract where model where the model second line in section into space into the space should not the rhs in eq be sum tilde y t i frac hat y t tilde y t i ei,6.0
473.json,this paper provides a theoretical framework for tying parameters between input word embeddings and output word representations in the softmax experiments on ptb shows significant improvement the idea of sharing or tying weights between input and output word embeddings is not new as noted by others in this thread which i see as the main negative side of the paper the proposed justification appears new to me though and certainly interesting i was concerned that results are only given on one dataset ptb which is now kind of old in that literature i am glad the authors tried at least one more dataset and i think it would be nice to find a way to include these results in the paper if accepted have you considered using character or sub word units in that context,7.0
473.json,this paper gives a theoretical motivation for tieing the word embedding and output projection matrices in rnn lms the argument uses an augmented loss function which spreads the output probability mass among words with close word embedding i see two main drawbacks from this framework the augmented loss function has no trainable parameters and is used for only for regularization this is not expected to give gains with large enough datasets the augmented loss is heavily engineered to produce the desired result of parameter tying it s not clear what happens if you try to relax it a bit by adding parameters or estimating y in a different way nevertheless the argument is very interesting and clearly written the simulated results indeed validate the argument and the ptb results seem promising minor comments section can you clarify if y is conditioned on the t example or on the entire history eq i is enumerated over v not v,8.0
561.json,this paper introduces a hierarchical clustering method using learned cnn features to build the tree of life the assumption is that the feature similarity indicates the distance in the tree the authors tried three different ways to construct the tree approximation central point minimum spanning tree and multidimensional scaling based method out of them mds works the best it is a nice application of using deep features however i lean toward rejecting the paper because the following reasons all experiments are conducted in very small scale the experiments include fish species canine species vehicle classes there are no quantitative results only by visualizing the generated tree versus the wordnet tree moreover the assumption of using wordnet is not quite valid wordnet is not designed for biology purpose and it might not reflect the true evolutionary relationship between species limited technical novelty most parts of the pipeline are standard e g use pretrained model for feature extraction use previous methods to construct hierarchical clustering i think the technical contribution of this paper is very limited,4.0
561.json,i like this paper in that it is a creative application of computer vision to biology or at least that would be a good narrative but i am not confident biologists would actually care about the tree of life built from this method there not really any biology in this paper either in methodology or evaluation it boils down to a hierarchical clustering of visual categories with ground truth assumed to be the wordnet hierarchy which may or may not be the biological ground truth inheritance relationships between species if that is even possible to define it probably is not for dog species which interbreed and it definitely is not for vehicles or the actual biological inheritance tree or what humans would do in the same task if we are just worried about visual relationships and not inheritance relationships then a graph is the right structure not a tree a tree is needlessly lossy and imposes weird relationships e g imagenet has a photo of a toy rabbit and by tree distance it is maximally distant from rabbit because the toy is in the devices top level hierarchy and the real rabbit is in the animal branch are those two images really as semantically unrelated as is possible our visual world is not a hierarchy our biological world can reasonably be defined as one one could define the task of trying to recover the biological inheritance tree from visual inputs although we know that would be tough to do because of situations like convergent evolution still one could evaluate how well various visual features can recover the hierarchical relationship of biological organisms this paper does not quite do that and even if it did it would still feel like a bit of a solution in search of a problem the paper says that this type of exercise can help us understand deep features but i am not sure sure how much it reveals i guess it a fair question to ask if a particular feature produces meaningful class to class distances but it not clear that the biological tree of life or the wordnet hierarchy is the right ground truth for that i would argue it not finally the paper mentions human baselines in a few places but i am not really seeing it experiments show that the proposed method using deep representation is very competitive to human beings in building the tree of life based on the visual similarity of the species and then later the reconstructed quality is as good as what human beings could reconstruct based on the visual similarity that the extent of the experiment a qualitative result and the declaration that it as good as humans could do,4.0
424.json,this paper first discusses a general framework for improving optimization of a complicated function using a series of approximations if the series of approximations are well behaved compared to the original function the optimization can in principle be sped up this is then connected to a particular formulation in which a neural network can behave as a simpler network at high noise levels but regain full capacity as training proceeds and noise lowers the idea and motivation of this paper are interesting and sound as mentioned in my pre review question i was wondering about the relationship with shaping methods in rl i agree with the authors that this paper differs from how shaping typically works by modifying the problem itself because in their implementation the architecture is what is shaped nevertheless the central idea in both cases is to solve a series of optimization problems of increasing difficulty therefore i strongly suggest including a discussion of the differences between shaping curriculum learning i am also not sure how this is different from shaping and the present approach the presentation of the method for neural networks lacks clarity in presentation improving this presentation will make this paper much easier to digest in particular alg can not be understood at the point that it is referenced please explain the steps to eq more clearly and connect to steps in alg define u x clearly before defining u x there are several concerns with the experimental evaluations there should be a discussion about why does not the method work for solving much more challenging network training problems such as thin and deep networks some specific concerns the mlps trained parity and pentomino are not very deep at all an experiment of training thin networks with systematically increasing depth would be a better fit to test this method network depth is well known to pose optimization challenges instead it is stated without reference that learning the mapping from sequences of characters to the word embeddings is a difficult problem for cases where the gain is primarily due to the regularization effect this method should be compared to other weight noise regularization methods i also suggest comparing to highway networks since there are thematic similarities in eq and it is possible that they can automatically anneal their behavior from simple to complex nets during training considering that they are typically initialized with a bias towards copying behavior for cifar experiment does the mollified model also use residual connections if so why in either case why does the mollified net actually train slower than the residual and stochastic depth networks this is inconsistent with the mlp results overall the ideas and developments in this paper are promising but it needs more work to be a clear accept for me,6.0
577.json,this paper proposes a method that attempts to understand what is happening within a neural network by using linear classifier probes which are inserted at various levels of the network i think the idea is nice overall because it allows network designers to better understand the representational power of each layer in the network but at the same time this works feels a bit rushed in particular the fact that the authors did not provide any results in real networks which are used to win competitions makes the results less strong since researchers who want to created competitive network architectures do not have enough evidence from this work to decides whether they should use it or not ideally i would encourage the authors to consider continuing this line of research and show how to use the information given by these linear classifiers to construct better network architectures unfortunately as is i do not think we have enough novelty to justify accepting this work in the conference,5.0
432.json,this is a very nicely written paper which unifies some value based and policy based regularized policy gradient methods by pointing out connections between the value function and policy which have not been established before the theoretical results are new and insightful and will likely be useful in the rl field much beyond the specific algorithm being proposed in the paper this being said the paper does exploit the theory to produce a unified version of q learning and policy gradient which proves to work on par or better than the state of art algorithms on the atari suite the empirical section is very well explained in terms of what optimization were done one minor comment i had was related to the stationary distribution used for a policy there are subtleties here between using a discounted vs non discounted distribution which are not crucial in the tabular case but will need to be addressed in the long run in the function approximation case this being said there is no major problem for the current version of the paper overall the paper is definitely worthy of acceptance and will likely influence a broad swath of rl as it opens the door to further theoretical results as well as algorithm development,9.0
432.json,nice paper exploring the connection between value based methods and policy gradients formalizing the relation between the softmax like policy induced by the q values and a regularized form of pg presentation although that seems to be the flow in the first part of the paper i think it could be cast as a extension generalization of the dueling q network for me that would be a more intuitive exposition of the new algorithm and findings small concern in general case derivation section eq the expectation s a is wrt to pi which is a function of theta that dependency seems to be ignored although it is key to the pg update derivation if these policies the sampling policy for the expectation and pi are close enough it usually okay but except for particular cases trust region methods co that generally not true thus you might end up solving a very different problem than the one you actually care solving results a comparison with the dueling architecture could be added as that would be the closest method it would be nice to see if and in which game you get an improvement overall strong paper good theoretical insights,7.0
672.json,the paper introduces the joint multimodal variational autoencoder a directed graphical model for modeling multimodal data with latent variable the model is rather straightforward extension of standard vae where two data modalities are generated from a shared latent representation independently in order to deal with missing input modalities or bi directional inference between two modalities the paper introduces modality specific encoder that is trained to minimize the kl divergence of latent variable distributions between joint and modality specific recognition networks the paper demonstrates its effectiveness on mnist and celeba datasets both in terms of test log likelihoods and the conditional image generation and editing the proposed method is rather straightforward extension of vae and therefore the model should inherent the probabilistic inference methods of vae for example for missing data modalities the model should be able to infer joint representation as well as filling in the missing modalities via iterative sampling as introduced by rezende et al given marginal improvement i am not convinced by the contribution of modality specific encoders in section in addition the inference methods introduced for generating figure looks somewhat unprincipled i am wondering the conditional image generation results by following more principled approach e g iterative sampling experimental results on joint image attribute generation is also missing,5.0
672.json,the proposed method of modeling multimodal datasets is a vae with an inference network for every combination of missing and present modalities the method is evaluated on modeling mnist and celeba datasets mnist is hardly a multimodal dataset the authors propose to use the labels as a separate modality that gets modeled with a variational autoencoder the reviewer finds this choice perplexing even then the modalities are never actually missing so the applicability of the suggested method is questionable in addition the differences in log likelihoods between different models are tiny and likely to be due to noise the other experiment reports log likelihood of models that were not trained to maximize log likelihood it is not clear what conclusions can be drawn from such comparison,3.0
737.json,the squeezenet paper came out in feb and i read it with interest it has a series of completely reasonable engineering suggestions for how to save parameter memory for cnns for object recognition imagenet the suggestions make a lot of sense and provide an excellent compression of about x versus alexnet looks like x if combined with han so very nice results definitely worth publishing since the arxiv paper came out people have noticed and worked to extend the paper this is already evidence that this paper will have impact and deserves to have a permanent published home on the negative side the architecture was only tested on imagenet unclear whether the ideas transfer to other tasks e g audio or text recognition and as with many other architecture tweaking papers there is no real mathematical or theoretical support for the ideas they are just sensible and empirically work oh the whole i think the paper deserves to appear at iclr being in the mainline of work on deep learning architectures,7.0
625.json,description this paper presents a reinforcement learning architecture where based on natural language input a meta controller chooses subtasks and communicates them to a subtask controller that choose primitive actions based on the communicated subtask the goal is to scale up reinforcement learning agents to large scale tasks the subtask controller embeds the subtask definition arguments into vectors by a multi layer perceptron including an analogy making regularization the subtask vectors are combined with inputs at each layer of a cnn cnn outputs given the observation and the subtask are then fed to one of two mlps one to compute action probabilities in the policy exponential falloff of mlp outputs and the other to compute termination probability sigmoid from mlp outputs the meta controller takes a list of sentences as instructions embeds them into a sequence of subtask arguments not necessarily a one to one mapping a context vector is computed by a cnn from the observation the previous sentence embedding the previous subtask and its completion state the subtask arguments are computed from the context vector through further mechanisms involving instruction retrieval from memory pointers and hard soft decisions whether to update the subtask or not training involves policy distillation actor critic training for the subtask controller and actor critic training for the meta controller keeping the subtask controller frozen the system is tested in a grid world where the agent moves and interacts with picks up transforms various item enemy types it is compared to a a flat controller not using a subtask controller and b subtask control by mere concatenation of the subtask embedding to the input with without the analogy making regularization evaluation the proposed architecture seems reasonable although it is not clear why the specific way of combining subtask embeddings in the subtask controller would be the right way to do it i do not feel the grid world here really represents a large scale task in particular the x size of the grid is very small this is disappointing since this was a main motivation of the work moreover the method is not compared to any state of the art alternatives this is especially problematic because the test is not on established benchmarks it is not really possible based on the shown results to put the performance in context of other works,4.0
625.json,this paper can be seen as instantiating a famous paper by the founder of ai john mccarthy on learning to take advice which was studied in depth by other later researchers such as jack mostow in the card game hearts the idea is that the agent is given high level instructions on how to solve a problem and must distill from it a low level policy this is quite related to how humans learn complex tasks in many domains e g driving where a driving instructor may provide advice such as keep a certain distance from the car in front a fairly complex neural deep learning controller architecture is used although the details of this system are somewhat confusing in terms of many details that are presented a simpler approach might have been easier to follow at least initially the experiments unfortunately are on a rather simplistic d maze and it would have been worthwhile to see how the approach scaled to more complex tasks of the sort usually seen in deep rl papers these days e g atari physics simulators etc nice overall idea somewhat confusing description of the solution and an inadequate set of experiments on a less than satisfactory domain of d grid worlds,5.0
449.json,looking through the comment section here i agree to a large degree with the author standpoint on many issues discussed points through in the authors comment below are in my opinion a good summary of the contributions of the paper while i do not think those contributions are groundbreaking i believe they are significant enough to merit acceptance the reason i am commenting here is because having looked at several comment sections for this iclr i am seeing a general trend that reviews have a strong focus on performance i e reviews tend to be very short and judge papers to a large degree on whether they are a few percentage points better or worse than the reported baseline e g see the comments the experimental evaluation is not convincing e g no improvement on svhn or the effect of drop path seems to vanish with data augmentation below i believe that papers should be judged more on their scientific contributions see points and below especially when those papers themselves state that their focus is on those scientific contributions not on amazing performance further i believe the trend to focus excessively on performance is problematic for a number of reasons the deep learning community has focused very heavily on a few datasets mnist imagenet cifar cifar svhn this means that at any time a large chunk of the deep learning literature is battling for sota titles hence expecting any new model to attain one of those titles is a very high bar it is an arbitrary standard say the sota on imagenet improves by a year then a paper that outperforms by in would underperform by in by the performance standard the same paper with the same ideas and the same scientific merit would have declined drastically in value over that one year is that really true how does one even draw a fair comparison on these standard datasets at this point the bag of tricks for neural networks includes drop out l l ensembling various forms of data augmentation various forms of normalization and initialization various non linearities various learning rate schedules various forms of pooling label smoothing gradient clipping etc etc there are a gazillion ways to eke out fractions of percentage points of performance and every single paper has a unique combination of tricks that they use for their model even though the tricks themselves are unrelated to the model hence the only truly fair comparison would be to compare against every reference model with the exact trick combination that the paper presenting the reference model used which would take an exorbitant amount of time what is worse many papers do not even report all of the tricks they used one would have to get the authors code and reverse engineer the model not to mention slight differences introduced by using e g tensorflow vs torch vs caffe in this light the request from one of the reviewers to have a baseline against which the improvements can be clearly demonstrated by making isolated changes seems unrealistic to me the ml community should not make excessive fine tuning of models mandatory for publication by requiring models to beat sota we force each author to fine tune their model ad nauseum which leads to an arms race to get a publications authors would spend ever more time fine tuning their models this can not only lead to training on the test set but also wastes the time of researcher that could be better spent exploring new ideas it gives too much power to bad research in science there is always a certain background rate of bad results published either the numbers are outright fake or the experimental protocol was invalid e g someone used the test set as a validation set or someone did an exorbitant number of random reruns and only published the best single result what is worse these bad results are far more likely to hold the sota title at any given time than a good result by requiring new publications to beat sota we give too much power to bad results it punishes authors for reporting many or strong baselines in this paper authors were careful to report many recent results table is thorough and now they are criticized for not beating all of those baselines i have a feeling that if the authors of this paper had been more selective about which baselines they report i e those that they can beat they would have received higher scores on the paper i have written an in depth review for another paper at this conference that used in my opinion very weak baselines and ended up getting high reviewer marks i do not think that was a coincidence the same arguments apply though i think to a lesser degree to judging models excessively on how many parameters they have or their runtime however i agree with reviewers that more information about how models compare in terms of those metrics would enhance this paper i would like to see a discussion of that in the final version in general i think this paper would benefit from an appendix with more details on model and training procedure i also agree with reviewers that layers which is the deepest that authors can go while improving test error table is not ultra deep hence putting ultra deep in the paper title seems exaggerated and i would recommend scaling back the language however i do not think being ultra deep layers is necessary because as veit et al showed networks that appear ultra deep might not be ultra deep in practice training an layer net that functions at test time without residual connections seems to be enough of an achievement in summary i think if a paper makes scientific contribution see points and below independent of performance then competitive performance should be enough for publication instead of requiring sota i believe this paper achieves that mark,7.0
776.json,this paper proposes a model for iteratively refining translation hypotheses this has several benefits including enabling the translation model to condition not only on left context but also on right context and potentially enabling more rapid and or accurate decoding the motivation given is that often translators and text generators generally use a process of refinement in generating outputs this is an important idea that is not currently playing much of a role in neural net models so this paper is a welcome contribution however while i think this is an important first step i do feel that the lack of in depth analysis suggests this paper is not quite ready for a final publication version for example there are many possible connections to prior work in nlp mt and other parts of ml that could better contextualize this work see specifics below more substantively the model in section could be interpreted as a globally normalized undirected crf translation model trained using a pseudo likelihood objective in this analysis the model squarely back in the context of traditional discriminative translation models which used undirected features and the decoding algorithm then looks more like a standard greedy hill climbing algorithm albeit with an extra heuristic model for selecting which variable to update which is also nothing unfamiliar my second criticism the limitations of the model are not well discussed for example the proposed editing procedure cannot obviously remove or insert a word from a translation while i think this is a reasonable assumption than can be made for the sake of tractability it is very unfortunate since missing or extra words esp function words are a common problem in the baseline models that are being used second the standard objections to absolute positional models vs relative positional models seem particularly crucial to bring up in this work especially since they might make some of the design decisions a bit more justifiable overall this is an initial step in an interesting direction but it needs more thorough analysis to demonstrate its value a more thorough analysis will also likely suggest some important model variants for example is a global translation model really the goal or is a post editing model that fixes outputs with more complex operations more ideal related work i think that more could be done to put this work in the context of what has come before and what is currently going on in other parts of ml the idea of iterative refinement has been proposed in other problems that have complex output spaces for example the draw model of gregor et al and the conditional adversarial network models used to refine images proposed recently by isola et al in nlp there have been several stochastic hill climbing approaches that have been proposed such as the work on parsing by zhang and lei et al who use random initial guesses and then do greedy hill climbing using a series of local refinements the structured prediction cascades of weiss and taskar not to mention general coarse to fine modeling strategies finally in mt arun et al who use a gibbs sampler to refine an initial guess to do decoding with a more complex model the use of an explicit error model is rather novel in the context of correction but i would point out that although the proposed architecture is different the discriminative word lexicon models of mauser et al and the neural version of the same by ha et al are similar in spirit there have also been a number of papers on automatic post editing including the shared task at wmt and there are not only standard test sets and baselines but also datasets that could actually be used to train a post editing model with human generated data minimally using the techniques they described could be a useful foil for the models presented in this paper the target sentence is also embedded in distributional space via a lookup table i think distributional space is a bit unclear maybe the target sentence is represented in terms of distributed word representations via a lookup table or something like that distributional suggests that the representations are derived from how the words are distributed in the corpus whereas you are learning these representations on this task which isn t modeling their distribution except only very indirectly section model in section the model computes the distribution over target word types at an absolute position i in the output sentence given the target language context and the source language context it is introduced as the model that is used to refine an existing hypothesis but it is not immediately clear that the training data for this model at least in this section are the gold standard translations training set could be interpreted in variety of ways this becomes clearer when reading later in the paper but it s a bit less clear when reading from the beginning for the first time the use of a fixed sized window for representing the target word in context also seems to make something like a model assumption since only the lexical features and not any alignment or positional features determine the attention this should be clarified since it will make the assumptions of the model more transparent and also suggest possible refinements to the model e g including representations of i and j as components of s j and t i which would allow model like responses to be learned although by leaving them out the model might behave a bit more like a relative positional model than an absolute positional model which is probably attractive finally some discussion for why a fixed window is used to represent the target sentence is worth including since a global context is apparently used to represent the source sentence the relationship between this training objective and pseudo likelihood pl besag might be worth mentioning since i believe this is just a pl objective for a certain global model this suggests alternative decoding algorithms or certainly a different analysis of the proposed decoding objective the section model conditions on the true context of a position in the true target the current target guess and the source i don t completely understand the rationale for this model since at test time only two of these variables are available and the replacement of yref with yg seems hard to justify,5.0
776.json,this work proposes to iteratively improve a sentence that has been generated from another mt system in this case a phrase based system the authors use a neural net that takes in the source sentence and a window of gold words around the current target word and predicts the current target word during testing the gold words are replaced with the generated words while this is an interesting area of research i am not convinced by the proposed approach and experimental evidence is lacking under the current framework it is all but impossible for the model to do anything more than a rudimentary word replacement e g it cannot change i went to the fridge even though i was not hungry to although i was not hungry i went to the fridge the fact that only words are edited on average supports this specific comments it would be interesting to see what the improvements are if the baseline model is a neural system it seems strange to me at least that t i and l y i k only look at a window of k words it means that when making the decision to change the i th word the model does not know what was generated outside of the window relatedly the idea of changing individual words based on local i e word level scores seems counterintuitive given that we have the full generated sentence do not we want a global score scoring at the sentence level could also make room for non greedy search strategies which could potentially facilitate richer edits how does the approach compare to a model that simply re ranks the k best output instead of editing did you consider learning an encoder decoder that takes in x yg and generates yref when decoding you can attend to both x and yg minor comments iteratively improving a generated text was also explored in,4.0
371.json,summary this paper proposes the neural physics engine npe a network architecture which simulates object interactions while npe decides to explicitly represent objects rather than video frames it incorporates knowledge of physics almost exclusively through training data it is tested in a toy domain with bouncing d balls the proposed architecture processes each object in a scene one at a time pairs of objects are embedded in a common space where the effect of the objects on each other can be represented these embeddings are summed and combined with the focus object state to predict the focus object change in velocity alternative baselines are presented which either forego the pairwise embedding for a single object embedding or encode a focus object neighbors in a sequence of lstm states npe outperforms the baselines dramatically showing the importance of architecture choices in learning to do this object based simulation the model is tested in multiple ways ability to predict object trajectory over long time spans is measured generalization to different numbers of objects is measured generalization to slightly altered environments difference shaped walls is measured finally the npe is also trained to predict object mass using only interactions with other objects where it also outperforms baselines comments i have one more clarifying question are the inputs to the blue box in figure b c the concatenation of the summed embeddings and state vector of object or is the input to the blue module some other combination of the two vectors section begins with first because physics does not change across inertial frames it suffices to separately predict the future state of each object conditioned on the past states of itself and the other objects in its neighborhood similar to fragkiadaki et al i think this is an argument to forego the visual representation used by previous work in favor of an object only representation this would be more clear if there were contrast with a visual representation as addressed in the paper this approach is novel though less so after taking into consideration the concurrent work of battaglia et al in nips titled interaction networks for learning about objects relations and physics this work offers a different network architecture and set of experiments as well as great presentation but the use of an object based representation for learning to predict physical behavior is shared overall evaluation this paper was a pleasure to read and provided many experiments that offered clear and interesting conclusions it offers a novel approach though less so compared to the concurrent work of battaglia et al which represents a significant step forward in the current investigation of intuitive physics,9.0
408.json,pros interesting training criterion cons missing proper asr technique based baselines comments the dataset is quite small roc curves for detection and more measurements e g eer would probably be helpful besides ap more detailed analysis of the results would be necessary e g precision of words seen during training compared to the detection performance of out of vocabulary words it would be interesting to show scatter plots for embedding vs orthographic distances,5.0
408.json,this paper proposes an approach to learning word vector representations for character sequences and acoustic spans jointly the paper is clearly written and both the approach and experiments seem reasonable in terms of execution the motivation and tasks feel a bit synthetic as it requires acoustics spans for words that have already been segmented from continuous speech a major assumption the evaluation tasks feel a bit synthetic overall and in particular when evaluating character based comparisons it seems there should also be phoneme based comparisons there a lot of discussion of character edit distance relative to acoustic span similarity it seems very natural to also include phoneme string edit distance in this discussion and experiments this is especially true of the word similarity test rather than only looking at levenshtein edit distance of characters you should evaluate edit distance of the phone strings relative to the acoustic embedding distances beyond the evaluation task the paper would be more interesting if you compared character embeddings with phone string embeddings i believe the last function could remain identical it just swapping out characters for phones as the symbol set finally in this topic the discussion and experiments should look at homophones as if not obvious what the network would learn to handle these the vocabulary size and training data amount make this really a toy problem although there are many pairs constructed most of those pairs will be easy distinctions the experiments and conclusions would be far stronger with a larger vocabulary and word segment data set with subsampling all pairs perhaps biased towards more difficult or similar pairs it seems this approach is unable to address the task of keyword spotting in longer spoken utterances if that the case please add some discussion as to why you are solving the problem of word embeddings given existing word segmentations the motivating example of using this approach to retrieve words seems flawed if a recognizer must be used to segment words beforehand,6.0
490.json,this work is basically a combined pointer network applied on language modelling the smart point is that this paper aims at language modelling with longer context where a memory of seen words especially the rare words would be very useful for predicting the rest of the sentences hence a combination of a pointer network and a standard language model would balance the copying seen words and predicting unseen words generally such as the combined pointer networks applied in sentence compression a vector representation of the source sequence would be used to compute the gate this paper instead introduces a sentinel vector to carry out the mixture model which is suitable in the case of language modelling i would be interested in the variations of sentinel mixture implementation though the current version has achieved very good results in addition the new wikitext language modelling dataset is very interesting it probably can be a more standard dataset for evaluating the continuously updated language model benchmarks than ptb dataset overall this is a well written paper i recommend it to be accepted,8.0
306.json,this work presents an lstm based meta learning framework to learn the optimization algorithm of a another learning algorithm here a nn the paper is globally well written and the presentation of the main material is clear the crux of the paper drawing the parallel between robbins monroe update rule and the lstm update rule and exploit it to satisfy the two main desiderata of few shot learning quick acquisition of new knowledge slower extraction of general transferable knowledge is intriguing several tricks re used from andrychowicz et al such as parameter sharing and normalization and novel design choices specific implementation of batch normalization are well motivated the experiments are convincing this is a strong paper my only concerns questions are the following can it be redundant to use the loss gradient and parameters as input to the meta learner did you do ablative studies to make sure simpler combinations are not enough it would be great if other architectural components of the network can be learned in a similar fashion number of neurons type of units etc do you have an opinion about this the related work section mainly focused on meta learning is a bit shallow meta learning is a rather old topic and similar approaches have been tried to solve the same problem even if they were not using lstms samy bengio phd thesis is all about this use of genetic programming for the search of a new learning rule for neural networks s bengio y bengio and j cloutier i am convince schmidhuber has done something make sure you find it and update related work section overall i like the paper i believe the discussed material is relevant to a wide audience at iclr,9.0
582.json,the problem of utilizing all available information across modalities about a product to learn a meaningful joint embedding is an interesting one and certainly seems like it a promising direction for improving recommender systems especially in the cold start scenario i am unaware of approaches combining as many modalities as proposed in this paper so an effective solution could indeed be significant however there are many aspects of the proposed architecture that seem sub optimal to me a major benefit of neural network based systems is that the entire system can be trained end to end jointly the proposed approach sticks together largely pre trained modules for different modalities this can be justifiable when there is very little training data available on which to train jointly with m product pairs however this does not seem to be the case for the amazon dataset although i have not worked with this dataset myself so perhaps i am missing something either way it not discussed at all in the paper i consider the lack of a jointly fine tuned model a major shortcoming of the proposed approach the discussion of pairwise residual units is confusing and not well motivated the residual formulation if i understand it correctly applies a relu layer to the concatenation of the modality specific embeddings giving a new similarity after dot products that can be added to the similarity obtained from the concatenation directly why not just have an additional fully connected layer that mixes the modality specific embeddings to form a final embedding perhaps of lower dimensionality this should at least be presented as a baseline if the pairwise residual unit is claimed as a contribution i do not find the provided explanation convincing in what way does the residual approach reduce parameter count more minor the choice of textcnn for the text embedding vectors seems fine although i wonder how an lstm based approach would perform however the details surrounding how it is used are obscured in the paper in response to a question the authors mention that it runs on the concatenation of the first words of the title and product description especially for the description this seems insufficiently long to contain a lot of information to me more care could be given to motivating the choices made in the paper finally i am not familiar with state of the art on this dataset do the comparisons accurately reflect it it seems only one competing technique is presented with none on the more challenging cold start scenarios minor detail in the second paragraph of page there is a reference that just says cite julian,3.0
701.json,this paper proposes a method of augmenting pre trained networks for one task with an additional inference path specific to an additional task as a replacement for the standard fine tuning approach pros the method is simple and clearly explained standard fine tuning is used widely so improvements to and analysis of it should be of general interest experiments are performed in multiple domains vision and nlp cons the additional modules incur a rather large cost resulting in x the parameters and roughly x the computation of the original network for the stiched network these costs are not addressed in the paper text and make the method significantly less practical for real world use where performance is very often important given these large additional costs the core of the idea is not sufficiently validated to me in order to verify that the improved performance is actually coming from some unique aspects of the proposed technique rather than simply the fact that a higher capacity network is being used some additional baselines are needed allowing the original network weights to be learned for the target task as well as the additional module outperforming this baseline on the validation set would verify that freezing the original weights provides an interesting form of regularization for the network training the full module stitched network from scratch on the source task then fine tuning it for the target task outperforming this baseline would verify that having a set of weights which never sees the source dataset is useful the method is not evaluated on imagenet which is far and away the most common domain in which pre trained networks are used and fine tuned for other tasks i ve never seen networks pre trained on cifar deployed anywhere and it s hard to know whether the method will be practically useful for computer vision applications based on cifar results often improved performance on cifar does not translate to imagenet in other contexts such as more theoretical contributions having results only on small datasets is acceptable to me but network fine tuning is far enough on the practical end of the spectrum that claiming an improvement to it should necessitate an imagenet evaluation overall i think the proposed idea is interesting and potentially promising but in its current form is not sufficiently evaluated to convince me that the performance boosts don t simply come from the use of a larger network and the lack of imagenet evaluation calls into question its real world application edit i had indeed missed the fact that the stanford cars does do transfer learning from imagenet thanks for the correction however the experiment in this case is only showing late fusion ensembling which is a conventional approach compared with the stitched network idea which is the real novelty of the paper furthermore the results in this case are particularly weak showing only that an ensemble of resnet vgg outperforms vgg alone which is completely expected given that resnet alone is a stronger base network than vgg resnet vgg resnet would be a stronger result but still not surprising demonstrating the stitched network idea on imagenet comparing with the corresponding vgg only or resnet only finetuning could be enough to push this paper over the bar for me but the current version of the experiments here do not sufficiently validate the stitched network idea in my opinion,4.0
717.json,this paper makes three main methodological contributions definition of neural feature nf as the pixel average of the top n images that highly activation a neuron ranking of neurons based on color selectivity ranking of neurons based on class selectivity the main weaknesses of the paper are that none of the methodological contributions are very significant and no singularly significant result arises from the application of the methods however the main strengths of the paper are its assortment of moderately sized interesting conclusions about the basic behavior of neural nets for example a few are indexing on class selectivity neurons we found highly class selective neurons like digital clock at conv cardoon at conv and ladybug at conv much before the fully connected layers as far as i know this had not been previously reported color selective neurons are found even in higher layers color selectivity in conv our main color axis emerge black white blue yellow orange cyan and cyan magenta curiously these two observations correlate with evidences in the human visual system shapley hawken great observation overall i d recommend the paper be accepted because although it s difficult to predict at this time there s a fair chance that one of the smaller conclusions would turn out to be important in hindsight a few years hence other small comments the cite for learning to generate chairs is wrong first two authors combined resulting in a confusing cite what exactly is the color selectivity index computing the opponent color space isn t well defined and it wasn t previously familiar to me intuitively it seems to be selecting for units that respond to a constant color but the highest color selectivity nf in fig i for a unit with two colors not one finally the very last unit lowest color selectivity is almost the same edge pattern but with white black instead of blue orange why are these considered to be so drastically different this should probably be more clearly described for the sake of argument imagine a mushroom sensitive neuron in conv that fires highly for mushrooms of any color but not for anything else if the dataset contains only red capped mushrooms would the color selectivity index for this neuron be high or low if it is high it s somewhat misleading because the unit itself actually isn t color selective the dataset just happens only to have red mushrooms in it it s a subtle point but worth considering and probably discussing in the paper,7.0
740.json,update i looked at the arxiv version of the paper it is much longer and appears more rigorous fig there is indeed more insightful however i am reviewing the submission and my overall assessment does not change this is not a minor incremental contribution and if you want to compress it into a conference submission of this type i would recommend choosing message you want to convey and focus on that as you say iclr submission focus on the parmac algorithm i would focus on this properly and remove or move to appendix all extensions and theoretical remarks and have an extra page on explaining the algorithm additionally make sure to clearly explain the relation of the arxiv paper in particular that the submission was a compressed version original review the submission proposes parmac based on mac method of auxiliary coordinates formulating a distributed variant of the idea related work in the part on convex erm and methods i would recommend citing general communication efficient frameworks cocoa ma et al and aide reddi et al i believe these works are most related to the practical objectives authors of this paper set while number of the papers cited are less relevant section explaining mac is quite clearly written but i do not find part on mac and em particularly useful section is much less clearly written i have trouble following notation particularly in the speedups part as different symbols were introduced at different places perhaps a quick summary or paragraph on notation in the introduction would be helpful in paragraph you write as if reader knew how data anything is distributed but this was not mentioned yet it is specified later it is not clear what is meant by submodel perhaps a more precise example pointing back to eqs would be useful as far as i understand from what is written there are p independent sets of submodels that traverse the machines in circular fashion i do not understand how are they initialized identically and more importantly i do not understand what would be a single output of the algorithm averaging does not seem to make sense since this is not addressed i suppose i get it wrong leaving me to guess what was actually meant the fact that i am not able to understand what is actually happening i see as major issue i do not like the later paragraphs on extensions model for speedup convergence and topologies i do not understand whether these are novel contributions or not as the authors refer to other work for details if these are novel the explanation is not sufficient particularly speedup part which contains undefined quantities e g t p or i can not find it if this is not novel it does not provide enough explanation to understand anything more compared with a its version compressed to of its size and referring to the other work the statement that we can recover the original convergence guarantees seems strong and i do not see why it should be trivial to show but author point to other work which i did not look at in topologies part claiming that something does true sgd without explaining what is true sgd seems very strange other statements in this section seem also very vague and unjustified unexplained experimental section seems to suggest that the method is interesting for binary autoencoders but i do not see how would i conclude anything about any other models parmac is also not compared to alternative methods only with itself focusing on scaling properties conclusion contains statements that are too strong or misleading based on what i saw in particular we analysed its parallel speedup and convergence seems ungrounded further the claim the convergence properties of mac remain essentially unaltered in parmac is unsupported regardless of the meaning of essentially unchanged in summary the method seems relevant for particular model class binary autoencoders but clarity of presentation is insufficient i would not be able to recreate the algorithm used in experiments and the paper contains a number of questionable claims,4.0
740.json,this paper proposes an extension of the mac method in which subproblems are trained on a distributed cluster arranged in a circular configuration the basic idea of mac is to decouple the optimization between parameters and the outputs of sub pieces of the model auxiliary coordinates optimization alternates between updating the coordinates given the parameters and optimizing the parameters given the outputs in the circular configuration because each update is independent they can be massively parallelized this paper would greatly benefit from more concrete examples of the sub problems and how they decompose for instance can this be applied effectively for deep convolutional networks recurrent models etc from a practical perspective there not much impact for this paper beyond showing that this particular decoupling scheme works better than others there also seem to be a few ideas worth comparing at least circular vs parameter server configurations decoupled sub problems vs parallel sgd parallel sgd also has the benefit that it extremely easy to implement on top of nn toolboxes so this has to work a lot better to be practically useful also it a bit hard to understand what exactly is being passed around from round to round and what the trade offs would be in a deep feed forward network assuming you have one sub problem for every hidden unit then it seems like in the w step different bits of the nn walk their way around the cluster taking sgd steps w r t the coordinates stored on each machine this means passing around the parameter vector for each hidden unit then there a synchronization step to gather the parameters from each submodel requiring a traversal of the circular structure then each machine updates it coordinates based on the complete model for a slice of the data this would mean for a feed forward network producing the intermediate activations of each layer for each data point so for something comparable to parallel sgd you could do the following put a mini batch of size b on each machine with parmac compared to running such mini batches in parallel completing steps above would then be roughly equivalent to one synchronized ps type implementation step distribute model to workers get p gradients back update model it would be really helpful to see how this compares in practice it hard for me to understand intuitively why the proposed method is theoretically any better than parallel sgd except for the issue of non smooth function optimization the decoupling also can fundamentally change the problem since you are not doing back propagation directly anymore so that seems like it would conflate things as well and it not necessarily going to just work for other types of architectures,6.0
605.json,the method overall seems to be a very interesting structural approach to variational autoencoders however it seems to lack motivation as well as the application areas sufficient to prove its effectiveness i see the attractiveness of using structural information in this context and i find it more intuitive than using a flat sequence representation especially when there is a clear structure in the data however experimental results seem to fail to be convincing in that regard one issue is the lack of a variety of applications in general the experiments seem to be very limited in that regard considering that the paper itself speaks about natural language applications it would be interesting to use the latent representations learned with the model for some other end task and see how much it impacts the success of that end task compared to various baselines in my opinion the paper has a potentially strong idea however in needs stronger results and possibly in a wider variety of applications as a proof of concept,3.0
384.json,summary the paper presents a deep neural network for the task of machine comprehension on the squad dataset the proposed model is based on two previous works match lstm and pointer net match lstm produces attention over each word in the given question for each word in the given passage and sequentially aggregates this matching of each word in the passage with the words in the question the pointer net is used to generate the answer by either generating each word in the answer or by predicting the starting and ending tokens in the answer from the provided passage the experimental results show that both the variants of the proposed model outperform the baseline presented in the squad paper the paper also shows some analysis of the results obtained such as variation of performance across answer lengths and question types strengths a novel end to end model for the task of machine comprehension rather than using hand crafted features significant performance boost over the baseline presented in the squad paper some insightful analyses of the results such as performance is better when answers are short why questions are difficult to answer weaknesses questions suggestions the paper does not show quantitatively how much modelling attention in match lstm and answer pointer layer helps so it would be insightful if authors could compare the model performance with and without attention in match lstm and with and without attention in answer pointer layer it would be good if the paper could provide some insights into why there is a huge performance gap between boundary model and sequence model in the answer pointer layer i would like to see the variation in the performance of the proposed model for questions that require different types of reasoning table in squad paper this would provide insights into what are the strengths and weaknesses of the proposed model w r t the type reasoning required could authors please explain why the activations resulting from h p i and h r  i in gi in equation are being repeated across dimension of q why not learn different activations for each dimension i wonder why bi ans ptr is not used in the ensemble model last row in table when it is shown that bi ans ptr improves performance by in f could authors please discuss and compare the dcr model in table in the paper in more detail review summary the paper presents a reasonable end to end model for the task of machine comprehension on the squad dataset which outperforms the baseline model significantly however it would be good if more analyses ablation studies insights are included regarding how much attention helps why is boundary model better than sequence model how does the performance change when the reasoning required becomes difficult,7.0
453.json,this paper proposed a proximal quasi newton s method to learn binary dnn the main contribution is to combine pre conditioning with binarization in a proximal framework it is interesting to have a proximal newton s method to interpret the different dnn binarization schemes this gives a new interpretation of existing approaches however the theoretical analysis is not very convincing or useful the formulated optimization problem is essentially a mixed integer programming even though the paper treats the integer part as a constraint and address it in proximal operators the constraint set is still discrete and there is no guarantee that the proximal newton algorithm could converge under practically useful conditions in practice it is hard to verify the assumption dt t k beta in theorem this relation could be hard to hold in dnn as the loss surface could be extremely complicated,7.0
392.json,this work proposes a new approach for image compression using auto encoders the results are impressive besting the state of the art in this field pros very clear paper it should be possible to replicate these results should one be inclined to do so the results when compared to other work in this field are very promising i need to emphasize and i think the authors should have emphasized this fact as well this is very new technology and it should not be surprising it not better than the state of the art in image compression it definitely better than other neural network approaches to compression though cons the training procedure seems clunky it requires multiple training stages freezing weights etc the motivation behind figure is a bit strange as it not clear what it trying to illustrate and may confuse readers it talks about effects on jpeg but the paper discusses a neural network architecture not dct quantization,8.0
686.json,the paper proposed a very complex compression and reconstruction method with additional parameters for reducing the memory footprint of deep networks the authors show that this complex proposal is better than simple hashed net proposal one question are you also counting the extra parameters for reconstruction network for the memory comparison otherwise the experiments are unfair since hashing and reconstruction cost will dominate the feed forward and back propagation updates it is imperative to compare the two methods on running time for hashed net this is quite simple yet it created an additional bottleneck please also show the impact on running time small improvements for a big loss in computational cost may not be acceptable i am not convinced that this method will be lightweight if we are allowed complicated compression and reconstruction then we can use any off shelf methods but the cost will be huge,5.0
405.json,update after going through the response from the author and the revision i increased my review score for two reasons i thank the reviewers for further investigating the difference between yours and the other work scheduled sampling unsupervised learning using lstm and providing some insights about it this paper at least shows empirically that pred scheme is better for high dimensional video and for long term predictions it would be good if the authors briefly discuss this in the final revision either in the appendix or in the main text the revised paper contains more comprehensive results than before the presented result and discussion in this paper will be quite useful to the research community as high dimensional video prediction involves large scale experiments that are computationally expensive summary this paper presents a new rnn architecture for action conditional future prediction the proposed architecture combines actions into the recurrent connection of the lstm core which performs better than the previous state of the art architecture oh et al the paper also explores and compares different architectures such as frame dependent independent mode and observation prediction dependent architectures the experimental result shows that the proposed architecture with fully prediction dependent training scheme achieves the state of the art performance on several complex visual domains it is also shown that the proposed prediction architecture can be used to improve exploration in a d environment novelty the novelty of the proposed architecture is not strong the difference between oh et al and this work is that actions are combined into the lstm in this paper while actions are combined after lstm in oh et al the jumpy prediction was already introduced by srivastava et al in the deep learning area experiment the experiments are well designed and thorough specifically the paper evaluates different training schemes and compares different architectures using several rich domains atari d worlds besides the proposed method achieves the state of the art results on many domains and presents an application for model based exploration clarity the paper is well written and easy to follow overall although the proposed architecture is not much novel it achieves promising results on atari games and d environments in addition the systematic evaluation of different architectures presented in the paper would be useful to the community reference nitish srivastava elman mansimov ruslan salakhutdinov unsupervised learning with lstms icml,7.0
540.json,this paper studies knowledge transfer problem from small capacity network to bigger one this is a follow up work of netnet iclr and netmorph icml comments this paper studies macroscopic problem with the morphing process composed by multiple atomic operations while the atomic operations are proposed in netnet and netmorph there has not been study of the general modularized process principally thus this paper asks a novel question the solution by composing multiple atomic transformations seems to be quite reasonable in the related work section it is better to change network morphism to knowledge transfer or in the subsection title most of these works are known as knowledge transfer and it helps to connect to the existing works the author shows experiments on variants of resnet while the experiment shows that initializing from resnet gives better error rate than the ones trained from scratch it is unclear what the source this paper studies knowledge transfer problem from small capacity network to bigger one this is a follow up work of netnet iclr and netmorph icml comments this paper studies macroscopic problem with the morphing process composed by multiple atomic operations while the atomic operations are proposed in netnet and netmorph there has not been study of the general modularized process principally thus this paper asks a novel question the solution by composing multiple atomic transformations seems to be quite reasonable in the related work section it is better to change network morphism to knowledge transfer or in the subsection title most of these works are known as knowledge transfer and it helps to connect to the existing works the author shows experiments on variants of resnet while the experiment shows that initializing from resnet gives better error rate than the ones trained from scratch it is unclear what the source is one major advantage of this type of knowledge transfer netnet netmorph is to speedup training and model exploration there seems to be no experiments demonstrate such advantage possibly due to the lose initialization of batchnorm this is the major drawback of this paper the method proposed by the author can in principle do quite complicated transformation e g transform an entire resnet from a single conv layer the experiment only consists of simple module transformations which in some way can be covered by atomic operations it would be more interesting to see what the results of more complicated transformations are even if they are not as effective in summary this paper studies a novel problem of knowledge transfer in a macroscopic level the method could be of interest to the iclr community the experiments should be improved comment to make the results more convincing and practically useful and i strongly encourage the authors to do so,6.0
540.json,this paper presents works on neural network cnn architecture morphing results are not reported on imagenet larger resnet and new network architecture such as xception and densenet which are maybe too new also most results are reported in small datasets and network which do not offer confidence in the usability in production systems my biggest issue is that computational time and effort for these techniques is not mentioned in detail we always want to be able to quantify the extra effort of understanding and using a new technique especially if the results are minor,7.0
540.json,the paper presents an interesting incremental approach for exploring new convolutional network hierarchies in an incremental manner after a baseline network has reached a good recognition performance the experiments are presented for the cifar and imagenet benchmarks by morphing various resnet models into better performing models with somewhat more computation although the baselines are less strong than those presented in the literature the paper claims significant error reduction for both imagenet and cifar the main idea of the paper is to rewrite convolutions into multiple convolutions while expanding the number of filters it is quite unexpected that this approach yields any improvements over the baseline model at all however for some of the basic tenets of network morphing experimental evidence is not given in the paper here are some fundamental questions raised by the paper how does the quality of morphed networks compares to those with the same topology trained from scratch how does the incremental training time after morphing relate to that of the network trained from scratch where is the extra computational cost of the morphed networks come from why is the quality of the baseline resnet models lag behind those that are reported in the literature and github e g the github resnet model is supposed to have top recall vs reported in the paper more evidence for the first three points would be necessary to evaluate the validity of the claims of the paper the paper is written reasonably well and can be understood quite well but the missing evidence and weaker baselines make it looks somewhat less convincing i would be inclined to revise up the score if a more experimental evidence were given for the main message of the paper see the points above,5.0
628.json,this paper is about submodular sum product networks applied to scene understanding spns have shown great success in deep linear models since the work of poon the authors propose an extension to the initial spns model to be submodular introducing submodular unary and pairwise potentials the authors propose a new inference algorithm the authors evaluated their results on stanford background dataset and compared against multiple baselines pros new formulation of spns new inference algorithm cons the authors did not discuss how the sspn structure is learned and how the generative process chooses the a symbol operation at each level the evaluations is lacking the authors only showed results on their own approach and baselines leaving out every other approach evaluations could have been also done on bsd for regular image segmentation hierarchical segmentation the idea is great however the paper needs more work to be published i would also recommend for the authors to include more details about their approach and present a full paper with extended experiments and full learning approach,4.0
487.json,from my original comments the results looks good but the baselines proposed are quite bad for instance in the table misclassification rate for a the result for the fc with floating point is well far from what we can obtain from this topology near to i would like to see significant compression levels on state of the art results or good baselines i can get with two fc hidden layers in cifar experiments i do not understand why sparsely connected single precision floating point is worse than sparsely connected binaryconnect so it is better to use binary than float again i think that in the experiments the authors are not using all the techniques that can be easily applied to float but not to binary gaussian noise or other regularizations therefore under my point of view the comparison between float and binary is not fair this is a critic also for the original papers about binary and ternary precision in fact with this convolutional network floating standard precision we can get lower that of error rate again bad baselines the authors reply still does not convince me i still think that the same technique should be applied on more challenging scenarios,6.0
468.json,the paper has two main contributions shows that uniform quantization works well with variable length huffman coding improves fixed length quantization by proposing the hessian weighted k means as opposed to standardly used vanilla k means the hessian weighting is well motivated and it is also explained how to use an efficient approximation for free when using the adam optimizer which is quite neat as opposed to vanilla k means one of the main benefits of this approach apart from improved performance is that no tuning on per layer compression rates is required as this is achieved for free to conclude i like the paper is not really novel but it does not seem other papers have done this before so it nice to know it works well and is quite neat and also works well the paper is easy to follow results are good my only complaint is that it a bit too long minor note i still do not understand the parts about storing additional bits for each binary codeword for layer indication when doing layer by layer quantization what is the problem of just having an array of quantized weight values for each layer i e q would store all quantized weights for layer q for layer etc and for each layer you would have the codebook so the only overhead over joint quantization is storing the codebook for each layer which is insignificant i do not understand the additional bit part but anyway this is really not a important as i do not think it affects the paper at all just authors might want to additionally clarify this point maybe i am missing something obvious but if i am then it likely some other people will as well,7.0
653.json,this paper theoretically justified a faster convergence in terms of average gradient norm attained after processing a fixed number of samples of using small mini batches for sgd or asgd with smaller number of learners this indicates that there is an inherent inefficiency in the speed up obtained with parallelizing gradient descent methods by taking advantage of hardware this paper looks good overall and makes some connection between algorithm design and hardware properties my main concern is that lemma looks incorrect to me the factor df s should be df s m for me please clarify this and check the subsequent theorem,6.0
653.json,this paper shows that when a larger mini batch is used in the serial setting the number of samples needed to be processed for the same convergence guarantee is larger a similar behavior is discussed for using multiple learners in asynchronous sgd this behavior has been known in convex optimization e g better mini batch algorithms via accelerated gradient methods nips there the convergence is of the form o sqrt bt t and so using bt samples only lead to sqrt b time improvement this paper extends a similar result to the nonconvex case but the underlying mathematics is mainly from ghadimi lan however this behavior is also known and indeed has been summarized in the deep learning textbook chapter hence the novelty is limited the theoretical results in this paper suggest that it is best not to use mini batch however using mini batch is often beneficial in practice as discussed in the deep learning textbook using mini batch allows using a larger learning rate note that this paper assumes the same learning rate for both mini batch sizes moreover multicore architectures allows parallel execution of mini batch almost for free hence the practical significance of the results is also limited other comments equation since the same number of samples s is processed and s mk where m is the mini batch size and k is the number of mini batches processed as mentioned in the first paragraph of section when two different mini batch sizes are considered ml and mh their k should differ however the same k is used on the lhs of figures and as convergence speed is of main interest why not show the training objective instead,4.0
595.json,this paper presents a small trick to improve the model quality of variational autoencoders further optimizing the elbo while initializing it from the predictions of the q network instead of just using those directly and the idea of using jacobian vectors to replace simple embeddings when interpreting variational autoencoders the idea of the jacobian as a natural replacement for embeddings is interesting as it does seem to cleanly generalize the notion of embeddings from linear models it would be interesting to see comparisons with other work seeking to provide context specific embeddings either by clustering or by smarter techniques like neelakantan et al efficient non parametric estimation of multiple embeddings per word in vector space or chen et al a unified model for word sense representation and disambiguation with the evidence provided in the experimental section of the paper it hard to be convinced that the jacobian of vae generated embeddings is substantially better at being context sensitive than prior work similarly the idea of further optimizing the elbo is interesting but not fully explored it unclear for example what is the tradeoff between the complexity of the q network and steps further optimizing the elbo in terms of compute versus accuracy overall the ideas in this paper are good but i would like to see them a little more fleshed out,5.0
700.json,the authors pointed out some limitations of existing deep architectures in particular hard to optimize on small or mid size datasets and proposed to stack marginal fisher analysis mfa to build deep models the proposed method is tested on several small to mid size datasets and compared with several feature learning methods the authors also applied some existing techniques in deep learning such as backprop denoising and dropout to improve performance the new contribution of the paper is limited mfa has long been proposed the authors fail to theoretically or empirically justify the stacking of mfas the authors did not include any deep architectures that requires backprop over multiple layers in the comparison which the authors set out to address instead all the methods compared were learned layer by layer will a randomly initialized deep model such as dbn or cnn perform poorly on these datasets it is also not clear how the authors came up with each particular model architecture and hyper parameters used in the different datasets the writing of the paper needs to be significantly improved a lot of details were omitted for example how is dropout applied in the mfa,4.0
645.json,the authors propose a method that extends the non linear two view representation learning methods and the linear multiview techniques and combines information from multiple sources into a new non linear representation learning techniques in general the method is well described and seems to lead to benefits in different experiments of phonetic transcription of hashtag recommendation even if the method is mostly a extension of classical tools the scheme learns a deep network for each view essentially the combination of the different sources of information seems to be effective for the studied datasets it would be interesting to add or discuss the following issues what is the complexity of the proposed method esp the representation learning part would there by any alternative solution to combine the different networks views that could make the proposed solution more novel the experimental settings especially in the synthetic experiments should be more detailed if possible the datasets should be made available to encourage reproducibility the related work is far from complete unfortunately especially from the perspective of the numerous multiview multi modal multi layer algorithms that have been proposed in the literature in different applications domaines like image retrieval or classification or bibliographic data for example authors like a kumar x dong ping yu chen m bronstein and many others have proposed works in that direction in the last years no need to compare to all these works obviously but a more complete description of the related could help appreciating better the true benefits of dgcca,6.0
645.json,this paper proposes a deep extension of generalized cca the main contribution of the paper is deriving the gradient update for the gcca objective i disagree with the claim that this is the first multiview representation learning technique that combines the flexibility of nonlinear representation learning with the statistical power of incorporating information from many independent resources or views r proposes a multiview representation learning method which is both non linear and capable of handling more than views this is very much relevant to what authors are proposing the objective function proposed in r maximizes the correlation between views and minimizes the self and cross reconstruction errors this is intuitively similar to nonlinear version of pca cca for multiple views comparing these methods is crucial to prove the usefulness of dgcca and the paper is incomplete without this comparison authors should also change their strong claim related work section is minimal there are significant advances in view non linear representation learning which are worth mentioning references r janarthanan rajendran mitesh m khapra sarath chandar balaraman ravindran bridge correlational neural networks for multilingual multimodal representation learning hlt naacl,5.0
665.json,paper summary this paper presents a new large scale machine reading comprehension dataset called ms marco it is different from existing datasets in that the questions are real user queries the context passages are real web documents and free form answers are generated by humans instead of spans in the context the paper also includes some analysis of the dataset and performance of qa models on the dataset paper strengths the questions in the dataset are real queries from users instead of humans writing questions given some context context passages are extracted from real web documents which are used by search engines to find answers to the given query answers are generated by humans instead of being spans in context it is large scale dataset with an aim of million queries current release includes queries paper weaknesses the authors say we have found that the distribution of actual questions users ask intelligent agents can be very different from those conceived from crowdsourcing them from the text but the statement is not backed up with any study the paper does not clearly present what additional information can today qa models learn from ms marco which they can not from existing datasets the paper should talk about what challenges are involved in obtaining a good performance on this dataset what are the human performances as compared to the models presented in the paper in section what are the train test splits the results are for the subset of ms marco where every query has multiple answers how big is that subset what is dssm mentioned in row table the authors should include in the paper how experiments in section prove that ms marco is a better dataset in table the performance of memory networks is already close to best passage does that mean there is not enough room for improvement there the paper seems to be written in hurry with partial analysis evaluation and various mistakes in the text preliminary evaluation the proposed dataset ms marco is unique from existing datasets as it is a good representative of the qa task encountered by search engines i think it can be a very useful dataset for the community to benefit from given the huge potential in the dataset this paper lacks the analysis and evaluation needed to present the dataset worth i think it can benefit a lot with a more comprehensive analysis of the dataset,6.0
665.json,this is a dataset paper that brings unique values over existing reading comprehension challenges unlike others ms marco is derived from query logs thus represents real questions that people ask rather than solicited questions that might be rather artificial in practical settings there are potential downsides of using query logs however it may be that people adapt their language and questions for search engines such that users ask questions that they know current search engines can reasonably answer thus it may be that people limit the complexity of questions or language or both i think authors could have addressed this concern by being more selective about the query logs by down sampling on simple questions that can be easily answered by keyword matching without any sophisticated reading comprehension and up sampling more complex questions that require at least paraphrasing and ideally synthesis of information taken from more than one sentences it s great that there are several new efforts to construct large scale reading comprehension challenges but my main concern is whether the majority of the questions can be answered through relatively easy text matching without intelligent reading or reasoning also the paper reads like the authors were running out of time before the deadline i would appreciate more analytic and quantitative comparisons against other existing datasets and more insights on the degree of challenges required to handle qas in ms marco for example the authors could collect statistics on qas exact match exists in the text snippet paraphrasing is required but otherwise the relevant answer is directly available in the text snippet requires synthesizing information taken from more than one sentences requires external knowledge the author response mentions that is unlikely but a more formal and complete analysis would be helpful,6.0
389.json,pros the authors are presenting an rnn based alternative to wavenet for generating audio a sample at a time rnns are a natural candidate for this task so this is an interesting alternative furthermore the authors claim to make significant improvement in the quality of the produces samples another novelty here is that they use a quantitative likelihood based measure to assess them model in addition to the ab human comparisons used in the wavenet work cons the paper is lacking equations that detail the model this can be remedied in the camera ready version the paper is lacking detailed explanations of the modeling choices it not clear why an mlp is used in the bottom layer instead of another rnn it not clear why r linear projections are used for up sampling instead of feeding the same state to all r samples or use a more powerful type of transformation as the authors admit their wavenet implementation is probably not as good as the original one which makes the comparisons questionable despite the cons and given that more modeling details are provided i think this paper will be a valuable contribution,8.0
736.json,this paper proposes a simple domain adaptation technique in which batch normalization is performed separately in each domain pros the method is very simple and easy to understand and apply the experiments demonstrate that the method compares favorably with existing methods on standard domain adaptation tasks the analysis in section shows that a very small number of target domain samples are needed for adaptation of the network cons there is little novelty the method is arguably too simple to be called a method rather it s the most straightforward intuitive approach when using a network with batch normalization for domain adaptation the alternative using the bn statistics from the source domain for target domain examples is less natural to me i guess this alternative is what s done in the inception bn results in table the analysis in section is superfluous except as a sanity check kl divergence between the distributions should be when each distribution is shifted scaled to n by bn section it s not clear to me what point is being made here overall there s not much novelty here but it s hard to argue that simplicity is a bad thing when the method is clearly competitive with or outperforming prior work on the standard benchmarks in a domain adaptation tradition that started with frustratingly easy domain adaptation if accepted sections and should be removed or rewritten for clarity for a final version,6.0
736.json,update i thank the authors for their comments i still think that the method needs more experimental evaluation for now it restricted to the settings in which one can use pre trained imagenet model but it also important to show the effectiveness in scenarios where one needs to train everything from scratch i would love to see a fair comparison of the state of the art methods on toy datasets e g see bousmalis et al ganin lempitsky in my opinion it a crucial point that does not allow me to increase the rating this paper proposes a simple trick turning batch normalization into a domain adaptation technique the authors show that per batch means and variances normally computed as a part of the bn procedure are sufficient to discriminate the domain this observation leads to an idea that adaptation for the target domain can be performed by replacing population statistics computed on the source dataset by the corresponding statistics from the target dataset overall i think the paper is more suitable for a workshop track rather than for the main conference track my main concerns are the following although the main idea is very simple it feels like the paper is composed in such a way to make the main contribution less obvious e g the idea could have been described in the abstract but the authors avoided doing so this one is from the pre review questions the authors are using much stronger base cnn which may account for the bulk of the reported improvement in order to prove the effectiveness of the trick the authors would need to conduct a fair comparison against competing methods it would be highly desirable to conduct this comparison also in the case of a model trained from scratch as opposed to reusing imagenet trained networks,4.0
464.json,the paper proposes to use reinforcement learning to learn how to compose the words in a sentence i e parse tree that can be helpful for the downstream tasks to do that the shift reduce framework is employed and rl is used to learn the policy of the two actions shift and reduce the experiments on four datasets sst sick imdb and snli show that the proposed approach outperformed the approach using predefined tree structures e g left to right right to left the paper is well written and has two good points firstly the idea of using rl to learn parse trees using downstream tasks is very interesting and novel and employing the shift reduce framework is a very smart choice because the set of actions is minimal shift and reduce secondly what shown in the paper somewhat confirms the need of parse trees this is indeed interesting because of the current debate on whether syntax is helpful i have the following comments it seems that the authors were not aware of some recent work using rl to learn structures for composition e g andreas et al because different composition functions e g lstm gru or classical recursive neural net have different inductive biases i was wondering if the tree structures found by the model would be independent from the composition function choice because rnns in theory are equivalent to turing machines i was wondering if restricting the expressiveness of the model e g reducing the dimension can help the model focus on discovering more helpful tree structures ref andreas et al learning to compose neural networks for question answering naacl,8.0
608.json,summary in this paper the authors explore the advantages disadvantages of using a sin activation function they first demonstrate that even with simple tasks using sin activations can result in complex to optimize loss functions they then compare networks trained with different activations on the mnist dataset and discover that the periodicity of the sin activation is not necessary for learning the task well they then try different algorithmic tasks where the periodicity of the functions is helpful pros the closed form derivations of the loss surface were interesting to see and the clarity of tone on the advantages and disadvantages was educational cons seems like more of a preliminary investigation of the potential benefits of sin and more evidence to support or in contrary is needed to conclude anything significant the results on mnist seem to indicate truncated sin is just as good and while it is interesting that tanh maybe uses more of the saturated part the two seem relatively interchangeable the toy algorithmic tasks are hard to conclude something concrete from,4.0
433.json,this paper presents a clever way of training a generative model which allows for exact inference sampling and log likelihood evaluation the main idea here is to make the jacobian that comes when using the change of variables formula from data to latents triangular this makes the determinant easy to calculate and hence learning possible the paper nicely presents this core idea and a way to achieve this by choosing special routings between the latents and data such that part of the transformation is identity and part some complex function of the input a deep net for example the resulting jacobian has a tractable structure this routing can be cascaded to achieve even more complex transformation on the experimental side the model is trained on several datasets and the results are quite convincing both in sample quality and quantitive measures i would be very happy to see if this model is useful with other types of tasks and if the resulting latent representation help with classification or inference such as image restoration in summary the paper is nicely written results are quite good and the model is interesting i am happy to recommend acceptance,8.0
433.json,this paper proposes a new generative model that uses real valued non volume preserving transformations in order to achieve efficient and exact inference and sampling of data points the authors use the change of variable technique to obtain a model distribution of the data from a simple prior distribution on a latent variable by carefully designing the bijective function used in the change of variable technique they obtain a jacobian that is triangular and allows for efficient computation generative models with tractable inference and efficient sampling are an active research area and this paper definitely contributes to this field while not achieving state of the art they are not far behind this does not change the fact that the proposed method is innovative and worth exploring as it tries to bridge the gap between auto regressive models variational autoencoders and generative adversarial networks the authors clearly mention the difference and similarities with other types of generative models that are being actively researched compared to autoregressive models the proposed approach offers fast sampling compared to generative adversarial networks real nvp offers a tractable log likelihood evaluation compared to variational autoencoders the inference is exact compared to deep boltzmann machines the learning of the proposed method is tractable it is clear that real nvp goal is to bridge the gap between existing and popular generative models the paper presents a lot of interesting experiments showing the capabilities of the proposed technique making the code available online will certainly contribute to the field is there any intention of releasing the code typo section we also use apply batch normalization,8.0
649.json,this paper analyzes dependency trees vs standard window contexts for word vector learning while that a good goal i believe the paper falls short of a thorough analysis of the subject matter it does not analyze glove like objective functions which often work better than the algorithms used here it does not compare in absolute terms to other published vectors or models it fails to gain any particularly interesting insights that will modify other people work it fails to push the state of the art or make available new resources for people,4.0
649.json,this paper evaluates how different context types affect the quality of word embeddings on a plethora of benchmarks i am ambivalent about this paper on one hand it continues an important line of work in decoupling various parameters from the embedding algorithms this time focusing on context on the other hand i am not sure i understand what the conclusion from these experiments is there does not appear to be a significant and consistent advantage to any one context type why is this are the benchmarks sensitive enough to detect these differences if they exist while i am ok with this paper being accepted i would rather see a more elaborate version of it which tries to answer these more fundamental questions,6.0
472.json,the work proposes to use the geometry of data that is considered to be known a priori in order to have more consistent sparse coding namely two data samples that are similar or neighbours should have a sparse code that is similar in terms of support the general idea is not unique but it is an interesting one if one admits that the adjacency matrix a is known a priori and the novelty mostly lies on the definition of the regularisation term that is an l norm while other techniques would mostly use l regularisation based on this idea the authors develop a new srsc algorithm which is analysed in detail and shown to perform better than its competitors based on l sparse coding regularisation and other schemes in terms of clustering performance inspired by lista the authors then propose an approximate solution to the srsc problem called deep srsc that acts as a sort of fast encoder here too the idea is interesting and seems to be quite efficient from experiments on usps data even if the framework seems to be strongly inspired from lista that scheme should however be better motivated by the limitations of srsc that should be presented more clearly overall the paper is well written and pretty complete it is not extremely original in its main ideas though but the actual algorithm and implementation seem new and effective,7.0
615.json,l sr seems to have o mn time complexity i miss this information in your paper your experimental results suggest that l sr does not outperform adadelta i suppose the same for adam given the time complexity of l sr the x axis showing time would suggest that l sr is much say m times slower the memory size of had the lowest minimum test loss over suggests that the main driven force of l sr was its momentum i e the second order information was rather useless,4.0
584.json,the authors propose a transfer learning approach applied to a number of nlp tasks the set of tasks appear to have an order in terms of complexity from easy syntactic tasks to somewhat harder semantic tasks novelty the way the authors propose to do transfer learning is by plugging models corresponding to each task in a way that respects the known hierarchy in terms of nlp complexity of those tasks in that respect the overall architecture looks more like a cascaded architecture than a transfer learning one there are some existing literature in the area first two google results found,6.0
642.json,this paper explores the performance area energy model accuracy tradeoff encountered in designing custom number representations for deep learning inference common image based benchmarks vgg googlenet etc are used to demonstrate that fewer than bits in a custom floating point representation can lead to improvement in runtime performance and energy efficiency with only a small loss in model accuracy questions does the custom floating point number representation take into account support for de normal numbers is the custom floating point unit clocked at the same frequency as the baseline bit floating point unit if not what are the different frequencies used and how would this impact the overall system design in terms of feeding the data to the floating point units from the memory comments i would recommend using the ieee half precision floating point bit sign bit exponent and bit mantissa as a baseline for comparison at this point it is well known in both the ml and the hw communities that bit floats are an overkill for dnn inference and major hw vendors already include support for ieee half precision floats in my opinion the claim that switching to custom floating point lead to a yy zz x savings in energy is misleading it might be true that the floating point unit itself might consume less energy due to smaller bit width of the operands however a large fraction of the total energy is spent in data movement to from the memories as a result reducing the floating point unit s energy consumption by a certain factor will not translate to the same reduction in the total energy a reader not familiar with such nuances for example a typical member of the ml community may be mislead by such claims on a similar note as comment the authors should explicitly mention that the claimed speedup is that of the floating point unit only and it will not translate to the overall workload speedup although the speedup of the compute unit is roughly quadratic in the bit width the bandwidth requirements scale linearly with bit width as a result it is possible that these custom floating point units may be starved on memory bandwidth in which case the claims of speedup and energy savings need to be revisited the authors should also comment on the complexities and overheads introduced in data accesses designing the various system buses data paths when the number representation is not byte aligned moving to a custom bit number representation for example can improve the performance and energy efficiency of the floating point unit but these gains can be partially eroded due to the additional overhead in supporting non byte aligned memory accesses,5.0
592.json,this paper replaces the gaussian prior often used in a vae with a group sparse prior they modify the approximate posterior function so that it also generates group sparse samples the development of novel forms for the generative model and inference process in vaes is an active and important area of research i do not believe the specific choice of prior proposed in this paper is very well motivated however i believe several of the conceptual claims are incorrect the experimental results are unconvincing and i suspect compare log likelihoods in bits against competing algorithms in nats some more detailed comments in table the log likelihoods reported for competing techniques are all in nats the reported log likelihood of cvae using k samples is not only higher than the likelihood of true data samples but is also higher than the log likelihood that can be achieved by fitting a k k means mixture model to the data eg as done in a note on the evaluation of generative models it should nearly impossible to outperform a k k means mixture on parzen estimation which makes me extremely skeptical of these evae results however if you assume that the evae log likelihood is actually in bits and multiply it by log to convert to nats then it corresponds to a totally believable log likelihood note that some parzen window implementations report log likelihood in bits is this experiment comparing log likelihood in bits to competing log likelihoods in nats also label units eg bits or nats in table it would be really really good to report and compare the variational lower bound on the log likelihood alternatively if you are concerned your bound is loose you can use ais to get a more exact measure of the log likelihood even if the parzen window results are correct parzen estimates of log likelihood are extremely poor they possess any drawback of log likelihood evaluation which they approximate and then have many additional drawbacks as well the mnist sample quality does not appear to be visually competitive also it appears that the images are of the probability of activation for each pixel rather than actual samples from the model samples would be more accurate but either way make sure to describe what is shown in the figure there are no experiments on non toy datasets i am still concerned about most of the issues i raised in my questions below briefly some comments on the authors response minibatches are constructed to not only have a random subset of training examples but also be balanced w r t to epitome assignment alg ln nice this makes me feel better about why all the epitomes will be used i do not think your response addresses why cvae would trade off between data reconstruction and being factorial the approximate posterior is factorial by construction there nothing in cvae that can make it more or less factorial for cvae to have zero contribution from the kl term of a particular zd in other words that unit is deactivated it has to have all the examples in the training set be deactivated kl term of zero for that unit this is not true a standard vae can set the variance to and the mean to kl term of for some examples in the training set and have non zero kl for other training examples the vae loss is trained on a lower bound on the log likelihood though it does have a term that looks like reconstruction error naively i would imagine that if it overfits this would correspond to data samples becoming more likely under the generative model see parzen concerns above it strange to train a binary model and then treat it probability of activation as a sample in a continuous space we can only evaluate the model from its samples i do not believe this is true you are training on a lower bound on the log likelihood which immediately provides another method of quantitative evaluation additionally you could use techniques such as ais to compute the exact log likelihood i do not believe parzen window evaluation is a better measure of model quality even in terms of sample generation than log likelihood,4.0
592.json,this paper proposes an elegant solution to a very important problem in vaes namely that the model over regularizes itself by killing off latent dimensions people have used annealing of the kl term and free bits to hack around this issue but a better solution is needed the offered solution is to introduce sparsity for the latent representation for every input only a few latent distributions will be activated but across the dataset many latents can still be learned what i didn t understand is why the authors need the topology in this latent representation why not place a prior over arbitrary subsets of latents that seems to increase the representational power a lot without compromising the solution to the problem you are trying to solve now the number of ways the latents can combine is no longer exponentially large which seems a pity the first paragraph on p is a mystery to me an effect of this samples how can under utilization of model capacity lead to overfitting the experiments are modest but sufficient this paper has an interesting idea that may resolve a fundamental issue of vaes and thus deserves a place in this conference,8.0
480.json,the paper presents a method for joint motion prediction and activity classification from sequences with two different applications motion of fruit flies and online handwriting recognition the method uses a classical encoder decoder pipeline with skip connections allowing direct communication between the encoder and the decoder on respective levels of abstraction motion is discretized and predicted using classification the model is trained on classification loss combined with a loss on motion prediction the goal is to leverage latter loss in a semi supervised setting from parts of the data which do not contain action labels the idea of leveraging predictions to train feature representations for discrimination is not new however the paper presents a couple of interesting ideas partially inspired from other work in other areas my biggest concern is with the experimental evaluation the experimental section contains a large amount of figures which visualize what the model has learned in a qualitative way however quantitative evaluation is rarer on the fly application the authors compare the classification performance with another method previously published by the first author again on the fly application the performance gain on motion prediction in figure c looks small compared to the baseline i am not sure it is significant i did not see any recognition results on the handwriting application has this part not been evaluated figure a is difficult to understand and to interpret the term besnet is used here without any introduction figure seems to tell multiple and different stories i would suggest splitting it into at least two different figures,6.0
603.json,pros using neural network on a new domain cons it is not clear how it is guaranteed that the network generates syntactically correct code questions comments how is the ntn ntnt top accuracy is computed maximizing the multiplied posterior probability of the two classifications were all combinations of ntn decision with all possible ntnt considered using unk is obvious and should be included from the very beginning in all models since the authors selected the size of the lexicon thus limited the possible predictions the question should then more likely be what is the optimal value of alpha for unk see also my previous comment on estimating and using unk section second paragraph compares numbers which are not comparable,5.0
551.json,summary this paper presents a differentiable histogram filter for state estimation tracking the proposed histogram filter is a particular bayesian filter that represents the discretized states using beliefs the prediction step is parameterized by a locally linear and translation invariant motion model while the measurement model is represented by a multi layered neural network the whole system is learned with both supervised and unsupervised objectives and experiments are carried out on two synthetic robot localization tasks d and d the major claim of this paper is that the problem specific model structure bayesian filter for state estimation should improve pure deep learning approach in data efficiency and generalization ability this paper has nice arguments about the importance of prior knowledge to deep learning approach for specific tasks an end to end histogram filter is derived for state estimation and unsupervised learning is possible in this model this paper seems to have a hidden assumption that deep learning rnn is a natural choice for recursive state estimation and the rest of paper is built upon this assumption including lstm baselines however this assumption itself may not be true because bayesian filter is a first established approach for this classic problem so it it more important to justify if deep learning is even necessary for solving the tasks presented this requests pure bayesian filter baselines in the experiments the derived histogram filter seems to be particularly designed for discretized state space it is not clear how well it can be generalized to continuous state space using the notation x more interestingly the observation is discrete binary as well which eventually makes it possible to derive a closed form measurement update model this setup might be too constrained generalizing to continuous observations is not a trivial task not even to mention using images as observations like haarnoja et al these design choices overall narrow down the scope of applicability,4.0
443.json,this paper proposes to use an ssnt model of p x y to allow for a noisy channel model of conditional generation that still allows for incremental generation of y the authors also propose an approximate search strategy for decoding and do an extensive empirical evaluation pros this paper is generally well written and the ssnt model is quite interesting and its application here well motivated furthermore the empirical evaluation is very well done and the authors obtain good results cons one might be concerned about whether the additional training and decoding complexity is warranted for instance one might plausibly obtain the benefits of the proposed approach by reranking full outputs from a standard seqseq model with a score combining p y x p x y and p y it worth noting that li et al naacl do something similar for conversation modeling at the same time being able to rerank during search may be helpful and so it might be nice to see some experiments addressing this other comments given that the main thrust of the paper is to provide a model for p x y the paper might be slightly clearer if section were presented from the perspective of modeling p x y instead of switching back to p y x as in the original yu et al paper it initially seems strange to suggest a noisy channel model as a way of addressing the explaining away problem since now you have an explicit uncalibrated p y term however since seqseq models appear to naturally do a lot of target side language modeling incorporating an explicit p x y term seems quite clever,7.0
785.json,this paper introduces a neural network architecture and training procedure for predicting the speed of a vehicle several seconds into the future based on video and vehicle state input the architecture allows several rnns to compete to make the best predictions with only the best prediction receiving back propagation training at each time step preliminary experimental results show that this scheme can yield reduced prediction error it is not clear how the best performing rnn is chosen for each time point at test time that is how is the integrated prediction obtained in fig is the prediction the one with minimum error over all of the output layers if so this means the prediction cannot be made until you already know the value to be predicted it seems possible that a larger generic rnn might be able to generate accurate predictions if i understand correctly the competitive architectures have many more parameters than the baseline is the improved performance here due to the competitive scheme or just a larger model a large amount of additional work is required to sustain the claim that this scheme is successfully extracting driver intentions it would be interesting to see if the scheme suitably extended can automatically infer the intention to stop at a stop sign vs slowing but not stopping due to a car in front say or to pass a car vs simply changing lanes adding labels to the dataset may enable this comparison more clearly more generally the intention of the driver seems more related to the goals they are pursuing at the moment there is a fair amount of work in inverse reinforcement learning that examines this problem some of it in the context of driving style as well,2.0
785.json,this paper proposes a neural network architecture for car state prediction while driving based on competitive learning competitive learning creates several duplicates of the baseline neural architecture and during training only updates the architecture with minimum loss the experiments compare the competitive learning approach to a single baseline architecture on a driving benchmark task the paper is understandable but could benefit from some copy editing the competitive learning approach seems rather adhoc and this paper feels quite incomplete without significant discussion and comparisons to ensembling much recent work has shown that duplicating and ensembling neural architectures can produce gains and it s not clear why competitive learning is better than ensembling it seems less theoretically sound to me there is a huge confound in the experiments due to the competitive learning architecture having many more free parameters than the baseline architecture again i think comparing to ensembling with the same number of architectures duplicated and perhaps comparing to a single baseline with larger hidden layers to make the total number of free parameters comparable is critical to validating the proposed approach the graphical model of the driving process depicted in figure seems nonsensical if e is observed then all variables are known given the dependencies shown further it is at best very poor notation to say that the driving action d decided at time t affects the vehicle state s at that same time it should be that st depends on d t also according to this figure the driving decision d does not depend on the observed vehicle state x which also seems invalid odd to have a paragraph break in abstract figure caption should include a brief explanation of the variables shown,2.0
394.json,paper summary this paper proposes a variant of dropout applicable to rnns in which the state of a unit is randomly retained as opposed to being set to zero this provides noise which gives the regularization effect but also prevents loss of information over time in fact making it easier to send gradients back because they can flow right through the identity connections without attenuation experiments show that this model works quite well it is still worse that variational dropout on penn tree bank language modeling task but given the simplicity of the idea it is likely to become widely useful strengths simple idea that works well detailed experiments help understand the effects of the zoneout probabilities and validate its applicability to different tasks domains weaknesses does not beat variational dropout but maybe better hyper parameter tuning will help quality the experimental design and writeup is high quality clarity the paper clear and well written experimental details seem adequate originality the proposed idea is novel significance this paper will be of interest to anyone working with rnns which is a large group of people minor suggestion as the authors mention zoneout has two things working for it the noise and the ability to pass gradients back without decay it might help to tease apart the contribution from these two factors for example if we use a fixed mask over the unrolled network different at each time step instead of resampling it again for every training case it would tell us how much help comes from the identity connections alone,8.0
681.json,this work proposes improvements to scattering networks a non linearity that allows fourier domain computation compact supported in the fourier domain representations and computing additional variance features the technical contributions seem worthwhile since and may result in better speed while may improve accuracy unfortunately they are poorly described and evaluated if the writing was clear and the evaluation more broad i would have recommended acceptance since the ideas have merit one of the biggest faults of the presentation is that many sentences are overly long and full of unnecessary obfuscating language e g the last paragraph of section though unfortunately this permeates the whole paper likewise most equations are made unnecessarily complicated for example eq does not need lines and so many indexes but just x x xl x l psil with the operator being element wise most of the hyperparameter dependencies and indexes are not necessary as well as the repetition of iterations the same reasoning can be applied to most equations to the argument of cardinality eq does not really help prove that variance is more informative in fact we could just as easily write that the cardinality of s concatenated with any other quantity is the cardinality of s another argument from machine learning theory would be better the authors should strive to make the arguments in the paper less hyperbolic and better substantiated the claims about finding invariants of any input abstract and fundamental structures last paragraph of section are not really backed up by any math how can we have any guarantees about singling out for example semantically relevant representations the learning procedures in machine learning give at least some guarantees while here the feature building seems a bit more heuristic this does not take away from the main idea but this part needs to be better researched,5.0
475.json,paper proposes a novel variational encoder architecture that contains discrete variables model contains an undirected discrete component that captures distribution over disconnected manifolds and a directed hierarchical continuous component that models the actual manifolds induced by the discrete variables in essence the model clusters the data and at the same time learns a continuous manifold representation for the clusters the training procedure for such models is also presented and is quite involved experiments illustrate state of the art performance on public datasets including mnist omniglot caltech overall the model is interesting and could be useful in a variety of applications and domains the approach is complex and somewhat mathematically involved it not exactly clear how the model compares or relates to other rbm formulations particularly those that contain discrete latent variables and continuous outputs as a prime example graham taylor and geoffrey hinton factored conditional restricted boltzmann machines for modeling motion style in proc of the th international conference on machine learning icml discussion of this should certainly be added,8.0
567.json,this paper proposes a multiview learning approach to finding dependent subspaces optimized for maximizing cross view similarity between neighborhoods of data samples the motivation comes from information retrieval tasks authors position their work as an alternative to cca based multiview learning note however that cca based techniques have very different purpose and are rather broadly applicable than the setting considered here main points i am not sure what authors mean by time complexity it would appear that they simply report the computational cost of evaluating the objective in equation is there a sense of how many iterations of the l bfgs method since that is going to be difficult given the nature of the optimization problem one would appreciate some sense of how hard or easy it is in practice to optimize the objective in and how that varies with various problem dimensions authors argue that scalability is not their first concern which is understandable but if they are going to make some remarks about the computational cost it better be clarified that the reported cost is for some small part of their overall approach rather than time complexity since authors position their approach as an alternative to cca they should remark about how cca even though a nonconvex optimization problem can be solved exactly with computational cost that is linear in the data size and only quadratic with dimensionality even with a naive implementation the method proposed in the paper does not seem to be tractable at least not immediately the empirical results with synthetic data are a it confusing first of all the data generation procedure is quite convoluted i am not sure why we need to process each coordinate separately in different groups and then permute and combine etc a simple benchmark where we take different linear transformations of a shared representation and add independent noise would suffice to confirm that the proposed method does something reasonable i am also baffled why cca does not recover the true subspace arguably it is the level of additive noise that would impact the recoverability however the proposed method is nearly exact so the noise level is perhaps not so severe it is also not clear if authors are using regularization with cca without regularization cca can be have in a funny manner this needs to be clarified,4.0
567.json,this paper presents an multi view learning algorithm which projects the inputs of different views linearly such that the neighborhood relationship transition probabilities agree across views this paper has good motivation to study multi view learning from a more information retrieval perspective some concerns the time complexity of the algorithm in its current form is high see last paragraph of page this might be the reason why the authors have conducted experiments on small datasets and using linear projections the proposed method does have some nice properties e g it does not require the projections to have the same dimension across views i like this while it more directly models neighborhood relationship than cca based approaches it is still not directly optimizing typical retrieval e g ranking based criteria on the other hand the contrastive loss in hermann and blunsom multilingual distributed representations without word alignment iclr is certainly a relevant information retrieval approach and shall be discussed and compared with my major concern about this paper is the experiments as i mentioned in my previous comments there are limited cases where linear mapping is more desirable than nonlinear mappings for dimension reduction while the authors have argued that linear projection may provide better interpretability i have not found empirical justification in this paper moreover one could achieve interpretability by visualizing the projections and see what variations of the input is reflected along certain dimensions this is commonly done for nonlinear dimension reduction methods i agree that the general approach here generalizes to nonlinear projections easily but the fact that the authors have not conducted experiments with nonlinear projections and comparisons with nonlinear variants of cca and other multi view learning algorithms limits the significance of the current paper,4.0
422.json,this is mainly a well written toy application paper it explains sgvb can be applied to state space models the main idea is to cast a state space model as a deterministic temporal transformation with innovation variables acting as latent variables the prior over the innovation variables is not a function of time approximate inference is performed over these innovation variables rather the states this is a solution to a fairly specific problem e g it does not discuss how priors over the beta can depend on the past but an interesting application nonetheless the ideas could have been explained more compactly and more clearly the paper dives into specifics fairly quickly which seems a missed opportunity my compliments for the amount of detail put in the paper and appendix the experiments are on toy examples but show promise section in our notation one would typically set betat wt though other variants are possible it s probably better to clarify that if ft and bt and not in betat they are not given a bayesian treatment but e g merely optimized section last paragraph a key contribution is forcing the latent space to fit the transition this seems rather trivial to achieve eq this interpretation implies the factorization of the recognition model the factorization is not implied anywhere i e you could in principle use q beta x q w x v q v,6.0
658.json,this paper tries to solve the problem of interpretable representations with focus on sum product networks the authors argue that spns are a powerful linear models that are able to learn parts and their combinations however their representations havent been fully exploited by generating embeddings pros the idea is interesting and interpretable models representations is an important topic generating embeddings to interpret spns is a novel idea the experiments are interesting but could be extended cons the author contribution is not fully clear and there are multiple claims that need support for example spns are indeed interpretable as is since the bottom up propagation of information from the visible inputs could be visualized at every stage and the top down parse could be also visualized as it has been done before amer todorovic another example proposition one claims that mpns are perfect encoder decoders since the max nodes always have one max value however what if it was uniformally distributed node or there are two equal values did the authors run into such cases did they address all edge cases a good comparison could have been against generative adversarial networks gans generative stochastic networks gsns and variational autoencoders too since they are the state of the art generative models rather than comparing with rbms and nade i would suggest that the authors take sometime to evaluate their approach against the suggested methods and make sure to clarify their contributions and eliminate over claiming statements i agree with the other comments raised by anon reviewer,6.0
658.json,the authors propose and evaluate using spn to generate embeddings of input and output variables and using mpn to decode output embeddings to output variables the advantage of predicting label embeddings is to decouple dependencies in the predicted space the authors show experimentally that using spn based embeddings is better than those produced by rbm this paper is fairly dense and a bit hard to read after the discussion the main contributions of the authors are they propose the scheme of learning spn over y and then using mpn to decode the output or just spns to embed x they propose how to decode mpn with partial data they perform some analysis of when their scheme will lead to perfect encoding decodings they run many many experiments comparing various ways of using their proposed method to make predictions on multi label classification datasets my main concerns with this paper are as follows the point of this paper is about using generative models for representation learning in their experiments the main task is discriminative e g predict multiple y from x the only discriminative baseline is a l regularized logistic regression which does not have any structure on the output it would be nice to see how a discriminative structured prediction method would do such as crf or belief propagation the many experiments suggest that their encoder decoder scheme is working better than the alternatives can you please give more details on the relative computation complexity of each method one thing i am still having trouble understanding is why this method works better than made and the other alternatives is it learning a better model of the distribution of y is it better at separating out correlations in the output into individual nodes does it have larger representations i think the experiments are overkill and if anything they way they are presented detract from the paper there already far too many numbers and graphs presented to be easy to understand if i have to dig through hundreds of numbers to figure out if your claim is correct the paper is not clear enough and i said this before in my comments please do not refer to q q etc these shortcuts let you make the paper more dense with fewer words but at the cost of readability i think i convinced myself that your method works i would love to see a table that shows for each condition a a baseline x y b one average result across datasets for your method and c one average result from a reasonable best competitor method please show for both the exact match and hamming losses as that will demonstrate the gap between independent linear prediction and structured prediction that would still be plenty of numbers but would make it much easier for the reader to verify your claims and you can put everything else in the appendix e g something like input predicted output decoder hamming exact match x p y crf xx xx xx xx this is your baseline spn ex p y n a xx xx xx xx x spn ey mpn xx xx xx xx given x predict ey then decode it with an mpn does a presentation like that make sense it just really hard and time consuming for me as a reviewer to verify your results the way you have laid them out currently,6.0
571.json,this paper extends boosting to the task of learning generative models of data the strong learner is obtained as a geometric average of weak learners which can themselves be normalized e g vae or un normalized e g rbms generative models genbgm or a classifier trained to discriminate between the strong learner at iteration t and the true data distribution discbgm this latter method is closely related to noise contrastive estimation gans etc the approach benefits from strong theoretical guarantees with strict conditions under which each boosting iteration is guaranteed to improve the log likelihood the downside of the method appears to be the lack of normalization constant for the resulting strong learner and the use of heuristics to weight each weak learner which seems to matter in practice from sec the discriminative approach further suffers from an expensive training procedure each round of boosting first requires generating a training set worth of samples from the previous strong learner where samples are obtained via mcmc the experimental section is clearly the weak point of the paper the method is evaluated on a synthetic dataset and a single real world dataset mnist both for generation and as a feature extraction mechanism for classification of these the synthetic experiments were the clearest in showcasing the method on mnist the baseline models are much too weak for the results to be convincing a modestly sized vae can obtain nats within hours on a single gpu clearly an achievable goal furthermore despite arguments to the contrary i firmly believe that mixing base learners is an academic exercise if only because of the burden of implementing k different models training algorithms this section fails to answer a more fundamental question is it better to train a large vae by maximizing the elbow or e g train iterations of boosting using vaes th the size of the baseline model experimental details are also lacking especially with respect to the sampling procedure used to draw samples from the bgm the paper would also benefit from likelihood estimates obtained via ais with regards to novelty and prior work there is also a missing reference to self supervised boosting by welling et al r after a cursory read through there seems to be strong similarities to the genbgm approach which ought to be discussed overall i am on the fence the idea of boosting generative models is intriguing seems well motivated and has potential for impact for this reason and given the theoretical contributions i am willing to overlook some of the issues highlighted above and hope the authors can address some of them in time for the rebuttal r,6.0
571.json,the authors propose two approaches to combine multiple weak generative models into a stronger one using principles from boosting the approach is simple and elegant and basically creates an unnormalized product of experts model where the individual experts are trained greedily to optimize the overall joint model unfortunately this approach results in a joint model that has some undesirable properties a unknown normalisation constant for the joint model and therefore an intractable log likelihood on the test set and it makes drawing exact samples from the joint model intractable these problems can unfortunately not be fixed by using different base learners but are a direct result of the product of experts formulation of boosting the experiments on dimensional toy data illustrate that the proposed procedure works in principle and that the boosting formulation produces better results than individual weak learners and better results than e g bagging but the experiments on mnist are less convincing without an undisputable measure like e g log likelihood it is hard to draw conclusions from the samples in figure and visually they look weak compared to even simple models like e g nade i think the paper could be improved significantly by adding a quantitative analysis investigating the effect of combining undirected e g rbm undirected e g vae and autoregressive e g nade models and by measuring the improvement over the number of base learners but this would require a method to estimate the partition function z or estimating some proxy,5.0
434.json,this paper extends batch normalization successfully to rnns where batch normalization has previously failed or done poorly the experiments and datasets tackled show definitively the improvement that batch norm lstms provide over standard lstms they also cover a variety of examples including character level ptb and text word level cnn question answering task and pixel level mnist and pmnist the supplied training curves also quite clearly show the potential improvements in training time which is an important metric for consideration the experiment on pmnist also solidly shows the advantage of batch norm in the recurrent setting for establishing long term dependencies i additionally also appreciated the gradient flow insight specifically the impact of unit variance on tanh derivatives showing it not just for batch normalization but additionally the toy task figure b was hugely useful overall i find this paper a useful additional contribution to the usage of batch normalization and would be necessary information for successfully employing it in a recurrent setting,8.0
526.json,it is interesting to derive such a bound and show it satisfies a regret bound along with empirical evidence on the cifar for cross entropy loss and auto encoder for mse loss at least empirically by comparing the observed training loss and taylor loss the better a particular optimizer performs training loss statement not a validation or observed test statement the smaller the difference between these two also shown is the regret loss is satisfied at different scales of the network by layer neuron and whole network the taylor approximation can be used to investigate activation configurations of the network and used to connect this to difficulty in optimizing at kinks in the loss surface along with an empirical study of exploration of activation surface of the sgd adam rmsprop optimizers the more exploration the better the resulting training loss not that it impacts the paper but the weaker performance of the sgd could be related to the fixed learning rate if we anneal this learning rate which should improve performance does this translate to more exploration and tightening between the actual loss and the taylor loss it might be useful to use a cross validation set for some of the empirical studies in the end we would like to say something about generalization of the resulting network is there a reason the subscript on the jacobian changes to al in the,7.0
361.json,this paper is about using bayesian neural networks to model learning curves that arise from training ml algorithms the application is hyper parameter optimization if we can model the learning curve we can terminate bad runs early and save time the paper builds on existing work that used parametric learning curves here the parameters of these learning curves form the last layer of a bayesian neural network this seems like a totally sensible idea i think the main strength of this paper is that it addresses an actual need based on my personal experience there is high demand for a working system to do early termination in hyperparameter optimization what i would like to know which i wish i would asked during pre review questions is whether the authors plan to release their code do you i sincerely hope so because i think the code would be a significant part of the paper contribution since the nature of the paper is more practical than conceptual the experiments in the paper seem thorough but the results are a bit underwhelming i am less interested in the part about whether the learning curves are actually modeled well and more interested in the impact on hyperparameter optimization i was hoping to see big speedups as a result of using this method but i am left feeling unsure how big the speedup really is instead of objective function vs iterations i would be more interested in the inverse plot number of iterations needed to get to a fixed objective function value since what i am really interested in is how much time i can save ideally there would also be some mention of real time as sometimes these hyperparameter optimization methods are themselves so slow that they end up being unusable finally one figure that i feel is missing is a histogram of termination times over different runs this would provide me with more intuition than all the other figures because it would tell me what fraction of runs are being terminated early and how early right now i have no sense of this except that at least some runs are clearly being terminated early since this is neccessary for the proposed method to outperform other methods overall i think this paper merits acceptance because it is a solid effort on an interesting problem the progress is fairly incremental but i can live with that,7.0
418.json,the paper introduces a technique for stabilizing the training of generative adversrial networks by unrolling the inner discriminator optimization in the gan loss function several steps and optimizing the generator with respect to the final state of this optimization process the experimental evidence that this actually helps is very compelling the d example shows a toy problem where this technique helps substantially the lstm mnist generator example shows that the procedure helps with stabilizing the training of an unusual architecture of generator and the image generation experiment while not being definitive is very convincing for future work it would be interesting to see whether a method with smaller memory requirements could be devised based on similar principles i strongly recommend to accept this paper,9.0
789.json,the authors propose to sample from vaes through a markov chain zt q z x x t xt p x z zt the paper uses confusing notation oversells the novelty ignoring some relevant previous results the qualitative difference between regular sampling and this gibbs chain is not very convincing judging from the figures it would be a great workshop paper perhaps more if the authors fix the notation fix the discussion to related work and produce more convincing perhaps simply upscaled figures comments rezende et al original vae paper already discusses the markov chain which is ignored in this paper notation is nonstandard confusing at page it s unclear what the authors mean with p x z which is approximated as q x z it s also not clear what s meant with q z at page q z is called the learned distribution while p z can in general also be a learned distribution it s not true that it s impossible to draw samples from q z one can sample x q x from the dataset then draw z q z x it not explained whether the analysis only applies to continuous observed spaces or also discrete observed spaces figures and are not very convincing,3.0
789.json,the authors argues that the standard ancestral sampling from stochastic autoencoders such as the variational autoencoder and the adversarial autoencoder imposes the overly restrictive constraint that the encoder distribution must marginally match the latent variable prior they propose as an alternative a markov chain monte carlo approach that avoids the need to specify a simple parametric form for the prior the paper is not clearly written most critically the notation the authors use is either deeply flawed or there are simple misunderstanding with respect to the manipulations of probability distributions for example the authors seem to suggest that both distributions q z x and q x z are parametrized for this to be true the model must either be trivially simple or an energy based model there is no indication that they are speaking of an energy based model another example of possible confusion is the statement that the ratio of distributions q z x p z i believe this is supposed to be a ratio of marginals q z p x overall it seems like there is a confusion of what q and p represent the standard notation used in vaes is to use p to represent the decoder distribution and q to represent the encoder distribution this seems not to be how the authors are using these terms nor does it seem like there is a single consistent interpretation the empirical results consist entirely of qualitative results samples and reconstructions from a single dataset celeba the samples are also not at all up to the quality of the sota models the interpolations shown in figures and both seems to look like interpolation in pixel space for both the vae model and the proposed dvae,3.0
623.json,the work presents some empirical observations to support the statement that the hessian of the loss functions in deep learning is degenerate but what does this statement refer to to my understanding there are at least three interpretations i the hessian of the loss functions in deep learning is degenerate at any point in the parameter space i e any network weight matrices ii the hessian of the loss functions in deep learning is degenerate at any critical point iii the hessian of the loss functions in deep learning is degenerate at any local minimum or any global minimum none of these interpretations is solidly supported by the observations provided in the paper more comments are as follows the authors state that we don t have much information on what the actual hessian looks like then i just wonder what hessian is investigated is it the actual one or approximate one please clarify and provide the references for computing the actual hessian it is not clear whether the optimization was done by a batch gradient descent algorithm i e batch back propagation bp algorithm or a stochastic bp algorithm if the training was done via a stochastic bp algorithm it is hard to conclude that the the neural network has been trained to its local minimum when it was done by a full batch bp algorithm what was the accumulating point was it local minimum or global minimum since the negative log likelihood function was used as at the end of training it is essentially a joint learning approach in both the newton weight matrices and the negative log likelihood vector certainly the whole loss function is not convex in these two parameters but if least squares error function is used at the end would it make any difference in claiming the degeneracy of the hessian finally the statement there are still negative eigenvalues even when they are small in magnitude is very puzzling potential reasons are a if the training algorithm did converge the accumulating points were not local minima i e they were saddle points b training algorithms did not converge or have not converged yet c the calculation of the actual hessian might be inaccurate,4.0
336.json,apologies for the late submission of this review and thank you for the author s responses to earlier questions this submission proposes an improved implementation of the pixelcnn generative model most of the improvements are small and can be considered as specific technical details such as the use of dropout and skip connections while others are slightly more substantial such as the use of a different likelihood model and multiscale analysis the submission demonstrates state of the art likelihood results on cifar my summary of the main contribution autoregressive type models of which pixelcnn is an example are a nice class of models as their likelihood can be evaluated in closed form a main differentiator for this type of models is how the conditional likelihood of one pixel conditioned on its causal neighbourhood is modelled in one line of work such as theis et al mcgsm theis et al spatial lstm the conditional distribution is modelled as a continuous density over real numbers this approach has limitations we know that in observed data pixel intensities are quantized to a discrete integer representation so a discrete distribution could give better likelihoods furthermore these continuous distributions have a tail and assign some probability mass outside the valid range of pixel intensities which may hurt the likelihood in more recent work by van den oord and colleagues the conditional likelihood is modelled as an arbitrary discrete distribution over the possible values for pixel intensities this does not suffer from the limitations of continuous likelihoods but it also seems wasteful and is not very data efficient the authors propose something in the middle by keeping the discretized nature of the conditional likelihood but restricting the discrete distribution to ones whose cdf that can be modeled as a linear combination of sigmoids this approach makes sense to me and is new in a way but it doesn t appear to be very revolutionary or significant to me the second somewhat significant modification is the use of downsampling and multiscale modelling as opposed to dilated convolutions the main motivation for the authors to do this is saving computation time while keeping the multiscale flexibility of the model the authors also introduce shortcut connections to compensate for the potential loss of information as they perform downsampling again i feel that this modification not particularly revolutionary multiscale image analysis with autoregressive generative models has been done for example in theis et al and several other papers overall i felt that this submission falls short on presenting substantially new ideas and reads more like documentation for a particular implementation of an existing idea,6.0
336.json,review this paper proposes five modifications to improve pixelcnn a generative model with tractable likelihood the authors empirically showed the impact of each of their proposed modifications using a series of ablation experiments they also reported a new state of the art result on cifar improving generative models especially for images is an active research area and this paper definitely contributes to it pros the authors motivate each modification well they proposed they also used ablation experiments to show each of them is important the authors use a discretized mixture of logistic distributions to model the conditional distribution of a sub pixel instead of a way softmax this allows to have a lower output dimension and to be better suited at learning ordinal relationships between sub pixel values the authors also mentioned it speeded up training time less computation as well as the convergence during the optimization of the model as shown in fig the authors make an interesting remark about how the dependencies between the color channels of a pixel are likely to be relatively simple and do not require a deep network to model this allows them to have a simplified architecture where you do not have to separate out all feature maps in groups depending on whether or not they can see the r g b sub pixel of the current location cons it is not clear to me what the predictive distribution for the green channel and the blue looks like more precisely how are the means of the mixture components linearly depending on the value of the red sub pixel i would have liked to see the equations for them minor comments in fig it is written sequence of layers but in the text section it says blocks of resnet layers what is the remaining layer in fig what does the first green square blue square which is not in the white rectangle represents is there any reason why the mixture indicator is shared across all three channels,7.0
766.json,paper summary the authors proposed to use edgeboxes fast rcnn with batch normalization for pedestrian detection review summary results do not cover enough datasets the reported results do not improve over state of the art writing is poor and overall the work lacks novelty this is a clear reject pros shows that using batch normalization does improve results cons only results on eth and inria should include caltech or kitti reported results are fair but not improving over state of the art overall idea of limited interest when considering works like s zhang cvpr fast r cnn for pedestrian detection and l zhang eccv faster r cnn for pedestrian detection issues with the text quality limited takeaways quality low clarity fair but poor english originality low significance low for acceptance at future conferences this work would need more polish improving over best known results on inra eth and caltech or kitti and ideally present additional new insights minor comments the text lacks polish e g influent influence has maken made is usually very important is important achieve more excellent results achieve better results etc please consider asking help from a native speaker for future submissions there are also non sense sentences such as it is computational citations should be in parentheses some of the citations are incorrect because the family name is in the wrong position e g joseph lim lawrence zitnick and rodrigo benenson,3.0
459.json,the paper proposed a tensor factorization approach for mtl to learn cross task structures for better generalization the presentation is clean and clear and experimental justification is convincing as mentioned including discussions on the effect of model size vs performance would be useful in the final version and also work in other fields related to this one question on sec to build the dmtrl one dnn per task is trained with the same architecture how important is this pretraining would random initialization also work here if the data is unbalanced namely some classes have very few examples how would that affect the model,7.0
662.json,the paper extends the ntm by a trainable memory addressing scheme the paper also investigates both continuous differentiable as well as discrete non differentiable addressing mechanisms pros extension to ntm with trainable addressing experiments with discrete addressing experiments on babi qa tasks cons big gap to memnn and dmn in performance code not available there could be more experiments on other real world tasks,7.0
399.json,paper strengths elegant use of moe for expanding model capacity and enabling training large models necessary for exploiting very large datasets in a computationally feasible manner the effective batch size for training the moe drastically increased also interesting experimental results on the effects of increasing the number of moes which is expected paper weaknesses there are many different ways of increasing model capacity to enable the exploitation of very large datasets it would be very nice to discuss the use of moe and other alternatives in terms of computational efficiency and other factors,6.0
376.json,contributions large scale experiments are used to measure the capacity and trainability of different rnn architectures capacity experiments suggest that across all architectures rnns can store between three and six bits of information per parameter with ungated rnns having the highest per parameter capacity all architectures are able to store approximately one floating point number per hidden unit trainability experiments show that ungated architectures rnn irnn are much harder to train than gated architectures gru lstm ugrnn rnn the paper also proposes two novel rnn architectures ugrnn and rnn experiments suggest that the ugrnn has similar per parameter capacity as the ungated rnn but is much easier to train and that deep layer rnn models are easier to train than existing architectures clarity the paper is well written and easy to follow novelty this paper is the first to my knowledge to empirically measure the number of bits of information that can be stored per learnable parameter the idea of measuring network capacity by finding the dataset size and other hyperparameters that maximizes mutual information is a particularly novel experimental setup the proposed ugrnn is similar but not identical to the minimal gated unit proposed by zhou et al minimal gated unit for recurrent neural networks international journal of automation and computing significance i have mixed feelings about the significance of this paper i found the experiments interesting but i don t feel that they reveal anything particularly surprising or unexpected about recurrent networks it is hard to see how any of the experimental results will change the way either that i think about rnns or the way that i will use them in my own future work on the other hand it is valuable to see intuitive results about rnns confirmed by rigorous experiments especially since few researchers have the computational resources to perform such large scale experiments the capacity experiments both per parameter capacity and per unit capacity essentially force the network to model random data for most applications of rnns however we do not expect them to work with random data instead when applied in machine translation or language modeling or image captioning or any number of real world tasks we hope that rnns can learn to model data that is anything but random it is not clear to me that an architecture s ability to model random data should be beneficial in modeling real world data indeed the experiments in section show that architectures vary in their capacity to model random data but the text experiments in section show that these same architectures do not significantly vary in their capacity to model real world data i do not think that the experimental results in the paper are sufficient to prove the significance of the proposed ugrnn and rnn architectures it is interesting that the ugrnn can achieve comparable bits per parameter as the ungated rnn and that the deep rnns are more easily trainable than other architectures but the only experiments on a real world task language modeling on text do not show these architectures to be significantly better than gru or lstm summary i wish that the experiments had revealed more surprising insights about rnns though there is certainly value in experimentally verifying intuitive results the proposed ugrnn and rnn architectures show some promising results on synthetic tasks but i wish that they showed more convincing performance on real world tasks overall i think that the good outweighs the bad and that the ideas of this paper are of value to the community pros the paper is the first of my knowledge to explicitly measure the bits per parameter that rnns can store the paper experimentally confirms several intuitive ideas about rnns rnns of any architecture can store about one number per hidden unit from the input different rnn architectures should be compared by their parameter count not their hidden unit count with very careful hyperparameter tuning all rnn architectures perform about the same on text language modeling gated architectures are easier to train than non gated rnns cons experiments do not reveal anything particularly surprising or unexpected the ugrnn and rnn architectures do not feel well motivated the utility of the ugrnn and rnn architectures is not well established,7.0
634.json,the paper presents a new exciting layerwise origin target synthesis method both for generating a large number of diverse adversarials as well as for understanding the robustness of various layers the methodology is then used to visualize the amount of perturbation necessary for producing a change for higher level features the approach to match the features of another unrelated image is interesting and it goes beyond producing adversarials for classification it can also generate adversarials for face recognition and other models where the result is matched with some instance from a database pro the presented approach is definitely sound interesting and original con the analyses presented in this paper are relatively shallow and do not touch the most obvious questions there is not much experimental quantitative evidence for the efficacy of this method compared with other approaches to produce adversarials the visualization is not very exciting and it is hard to any draw any meaningful conclusions from them it would definitely improve the paper if it would present some interesting conclusions based on the new ideas,6.0
634.json,this paper presents a relatively novel way to visualize the features hidden units of a neural network and generate adversarial examples the idea is to do gradient descent in the pixel space from a given hidden unit in any layer this can either be done by choosing a pair of images and using the difference in activations of the unit as the thing to do gradient descent over or just the activation itself of the unit for a given image in general this method seems intriguing here are some comments it s not clear that some of the statements at the beginning of sec are actually true re positive negative signs and how that changes or does not change the class mathematically i don t see why that would be the case moreover the contradictory evidence from mnist vs faces supports my intuition the authors use the pass score through the paper but only given an intuition citation for it i think it s worth explaining what it actually does in a sentence or two the pass score seems to have some but not complete correlation with l l infty or visual estimation of how good the adversarial examples are i am not sure what the take home message from all these numbers is in general lots cannot produce high quality adversarial examples at the lower layers sec seems false for mnist no i would have liked this work to include more quantitative results e g extract adversarial examples at different layers add them to the training set train networks compare on test set in addition to the visualizations present that to me is the main drawback of the paper in addition to basically no comparisons with other methods it s hard to judge the merits of this work in vacuum edit after rebuttal thanks to the authors for addressing the experimental validation concerns i think this makes the paper more interesting so revising my score accordingly,6.0
771.json,this paper proposes a method for estimating the context sensitivity of paraphrases and uses that to inform a word embedding learning model the main idea and model are presented convincingly and seem plausible the main weaknesses of the paper are shortcomings in the experimental evaluation and in the model exploration the evaluation does not convincingly determine whether the model is a significant improvement over simpler methods particularly those that do not require the paraphrase database likewise the model section did not convince me that this was the most obvious model formulation to try the paper would be stronger if model choices were explained more convincingly or better yet alternatives were explored on balance i lean towards rejecting the paper and encouraging the authors to submit a revised and improved version at a near point in the future detailed minor points below while the paper is grammatically mostly correct it would benefit from revision with the help of a native english speaker in its current form long sections are very difficult to understand due to the unconventional sentence structure the tables need better and more descriptive labels the results are somewhat inconclusive particularly in the analogy task in table it is surprising that cbow does better on the semantic aspect of the task than your embeddings which are specifically tailored to be good at this why was enriched cbow not included in the analogy task in the related work section several papers are mentioned that learn embeddings from a combination of lexica and corpora yet it is repeatedly said that this was the first work of such a kind that there has not been enough work on this that feels a little misleading,5.0
771.json,this paper tries to leverage an external lexicon knowledge base to improve corpus based word representations by determining in a fuzzy way which potential paraphrase is the most appropriate in a particular context i think this paper is a bit lost in translation the grammatical and storytelling styles made it really difficult for me to concentrate and even unintelligible at times one of the most important criteria in a conference paper is to communicate one ideas clearly unfortunately i do not feel that this paper meets that standard in addition the evaluation is rather lacking there are many ways to evaluate word representations and google analogy dataset has many issues see for example linzen paper from repeval as well as drozd et al coling finally this work does not provide any qualitative result or motivation why does this method work better where does it fail what have we learned about word representations lexicons corpus based methods in general,3.0
321.json,interesting work on hierarchical control similar to the work of heess et al experiments are strong and manage to complete benchmarks that previous work could not analysis of the experiments is a bit on the weaker side like other reviewers i find the use of the term intrinsic motivation somewhat inappropriate mostly because of its current meaning in rl pre training robots with locomotion by rewarding speed or rewarding grasping for a manipulating arm is very geared towards the tasks they will later accomplish the pre training tasks from heess et al while not identical are similar the mutual information regularization is elegant and works generally well but does not seem to help in the more complex mazes and the authors note this is there any interpretation or analysis for this result the factorization between sagent and srest should be clearly detailed in the paper duan et al specify sagent but for replicability srest should be clearly specified as well did i miss it it would be interesting to provide some analysis of the switching behavior of the agent more generally some further analysis of the policies failure modes effects of switching time on performance would have been welcome,7.0
622.json,this paper extend the spin glass analysis of choromanska et al a to res nets which yield the novel dynamic ensemble results for res nets and the connection to batch normalization and the analysis of their loss surface of res nets the paper is well written with many insightful explanation of results although the technical contributions extend the spin glass model analysis of the ones by choromanska et al a the updated version could eliminate one of the unrealistic assumptions and the analysis further provides novel dynamic ensemble results and the connection to batch normalization that gives more insightful results about the structure of res nets it is essential to show this dynamic behaviour in a regime without batch normalization to untangle the normalization effect on ensemble feature hence authors claim that steady increase in the l norm of the weights will maintain the this feature but setting for figure is restrictive to empirically support the claim at least results on cifar without batch normalization for showing effect of l norm increase and results that support claims about theorem would strengthen the paper this work provides an initial rigorous framework to analyze better the inherent structure of the current state of art res net architectures and its variants which can stimulate potentially more significant results towards careful understanding of current state of art models rather than always to attempting to improve the performance of res nets by applying intuitive incremental heuristics it is important to progress on some solid understanding too,7.0
527.json,brief summary this paper explores an extension of multiplicative rnns to the lstm type of models the resulting proposal is very similar to authors show experimental results on character level language modeling tasks in general i think the paper is well written and the explanations are quite clear criticisms in terms of contributions the paper is weak the motivation makes sense however very similar work has been done in and already an extension over because of that this paper mainly stands as an application paper the results are encouraging on the other hand they are still behind the state of art without using dynamic evaluation there are some non standard choices on modifications on the standard algorithms such as l parameter of rmsprop and multiplying output gate before the nonlinearity the experimental results are only limited to character level language modeling only an overview of the review pros a simple modification that seems to reasonably well in practice well written cons lack of good enough experimental results not enough contributions almost trivial extension over existing algorithms non standard modifications over the existing algorithms wu y zhang s zhang y bengio y salakhutdinov rr on multiplicative integration with recurrent neural networks inadvances in neural information processing systems pp sutskever i martens j hinton ge generating text with recurrent neural networks inproceedings of the th international conference on machine learning icml pp,4.0
527.json,pros clearly written new model mlstm which seems to be useful according to the results some interesting experiments on big data cons number of parameters in comparisons of different models is missing mlstm is behind some other models in most tasks,6.0
570.json,summary the paper presents an approach neural answer construction model for the task of answering non factoid questions in particular love advice questions the two main features of the proposed model are the following it incorporates the biases of semantics behind questions into word embeddings in addition to optimizing for closeness between questions and answers it also optimizes for optimum combination of sentences in the predicted answer the proposed model is evaluated using the dataset from a japanese online qa service and is shown to outperform the baseline model tan et al by relatively absolutely the paper also experiments with few other baseline models ablations of the proposed model strengths the two motivations behind the proposed approach need to understand the ambiguous use of words depending on context and need to generate new answers rather than just selecting from answers held by qa sites are reasonable the novelty in the paper involves the following incorporating biases of semantics behind questions into word embeddings using paragraphvec like model modified to take as inputs words from questions question title token and question category token modelling optimum combination of sentences conclusion and supplement sentences in the predicted answer designing abstract scenario for answers inspired by automated web service composition framework rao su and extracting important topics in conclusion sentence and emphasizing them in supplemental sentence using attention mechanism attention mechanism is similar to tan et al the proposed method is shown to outperform the current best method tan et al by relatively absolutely which seems to be significant improvement the paper presents few ablations studies that provide insights on how much different components of the model such as incorporating biases into word embeddings incorporating attention from conclusion to supplement are helping towards performance improvement weaknesses suggestions questions how are the abstract patterns determined i e how did the authors determine that the answers to love advice questions generally constitute of sympathy conclusion supplement for conclusion and encouragement how much is the improvement in performance when using abstract patterns compared to the case when not using these patters i e when candidate answers are picked from union of all corpuses rather than picking from respective corpuses corpuses for sympathy conclusion etc it seems that the abstract patterns are specific to the type of questions so the abstract patterns for love advice will be different from those for business advice thus it seems like the abstract patterns need to be hand coded for different types and hence one model cannot generalize across different types the paper should present explicit analysis of how much combinational optimization between sentences help comparison between model performance with and without combinational optimization keeping rest of the model architecture same the authors could also plot the accuracy of the model as a function of the combinational optimization scores this will provide insights into how significant are the combinational optimization scores towards overall model accuracy paper says that current systems designed for non factoid qa cannot generalize to questions outside those stored in qa sites and claims that this is one of the contributions of this paper in order to ground that claim the paper should show experimentally how well the proposed method generalized to such out of domain questions although the questions asked by human experts in the human evaluation were not from the evaluation datasets the paper should analyze how different those questions were compared to the questions present in the evaluation datasets for human evaluation were the outputs of the proposed model and that of the qa lstm model judged each judged by both the human experts or one of the human experts judged the outputs of one system and the other human expert judged the outputs of the other system if both the sets of outputs were each judged by both the human experts how were the ratings of the two experts combined for every questions i wonder why the authors did not do a human evaluation where they just ask human workers not experts to compare the output of the proposed model with that of the qa lstm model which of the two outputs they would like to hear when asking for advice such an evaluation would not get biased by whether each sentence is good or not whether the combination is good or not looking at the qualitative examples in table i personally like the output of the qa lstm more than that of the proposed model because they seem to provide a direct answer to the question e g for the first example the output of the qa lstm says you should wait until you feel excited whereas the output of the proposed model says it is better to concentrate on how to confess your love to her which seems a bit indirect to the question asked given a question is the ground truth answer different in the two tasks answer selection and answer construction the paper mentions that attentive lstm tan et al is evaluated as the current best answer selection method section so why is its accuracy lower than that of qa lstm in table the authors explain this by pointing out the issue of questions being very long compared to answers and hence the attention being noisy but did these issues not exist in the dataset used by tan et al the paper says the proposed method achieves gain over current best in conclusion section where they refer to qa lstm as the current best method however in the description of attentive lstm section the paper mentions that attention lstm is the current best method so could authors please clarify the discrepancy minor correction remove space between and in abstract review summary the problem of non factoid qa being dealt with in the paper is an interesting and useful problem to solve the motivations presented in the paper behind the proposed approach are reasonable the experiments show that the proposed model outperforms the baseline model however the use of abstract patterns to determine the answer seems like hand designing and hence it seems like these abstract patterns need to be designed for every other type of non factoid question and hence the proposed approach is not generalizable to other types also the paper needs more analysis of the results to provide insights into the contribution of different model components,4.0
659.json,this paper proposes a sequence transduction model that first uses a traditional statistical alignment methods to provide alignments for an encoder decoder type model the paper provides experiments on a number of morphological inflection generation datasets they shows an improvement over other models although they have much smaller improvements over a soft attention model on some of their tasks i found this paper to be well written and to have very thorough experiments analysis but i have concerns that this work is not particularly different from previous approaches and thus has a more focused contribution that is limited to its application on this type of shorter input the authors suggest that their approach is sufficient for shorter sequences but do not compare against the approach of chorowski et al or jailty el at in summary i found this paper to be well executed well written but it novelty and scope too small that said i feel this work would make a very good short paper,5.0
659.json,the paper proposes an approach to sequence transduction for the case when a monotonic alignment between the input and the output is plausible it is assumed that the alignment can be provided as a part of training data with chinese restaurant process being used in the actual experiments the idea makes sense although its applicability is limited to the domains where a monotonic alignment is available but as discussed during the pre review period there has been a lot of strongly overlapping related work such as probabilistic models with hard alignment sequence transduction with recurrent neural network graves et al and also attempts to use external alignments in end to end models a neural transducer jaitly et al that said i do not think the approach is sufficiently novel i also have a concern regarding the evaluation i do not think it is fair to compare the proposed model that depends on external alignment with the vanilla soft attention model that learns alignments from scratch in a control experiment soft attention could be trained to match the external alignment such a pretraining could reduce overfitting on the small dataset the one on which the proposed approach brings the most improvement on a larger dataset especially sigmorphon the improvements are not very big and are only obtained for a certain class of languages to sum up two main issues are a lack of novelty b the comparison of a model trained with external alignment and one without it,4.0
589.json,the paper proposes to combine graph convolution with rnns to solve problems in which inputs are graphs the two key ideas are i a graph convolutional layer is used to extract features which are then fed in an rnn and ii matrix multiplications are replaced by graph convolution operations i is applied to language modelling yielding lower perplexity on penn treebank ptb compared with lstm ii outperformed lstm cnn on the moving mnist both two models ideas are actually trivial and in line with the current trend of combining different architectures for instance the idea of replacing matrix multiplications by graph convolution is a small extension for shi et al regarding to the experiment on ptb section i am skeptical about the way the experiment carried out the reason is that instead of using the given development set to tune the models the authors blindly used an available configuration which is for a different model pros good experimental results cons ideas are quite trivial the experiment on ptb was carried out improperly,4.0
474.json,summary the paper presents an advanced self learning model that extracts compositional rules from bach chorales which extends their previous work in the rule hierarchy in both conceptual and informational dimensions adaptive d memory selection which assumes the features follow dirichlet distribution sonority column of midi numbers acts as word in language model unigram statistics have been used to learn the fundamental rules in music theory while n grams with higher order help characterize part writing sonorities have been clustered together based on feature functions through iterations the partition induced by the features is recognized as a rule if it is sufficiently significant as a result two sample syllabi with different difficulty strides and satisfactory gaps have been generated in terms of sets of learned rules quality a strengths in the paper the exploration of hierarchies in two dimensions makes the learning process more cognitive and interpretable the authors also demonstrate an effective memory selection to speed up the learning b flaws the paper only discussed n which might limit the learning and interpretation capacities of the proposed model failing to capture long distance dependence of music in the replies to questions the authors mentioned they had experimented with max n but i am not sure why related results were not included in the paper besides the elaborated interpretation of results a survey seeking the opinions of students in a music department might make the evaluation of system performance more persuasive clarity a pros the paper clearly delivers an improved automatic theorist system which learns and represents music concepts as well as thoroughly interprets and compares the learned rules with music theory proper analogies and examples help the reader perceive the ideas more easily b cons although detailed definitions can be found in the authors previous mus rover i papers it would be great if they had described the optimization more clearly in figure and related parts the conceptual hierarchy filter row in equations the prime symbol should appear in the subscript originality the representation of music concepts and rules is still an open area the paper investigate the topic in a novel way it illustrates an alternative besides other interpretable feature learning methods such as autoencoders gan etc significance it is good to see some corresponding interpretations for the learned rules from music theory the authors mentioned students in music could and should be involved in the self learning loop to interact which is very interesting i hope their advantages can be combined in the practice of music theory teaching and learning,8.0
618.json,i sincerely apologize for the late review the first part has a strong emphasis on the technical part it could benefit from some high level arguments on what the method aims to achieve what limitation is there to overcome i may have misunderstood the contribution in which case please correct me that the main novel part of the paper is the suggestion to learn the group parameterizations instead of pre fixing them so instead of applying it to common spatial filters as in de brabandere et al it is applied to steerable frames the first contribution suggests that general frame bases are better suited to represent sensory input data than the commonly used pixel basis the experiments on cifar indicate that this is not true in general considering the basis as a hyper parameter expensive search has to be conducted to find that the gauss frame gives better results i assume this does not suggest that the gauss frame is always better at least there is weak evidence on a single network presented maybe the first contribution has to be re stated further is the pixel network representation corrected for the larger number of parameters as someone who is interested in using this what are the runtime considerations i would strongly suggest to improve fig the figure uses w several times in different notations and depictions it mixes boxes single symbols and illustrative figures it took some time to decipher the figure and its flow summary the paper is sufficiently clear technical at many places and readability can be improved e g the introduction of frames in the beginning lacks motivation and is rather unclear to someone new to this concept the work falls in the general category of methods that impose knowledge about filter transformations into the network architecture for me that has always two sides the algorithmic and technical part there are several ways to do this and the practical side should i do it this is a possible approach to this problem but after the paper i was a bit wondering what i have learned i am certainly not inspired based on the content of the paper to integrate or build on this work i am lacking insights into transformational parameters that are relevant for a problem while the spatial transformer network paper was weaker on the technical elegance side it provided exactly this an insight into the feature transformation learned by the algorithm i am missing this here e g from table i learn that among four choices one works empirically better what is destroyed by the x py p and hermite frames that the resnet is not able to recover from you can construct network architectures that are the superset of both so that inferior performance could be avoided the algorithm is clear but it is similar to the dynamic filter networks paper and i am unfortunately not convinced about the usefulness of this particular formulation i would expect a stronger paper with more insights into transformations and comparisons to standard techniques a clear delineation of when this is advised,4.0
403.json,the paper introduces a novel memory mechanism for ntms based on differentiable lie groups this allows to place memory elements as points on a manifold while still allowing training with backpropagation it a more general version of the ntm memory and possibly allows for training a more efficient addressing schemes pros novel and interesting idea for memory access nicely written cons need to manually specify the lie group to use it would be better if network could learn the best way of accessing memory not clear if this really works better than standard ntm compared only to simplified version not clear if this is useful in practice no comparison on real tasks,6.0
546.json,the authors attempt to extract analytical equations governing physical systems from observations an important task being able to capture succinct and interpretable rules which a physical system follows is of great importance however the authors do this with simple and naive tools which will not scale to complex tasks offering no new insights or advances to the field the contribution of the paper and the first four pages of the submission can be summarised in one sentence learn the weights of a small network with cosine sinusoid and input elements products activation functions s t the weights are sparse l the learnt network weights with its fixed structure are then presented as the learnt equation this research uses tools from literature from the s i have not seen the abbreviation ann page for a long time and does not build on modern techniques which have advanced a lot since then i would encourage the authors to review modern literature and continue working on this important task,3.0
792.json,the paper introduced a regularization scheme through soft target that are produced by mixing between the true hard label and the current model prediction very similar method was proposed in section from hinton et al distilling the knowledge in a neural network pros comprehensive analysis on the co label similarity cons weak baselines i am not sure the authors have found the best hyper parameters in their experiments i just trained a layer fully connected mnist model with hidden units without any regularizer and achieved acc using adam and he initialization where the paper reported for such architecture the authors failed to bring the novel idea it is very similar to hinton et al this is probably not enough for iclr,4.0
511.json,i have no familiarity with the hji pde i have only dealt with parabolic pde such as diffusion in the past so the details of transforming this problem into a supervised loss escape me therefore as indicated below my review should be taken as an educated guess i imagine that many readers of iclr will face a similar problem as me and so if this paper is accepted at the least the authors should prepare an appendix that provides an introduction to the hji pde at a high level my comments are it seems that another disadvantage of this approach is that a new network must be trained for each new domain including domain size system function f x or boundary condition if that is correct i wonder if it worth the trouble when existing tools already solve these pde can the authors shed light on a more unifying approach that would require minimal changes to generalize across pde how sensitive is the network result to domains of different sizes it seems only a single size x was tested do errors increase with domain size how general is this approach to pde of other types e g diffusion,5.0
442.json,update raised the score because i think the arguments about adversarial examples are compelling i think that the paper convincingly proves that this method acts as a decent regularizer but i am not convinced that it a competitive regularizer for example i do not believe that there is sufficient evidence that it gives a better regularizer than dropout normalization etc i also think that it will be much harder to tune than these other methods discussed in my rebuttal reply summary if i understand correctly this paper proposes to take the bottleneck term from variational autoencoders which pulls the latent variable towards a noise prior like n and apply it in a supervised learning context where the reconstruction term log p x z is replaced with the usual supervised cross entropy objective the argument is that this is an effective regularizer and increases robustness to adversarial attacks pros the presentation is quite good and the paper is easy to follow the idea is reasonable and the relationship to previous work is well described the robustness to adversarial examples experiment seems convincing though i am not an expert in this area is there any way to compare to an external quantitative baseline on robustness to adversarial examples this would help a lot since i am not sure how the method here compares with other regularizers in terms of combatting adversarial examples for example if one uses a very high dropout rate does this confer a comparable robustness to adversarial examples perhaps at the expense of accuracy cons mnist accuracy results do not seem very strong unless i am missing something the maxout paper from icml listed many permutation invariant mnist results with error rates below so the error rate listed here does not necessarily prove that the method is a competitive regularizer i also suspect that tuning this method to make it work well is harder than other regularizers like dropout there are many distinct architectural choices with this method particularly in how many hidden layers come before and after z for example the output could directly follow z or there could be several layers between z and the output as far as i can tell the paper says that p y z is a simple logistic regression i e one weight matrix followed by softmax but it not obvious why this choice was made did it work best empirically other i wonder what would happen if you trained against the discovered adversarial examples while also using the method from this paper would it learn to have a higher variance p z x when presented with an adversarial example,7.0
442.json,summary the paper deep variational information bottleneck explores the optimization of neural networks for variational approximations of the information bottleneck ib tishby et al on the example of mnist the authors show that this may be used for regularization or to improve robustness against adversarial attacks review the ib is potentially very useful for important applications regularization adversarial robustness and privacy are mentioned in the paper combining the ib with recent advances in deep learning to make it more widely applicable is an excellent idea but given that the theoretical contribution is a fairly straight forward application of well known ideas i would have liked to see a stronger experimental section since the proposed approach allows us to scale ib a better demonstration of this would have been on a larger problem than mnist it is also not clear whether the proposed approach will still work well to regularize more interesting networks with many layers why is dropout not included in the quantitative comparison of robustness to adversarial examples figure how was the number of samples chosen what are the error bars in figure a on page the authors claim the posterior covariance becomes larger as beta decreases increases is this really the case it s hard to judge based on figure since the figures are differently scaled it might be worth comparing to variational fair autoencoders louizos et al which also try to learn representations minimizing the information shared with an aspect of the input the paper is well written and easy to follow,6.0
317.json,the paper presents an amortised map estimation method for sr problems by learning a neural network which learns to project to an affine subspace of sr solutions which are consistent with the lr method the method enables finding propoer solutions with by using a variety of methods gans noise assisted and density assisted optimisation results are nicely demonstrated on several datasets i like the paper all in all though i feel the writing can be polished by quite a bit and presentation should be made clearer it was hard to follow at times and considering the subject matter is quite complicated making it clearer would help also i would love to see some more analysis of the resulting the networks what kind of features to they learn,7.0
747.json,the authors propose to measure feature importance or specifically which pixels contribute most to a network s classification of an image a simple albeit not particularly effective heuristic for measuring feature importance is to measure the gradients of the predicted class wrt each pixel in an input image i this assigns a score to each pixel in i that ranks how much the output prediction would change if a given pixel were to change in this paper the authors build on this and propose to measure feature importance by computing gradients of the output wrt scaled version of the input image alpha i where alpha is a scalar between and then summing across all values of alpha to obtain their feature importance score here the scaling is simply linear scaling of the pixel values alpha is all black image alpha is original image the authors call these scaled images counterfactuals which seems like quite an unnecessarily grandiose name for literally a scaled image the authors show a number of visualizations that indicate that the proposed feature importance score is more reasonable than just looking at gradients only with respect to the original image they also show some quantitative evidence that the pixels highlighted by the proposed measure are more likely to fall on the objects rather than spurious parts of the image in particular see figure the method is also applied to other types of networks the quantitative evidence is quite limited and most of the paper is spent on qualitative results while the goal of understanding deep networks is of key importance it is not clear whether this paper really help elucidate much the main interesting observation in this paper is that scaling an image by a small alpha i e creating a faint image places more importance on pixels on the object related to the correct class prediction beyond that the paper builds a bit on this but no deeper insight is gained the authors propose some hand wavy explanation of why using small alpha faint image may force the network to focus on the object but the argument is not convincing it would have been interesting to try to probe a bit deeper here but that may not be easy ultimately it is not clear how the proposed scheme for feature importance ranking is useful first it is still quite noisy and does not truly help understand what a deep net is doing on a particular image performing a single gradient descent step on an image or on the collection of scaled versions of the image hardly begins to probe the internal workings of a network moreover as the authors admit the scheme makes the assumption that each pixel is independent which is clearly false considering the paper presents a very simple idea it is far too long the main paper is pages up to with references and appendix in general the writing is long winded and overly verbose it detracted substantially from the paper the authors also define unnecessary terminology gradients of coutnerfactuals sounds quite fancy but is not very related to the ideas explored in the writing i would encourage the authors to tighten up the writing and figures down to a more readable page length and to more clearly spell out the ideas explored early on,3.0
481.json,this paper has two main contributions applying adversarial training to imagenet a larger dataset than previously considered comparing different adversarial training approaches focusing importantly on the transferability of different methods the authors also uncover and explain the label leaking effect which is an important contribution this paper is clear well written and does a good job of assessing and comparing adversarial training methods and understanding their relation to one another a wide range of empirical results are shown which helps elucidate the adversarial training procedure this paper makes an important contribution towards understand adversarial training and believe iclr is an appropriate venue for this work,7.0
710.json,this paper applies hdp hmm to challenging bioacoustics segmentation problems including humpback whole sound and bird sound segmentation although the technique itself is not novel the application of this data driven method to bioacoustics segmentation is quite challenging and may yield some scientific findings and this is a valuable contribution to the bioacoustics field my concern for this paper is that it does not have fair comparison of the other simple methods including bic and aic and it is better to provide such comparisons especially as the authors pointed out the computational cost of hdp hmm is a big issue and the other simple methods may solve this issue,5.0
655.json,this work proposes to use basic probability assignment to improve deep transfer learning a particular re weighting scheme inspired by dempster shaffer and exploiting the confusion matrix of the source task is introduced the authors also suggest learning the convolutional filters separately to break non convexity the main problem with this paper is the writing there are many typos and the presentation is not clear for example the way the training set for weak classifiers are constructed remains unclear to me despite the author previous answer i do not buy the explanation about the use of both training and validation sets to compute bpa also i am not convinced non convexity is a problem here and the author does not provide an ablation study to validate the necessity of separately learning the filters one last question is cifar has three channels and mnist only one how it this handled when pairing the datasets in the second set of experiments overall i believe the proposed idea of reweighing is interesting but the work can be globally improved clarified i suggest a reject,3.0
593.json,this paper proposed a variant of the semi supervised vae model which leads to a unified objective for supervised and unsupervised vae this variant gives software implementation of these vae models more flexibility in specifying which variables are supervised and which are not this development introduces a few extra terms compared to the original semi supervised vae formulation proposed by kingma et al from the experiment results it seems that these terms do not do much as the new formulation and the performance difference between the proposed method and kingma et al are not very significant figure therefore the benefit of the new formulation is likely to be just software engineering flexibility and convenience this flexibility and convenience is nice to have but it is better to demonstrate a few situations where the proposed method can be applied while for other previous methods it is non trivial to do the paper title and the way it is written make me expect a lot more than what is currently in the paper i was expecting to see for example structured hidden variable model for the posterior page top or really structured interpretation of the generative model title but i did not see any of these the main contribution of this paper a variant of the semi supervised vae model is quite far from these aside from these the plug in estimation for discrete variables only works when the function h x y is a continuous function of y if however h x y is not continuous in y for example h takes one form when y and another form when y then the approach of using expectation y to replace y will not work therefore the plug in estimation has its limitations,6.0
593.json,this paper introduces a variant of the semi supervised variational auto encoder vae framework the authors present a way of introducing structure observed variables inside the recognition network i find that the presentation of the inference with auxiliary variables could be avoided as it actually makes the presentation unnecessarily complicated specifically the expressions with auxiliary variables are helpful for devising a unified implementation but modeling wise one can get the same model without these auxiliary variables and recover a minimal extension of vae where part of the generating space is actually observed the observed variables mean that the posterior needs to also condition on those so as to incorporate the information they convey the way this is done in this paper is actually not very different from kingma et al and i am surprised that the experiments show a large deviation in these two methods results given the similarity of the models it would be useful if the authors could give a possible explanation on the superiority of their method compared to kingma et al by the way i was wondering if the experimental setup is the same as in kingma et al for the results of fig bottom the authors mention that they use cnns for feature extraction but from the paper it not clear if kingma et al do the same on a related note i was wondering the same for the comparison with jampani et al in particular is that model also using the same rate of supervision for a fair comparison the experiment in section is interesting and demonstrates a useful property of the approach the discussion of the supervision rate and the pre review answer is helpful in giving some insight about what is a successful training protocol to use in semi supervised learning overall the paper is interesting but the title and introduction made me expect something more from it from the title i expected a method for interpreting general deep generative models instead the described approach was about a semi supervised variant of vae naturally including labelled examples disentangles the latent space but this is a general property of any semi supervised probabilistic model and not unique to the approach described here moreover from the intro i expected to see a more general approximation scheme for the variational posterior similar to ranganath et al which trully allows very flexible distributions however this is not the case here given the above the contributions of this paper are in defining a slight variant of the semi supervised vae and perhaps more importantly formulating it in a way that is amendable to easier automation in terms of software but methodologically there is not much contribution to the current literature the authors mention that they plan to extend the framework in the probabilistic programming setting it seems indeed that this would be a very promising and useful extension minor note three of kingma papers are all cited in the main text as kingma et al causing confusion i suggest using kingma et al a etc,5.0
439.json,the paper presents a technique to combine deep learning style input output training with search techniques to match the input of a program to the provided output orders of magnitude speedup over non augmented baselines are presented summary the proposed search for source code implementations based on a rather small domain specific language dsl is compelling but also expected to some degree quality the paper is well written clarity some of the derivations and intuitions could be explained in more detail but the main story is well described originality the suggested idea to speed up search based techniques using neural nets is perfectly plausible significance the experimental setup is restricted to smaller scales but the illustrated improvements are clearly apparent details the employed test set of programs seems rather small in addition the authors ensure that the test set programs are semantically disjoint from the training set programs could the authors provide additional details about the small size of the test set and how to the disjoint property is enforced the length of the programs is rather small at this point in time a more detailed ablation regarding the runtime seems useful the search based procedure is probably still the computationally most expensive part hence the neural net provides some additional prior information rather than tackling the real task,6.0
706.json,this paper proposes a piecewise constant parameterisation for neural variational models so that it could explore the multi modality of the latent variables and develop more powerful neural models the experiments of neural variational document models and variational hierarchical recurrent encoder decoder models show that the introduction of the piecewise constant distribution helps achieve better perplexity on modelling documents and seemly better performance on modelling dialogues the idea of having a piecewise constant prior for latent variables is interesting but the paper is not well written even pages long and the design of the experiments fails to demonstrate the most of the claims the detailed comments are as follows the author explains the limitations of the vaes with standard gaussian prior in the last paragraph of and the last paragraph of hence a multimodal prior would help the vaes overcome the issues of optimisation however there is a lack of evidence showing the multimodality of the prior helps break the bottleneck in the last paragraph of the author claimed the decoder parameter matrix is directly affected by the latent variables but what the connects the decoder is a combination of a piecewise constant and gaussian latent variables no matter what is discovered in the experiments it only shows z,4.0
643.json,the paper is interesting it relates findings from neurscience and biology to a method for sparse coding that is adaptive and able to automatically generate or even delete codes as new data is coming from a nonstationary distribution i have a few points to make the algorithm could be discussed more to give a more solid view of the contribution the technique is not novel in spirit codes are added when they are needed and removed when they dont do much is there a way to relate the organization of the data to the behavior of this method in this paper buildings are shown first and natural images which are less structured more difficult later is this just a way to perform curriculum learning what happens when data simply changes in structure with no apparent movement from simple to more complex e g from flowers to birds to fish to leaves to trees etc in a way it makes sense to see an improvement when the training data has such a structure by going from something artificial and simpler to a more complex less structured domain the paper is interesting the idea useful with some interesting insights i am not sure it is ready for publication yet,5.0
375.json,this is a good paper with an interesting probabilistic motivation for weighted bag of words models the hopefully soon added comparison to wang and manning will make it stronger though it is sad that for sufficiently large datasets nb svm still works better in the second to last paragraph of the introduction you describe a problem of large cooccurrence counts which was already fixed by the glove embeddings with their weighting function f minor comments the capturing the similarities typo in line of intro recently wieting et al learned use citet instead of parenthesized citation,7.0
621.json,this paper proposed coconet which is a neural autoregressive model with convolution to do music composition task this paper also proposed to use blocked gibbs sampling instead of the ancestral sampling of the original nade model to generate better pieces of music the experimental results showed that the nll of coconet is better than the other baselines and the human evaluation task by amazon s mechanical turk illustrated that the model can generate compelling music in general i think the paper is good using nade based model with convolution operations on music generation tasks and using blocked gibbs sampling contains some kind of novelty however the novelty of the paper is incremental since the blocked gibbs sampling for nade model is already proposed by yao et al and the using nade based model for music modeling has also been proposed by boulanger lewandowski et al,6.0
564.json,i think the backbone of the paper is interesting and could lead to something potentially quite useful i like the idea of connecting signal processing with recurrent network and then using tools from one setting in the other however while the work has nuggets of very interesting observations i feel they can be put together in a better way i think the writeup and everything can be improved and i urge the authors to strive for this if the paper does not go through i think some of the ideas of how to connect to the past are interesting it would be nice to have more experiments or to try to understand better why this connections help and how,6.0
657.json,this paper describes how to approximate the fasttext approach such that its memory footprint is reduced by several orders of magnitude while preserving its classification accuracy the original fasttext approach was based on a linear classifier on top of bag of words embeddings this type of method is extremely fast to train and test but the model size can be quite large this paper focuses on approximating the original approach with lossy compression techniques namely the embeddings and classifier matrices a and b are compressed with product quantization and an aggressive dictionary pruning is carried out experiments on various datasets either with small or large number of classes are conducted to tune the parameters and demonstrate the effectiveness of the approach with a negligible loss in classification accuracy an important reduction in term of model size memory footprint can be achieved in the order of folds compared to the original size the paper is well written overall the goal is clearly defined and well carried out as well as the experiments different options for compressing the model data are evaluated and compared e g pq vs lsh which is also interesting nevertheless the paper does not propose by itself any novel idea for text classification it just focuses on adapting existing lossy compression techniques which is not necessarily a problem specifically it introduces a straightforward variant of pq for unnormalized vectors dictionary pruning is cast as a set covering problem which is np hard but a greedy approach is shown to yield excellent results nonetheless hashing tricks and bloom filter are simply borrowed from previous papers these techniques are quite generic and could as well be used in other works here are some minor problems with the paper it is not made clear how the full model size is computed what is exactly in the model which proportion of the full size do the a and b matrices the dictionary and the rest account for it is hard to follow where is the size bottleneck which also seems to depend on the target application i e small or large number of test classes it would have been nice to provide a formula to calculate the total model size as a function of all parameters k b for pq and k for dictionary number of classes some parts lack clarity for instance the greedy approach to prune the dictionary is exposed in less than lines top of page though it is far from being straightforward likewise it is not clear why the binary search used for the hashing trick would introduce an overhead of a few hundreds of kb overall this looks like a solid work but with potentially limited impact research wise,6.0
728.json,the paper starts by pointing out the need for methods that perform both state and temporal representation learning for rl and which allow gaining insight into what is being learned perhaps in order to allow a human operator to intervene if necessary this is a very important goal from a practical point of view and it is great to see research in this direction for this reason i would like to encourage the authors to pursue this further however i am not at all convinced that the current incarnation of this work is the right answer part of the issues are more related to presentation part may require rethinking in order to get the interpretability the authors opt for some fairly specific ways of performing abstraction for example their skills always start in a single skill initiation state and likewise end in one state this seems unnecessarily restrictive and it is not clear why this restriction is needed other than convenience similarly clustering is the basis for forming the higher level states and there is a specific kind of clustering used here again it is not clear why this has to be done via clustering as opposed to other methods ensuring temporal coherence in the particular form employed also seems restrictive there is a reference to supplementary material where some of these choices are explained but i could not find this in the posted version of the paper the authors should either explain clearly why these specific choices are necessary or even better try to think if they can be relaxed while still keeping interpretability from a presentation point of view the paper would benefit from formal definitions of amdp and samdp as well as from formal descriptions of the algorithms employed in constructing these representations eg bellman equations for the models and update rules for the algorithms learning them while intuitions are given the math is not precisely stated the overhead of constructing an samdp computation time and space should be clarified as well the experiments are well carried out and it is nice to have both gridworld experiments where visualization are easy to perform and understand as well as atari games gridworld still have their place despite what other reviewers might say the results are positive but because the proposed approach has many moving parts which rely on specific choices significance and general ease of use are unclear at this point perhaps having the complete supplementary would have helped in this respect small comment the two lines after eq contain typos in the notation and a wrong sign in the equation,4.0
682.json,based on previous work such as the stepped sigmoid units and relu hidden units for discriminatively trained supervised models a leaky relu model is proposed for generative learning pro what is interesting is that unlike the traditional way of first defining an energy function and then deriving the conditional distributions this paper propose the forms of the conditional first and then derive the energy function however this general formulation is not novel to this paper but was generalized to exponential family glms earlier con because of the focus on specifying the conditionals the joint pdf and the marginal p v becomes complicated and hard to compute on the experiments it would been nice to see a rbm with binary visbles and leaky relu for hiddens this would demonstrate the superiority of the leaky relu hidden units in addition there are more results on binary mnist modeling with which the authors can compare the results to while the authors is correct that the annealing distribution is no longer gaussian perhaps cd or faast pcd experiments can be run to compare agains the baseline rbm trained using fast pcd this paper is interesting as it combines new hidden function with the easiness of annealed ais sampling however the baseline comparisons to stepped sigmoid units nair hinton or other models like the spike and slab rbms and others are missing without those comparisons it is hard to tell whether leaky relu rbms are better even in continuous visible domain,5.0
682.json,this paper proposed a new variant of rbm which has a nonlinearity of leaky relu in contrast to the sigmoid function nonlinearity in rbm by gradually annealing the leakiness coefficient corresponding to from gaussian to non gaussian model the authors can sample from their model with a higher mixing rate with the same idea annealing leakiness they show they can estimate the partition function of the new model more accurately main comments the proposed model can only account for real valued data however rbm is primarily used to model binary data real valued rbm gaussian rbm is not a well recognized model for real valued data so to demonstrate the superiority of the model the author should also include the comparison with binary data and it is also not enough to only compare two datasets for a newly proposed model the claim that the marginal distribution of visible variables is truncated gaussian is incorrect for a truncated normal the values of variables are constrained to be within some region e g requiring variable v from the region a,5.0
378.json,this paper proposes a novel exploration strategy that promotes exploration of under appreciated reward regions proposed importance sampling based approach is a simple modification to reinforce and experiments in several algorithmic toy tasks show that the proposed model is performing better than reinforce and q learning this paper shows promising results in automated algorithm discovery using reinforcement learning however it is not very clear what is the main motivation of the paper is the main motivation better exploration for policy gradient methods if so authors should have benchmarked their algorithm with standard reinforcement learning tasks while there is a huge body of literature on improving reinforce authors have considered a simple version of reinforce on a non standard task and say that urex is better if the main motivation is improving the performance in algorithm learning tasks then the baselines are still weak authors should make it clear which is the main motivation also the action space is too small in the beginning authors raise the concern that entropy regularization might not scale to larger action spaces so a comparison of ment and urex in a large action space problem would give more insights on whether urex is not affected by large action space after rebuttal i missed the action sequences argument when i pointed about small action space issue for question regarding weak baseline there are several tricks used in the literature to tackle high variance issue for reinforce for example see mnih gregor i have increased my rating from to i still encourage the authors to improve their baseline,7.0
689.json,this paper proposes a generative model for mixtures of basic local structures where the dependency between local structures is a tensor they use tensor decomposition and the result of their earlier paper on expressive power of cnns along with hierarchical tucker to provide an inference mechanism however this is conditioned on the existence of decomposition the authors do not discuss how applicable their method is for a general case what is the subspace where this decomposition exists is efficient has low approximation error their answer to this question is that in deep learning era these theoretical analysis is not needed while this claim is subjective i need to emphasize that the paper does not clarify this claim and does not mention the restrictions hence from theoretical perspective the paper has flaws and the claims are not justified completely some claims cannot be justified with the current results in tensor literature as the authors also mentioned in the discussions therefore they should have corrected their claims in the paper and made the clarifications that this approach is restricted to a clear subclass of tensors if we ignore the theoretical aspect and only consider the paper from empirical perspective the experiments the appear in the paper are not enough to accept the paper mnist and cifar are very simple baselines and more extensive experiments are required also the experiments for missing data are not covering real cases and are too synthetic also the paper lacks the extension beyond images since the authors repeatedly mention that their approach goes beyond images and since the theory part is not complete those experiments are essential for acceptance of this paper,4.0
328.json,this paper fits models to spike trains of retinal ganglion cells that are driven by natural images i think the title should thus include the word activity at the end for otherwise it is actually formally incorrect anyhow this paper proposes more specifically a recurrent network for this time series prediction and compares it to what seems to be the previous approach of a generalized linear model overall the stated paradigm is that when one can predict the spikes well then one can look into the model and learn how nature does it in general the paper sounds plausible though i am not convinced that i learned a lot the results in figure show that the rnn model can predict the spikes a bit better so this is nice but now what you have shown that a more complicated model can produce better fits to the data though there are of course still some variations to the real data your initial outline was that a better predictive model helps you to better understand the neural processing in the retina so tell us what you learned i am not a specialist of the retina but i know that there are several layers and recurrencies in the retina so i am not so surprised that the new model is better than the glm it seems that more complicated recurrent models such as lstm do not improve the performance according to a statement in the paper however comparisons on this level are also difficult as a more complex models needs more data hence i would actually expect that more layers and even a more detailed model of the retina could eventually improve the prediction even further i was also a bit puzzled that all the neurons in the network share all the same parameters weights while the results show that these simplified models can capture a lot of the spike train characteristics couldn t a model with free parameters eventually outperform this one with correspondingly more training data,4.0
684.json,this paper proposes a model based reinforcement learning approach focusing on predicting future rewards given a current state and future actions this is achieved with a residual recurrent neural network that outputs the expected reward increase at various time steps in the future to demonstrate the usefulness of this approach experiments are conducted on atari games with a simple playing strategy that consists in evaluating random sequences of moves and picking the one with highest expected reward and low enough chance of dying interestingly out of the games tested one of them exhibits better performance when the agent is trained in a multitask setting i e learning all games simultaneously hinting that transfer learning is occurring this submission is easy enough to read and the reward prediction architecture looks like an original and sound idea there are however several points that i believe prevent this work from reaching the iclr bar as detailed below the first issue is the discrepancy between the algorithm proposed in section vs its actual implementation in section experiments in section the output is supposed to be the expected accumulated reward in future time steps as a single scalar while in experiments it is instead two numbers one which is the probability of dying and another one which is the probability of having a higher score without dying this might work better but it also means the idea as presented in the main body of the paper is not actually evaluated and i guess it would not work well as otherwise why implement it differently in addition the experimental results are quite limited only on games that were hand picked to be easy enough and no comparison to other rl techniques dqn friends i realize that the main focus of the paper is not about exhibiting state of the art results since the policy being used is only a simple heuristic to show that the model predictions can ne used to drive decisions that being said i think experiments should have tried to demonstrate how to use this model to obtain better reinforcement learning algorithms there is actually no reinforcement learning done here since the model is a supervised algorithm used in a manually defined hardcoded policy another question that could have been addressed but was not in the experiments is how good these predictions are e g classification error on dying probability mse on future rewards compared to simpler baselines finally the paper previous work section is too limited focusing only on dqn and in particular saying very little on the topic of model based rl i think a paper like for instance action conditional video prediction using deep networks in atari games should have been an obvious must cite minor comments notations are unusual with a denoting a state rather than an action this is potentially confusing and i see no reason to stray away from standard rl notations using a dot for tensor concatenation is not a great choice either since the dot usually indicates a dot product the ri in is a residual that has nothing to do with ri the reward ci is defined as the control that was performed at time i but instead it seems to be the control performed at time i there is a recurrent confusion between mean and median in x should not be used in observation since the x from fig does not go through layer normalization the inequality in observation should be about xi not xi observation with its proof takes too much space for such a simple result in the first rj should be ri the probability of dying comes out of nowhere in since we do not know yet it will be an output of the model our approach is not able to learn from good strategies did you mean only from good strategies please say that in fig fc means fully connected it would be nice also to say how the architecture of fig differs from the classical dqn architecture from mnih et al please clarify rj as per your answer in openreview comments table says after one iteration but has prl iteration in it which is confusing figure shows that not only there is no degradation in pong and demon attack to me it seems to be a bit worse actually a model that has learned only from random play is able to play at least times better not clear where this comes from demon attack plot in figure c shows a potential problem we mentioned earlier where was it mentioned,4.0
734.json,update i have read the replies on this thread my opinion has not changed the authors propose deep vcca a deep version of the probabilistic cca model by using likelihoods parameterized by nonlinear functions neural nets variational inference is applied with an inference network and reparameterization gradients an additional variant termed vcca private is also introduced which includes local latent variables for each data point view a connection to the multi view auto encoder is also shown since the development of black box variational inference and variational auto encoders the methodology in model specific papers like this one are arguably not very interesting the model is a straightforward extension of probabilistic cca with neural net parameterized likelihoods inference is mechanically the same as any black box approach using the reparameterization gradient and inference networks the approach also uses a mean field approximation which is quite old given the many recent developments in more expressive approximations see e g rezende and mohamed tran et al the connection to multi view auto encoders is at first insightful but no more than the difference between map and variational inference this is a well known insight in the abstract the authors argue that the key distinction is the additional sampling but ultimately what matters is the kl regularizer even with noisy samples the variances of a normal variational approximation would collapse to zero and thus become a point mass approximation equivalent to optimizing a point estimate from the mvae objective i suspect the authors know this to some degree due to their remarks in the paper but it is unclear that said i think the paper has strong merits in application the experiments are strong comparing to alternative multi view approaches under a number of interesting data sets while the use of private variables is simple they demonstrate how it can successfully disentangle the per view latent representation from the shared view it would have been preferable to compare to methods using probabilistic inference such as full bayes for the linear cca there are also a number of approximations taken to almost be standard in the paper which may not be necessary such as the use of a mean field family or the use of an inference network to separate out how much the approximate inference is influencing the fit of the model i strongly recommend using mcmc and non amortized variational inference on at least one experiment rezende d j mohamed s variational inference with normalizing flows presented at the international conference on machine learning tran d ranganath r blei d m the variational gaussian process presented at the international conference on learning representations,5.0
775.json,the paper is straightforward easy to read and has clear results since all these parameterisations end up outputting torques it seems like there should not be much difference between them there is a known function that convert from one representation to another or at least to torques is it not possible that the only reason proportional control is a little better is that the tracking cost is a function of positions would we get the same result if there was no reference pose cost only a locomotion cost would we get the same result if the task was to spin a top my guess is no this work is interesting but not likely to generalise to other scenarios and in that sense is rather limited the video is nice,6.0
775.json,paper studies deep reinforcement learning paradigm for controlling high dimensional characters experiments compare the effect different control parameterizations torques muscle activations pd control with target joint positions and target joint velocities have on the performance of reinforcement learning and optimized control policies evaluated are different planer gate cycle trajectories it is illustrated that more abstract parameterizations are in fact better and result in more robust and higher quality policies significance originality the explored parameterizations are relatively standard in humanoid control the real novelty is systematic evaluation of the various parameterizations i think this type of study is important and insightful however the findings are very specific to the problem and specific tested architecture its not clear that findings will transferable to other networks on other control problems domains as such for the iclr community this may have limited breadth and perhaps would have broader appeal in robotics graphics community clarity the paper is well written and is pretty easy to understand for someone who has some background with constrained multi body simulation and control experiments experimental validation is lacking somewhat in my opinion given that this is a fundamentally experimental paper i would have liked to see more analysis of sensitivity to various parameters and analysis of variance of performance when policy is optimized multiple times,6.0
325.json,this paper trains a generative model which transforms noise into model samples by a gradual denoising process it is similar to a generative model based on diffusion unlike the diffusion approach it uses only a small number of denoising steps and is thus far more computationally efficient rather than consisting of a reverse trajectory the conditional chain for the approximate posterior jumps to q z x and then runs in the same direction as the generative model this allows the inference chain to behave like a perturbation around the generative model that pulls it towards the data this also seems somewhat related to ladder networks there is no tractable variational bound on the log likelihood i liked the idea and found the visual sample quality given a short chain impressive the inpainting results were particularly nice since one shot inpainting is not possible under most generative modeling frameworks it would be much more convincing to have a log likelihood comparison that does not depend on parzen likelihoods detailed comments follow sec theta the theta be the theta t the theta t be the what we will be using which we will be doing i like that you infer q z x and then run inference in the same order as the generative chain this reminds me slightly of ladder networks q having learned q paragraph break having learned sec learn to inverse learn to reverse sec for each experiments for each experiment how sensitive are your results to infusion rate sec appears to provide more accurate models i do not think you showed this there no direct comparison to the sohl dickstein paper fig neat,8.0
598.json,there have been numerous works on learning from raw waveforms and training letter based ctc networks for speech recognition however there are very few works on combining both of them with purely convnet as it is done in this paper it is interesting to see results on a large scale corpus such as librispeech that is used in this paper though some baseline results from hybrid nn hmm systems should be provided to readers it is unclear how this system is close to state of the art only from table the key contribution of this paper may be the end to end sequence training criterion for their ctc variant where the blank symbol is dropped which may be viewed as sequence training of ctc as h sak et al learning acoustic frame labeling for speech recognition with recurrent neural networks however instead of generating the denominator lattices using a frame level trained ctc model first this paper directly compute the sequence level loss by considering all the competing hypothesis in the normalizer therefore the model is trained end to end from this perspective it is closely related to d povey lf mmi for sequence training of hmms as another reviewer has pointed out references and discussions on that should be provided this approach should be more expensive than frame level training of ctcs however from table the authors implementation is much faster did the systems there use the same sampling rate you said at the end of that the step size for your model is ms is it also the same for baidu ctc system also have you tried increasing the step size e g to ms or ms as people have found that it may work equally better while significantly cut down the computational cost,6.0
598.json,this submission proposes a letter level decoder with a variation of the ctc approach they call asg where the blank symbol is dropped and replaced by letter repetition symbols and where explicit normalization is dropped both the description of a letter level model though not novel as well as the ctc variant are interesting the approach is evaluated on the librispeech task the authors claim that their approach is competitive they compare their modelling variant asg to ctc but a comparison of the letter level approach to available word level results are missing compared to the results obtained in panayotov et al the performance obtained here seems only comparable to word level gmm hmm models but worse than word level hybrid dnn hmm models though panayotov et al also appled speaker adaptation which was not done as far as i can see i suggest to add a comparison to panyotov results in addition to mentioning baidu results on librispeech which are not comparable due to much larger amounts of training data to allow readers to get a quantitative idea as pointed out by the authors in the text baidu gpu implementation for ctc is more aimed at larger vocabularies therefore the comparison to gpu in tables a c do not seem to be helpful for this work without further discussing the implementations you are using quite a huge analysis window nearly s even though other authors also use windows up to s to s e g mrasta features some comments on how you arrive at such a large window and what advantages you observe for it would be interesting the submission is well written though more details on the experiences with using non normalized transition scores and beam pruning would be desirable table would be better readable if the units of the numbers shown in a b c would be shown within the tables and not only in the caption prior partial publications of this work your nips end to end workshop paper should clearly be mentioned referenced what do you mean by transition scalars i do not repeat further comments here which were already given in the pre review period minor comments sec end of nd sentence train properly the model train the model properly end of same paragraph boostrap bootstrap such errors should be avoided by performing an automatic spell check sec bayse bayes definition of logadd is wrong see comment applies also for your nips end to end workshop paper line before eq all possible sequence of letters all possible sequences of letters plural sec first line threholding thresholding spell check figure mention the corpus used here dev,7.0
448.json,this paper presents a mathematical analysis of how information is propagated through deep feed forward neural networks with novel analysis addressing the problem of vanishing and exploding gradients in the backward pass of backpropagation and the use of the dropout algorithm the paper is clear and well written the analysis is thorough and the experimental results showing agreement with the model are very nice,8.0
673.json,i find this paper not very compelling the basic idea seems to be that we can put a fast neighbor searcher into a memory augmented net to make the memory lookups scalable however this was precisely the point of rae et al there are a number of standardized neighbor searchers i do not understand why the authors choose to use their own which they do not benchmark against the standards moreover they test on a problem where there is no clear need for vector based fast nn because one can use hashing on the text i also find the repeated distinction between mips and nns distracting most libraries that can do one can do the other or inputs can be modified to switch between the problems indeed the authors do this when they convert to the mcss problem,4.0
366.json,this paper proposes the use of neural variational inference method for topic models the paper shows a nice trick to approximate dirichlet prior using softmax basis with a gaussian and then the model is trained to maximize the variational lower bound also the authors study a better way to alleviate the component collapsing issue which has been problematic for continuous latent variables that follow gaussian distribution the results look promising and the experimental protocol sounds fine minor comments please add citation to or for neural variational inference and for vae a typo in this approximation to the dirichlet prior p θ α is results in the distribution it should be this approximation to the dirichlet prior p θ α results in the distribution in table it is written that dmfvi was trained more than hrs but failed to deliver any result but why not wait until the end and report the numbers in table why are the perplexities of lda collapsed gibbs and nvdm are lower while the proposed models prodlda generates more coherent topics what is your intuition on this how does the training speed until the convergence differs by using different learning rate and momentum scheduling approaches shown as in figure it may be also interesting to add some more analysis on the latent variables z component collapsing and etc although your results indirectly show that the learning rate and momentum scheduling trick removes this issue overall the paper clearly proposes its main idea explain why it is good to use nvi and its experimental results support the original claim it explains well what are the challenges and demonstrate their solutions minh et al neural variational inference and learning in belief networks icml rezende et al stochastic backpropagation and approximate inference in deep generative models icml,7.0
366.json,this is an interesting paper on a vae framework for topic models the main idea is to train a recognition model for the inference phase which because of so called amortized inference can be much faster than normal inference where inference must be run iteratively for every document some comments eqn i find the notation p theta h alpha awkward why not p h alpha the generative model seems agnostic to document length meaning that the latent variables only generate probabilities over word space however the recognition model is happy to radically change the probabilities q z x if the document length changes because the input to q changes this seems undesirable maybe they should normalize the input to the recognition network the prodlda model might well be equivalent to exponential family pca or some variant thereof,6.0
537.json,this paper addresses the problem of decoding barcode like markers depicted in an image the main insight is to train a cnn from generated data produced from a gan the gan is trained using unlabeled images and leverages a d model that undergoes learnt image transformations e g blur lighting background the parameters for the image transformations are trained such that it confuses a gan discriminator a cnn is trained using images generated from the gan and compared with hand crafted features and from training with real images the proposed method out performs both baselines on decoding the barcode markers the proposed gan architecture could potentially be interesting however i won t champion the paper as the evaluation could be improved a critical missing baseline is a comparison against a generic gan without this it s hard to judge the benefit of the more structured gan also it would be worth seeing the result when one combines generated and real images for the final task a couple of references that are relevant to this work for object detection using rendered views of d shapes a xingchao peng baochen sun karim ali kate saenko learning deep object detectors from d models iccv b deep exemplar d d detection by adapting from real to rendered views francisco massa bryan c russell mathieu aubry cvpr the problem domain decoding barcode markers on bees is limited it would be great to see this applied to another problem domain e g object detection from d models as shown in paper reference a where direct comparison against prior work could be performed i found the writing to be somewhat vague throughout for instance on first reading of the introduction it is not clear what exactly is the contribution of the paper minor comments fig are these really renders from a d model the images look like d images perhaps spatially warped via a homography page chapter section in table what is the loss used for the dcnn fig a the last four images look like they have strange artifacts can you explain these,5.0
496.json,this paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data e g character word their approach does not require boundary information to segment the sequence in meaningful groups like in chung et al their model is organized as a set of layers that aim at capturing the information form different level of abstraction the lowest level activate the upper one and decide when to update it based on a controller or state cell called c a key feature of their model is that c is a discrete variable allowing potentially fast inference time however this makes their model more challenging to learn leading to the use of the straight through estimator by hinton the experiment section is thorough and their model obtain competitive performance on several challenging tasks the qualitative results show also that their model can capture natural boundaries overall this paper presents a strong and novel model with promising experimental results on a minor note i have few remarks complaints about the writing and the related work in the introduction one of the key principles of learning in deep neural networks as well as in the human brain please provide evidence for the human brain part of this claim for modelling temporal data the recent resurgence of recurrent neural networks rnn has led to remarkable advances i believe you re missing mikolov et al in the references in spite of the fact that hierarchical multiscale structures naturally exist in many temporal data missing reference to lin et al in the related work a more recent model the clockwork rnn cw rnn koutník et al extends the hierarchicalrnn el hihi bengio it extends the narx model of lin et al not the el hihi bengio while the above models focus on online prediction problems where a prediction needs to be made i believe there is a lot of missing references in particular to socher s work or older recursive networks the norm of the gradient is clipped with a threshold of pascanu et al this is not the first work using gradient clipping i believe it was introduced in mikolov et al missing references recurrent neural network based language model mikolov et al learning long term dependencies in narx recurrent neural networks lin et al sequence labelling in structured domains with hierarchical recurrent neural networks fernandez et al learning sequential tasks by incrementally adding higher orders ring,8.0
316.json,this paper discusses how to guarantee privacy for training data in the proposed approach multiple models trained with disjoint datasets are used as teachers model which will train a student model to predict an output chosen by noisy voting among all of the teachers the theoretical results are nice but also intuitive since teachers result are provided via noisy voting the student model may not duplicate the teacher behavior however the probabilistic bound has quite a number of empirical parameters which makes me difficult to decide whether the security is guaranteed or not the experiments on mnist and svhn are good however as the paper claims the proposed approach may be mostly useful for sensitive data like medical histories it will be nice to conduct one or two experiments on such applications,7.0
419.json,this paper presents topicrnn a combination of lda and rnn that augments traditional rnn with latent topics by having a switching variable that includes excludes additive effects from latent topics when generating a word experiments on two tasks are performed language modeling on ptb and sentiment analysis on imbd the authors show that topicrnn outperforms vanilla rnn on ptb and achieves sota result on imdb some questions and comments in table how do you use lda features for rnn rnn lda features i would like to see results from lstm included here even though it is lower perplexity than topicrnn i think it still useful to see how much adding latent topics close the gap between rnn and lstm the generated text in table are not meaningful to me what is this supposed to highlight is this generated text for topic trading what about the imdb one how scalable is the proposed method for large vocabulary size k what is the accuracy on imdb if the extracted features is used directly to perform classification instead of being passed to a neural network with one hidden state i think this is a fairer comparison to bow lda and svm methods presented as baselines,7.0
462.json,this paper proposes a new idea to help defending adversarial examples by training a complementary classifier to detect them the results of the paper show that adversarial examples in fact can be easily detected moreover such detector generalizes well to other similar or weaker adversarial examples the idea of this paper is simple but non trivial while no final scheme is proposed in the paper how this idea can help in building defensive systems it actually provides a potential new direction based on its novelty i suggest an acceptance my main concern of this paper is about its completeness no effective method is reported in the paper to defend the dynamic adversaries it could be difficult to do so but rather the paper doesn t seem to put much effort to investigate this part how difficult it is to defend the dynamic adversaries is an important and interesting question following the conclusions of this paper such investigation may essentially help improve our understanding of adversarial examples that being said the novelty of this paper is still significant minor comment the paper needs to improve its clarity some important details are skipped in the paper for example the paper should provide more details about the dynamic adversaries and the dynamic adversary training method,7.0
383.json,the paper looks solid and the idea is natural results seem promising as well i am mostly concerned about the computational cost of the method days on gpus for relatively tiny datasets is quite prohibitive for most applications i would ever encounter i think the main question is how this approach scales to larger images and also when applied to more exotic and possibly tiny datasets can you run an experiment on caltech for instance i would be very curious to see if your approach is suitable for the low data regime and areas where we all do not know right away how a suitable architecture looks like for cifar mnist and svhn everyone knows very well what a reasonable model initialization looks like if you show proof that you can discover a competitive architecture for something like caltech i would recommend the paper for publication minor resnets should be mentioned in table,6.0
356.json,this paper sets out to tackle the program synthesis problem given a set of input output pairs discover the program that generated them the authors propose a bipartite model with one component that is a generative model of tree structured programs and the other component an input output pair encoder for conditioning they consider applying many variants of this basic model to a flashfill dsl the experiments explore a practical dataset and achieve fine numbers the range of models considered carefulness of the exposition and basic experimental setup make this a valuable paper for an important area of research i have a few questions which i think would strengthen the paper but think it worth accepting as is questions comments the dataset is a good choice because it is simple and easy to understand what is the effect of the rule based strategy for computing well formed input strings clarify what backtracking search is i assume it is the same as trying to generate the latent function in general describing the accuracy as you increase the sample size could be summarize simply by reporting the log probability of the latent function perhaps it worth reporting that not sure if i missed something,7.0
356.json,this paper proposes a model that is able to infer a program from input output example pairs focusing on a restricted domain specific language that captures a fairly wide variety of string transformations similar to that used by flash fill in excel the approach is to model successive extensions of a program tree conditioned on some embedding of the input output pairs extension probabilities are computed as a function of leaf and production rule embeddings one of the main contributions is the so called recursive reverse recursive neural net which computes a globally aware embedding of a leaf by doing something that looks like belief propagation on a tree but training this operation in an end to end differentiable way there are many strong points about this paper in contrast with some of the related work in the deep learning community i can imagine this being used in an actual application in the near future the rnn idea is a good one and the authors motivate it quite well moreover the authors have explored many variants of this model to understand what works well and what does not finally the exposition is clear even if it is a long paper which made this paper a pleasure to read some weaknesses of this paper the results are still not super accurate perhaps because the model has only been trained on small programs but is being asked to infer programs that should be much longer and it s unclear why the authors did not simply train on longer programs it also seems that the number of i o pairs is fixed so if i had more i o pairs the model might not be able to use those additional pairs and based on the experiments more pairs can hurt overall however i would certainly like to see this paper accepted at iclr other miscellaneous comments too many e s in the expansion probability expression might be better just to write softmax there is a comment about adding a bidirectional lstm to process the global leaf representations before calculating scores but no details are given on how this is done as far as i can see the authors claim that using hyperbolic tangent activation functions is important i d be interested in some more discussion on this and why something like relu would not be good it s unclear to me how batching was done in this setting since each program has a different tree topology more discussion on this would be appreciated related to this it would be good to add details on optimization algorithm sgd adagrad adam learning rate schedules and how weights were initialized at the moment the results are not particularly reproducible in figure unsolved benchmarks it would be great to add the program sizes for these harder examples i e did the approach fail because these benchmarks require long programs or was it some other reason there is a missing related work by piech et al learning program embeddings where the authors trained a recursive neural network that matched abstract syntax trees for programs submitted to an online course to predict program output but did not synthesize programs,8.0
478.json,pros new representation with nice properties that are derived and compared with a mathematical baseline and background a simple algorithm to obtain the representation cons the paper sounds like an applied maths paper but further analysis on the nature of the representation could be done for instance by understanding the nature of each layer or at least the first,8.0
400.json,this paper proposes a variant of a recurrent neural network that has two orthogonal temporal dimensions that can be used as a decoder to generate tree structures including the topology in an encoder decoder setting the architecture is well motivated and i can see several applications in addition to what is presented in the paper that need to generate tree structures given an unstructured data one weakness of the paper is the limitation of experiments ifttt dataset seems to be an interesting appropriate application and there is also a synthetic dataset however it would be more interesting to see more natural language applications with syntactic tree structures still i consider the experiments sufficient as a first step to showcase a novel architecture a strength is that the authors experiment with different design decisions when building the topology predictor components of the architecture about when how to decide to terminate as opposed to making a single arbitrary choice i see future applications of this architecture and it seems to have interesting directions for future work so i suggest its acceptance as a conference contribution,6.0
400.json,authors response well answered my questions thanks evaluation not changed this paper proposes a neural model for generating tree structure output from scratch the model does separate the recurrence between depths and siblings separate the topology and label generation and outperforms previous methods on a benchmark ifttt dataset compared to previous tree decoding methods the model avoids manually annotating subtrees with special tokens and thus is a very good alternative to such problems the paper does solid experiments on one synthetic dataset and outperforms alternative methods on one real world ifttt dataset there are couple of interesting results in the paper that i believe is worth further investigation firstly on the synthetic dataset the precision drops rapidly with the number of nodes is it because that the vector representation of the sequential encoder fails to provide sufficient information of long sequences such that the tree decoder can not do a good job or is it because that such tree decoder is not tolerant to the long sequence input i e large tree structure i believe that it is important to understand this before a better model can be developed for example if it is the fault of encoder maybe an attention layer can be added as in a seq to seq model to preserve more information of the input sequence moreover besides only showing how the precision changes with the number of nodes in the tree it might be interesting to investigate how it goes with number of depths number of widths symmetricity etc moreover as greedy search is used in decoding it might be interesting to see how it helps if it does to use beam search in tree decoding on the ifttt dataset listing more statistics about this dataset might be helpful for better understanding the difficulty of this task how deep are the trees how large are the vocabularies on both language and program sides the paper is well written except for minor typo as mentioned in my pre review questions in general i believe this is a solid paper and more can be explored in this direction so i tend to accept it,7.0
482.json,this is a well written organized and presented paper that i enjoyed reading i commend the authors on their attention to the narrative and the explanations while it did not present any new methodology or architecture it instead addressed an important application of predicting the medications a patient is using given the record of billing codes the dataset they use is impressive and useful and frankly more interesting than the typical toy datasets in machine learning that said the investigation of those results was not as deep as i thought it should have been in an empirical applications paper despite their focus on the application i was encouraged to see the authors use cutting edge choices eg keras adadelta etc in their architecture a few points of criticism the numerical results are in my view too brief fig is anecdotal fig is essentially a negative result tsne is only in some places interpretable so that leaves table i recognize there is only one dataset but this does not offer a vast amount of empirical evidence and analysis that one might expect out of a paper with no major algorithmic theoretical advances to be clear i do not think this is disqualifying or deeply concerning i simply found it a bit underwhelming to be constructive re the results i would recommend removing fig and replacing that with some more meaningful analysis of performance i found fig to be mostly uninformative other than as a negative result which i think can be stated in a sentence rather than in a large figure there is a bit of jargon used and expertise required that may not be familiar to the typical iclr reader i saw that another reviewer suggested perhaps iclr is not the right venue for this work while i certainly see the reviewer point that a medical or healthcare venue may be more suitable i do want to cast my vote of keeping this paper here our community benefits from more thoughtful and in depth applications instead i think this can be addressed by tightening up those points of jargon and making the results more easy to evaluate by an iclr reader that is as it stands now researchers without medical experience have to take your results after table on faith rather than getting to apply their well trained quantitative eye overall a nice paper,7.0
498.json,summary the paper explains dropout with a latent variable model where the dropout variable or depending on which units should be dropped is not observed and is accordingly marginalised maximum likelihood under this model is not tractable but standard dropout then corresponds to a simple monte carlo approximation of ml for this model the paper then introduces a theoretical framework for analysing the discrepancy called inference gap between the model at training model ensemble or here the latent variable model and the model at testing where usually what should be an expectation over the activations over many models becomes the activation of one model with averaged weights this framework introduces several notions e g expectation linearity which allow the study of which transition functions and more generally layers can have a small inference gap theorem gives a bound on the inference gap finally a new regularisation term is introduced to account for minimisation of the inference gap during learning experiments are performed on mnist cifar and cifar and show that the method has the potential to perform better than standard dropout and at the level of monte carlo dropout the standard method to compute the real dropout outputs consistently with the training assumption of an ensemble of course quite expensive computationally the study gives a very interesting theoretical model for dropout as a latent variable model where standard dropout is then a monte carlo approximation this is very probably widely applicable to further studies of dropout the framework for the study of the inference gap is interesting although maybe somewhat less widely applicable the proposed model is convincing although it is tested on simple datasets the gains are relatively small and there is an increased computational cost during training because a new hyper parameter is introduced p line typo expecatation,8.0
498.json,this paper puts forward a not entirely new but also not sufficiently understood interpretation of dropout regularization the authors derive useful theorems that estimate or put bounds on key quantities that are of interest when analyzing dropout regularized networks from their perspective they furthermore introduce an explicit regularization term that should have a well understood impact on these key quantities in the experimental section they convincingly show that the proposed regularization indeed has the expected effect and that their perspective on dropout is therefore useful and meaningful their proposed regularization also seems to have a positive impact on the models performance but they demonstrate this only on rel small scale benchmark problems i therefore don t belief that this approach will have a large impact on how practitioner train models but their general perspective is well aligned with the recently proposed idea of dropout as a bayesian approximation and the insights and theorems in this paper might enable future work in that direction,8.0
749.json,the paper formulates a number of rules for designing convolutional neural network architectures for image processing and computer vision problems essentially it reads like a review paper about modern cnn architectures it also proposes a few new architectural ideas inspired by these rules these are experimentally evaluated on cifar and cifar but seem to achieve relatively poor performance on these datasets table so their merit is unclear to me i am not sure if such a collection of rules extracted from prior work warrants publication as a research paper it is not a bad idea to try and summarise some of these observations now that cnns have been the model of choice for computer vision tasks for a few years and such a summary could be useful for newcomers however a lot of it seems to boil down to common sense e g the rest of it might be more suited for an introduction to training cnns course blog post it also seems to be a bit skewed towards recent work that was fairly incremental e g a lot of attention is given to the flurry of resnet variants the paper states that it is universal in all convolutional neural networks that the activations are downsampled and the number of channels increased from the input to the final layer which is wrong we already discussed this previously re my question about design pattern but i think the answer that was given the nature of design patterns is that they only apply some of the time does not excuse making such sweeping claims this should probably be removed we feel that normalization puts all the layer input samples on more equal footing which allows backprop to train more effectively section nd paragraph is very vague language that has many possible interpretations and should probably be clarified it also seems odd to start this sentence with we feel as this does not seem like the kind of thing one should have an opinion about such claims should be corroborated by experiments and measurements there are several other instances of this issue across the paper the connection between taylor series and the proposed taylor series networks seems very tenuous and i do not think the name is appropriate the resulting function is not even a polynomial as all the terms represent different functions f x g x h x is not a particularly interesting object it is just a nonlinear function of x overall the paper reads like a collection of thoughts and ideas that are not very well delineated and the experimental results are unconvincing,3.0
749.json,the authors have grouped recent work in convolutional neural network design specifically with respect to image classification to identify core design principles guiding the field at large the principles they produce along with associated references include a number of useful and correct observations that would be an asset to anyone unfamiliar with the field the authors explore a number of architectures on cifar and cifar guided by these principles the authors have collected a quality set of references on the subject and grouped them well which is valuable for young researchers clearly the authors explored a many of architectural changes as part of their experiments and publicly available code base is always nice overall the writing seems to jump around a bit and the motivations behind some design principles feel lost in the confusion for example design pattern increase symmetry argues for architectural symmetry as a sign of beauty and quality is presented as one of core design principles without any further justification similarly design pattern over train includes any training method where the network is trained on a harder problem than necessary to improve generalization performance of inference is presented in the middle of a paragraph with no supporting references or further explanation the experimental portion of this paper feels scattered with many different approaches being presented based on subsets of the design principles in general these approaches either are minor modifications of existing networks different fractalnet pooling strategies or are novel architectures that do not perform well the exception being the fractal of fractal network which achieves slightly improved accuracy but also introduces many more network parameters increased capacity over the original fractalnet preliminary rating it is a useful and perhaps noble task to collect and distill research from many sources to find patterns and perhaps gaps in the state of a field however some of the patterns presented do not seem well developed and include principles that are poorly explained furthermore the innovative architectures motivated by the design principles either fall short or achieve slightly better accuracy by introducing many more parameters fractal of fractal networks for a paper addressing the topic of higher level design trends i would appreciate additional rigorous experimentation around each principle rather than novel architectures being presented,3.0
773.json,this paper applies rfn for biclustering to overcome the drawbacks in fabia the proposed method performs best among biclustering methods however my first concern is that from the methodological point of view the novelty of the proposed method seems small the authors replied to the same question which another reviewer gave but the replies were not so convincing this paper was actually difficult for me to follow for instance in figure a bicluster matrix is constructed as an outer product of h and w h is a hidden unit but what is w i could not find any definition furthermore i could not know how h is estimated in this method therefore i do not understand how this method performs biclustering totally i am not sure that this paper is suitable for publication prons empirical performance is good cons novelty of the proposed method some description in the paper is unclear,5.0
745.json,this paper propose a parallel mechanism for stochastic gradient descent method sgd in case of gradient can be computed via linear operations including least square linear regression and polynomial regression problems the motivation is to recover the same effect compared with sequential sgd by using a proposed sound combiner to make such combiner more efficient the authors also use a randomized projection matrix to do dimension reduction experiments shows the proposed method has better speedup than previous methods like hogwild and allreduce i feel that there might be some fundamental misunderstanding on sgd the combiner matrixm generate above can be quite large and expensive to compute the sequential sgd algorithm maintains and updates the weight vector w and thus requires o f space and time where f is the number of features in contrast m is a f f matrix and consequently the space and time complexity of parallel sgd is o f in practice this would mean that we would need o f processors to see constant speedups an infeasible proposition particularly for datasets that can have thousands if not millions of features i do not think one needs o f space and complexity for updating mi v where v is an f dimensional vector note that mi is a low rank matrix in the form of i ai ai the complexity and space can be reduced to o f if compute it by o v ai ai v equivalently if mi is defined in the form of the product of n number of rank matrices the complexity and space complexity is o fn in the context of this paper n should be much smaller than f i seriously doubt that all author assumptions experiments and strategies in this paper are based on this incorrect assumption on space and complexity of sgd why one can have speedup is unclear for me it is unclear what computations are in parallel and why this sequential algorithms can bring speedup if mi v is computed in the most efficient way i suggest authors to make the following changes to make this paper more clear and theoretically solid provide computational complexity per step of the proposed algorithm convergence rate analysis convergence analysis is not enough we would like to see how the dimension reduction can affect the complexity,4.0
778.json,this paper addresses to reduce test time computational load of dnns another factorization approach is proposed and shows good results the comparison to the other methods is not comprehensive the paper provides good insights,6.0
739.json,the authors present here a new algorithm for the effective calculation of polynomial features on sparse matrices the key idea is to use a proper mapping between matrices and their polynomial versions in order to derive an effective csr expansion algorithm the authors analyse the time complexity in a convincing way with experiments overall the algorithm is definitely interesting quite simple and nice with many possible applications the paper is however very superficial in terms of experiments or applications of the proposed scheme most importantly the fit with the main scope of iclr is far from obvious with this work that should probably re submitted to better targets,3.0
739.json,the paper is beyond my expertise i cannot give any solid review comments regarding the techniques that are better than an educated guess however it seems to me that the topic is not very relevant to the focus of iclr also the quality of writing requires improvement especially literature review and experiment analysis,3.0
597.json,this paper applies the pointer network architecture wherein an attention mechanism is fashioned to point to elements of an input sequence allowing a decoder to output said elements in order to solve simple combinatorial optimization problems such as the well known travelling salesman problem the network is trained by reinforcement learning using an actor critic method with the actor trained using the reinforce method and the critic used to estimate the reward baseline within the reinforce objective the paper is well written and easy to understand its use of a reinforcement learning and attention model framework to learn the structure of the space in which combinatorial problems of variable size can be tackled appears novel importantly it provides an interesting research avenue for revisiting classical neural based solutions to some combinatorial optimization problems using recently developed sequence to sequence approaches as such i think it merits consideration for the conference i have a few comments and some important reservations with the paper i take exception to the conclusion that the pointer network approach can handle general types of combinatorial optimization problems the crux of combinatorial problems for practical applications lies in the complex constraints that define feasible solutions e g simple generalizations of the tsp that involve time windows or multiple salesmen for these problems it is no longer so simple to exclude possible solutions from the enumeration of the solution by just striking off previously visited instances in fact for many of these problems finding a single feasible solution might in general be a challenge it would be relevant to include a discussion of whether the neural combinatorial optimization approach could scale to these important classes of problems and if so how my understanding is that this approach as presented would be mostly suitable for assignment problems with a very simple constraint structure the operations research literature is replete with a large number of benchmark problems that have become standard to compare solver quality for instance tsplib contains a large number of tsp instances,6.0
597.json,this paper proposes to use rnn and reinforcement learning for solving combinatorial optimization problems the use of pointer network is interesting as it enables generalization to arbitrary input size the proposed method also fintunes on test examples with active search to achieve better performance the proposed method is theoretically interesting as it shows that rnn and rl can be combined to solve combinatorial optimization problems and achieve comparable performance to traditional heuristic based algorithms however the lack of complexity comparison against baselines make it impossible to tell whether the proposed method has any practical value the matter is further complicated by the fact that the proposed method runs on gpu while baselines run on cpu it is hard to even come up with a meaningful unit of complexity money spent on hardware and electricity per instance may be a viable option further more the performance comparisons should be taken with a grain of salt as traditional heuristic based algorithms can often give better performance if allowed more computation which is not controlled across algorithms,6.0
554.json,this paper combines drqn with eligibility traces and also experiment with the adam optimizer for optimizing the q network this direction is worth exploring and the experiments demonstrate the benefit from using eligibility traces and adam on two atari games the methods themselves are not novel thus the primary contributions are applying eligibility traces and adam to drqn and the experimental evaluation the paper is well written and easy to understand the experiments provide quantitative results and detailed qualitative intuition for how and why the methods perform as they do however with only two atari games in the results it is difficult to tell how well it the method would perform more generally showing results on several more games and or other domains would significantly improve the paper showing error bars from multiple random seeds would also improve the paper,4.0
333.json,this paper provides an interesting analysis of the conditions which enable generation of natural looking textures the results is quite surprising and analysis is quite thorough i do think the evaluation methods require more work but as other reviewers mentioned this could be an interesting line of work moving forwards and does not take too much from this current paper which i think should be accepted,8.0
756.json,this paper presents a method for embedding data instances into a low dimensional space that preserves some form of similarity although the paper presents this notion as new basically every pre trained embedding be it auto encoders or wordvec has been doing the same representing items in a low dimensional space that inherently encodes their similarities even when looking at the specific case of word context embeddings the method is not novel either this method is almost identical to one of the similarity functions presented in a simple word embedding model for lexical substitution melamud et al the novelty claim must be more accurate and position itself with respect to existing work in addition i think the evaluation could be done better there are plenty of benchmarks for word embeddings in context for example,3.0
486.json,the paper develops a simple and reasonable algorithm for graph node prediction classification the formulations are very intuitive and lead to a simple cnn based training and can easily leverage existing gpu speedups experiments are thorough and compare with many reasonable baselines on large and real benchmark datasets although i am not quite aware of the literature on other methods and there may be similar alternatives as link and node prediction is an old problem i still think the approach is quite simple and reasonably supported by good evaluations,7.0
691.json,this paper introduces a new reinforcement learning environment called the retro learning environment that interfaces with the open source libretro api to offer access to various emulators and associated games i e similar to the atari arcade learning environment but more generic the first supported platform is the snes with games more consoles and games may be added later authors argue that snes games pose more challenges than atari s due to more complex graphics ai and game mechanics several dqn variants are evaluated in experiments and it is also proposed to compare learning algorihms by letting them compete against each other in multiplayer games i like the idea of going toward more complex games than those found on atari and having an environment where new consoles and games can easily be added sounds promising with openai universe and deepmind lab that just came out though i am not sure we really need another one right now especially since using roms of emulated games we do not own is technically illegal it looks like this did not cause too much trouble for atari but it might start raising eyebrows if the community moves to more advanced and recent games especially some nintendo still makes money from besides the introduction of the environment it is good to have dqn benchmarks on five games but this does not add a lot of value the authors also mention as contribution a new benchmarking technique allowing algorithms to compete against each other rather than playing against the in game ai but this seems a bit exaggerated to me the idea of pitting ais against each other has been at the core of many ai competitions for decades so it is hardly something new the finding that reinforcement learning algorithms tend to specialize to their opponent is also not particular surprising overall i believe this is an ok paper but i do not feel it brings enough to the table for a major conference this does not mean however that this new environment wo not find a spot in the now somewhat crowded space of game playing frameworks other small comments there are lots of typos way too many to mention them all it is said that infinite mario still serves as a benchmark platform however as far as i know it had to be shutdown due to nintendo not being too happy about it rle requires an emulator and a computer version of the console game rom file upon initialization rather than a rom file only the emulators are provided with rle how is that different from ale that requires the emulator stella which is also provided with ale why is there no dqn dddqn result on super mario it is not clear if figure displays the f zero results using reward shaping or not the du et al reference seems incomplete,4.0
556.json,i appreciate the work but i do not think the paper is clear enough moreover the authors say local minimia times but do not show except for figure that the solutions found are not necessarily local minima the authors do not talk about that fact that slices of a non convex problem can look like the ones they show it is well known that the first order methods may just fail to deal with certain non convex ill conditioned problems even in low dimensional noiseless cases the place solution where they fail to make progress is not necessarily a local minimum some sentences like the one given below suggest that the study is too superficial one of the interesting empirical observation is that we often observe is that the incremental improvement of optimization methods decreases rapidly even in non convex problems,4.0
612.json,paper summary this paper makes two contributions a model for next step prediction where the inputs and outputs are in the space of affine transforms between adjacent frames an evaluation method in which the quality of the generated data is assessed by measuring the reduction in performance of another model such as a classifier when tested on the generated data the authors show that according to this metric the proposed model works better than other baseline models including the recent work of mathieu et al which uses adversarial training strengths this paper attempts to solve a major problem in unsupervised learning with videos which is evaluating them the results show that using mse in transform space does prevent the blurring problem to a large extent which is one of the main aims of this paper the results show that the generated data reduces the performance of the cd model on ucf to a much less extent than other baselines the paper validates the assumption that videos can be approximated to quite a few time steps by a sequence of affine transforms starting from an initial frame weaknesses the proposed metric makes sense only if we truly just care about the performance of a particular classifier on a given task this significantly narrows the scope of applicability of this metric because arguably one the important reasons for doing unsupervised learning is to come up a representation that is widely applicable across a variety of tasks the proposed metric would not help evaluate generative models designed to achieve this objective it is possible that one of the generative models being compared will interact with the idiosyncrasies of the chosen classifier in unintended ways therefore it would be hard to draw strong conclusions about the relative merits of generative models from the results of such experiments one way to ameliorate this would be to use several different classifiers cd dual stream network other state of the art methods and show that the ranking of different generative models is consistent across the choice of classifier adding such experiments would help increase certainty in the conclusions drawn in this paper using only or input frames sampled at fps seems like very little context if we really expect the model to extrapolate the kind of motion seen in ucf the idea of working in the space of affine transforms would be much more appealing if the model can be shown to really generated non trivial motion patterns currently the motion patterns seem to be almost linear extrapolations the model that predicts motion does not have access to content at all it only gets access to previous motion it seems that this might be a disadvantage because the motion predictor cannot use any cues like object boundaries or decide what to do when two motion fields collide it is probably easier to argue about occlusions in content space quality clarity the paper is clearly written and easy to follow the assumptions are clearly specified and validated experimental details seem adequate originality the idea of generating videos by predicting motion has been used previously several recent papers also use this idea however the exact implementation in this paper is new the proposed evaluation protocol is novel significance the proposed evaluation method is an interesting alternative especially if it is extended to include multiple classifiers representative of different state of the art approaches given how hard it is to evaluate generative models of videos this paper could help start an effort to standardize on a benchmark set minor comments and suggestions in the caption for table each column shows the accuracy on the test set when taking a different number of input frames as input input here refers to the input to the classifier output of the next step prediction model however in the next sentence our approach maps times patches into times with stride and it takes frames at the input here input refers to the input to the next step prediction model it might be a good idea to rephrase these sentences to make the distinction clear in order to better understand the space of affine transform parameters it might help to include a histogram of these parameters in the paper this can help us see at a glance what is the typical range of these parameters should we expect a lot of outliers etc in order to compare transforms a and b instead of a b one could consider a b being close to identity as the metric did the authors try this the performance of the classifier on ground truth data is an upper bound on the performance of any generative model this is not strictly true it is possible though highly unlikely that a generative model might make the data look cleaner sharper or highlight some aspect of it which could improve the performance of the classifier even compared to ground truth this is especially true if the the generative model had access to the classifier it could then see what makes the classifier fire and highlight those discriminative features in the generated output overall this paper proposes future prediction in affine transform space this does reduce blurriness and makes the videos look relatively realistic at least to the cd classifier however the paper can be improved by showing that the model can predict more non trivial motion flows and the experiments can be strengthened by adding more classifiers besides than cd,6.0
632.json,summary the paper propose a new scoring function for knowledge base embedding the scoring function called transgaussian is an novel take on or a generalization of the well known transe scoring function the proposed function is tested on two tasks knowledge base completion and question answering overall judgment while i think this proposed work is very interesting and it is an idea worth to explore further the presentation and the experimental section of the paper have some problems regarding the presentation as far as i understand this is not an attention model as intended standardly in the literature plus it has hardly anything to share with memory networks neural turing machines the parallel that the authors try to make is not very convincing regarding the experimental section for a fair comparison the authors should test their model on standard benchmarks reporting state of the art models finally the paper lack of discussion of results and insights on the behavior of the proposed model detailed comments in section when the authors calculate mu context do not they loose the order of relations and if it is so does it make any sense,4.0
697.json,dear authors the authors response clarified some of my confusion but i still have the following question the response said a first contribution is a different formulation you divide the word embedding learning into two steps step looks for a low rank x by riemannian optimization step factorizes x into two matrices w c you are claiming that your model outperforms previous approaches that directly optimizes over w c but since the end result the factors is the same can the authors provide some intuition and justification why the proposed method works better as far as i can see though parameterized differently the first step of your method and previous methods sgd are both optimizing over low rank matrices admittedly riemannian optimization avoids the rotational degree of freedom the invertible matrix s you are mentioning in sec but i am not certain at this point this is the source of your gain learning curves of objectives would help to see if riemannian optimization is indeed more effective another detail i could not easily find is the following you said a disadvantage of other approaches is that their factors w and c do not directly reflect similarity did you try to multiply the factors w and c from other optimizers and then factorize the product using the method in section and use the new w for your downstream tasks i am not sure if this would cause much difference in the performance overall i think it is always interesting to apply advanced optimization techniques to machine learning problems the current paper would be stronger from the machine learning perspective if more thorough comparison and discussion as mentioned above are provided on the other hand my expertise is not in nlp and i leave it to other reviewers to decide the significance in experimental results,5.0
