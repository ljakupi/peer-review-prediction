review_file_id,comments,score
729.json,strength propose kernel based method capture high order pattern differentiting different type rumor evaluating similarity propagation tree structure weakness maybe math always clear sect general discussion propose propagation tree kernel kernel based method capture high order pattern differentiating type rumor evaluating similarity propagation tree structure proposed approach detects rumor quickly higher accuracy compared obtained state method data made public research purpose typo need fixed subgraph table show missing information need added published information need format figure small,3
338.json,strength related work quite thorough comparison approach presented make hypothesis stronger evaluation section also extensive thus experiment convincing weakness section clear exactly dataset used training furthermore give starting date collecting testing data information related size dataset time frame data collected might also give insight statistic given section table number reviewer slightly lower number review posted least hotel mean reviewer posted review labeled dataset compare full dataset table exact number reviewer table know percentage labeled reviewer also interesting know many review made person average reviewer post review much info learn benefit thorough discussion general discussion focus identifying spam review assumption deal cold start problem enough information draw conclusion proposes neural network learns represent review jointly using embedded textual information behaviour information overall well written compelling typo grammar reviewer provide jindal make first step work quite could past tense refer usage short form instead prefered long form following sentence clear rephrased reviewer posted review filter immediately historical review provided,3
343.json,strength well organized easy understand provides detailed comparison various experimental setting show state performance weakness experiment compare previous supervised approach proposed method semi supervised approach even training data enough train general discussion adopts training approach improve chinese word segmentation based transition based neural word segmentation aim train incoming character external resource punctuation soft segmentation heterogeneous training data multi task learning cast external source auxiliary classification task experimental show proposed method achieves state performance seven datasets well written easy understand number experiment prove effectiveness proposed method however exist issue proposed method semi supervised learning us external resource train character furthermore us another heterogeneous training datasets even us datasets training nevertheless baseline experiment based supervised learning general performance semi supervised learning better supervised learning semi supervised learning make plentiful auxiliary information experiment compared proposed method semi supervised approach post author response reviewer concerned used additional gold labeled dataset pretrain character embeddings baseline experiment used label information label predicted automatically base model pointed insisting superiority method circumstance thus even gold dataset used train segmentation directly seems unfair comparison proposed method used another gold dataset train character embeddings,2
752.json,strength demonstrates seqseq model comparatively effectively applied task parsing realization linearization engineered processed version graph associated sentence combined paired training iterative back translation monolingual data combined fine tuning parsing performance worse reported paper pust paper used additional semantic information task realization demonstrates utilizing additional monolingual data back translation effective relative seqseq information note comparing realization previous seqseq work realization task weakness high level main weakness aim empirical comparison comparing work multiple aspect dimension changing time case comparable access different information complicating comparison example realization table pbmt pourdamghani apparently trained ldct consists sentence compared trained ldce consists sentence according http download html used making claim point improvement state pbmt line line qualified caption table make valid comparison approach pbmt need evaluated using training data general discussion overlap sentence gigaword sample test sentence ldce apparently ldce contains data proxy report data deft narrative text source data corpus ldce accessible account http catalog upenn ldce seems ldce contains data gigaword http catalog upenn ldce apparently corpus ldct also contained newswire article selected english gigaword corpus fifth edition publicly accessible link http catalog upenn doc ldct readme please check test contamination line modification encoder make significant difference effectiveness motivation behind change please make clear appendix fine replication purpose whether implementation based existing seqseq framework line final sequence length used consider adding detail appendix please label column table presumably test also mismatch table text table summarizes development different round self training appears second round self training shown column table labeled column camr instead last line table http aclweb anthology configuration verb wiki look like second last table camr wang currently used note approach handle wikification information introduced ldce stochastic missing reference example line seems like hypothesis tested empirically rather forgone conclusion implied given extra page please concluding section performing decoding using beam search follow line appear actual vocabulary size used experiment mentioned preprocessing remaining unseen token test word unknown word replacement mechanism using attention weight described section ever used realization case study interest performance phenomenon known limitation quantification tense http github amrisi guideline blob master benefit brief discussion perhaps couple sentence motivating opposed semantic formalism well human annotated information signal might useful opposed learning seqseq directly task machine translation future work taken directly account score given review since applicable formally published eacl proceeding parsing account difference previous seqseq approach namely peng table difference effectiveness driven architecture preprocessing linearization data combination thereof consider isolating difference incidentally citation peng addressing data sparsity issue neural parsing apparently peng http eacl index program accepted paper http arxiv flipped reference section proofreading necessarily order occurrence note provided reference influence scoring outperform state outperform state zhou extend zhou extend puzikov puzikov based feature based feature language pair creating language pair creating using back translation system human translation using back translation system human translation probbank style palmer propbank style palmer independent parameter independent parameter token token maintaining embedding size maintaining embedding size table similar table similar realizer realizer notation line set defined never subsequently referenced however could used place line referring vocabulary,3
752.json,self training train seqseq based parser using small annotated corpus large amount unlabeled data train similar seqseq based text generator using annotated corpus automatic amrs produced parser unlabeled data careful delexicalization named entity task avoid data sparsity first sucessful application seqseq model parsing generation generation probably improves upon state general really liked approach well experiment final performance analysis method used revolutionary cleverly combined achieve practial description approach quite detailed believe possible reproduce experiment without significant problem approach still requires handcrafting believe overcome future taking good direction resolved response however made aware another reviewer data overlap gigaword semeval dataset potentially serious problem significant overlap test invalidate generation main achievemnt unless made sure test sentence made training gigaword cannot accept resolved response another question raised another reviewer fully agree point claim comparing system tested earlier version dataset could probably still claim improvement state sure accept point claim direct comparison pourdamghani also tested system older dataset version obtained pourdamghani score newer version otherwise minor comment experiment statistical significance test advisable even performance difference generation linearization order experiment repeated several time different random seed overcome bias particular random order chosen form definitely could improved dense point proofreading independent person preferably english native speaker advisable especially improvement luong could explained detail consider adding figure experiment description missing vocabulary size used importantly missed formal conclusion much end abruptly qualitative described give final overview work future work note minor factual note make clear jamr aligner whole parser also recorded mapping also testing parser gigaword improves seqseq model point voter figure person vote minor writing note rewording simplifying text near inter sentitial punctuation sometimes confusing correspond experience english syntax lot excessive well missing comma typo footnote missing full stop linearization description redundant could refer sect refering algorithm figure near enclose reference bracket rather comma think nice provide reference multi bleu script also mention remove variable footnote consider renaming sect linearization evaluation order table seems confusing especially system explicitly marked expect system bottom also table apparently list development score even though description say otherwise label table confusing read table reading text figure entirely visible distinguish month name month number state bibliography lack proper capitalization title abbreviation proper name capitalized curly brace prevent bibtex lowercasing everything peng citation listed improperly actually four summary present first competitive neural parsing probably state generation using seqseq model clever preprocessing exploiting large unlabelled corpus even though revision text advisable liked like conference resolved response however sure comparison previous state generation entirely sound importantly whether good actually caused data overlap gigaword additional training test comment response thank addressing major problem happy explanation raised score assuming reflect discussion final,3
494.json,strength nice clear application linguistics idea distributional semantics demonstrate clear improvement intrinsic extrinsic eval weakness fairly straightforward extension existing retrofitting work nice additional baseline character embeddings general discussion describes morph fitting type retrofitting vector space focus specifically incorporating morphological constraint vector space framework based idea attract repel constraint attract constraint used pull morphological variation close together look looking repel constraint used push derivational antonym apart responsible irresponsible test algorithm multiple different vector space several language show consistent improvement intrinsic evaluation simlex simverb also test extrinsic task dialogue state tracking demonstrate measurable improvement using morphologically unaware word embeddings think nice simple clean incorporate linguistic knowledge distributional model semantics empirical convincing question comment nothing feel prevent published comment really understand need morph simlex evaluation seems suspect create dataset using algorithm ultimately evaluate seems brainer well dataset constructed making assumption make think need include dataset since potentially erroneous evaluation cause confusion convincing enough standard datasets really liked morph baseline thank including liked baseline based character embeddings since seems fashionable currently side step dealing morphological variation mentioned related work better actually compare empirically ideally vector space morphological variant close together assign specific semantics different inflection evidence geometry space meaningful looking look walk walking nice analysis suggests meaningful space better embeddings,3
494.json,propose morph fitting method retrofit given trained word embeddings based morphologically driven objective pull inflectional form word together slow slowing push derivational antonym apart expensive inexpensive improve representation frequency inflection word well mitigate tendency corpus based word embeddings assign similar representation antonym method based relatively simple manually constructed morphological rule demonstrated english german italian russian experiment include intrinsic word similarity benchmark showing notable performance improvement achieved applying morph fitting several different corpus based embeddings performance improvement yielding state also demonstrated german italian extrinsic task dialog state tracking strength proposed method simple show nice performance improvement across number evaluation several language compared previous knowledge based retrofitting approach faruqui relies manually constructed rule instead large scale knowledge base ontology like previous retrofitting approach method easy apply existing set embeddings therefore seems like software intend release could useful community method experiment clearly described weakness hoping analysis morph fitted embeddings worked better evaluation well corresponds intuitive motivation introduce synthetic word similarity evaluation dataset morph simlex create applying presumably semantic meaning preserving morphological rule simlex generate many pair morphological variability manually annotate pair rather original similarity judgement simlex obvious caveat dataset similarity score presumed therefore le reliable furthermore fact dataset generated rule used work morph word embeddings mean reported dataset work taken grain salt clearly state soricut mentioned future source morphological knowledge fact also alternative approach proposed generating morphologically aware word representation present differentiate work evaluation include strong morphologically informed embedding baseline general discussion exception noted like work think represents nice contribution community presented simple approach showed yield nice improvement using various common embeddings several evaluation four different language happy conference minor comment line found phrasing unclear query linguistic constraint section suggest elaborate little delta used based wieting seemed mostly addition repel part line method cost function consists three term suggest spell equation line equation following one vector representation word suggest denote somehow also vector normalized process also computing nearest neighbor example cosine product please share detail line suggest move text section make note fine tune params main text footnote line create creates seems like wrong example rule read author response,3
375.json,address network embedding problem introducing neural network us network structure associated text node attention vary textual representation based text neighboring node strength leverage network text construct latent representation mutual attention approach seems sensible relatively thorough evaluation provided multiple datasets baseline evaluation task weakness like many paper network embedding literature neural network technique inspired word embeddings construct latent representation node network previous line work statistical probabilistic modeling network ignored particular network embedding paper need start citing comparing work latent space peter hoff subsequent paper statistical probabilistic machine learning publication venue hoff raftery handcock latent space approach social network analysis amer statist assoc latent space network embeds node dimensional latent space written back date neural network based network embeddings given differing representation social network actor different role really cite compare mixed membership stochastic blockmodel mmsb airoldi blei fienberg xing mixed membership stochastic blockmodels journal machine learning research mmsb allows node randomly select different role deciding whether form edge general discussion aforementioned statistical model leverage text scalable neural network implementation based negative sampling based well principled generative model instead heuristic neural network objective function algorithm recent extension model inference algorithm scalable leverage text difference performance cene cane figure statistically insignificant related question experiment repeated random train test split grid search hyperparameter value mentioned section performed evaluation test problematic validation training,3
16.json,strength try information argument usually ignored actually quite important improve performance event detection framework clear simple help supervised attention mechanism important method used many task machine translation performance system outperforms baseline significantly weakness attention vector simply summation attention vector part maybe attention vector could calculated appropriate approach supervised attention mechanism strategy proposed quite straightforward complicated strategy work better tried general discussion although place improved proposed quite effective framework performance good experiment solid considered accepted,3
288.json,strength well written show stylometric analysis help reasoning like text classification important implication design datasets important implication many text classification task weakness weakness true absence definition style concept general discussion describes experiment explore relationship writing task writing style particular controlling vocabulary topic show feature used authorship attribution style analysis long towards distinguishing natural ending story ending added different author purposefully incoherent ending added different author great read definitely merit accepted lucidly written clearly explains done well known simple feature simple classifier prove obvious hypothesis intuitively obvious writing task greatly constraint style however proven clear manner controlled setting finding impressive particularly like section discussion implication design task think influential well cited great work good minor suggestion defining mean style early seem mean level easily computable lexical syntactic feature usage somewhat misleading anyone outside computational stylometrics chosen stylistic feature make sense however option feature tried work think short discussion choice feature informative,4
699.json,proposes encoder decoder framework keyphrase generation experimental show proposed outperforms baseline supervised data available strength well organized easy follow intuition proposed method clear includes enough detail replicate experiment although application encoder decoder copy mechanism straightforward experimental reasonable support claim generation absent keyphrases presented weakness said little surprise proposed approach also described section trained transfer well domain go unsupervised model contribution maintain training corpus good quantity quality explicitly stated general discussion like read pleased accepted like know training corpus size variation affect performance proposed method also beneficial actual value along example figure copyrnn experience running copynet copying mechanism sometimes work unexpectedly sure happens,3
676.json,strength proposed method save memory improve decoding speed cpu without losing little loss performance weakness since determination convolutional code algorithm algorithm affect final performance think better explore good method think argument experiment show proposed achieves translation accuracy approach softmax reducing memory usage order also improving decoding speed cpu abstract rigorous know experiment setting binary hybrid aspec corpus show improvement decoding speed cpu bleu score valid conclusion general discussion proposes efficient prediction method neural machine translation predicts binary code word reduce complexity prediction also proposed improved error correction binary code method improve prediction accuracy hybrid softmax binary balance prediction accuracy efficiency proposed method save memory improve decoding speed without losing little loss performance think good,3
676.json,strength high originality proposing fundamentally different predicting word vocabulary efficient softmax layer comparable performance successful approach could impactful speed prediction nice read great diagram clearly presented like cross referencing model diagram table including loss curve appreciated weakness though possible time remaining good comparison bleu score previous related work like hierarchical softmax differentiated softmax lacking linguistic perspective proposed method compared softmax layer hierarchical differentiated softmax binary code prediction natural predict word le similar human might retrieve word memory theoretical reason believe binary code based approach le suited task softmax layer though promise faster training speed introduction table show modest le speedup training presumably much training iteration time consumed part network useful time needed output layer computation general discussion nice survey prior work explicitly related method desideratum introduction specify satisfy kind analysis qualitative strength weakness binary code prediction welcome kind mistake system make compare standard softmax hierarchical differentiated softmax level comment equation difference consider defining gpgpu table highlight best bleu score bold equation remind reader defined equation function confused first appear appear right know,3
226.json,strength proposed semi automated framework human generation auto expansion human post editing construct compositional semantic similarity evaluation data proposed framework used create polish compositional semantic similarity evaluation data useful future work developing polish compositional semantic model weakness proposed framework tested language clear whether framework portable language example proposed framework relies dependency parser available language poor performance language number sentence pair edited leader judge reported correctness efficiency automatic expansion framework evaluated fact post edited pair need post editing worrying quite number grammatical mistake example complete exhaustive list line displayed image picture displayed image picture line similarly similar proofread pas needed general discussion,3
524.json,strength elaborate evaluation data creation evaluation scheme range compared technique baseline simple complex weakness depth analysis beyond overall evaluation general discussion compare several technique robust hpsg parsing since main contribution novel parsing technique empirical evaluation like depth analysis summarized table nice show representative example sentence sketch analysis compared method behaved differently please precision recall figure table score result mixed effect overall partial coverage parse ranking efficiency search overall coverage figure table helpful addition recall table make situation clearer minor comment pacnv table pacnv described,2
524.json,strength well written weakness although title abstract suggest robust parsing method hpsg compared actual comparison limited technique applied single grammar past choice made create treebank sentence coverage grammar since quite idiosyncratic respect fear interesting researcher working precision grammar framework lack comparison robustness technique routinely applied system based precision grammar various system based alpage system french alpino dutch probably spirit reference supertagging dridan supertagging whereas supertagging precision grammar system proposed least decade earlier lack enough detail make replicable various detail spelled limit resource allocation perhaps importantly technique compared robust unification actual evaluation metric refers another still preparation actual various technique somewhat disappointing exception csaw method resulting parsing speed extreme sometimes much slower baseline method baseline method method standard resource limitation apply csaw method faster accurate case method introduced existing pcfg approximation technique interesting idea representative dataset consisting sentence coverage grammar case comparison real baseline system standard setting could obtained methodological issue datasets semcor wsjab consist sentence older version could parse newer version could reason problem datasets clearly much biased suprise therefore various technique obtain much better datasets reviewer somewhat meaningless minor used explained reverseability general discussion,1
524.json,strength technique creating dataset evaluation coverage item could possibly used evaluation grammar well writing engaging clear pleasant surprise compared typical publication weakness evaluation datasets used small hence convincing particularly alchemy dataset best obtained disappointing score coverage score virtually deeper analysis instance breakdown type error type grammatical construction interesting still clear reviewer proportion coverage item various factor running resource lack coverage genuine grammatical construction long tail lack coverage extra grammatical factor like interjection disfluency lack lexical coverage general discussion address problem robustness lack coverage hand written hpsg grammar english resource grammar compare several approach increasing coverage also present creative way obtaining evaluation datasets trivial issue fact gold standard evaluation data definition available coverage input although hand written precision grammar much fashion long time superseded statistical treebank based grammar important continue research opinion advantage high precision deep semantic analysis provided grammar reproduced handwritten grammar reason giving score despite shortcoming mentioned,2
318.json,work showed word representation learning benefit sememes used appropriate attention scheme hypothesized sememes essential regularizer task proposed detects word sens learn representation simultaneously though experimental indicate benefit exact gain unclear since qualitative case study couple example done overall well written well structured last paragraph introduction section tried tell three contribution work novelty work rather contribution main contribution work show learn better word representation unsure modeling sememe information competitive baseline neither contribution novelty three strategy tried modeling make sense intuitively ranked term well work good explaining experimental supported intuition reviewer also see fourth strategy rather baseline inspired chen many system assume sense word given context many time performed better unless missed clarify otherwise seems exactly like difference target word represented probable sense rather taking attention weighted average sens still attention based scheme sense maximum attention weight chosen though clearly mentioned target word represented chosen sense embedding function explain selection datasets training evaluation task reference page sogou text corpus help reviewer know chinese language unclear exact dataset used several datasets mentioned page word similarity datasets used different like rare word another since different model performed differently datasets choice datasets allow evaluating work make reviewer wonder next question proposed state chinese word similarity schnabel report score wordsim data using cbow word embeddings reviewer need clarification parameter like vocabulary size word sogou contains billion unique word word sens many word type hownet notation used clear embeddings sens sememes different word shared reviewer hope case dimensional embeddings used sememes better complexity parameter also discussed lack space experiment discussion lack insight observation performing best also claimed word lower frequency learned better sememes without evaluating rare word dataset read author response,3
318.json,strength proposes hownet enrich embedings idea interesting give good weakness interesting sure contibution important enough long also comparision work fair compare system manually developed resource understandable help improvement english general discussion,2
477.json,strength motivation well described provides detailed comparison various model across diverse language weakness conclusion biased selected language experiment cover claim completely general discussion issue simple fundamental question word representation subunit word suitable represent morphology compose unit answer question applied word representation various subunit character character trigram morphs composition function lstm simple addition language modeling task find best combination addition evaluated task language language typologically diverse different according word representation composition function experimental concluded character level representation effective still imperfective comparing explicit knowledge morphology another conclusion character trigram show reliable perplexity majority language however leaf issue behind first could selection bias experimental language chose language four category three language category basic question language claimed language representative category language category tendency word representation composition function proved instance even language belonging typology agglutinative show different therefore least seems better focus language tested instead drawing general conclusion language claim experiment language modeling best task prove claim chance claim break task explanation issue needed section evaluated proposed method arabic reason experiment performed arabic plenty language automatic morphological analyzer japanese turkish considers character trigram among various gram good reason choose character trigram always better character bigram character fourgram general language modeling gram affected corpus size factor minor typo missing reference introduction line page root patter root pattern line page,2
134.json,strength well written easy understand method interesting weakness evaluation obtained might problematic comment general discussion proposes system argumentation mining using neural network problem using approach sequence labeling dependency parsing also includes experimenting multitask learning setting sequence labeling approach clearly explains motivation behind proposed existing method based manual feature engineering manual design constraint however proposed avoids manual effort moreover jointly learns subtasks argumentation mining therefore avoids error back propagation problem pipeline method except missing detail mentioned method explained clearly experiment substantial comparison performed properly interesting main concern small size dataset large capacity used lstm based recurrent neural network blcc dataset includes around essay training essay testing size development however mentioned also supplementary material worrying number essay left training crucial problem total number tag training data probably thousand compare standard sequence labeling task hundred thousand sometimes million tag available reason sure parameter trained properly also analyze overfitting problem interesting training development loss value training parameter update epoch also provided information seen evidence overfitting line explanation tagger simpler local model thus need le training data le prone overfitting reason sure model stable enough mean standard deviation multiple run different initialization parameter need included statistical significance test also provide information stability model reliability without test hard better superiority proposed method chance understand neural network used modeling task regularization technique however since size dataset small need attention regularization method mention regularization supplementary material mention briefly regularization lstm problem need addressed properly instead current hyper parameter optimization method described supplementary material consider using bayesian optimization method also move information trained word embeddings error analysis supplementary material extra page enough please include inter annotator agreement score describing dataset relevant information information provide insight performance system available room improvement please consider illustrating figure different color make quality better black white print edit thanks answering question increased recommendation score please include score range also report mean variance different setting still concerned stability example large variance kiperwasser setting need analyzed properly even change range relatively large including score range help replicating work,3
134.json,work describes joint neural approach argumentation mining several approach explored including casting problem dependency parsing problem trying several different parser casting problem sequence labeling problem multi task learning based sequence labeling underneath neural labeling entity relation lstm based state model approach evaluated using defined concept relation dependency based solution work well labeling solution effective lstm performs well especially paragraph level labeling lstm model outperform approach comprehensive supplement given technicality training model optimizing hyper parameter also shown sequence labeling model greatly improved multitask approach claim task helping relation task aper thorough investigation neural based approach argumentation mining major remark concern data wondering problem essay train test might topic consequently writer might similar argument essay leading information leakage train test turn might give overly optimistic performance estimate though think issue present model unfair advantage still something discus concern best model lstm acutally application related work however given relative success sequence based model experiment useful lesson learned think work deserves published minor remark question guess arguing possible reconstruct full graph tree output still part quite clear ordering section tagging dependency based using tagging much easier follow order first reversed time forgotten stagt stood mean couple jointly model coupling required jointly checked miwa bansal could find confusing system couple relation info entity info best guess mean learns task edge tree task label edge thus decoupling case recommend make part clearer score paragraph essay setting comparable particular relation task wondering paragraph based model might miss cross paragraph relation default never consider,3
122.json,strength proposes novel approach dialogue state tracking benefit representing slot value trained embeddings learns compose distributed representation user utterance dialogue context experiment performed datasets show consistent significant improvement baseline previous delexicalization based approach alternative approach xavier glove program training word embeddings investigated weakness although main motivation using embeddings generalize complex dialogue domain delexicalization scale datasets used seem limited wonder approach compare without separate slot tagging component complex dialogue example computing similarity utterance slot value pair actually limit estimation span slot value applicable even value match think example intro misleading dialogue state also include restaurantname house brings another question resolution coreference impact task general discussion overall trained word embeddings great idea specific approach using exciting,3
122.json,present neural network based framework dialogue state tracking main contribution work learning representation user utterance system output also ontology entry based trained word vector particularly utterance representation compared different neural network model learned representation combined finally used downstream network make binary decision given slot value pair experiment show proposed framework achieved significant performance improvement compared baseline delexicalized approach generally quality work clear goal reasonable idea improved previous study seem well organized effectively deliver detail especially reader familiar area first formal definition need given beginning clear enough could confusing coupling suggestion provide general architecture dialogue system described section rather section followed problem definition focusing relationship component including policy learning also help improve readability notation used throughout defined earlier section symbol used much earlier description comment question possible perform separate term joint could misleading able handle task could please provide statistic many error corrected original dstc dataset huge experiment could include comparison also published work including dstc entry using dataset think using rnns lstms learn sequential aspect learning utterance representation considering recent success recurrent network problem could effective well detail semantic dictionary used baseline help imply cost building kind resource manually great could give sample correctly predicted baseline solved proposed model,3
56.json,strength present extension many popular method learning vector representation text original method skip gram negative sampling glove based approach currently word cooccurrence statistic approach could extended gram based statistic gram based statistic increase complexity every algorithm vocabulary embeddings context space many time larger present method learn embeddings ngrams ngram context efficiently computes embeddings similarity analogy task present strong weakness loved experiment real task embeddings used input beyond experiment presented made stronger general discussion even aforementioned weakness think nice read author response,3
56.json,strength idea train wordvec type model ngrams specifically bigram instead word excellent range experimental setting four wordvec type algorithm several word bigram condition cover quite ground qualitative inspection bigram embeddings interesting show potential type multi word expression weakness benefit check native speaker english especially regarding article description similarity analogy task come strange place datasets general discussion done point well could clarified start simply generalization original wordvec idea redefining word ngram unigram also using bigram good give rationale larger ngrams used read author response,3
56.json,modifies existing word embedding algorithm glove skip gram ppmi include ngram ngram cooccurance statistic deal large computational cost storing expensive matrix propose algorithm us different strategy collect count strength proposed work seems like natural extension existing work learning word embeddings integrating bigram information expect capture richer syntactic semantic information weakness propose learning embeddings bigram bibi case actually evaluate embeddings learned bigram except qualitative evaluation table quantitative evaluation paraphrasing related task include bigram representation could good contribution evaluation convincing show consistent trend improvement necessarily statistically significant read clunkily significant grammar spelling error need major editing pas general discussion extension standard embedding learning technique include information bigram bigram coocurance work interesting natural extension existing work evaluation method leaf open question apart one mentioned weakness minor question significant difference overlap overlap case interested finding quantitative difference shown task read author response look forward seeing revised version,2
335.json,present gated self matching network reading comprehension style question answering three component solution introduces gated attention based recurrent network obtain question aware representation passage add additional gate attention based recurrent network determine importance passage part attend one relevant question word well character embeddings handle word overall component inspired wang jiang proposes self matching attention mechanism improve representation question passage looking wider passage context necessary infer answer component completely novel output layer us pointer network locate answer boundary also inspired wang jiang overall like think make nice contribution strength clearly break network three component descriptive purpose relates prior work mention novelty respect sound empirical analysis describing impact component ablation study appreciated impressive weakness describes single ensemble could find detail ensemble created believe might ensemble character based word based please describe rebuttal general discussion along ablation study nice qualitative analysis describing example case component gating character embedding self embedding become crucial simple question right adding component help form appendix supplementary,3
636.json,work proposes apply dilated convolution sequence tagging specifically named entity recognition also introduces novel idea sharing dilated convolution block predicting tag convolution level think prove useful community performs extensive ablation experiment show effectiveness approach found writing clear experiment exceptionally thorough strength extensive experiment various architecture lstm lstm novel architectural training idea sharing block weakness applied english concern since title seems reference sequence tagging directly section could clearer example presume padding make sure output resolution block input resolution might good mention think ablation study number layer perf might interesting response author rebuttal thank much thoughtful response given agreed make content specific opposed sequence tagging revised score upward,3
636.json,strength main strength promised speed advantage accuracy level weakness presentation approach leaf desired section need much clearer concept definition explaining architecture parameterization particular section parameter tieing used need crystal clear since main contribution experiment supporting vast speed improvement promised need presented table good great speed nothing transformative general discussion exactly viterbi prediction term concept established reader could guess must better phrase reference wei typo,2
266.json,strength interesting comprehensive study effect using special domain corpus training word embeddings clear explanation assumption contribution methodology thorough evaluation various aspect proposal weakness conclusion fully backed numerical claim catalan improvement using specific corpus training word vector pronounced english sure conclusion made based table none combination method outperform baseline dimension vector general discussion present simple interesting experiment suggest word vector trained using skip gram method largely benefit relevant domain subjective corpus answer important question benefit practitioner natural language processing also well written clearly organized,2
180.json,present dataset annotation product coming online cybercrime forum clear well written experiment good every hypothesis tested compared however concern took liberty change font size line spacing abstract enabling longer abstract content page requirement think fit tagging chunking parsing area information extraction problem difficulty annotation sombody related product basic basic indeed tool available nowadays sure possible elaborate baseline without much extra work domain adaptation experiment corroborate already know user generated data forum video game different type user gender leading different text give highlight specific problem,2
657.json,strength address part problem interpreting long short term memory lstm neural network model trained categorize written justification value affirmation essay definitely interesting research question want rely approach standard experimental psychology furthermore also validating sociological assumption study weakness main weakness lie fact goal clear enough overall ambitious goal forward approach experimental psychology interpret lstms however clear methodology presented hand goal validate sociological assumption studying relationship gender marker written justification independently claim expected gender difference function theory gendered self construal proven study general discussion study interesting suffers several weak argument first fact probability shift token lstm network correlated corresponding coefficient proof probability valid way interpret indeed coefficient reveal part happening decision function classifie coefficient provides interpretation correlated coefficient provides explanation another furthermore correlation coefficient high point forward really backed mentioned another problem lie fact seem hesitate goal better clearly state goal develop concerning relation experimental psychology priori important part interesting develop better explain multilevel bayesian model used quantify gender based self construal assumption difficult ass whether methodology used really appropriate without detail important aspect method detailed,1
483.json,present application pointer network recurrent neural network original used solving algorithmic task subtasks argumentation mining determining type argument component finding link achieves state strength thorough review prior specific formulation argument mining handled simple effective modification existing make suitable task mostly explained clearly strong compared prior task weakness formulation argumentation mining several proposed subtask division mentioned example claim detected classified supporting evidence detected furthermore applied neural network task inaccurate claimed abstract work first based approach argumentation mining thing must improved presentation pooling method used embedding feature line equation line clear enough random variable representing type identity supposedly modeled latter feature representation need defined furthermore seems like equation conditional probability several unclear thing table first three first baseline evaluated macro individual score missing explained text second presented table actually joint three mentioned dataset experiment described table performed general discussion lengthier introduction pointer network mentioning recurrent neural network general benefit reader unfamiliar sequence sequence model also citation sutskever line first mention term difference respect recursive neural network explained paragraph starting line tree structure activation requires explanation citation still enough well known explained label sentence hyperparameters obtained appropriate decision early stopping link prediction accuracy explained average type accuracy example inference test time briefly explained benefit detail specify length measured word referent neither unclear minimum maximum performance amount training data indeed surprising model also achieved almost especially surprising usually need data good could alternatively show structural cue le important task minor typo corrected show line rinott ruty show evidence automatic method context dependent evidence detection emnlp laha anirban vikas raykar empirical evaluation various deep learning architecture sequence classification task coling,3
21.json,clearly written claim well supported related work particular thorough clearly establishes proposed work fit field main question method phrase mentioned section word representation discussed phrase representation derived explicit connection indirectly connected tanh scoring function learned matrix compare another like furthermore benefit drawback linking together directly enforcing measure dissimilarity additionally statistical significance observed improvement valuable typographical comment line word phase pair word phrase pair line propose alternate wording instead entity translated entity mapped first read translation operation vector space think exactly described line slightly improvement measure slight improvement measure line extraneous comma citation line case likely case guessing line extraneous period comma citation,3
21.json,strength interesting research problem method look quite formal released dataset submission design experiment good weakness advantage disadvantage transductive learning discussed general discussion introduce transductive learning approach chinese hypernym prediction quite interesting problem establish mapping entity hypernym embedding space directly sound also quite novel well written easy follow first part method preprocessing using embeddings widely used method initial stage still normal preprocess input data transductive optimization framework linear mapping utilizing labeled unlabeled data attached supplementary note method make clear experimental shown effectiveness proposed method also released dataset contributes similar research researcher future,3
440.json,strength present extension parsing include dependency information achieving maintaining speed tractability impressive feature approach ability precompute attachment nice trick also really appreciated evaluation effect head rule normal form violation love detail remaining case weakness like analysis certain dependency structure particularly interested coordination relative clause handled predicate argument structure odds dependency structure normally used dependency parser general discussion happy work feel nice contribution literature thing missing depth analysis type construction improvement english japanese discussion mentioned reconciling pred dependency parser,3
440.json,describes state parsing decomposes tagging dependency score efficient decoding algorithm interestingly slightly outperforms expressive global parsing presumably factorization make learning easier great also report another language showing large improvement existing work japanese parsing surprising original result modeling first word constituent head substantially outperforms linguistically motivated head rule overall good make nice contribution suggestion liked dependency supertagging model interact good include baseline simpler variation conditioning head dependency achieves state japanese large margin however le work data also possible train parser data comparison lewis zettlemoyer explore combined dependency supertagging model worth citing,3
769.json,present novel framework modelling symmetric collaborative dialogue agent dynamically extending knowledge graph embeddings task rather simple dialogue agent human human human talk mutual friend underlying knowledge base party dialogue associated knowledge graph item knowledge graph embeddings dynamically updated conversation used generate answer strength novel goal directed open ended dialogue presented evaluation metric show clear advantage presented weakness term presentation mathematical detail embeddings computed sufficiently clear done extensive evaluation actually compared system based dialogue manager current state goal oriented system finally clear approach scale complex problem actually agent operates judging table general discussion overall think good theoretical aspect better presented give accept,2
627.json,present dialogue agent belief tracker dialogue manager jointly optimised using reinforce algorithm learns interaction user simulator training phase first imitation learning phase system initialised using supervising learning rule based reinforcement learning phase system jointly optimised using objective strength present framework differentiable access integrated joint optimisation biggest contribution weakness firstly truly system considering response generation handcrafted rather learnt also actually overfits simulator performs poorly human evaluation begs question whether actually selling idea learning soft access soft access actually brings consistent improvement however idea learning much tried explain merit figure also fail difference addition motivate reason using reinforce algorithm known suffer high variance problem attempt improve using baseline perhaps considering natural actor critic algorithm known perform better general discussion apart mentioned weakness think experiment solid generally acceptable however crystallised around idea actually improves performance soft access idea learning better,3
365.json,update reading author response alignment hidden unit match intuition experience willing believe wrong case discussing alignment important maybe sanity checking alignment go away initialize different seed saying different little better performing error reduction wonder ensemble seems like ensembling provide nice boost failure across model distinct right anyhow solid appreciate author response raise review score strength evidence attention connection interesting method appropriate model perform well relative state weakness critical detail provided model particularly novel general discussion present method historical text normalization performs well primary contribution end hypothesis attention mechanism task learned multi task learning auxiliary task pronunciation task connection attention interesting major area improvement first given almost explanation pronunciation task somehow require attention mechanism similar used normalization task task normalization pronunciation related mentioned spelling variation often stem variation pronunciation task result implicit attention mechanism fact hampered inclusion explicit attention mechanism remains mystery leave question unanswered least suggestion answer strengthen concern clarity writing clear number detail omitted important description attention mechanism given central role method play described detail rather referring previous work understand paragraph question included compare output vector model figure output dimension understand hidden layer dimension model ever comparable usually hidden state organized completely different every least permuted really understand figure kappa statistic attention need compared statistic model base upper bound across data set lastly analysis seems imply attention approach make large change comparing experimental improvement accuracy either quite small seems like contradiction,3
365.json,summary applies sequence sequence seqseq approach german historical text normalization showed using grapheme phoneme generation auxiliary task multi task learning seqseq framework improves performance argue approach replaces need attention menchanism showing experimentally attention mechanism harm performance also tried show statistical correlation weight normalizer attention based strength novel application seqseq historical text correction although applied recently sentence grammatical error identification showed using grapheme phoneme auxiliary task setting improves text normalization accuracy weakness instead arguing approach replaces attention mechanism think investigate attention work perhaps modify attention mechanism harm performance think reference past seqseq work work also worked attention seqseq model tested german historical text data document interesting evaluate approach another language data reference allen schmaltz yoon alexander rush stuart shieber sentence level grammatical error identification sequence sequence correction proceeding workshop innovative building educational application minh thang luong ilya sutskever quoc oriol vinyals lukasz kaiser multi task sequence sequence learning iclr dong daxiang dianhai wang haifeng multi task learning multiple language translation reply rebuttal keeping review score mean object accepting however raising score reason respond question paper seqseq also avoided using attention mechanism term novelty main novelty lie applying text normalization always easier show something attention seqseq working value finding fails changing attention mechanism work,2
365.json,strength well written solid experimental setup intriguing qualitative analysis weakness except qualitative analysis belong better application area since model particularly application novelty general discussion present sequence sequence attention mechanism auxiliary phonetic prediction task tackle historical text normalization none used model technique seem never used problem showing improvement state seem like better application track except final analysis link attention multi task learning claiming produce similar effect hypothesis intriguing supported wealth evidence least presented task question analysis though section assuming hidden layer space model aligned safe section mean error model resolve independently like symmetric difference combine model error resolved anymore different vein comparison azawi reading author response feeling concerned claim alignment hidden space model accepted strongly encourage make clear discussion shared think alignment hold practice,3
220.json,strength great well written interesting creative method good enlightening comparison earlier approach addition corpus carefully annotated prove valuable resource researcher appreciated qualitative discussion section many paper give present table without much discussion really provides insight reader weakness section sentence rest input zero quite enigmatic look figure extra sentence explaining going helpful furthermore figure input layer lstms say embeddings also network taking dependency label input surely wrong correct please explain mean general discussion concerning comment lstms excellent modelling language sequence type comment seems strange sequential problem sense datapoint feed network word example next example nothing preceding lstm architecture could still superior course reason state misunderstood something interested hear comment point,4
467.json,strength present iterative method induce bilingual word embeddings using large monolingual corpus starting automatically obtainable numeral mapping language compared state using larger bilingual dictionary parallel comparable corpus obtained presented method relies little manually prepared input exciting impressive weakness liked discussion error method possibly discussion method could adjusted deal general discussion frequency seed monolingual corpus matter interesting partial sense number iteration evolution mapping word language word happens different translation word like different sens difference german english prevalence compound german happens compound mapped onto preprocessing step splitting compound help using maybe corpus internal unigram information upper bound approach analysis error word counterpart language interesting also interesting discussion error come could addressed presented approach,3
467.json,work proposes self learning bootstrapping approach learning bilingual word embeddings achieves competitive task bilingual lexicon induction cross lingual word similarity although requires minimal amount bilingual supervision method lead competitive performance even seed dictionary extremely small dictionary item constructed without language pair specific information relying numeral shared language well written admirably even find work eclectic sense original contribution breakthrough finding hort idea opinion connects dot prior work drawing inspiration modelling component variety previous paper subject including embedding work self learning bootstrapping fully recognized current version liked general research question could pursued work along partial recognition related work lack comparison several relevant baseline main concern regarding fixed updated version self learning bootstrapping bilingual vector space work first tackle limited setup learning cross lingual embeddings although first miceli barone work first truly bootstrapping self learning approach learning cross lingual embeddings however idea bootstrapping bilingual vector space reapplied learning embeddings body work used exactly idea traditional count based bilingual vector space suggest check work peirsman pado naacl vulic moens emnlp recognize fact proposed bootstrapping approach novel domain also related work ellen riloff group bootstrapping semantic lexicon monolingual setting relation artetxe might missing something seems proposed bootstrapping algorithm fact iterative approach repeatedly utilises previously proposed formulation artetxe difference reparametrization line clear whether bootstrapping approach draw performance reparametrization whether work previous parametrization performance product algorithm parametrization perhaps explicit statement text needed fully understand going comparison prior work several relevant paper mentioned discussed current version instance recent work duong emnlp learning crosslingual word embeddings without bilingual corpus seems related work basic word overlap title reveals least discussed compared another work also relies mapping seed lexicon also partially analyzes setting hundred seed lexicon pair work vulic korhonen role seed lexicon learning bilingual word embeddings paper might also help provide detail future work section selection reliable translation pair might boost performance iterative process another relevant work appeared recently smith iclr discus offline bilingual word vector orthogonal transformation inverted softmax also discus learning bilingual embeddings limited setting relying shared word cognate language pair side note interesting report obtained using shared word language word definitely exist three language pair used experiment also enable direct comparison work smith iclr rely setup seed dictionary size bilingual lexicon induction seems proposed algorithm discussed section almost invariant starting seed lexicon yielding similar final score regardless starting point intriguing finding also seems suggest utter limitation current offline approach seem ceiling setup discussed vulic korhonen showed cannot really improve simply collecting seed lexicon pair work suggests number starting pair good enough reach near optimal performance also similar number reported arxiv lazaridou like discussion break ceiling improve offline method smith iclr seem report higher number dataset interesting link work work smith word state future work plan fine tune method learn without bilingual evidence admirable philosophically driven feat pragmatic point view seems pragmatic detect plateau ceiling seems linear mapping approach regardless number used seed lexicon pair figure convergence criterion training efficiency convergence criterion although crucial entire algorithm term efficiency efficacy mentioned side note entirely clear whole procedure terminates suspect vanishing variation crosslingual word similarity performance criterion stop procedure make method applicable language cross lingual word similarity dataset might missing given current description fully understand procedure stop finnish given crosslingual word similarity dataset english finnish minor finnish corpus corpus line http clarin repository xmlui handle since claim method could work seed dictionary containing shared numeral interesting include additional language pair share alphabet english russian english bulgarian even something distant arabic hindi response like thank investing time response helped clarify doubt point raised initial review hope indeed clarify point final version given opportunity,3
563.json,strength idea investigate type relation lexical item interesting challenging make good argument going beyond analogy testing make sense weakness justify otherwise contextualize choice clustering evaluation rather using classification task despite fact classification task straightforward evaluate attempt made explain overall level well human task given word context general discussion read response,3
563.json,investigates application distributional vector meaning task involve identification semantic relation similar analogical reasoning task mikolov given expression form france london approximated simple vector arithmetic operation london france argue simple method capture specific form analogy present measure aim identifying wider range relation effective admit find idea single vector space able capture number semantic relationship analogy rather radical infeasible mention number study already suggest opposite reason quite simple behind model lie form distributional hypothesis word similar context similar meaning pose certain limitation expressive ability example word like small always considered semantically similar vector perspective although express opposite meaning since occur similar context cannot example given figure relevant nature vector space semantic matter certain analogy king woman queen asking word space capture relationship form claw hence hospital wall make much sense motivation behind main proposal similarity measure involves form cross comparison vector word vector representing context word clearly explained measure tested relation category semeval task rather unsatisfactory almost case simple baseline take account partial similarity tested word pair present high performance difference best performing seems statistically insignificant methodological experimental perspective weakness current form seems describe work progress inclined presentation formatting issue latex style fixed case accepted response thank clarification still comfortable idea metric vector space try capture semantic relational similarity think present enough experimental evidence method work agree reviewer appropriate format work short,1
563.json,present comparison several vector combination technique task relation classification strength clearly written easy understand weakness main complaint significance contribution believe might suitable short certainly full length unfortunately little original thought significantly strong experimental back contribution similarity metric adapted previous work seem sensitive choice cluster majorly outperforms naive baseline number cluster exact value data beforehand think relation classification clustering semantic vector space model interesting challenging problem work might useful experimental nugget future reference vector combination comparison technique short unfortunately substance merit full length,1
447.json,proposed explore discourse structure defined rhetorical structure theory improve text categorization attention mechanism employed compute representation text experiment various dataset show effectiveness proposed method comment table show unlabeled performs better four five datasets full explain intuitively incorporating additional relation label bring benefit performance relation labelling hurt performance instead also transforms tree dependency structure process step instead transforming keep original tree structure train hierarchical experimental datasets instead comparing dataset previous work want experiment common datasets used previous work,3
369.json,detail method achieving translation morphologically impoverished language chinese morphologically rich one spanish step process first system translates simplified version target language second system chooses morphological feature generated target word inflects word based feature wish apply work language pair believe issue addressed work important addressed problem current system approach taken different many modern approach based character level model instead harkens back approach factored translation model koehn hoang translating morphologically rich language synthetic phrase chahuneau unfortunately uncited also rather suspicious fact present meteor bleu qualitative improvement bleu score rise perhaps could argue believe approach still plus back claim meteor example sentence furthermore repeatedly talk gender number linguistic feature seek correctly handle seem completely overlook person perhaps first second person pronoun verb rarely occur news certainly point least merit brief discussion also like discussion rescoring hurt gender accuracy good reranker learn keep best finally content good overall huge amount spelling grammar word choice style error render unfit publication current form dump error found overall like work future conference hopefully language pair evaluation metric proofreading general error dump line zhand zhang line whole related work section consistent cite newcite appropriate feel like filler important mention worth mentioning line popular phrase based system moses pbmt system general line software line academic commercial level definitely pluralized even level line morphology based simplified target make sound like simplified target us morphology perhaps mean morphologically simplified target line decide morphological simplification table extra space cuestin first line titulado last line table perhaps highlight difference line table somehow simplification carried simplifier hand written existing tool line line train train line architecture inspired collobert proposal inspires architecture line drop comma line equation make look like word share word vector line could also casas blancas right system choose form remind reader source side conditioning line graph lattice perhaps specifically sausage lattice line insert similiar producirse line misspelled syllable line like example clarity palabras llanas palabras estrjulas handle three special case line sentence longer word line mean determiner mean tool line sure line trained instead trained line corpus copora line size size line bigger embedding size help hardly unreasonable training time line seven five best value line increased table hyperparameters ordinary parameter line coverage exceeds line descending line quadratic line space cite line large margin instead large line line standard phrase based citation list year tool actually released,0
105.json,strength idea hard monotonic attention substantially different others weakness experiment morphological inflection generation somewhat mixed proposed effective amount training data small celex also effective alignment mostly monotonic le context sensitive russian german spanish general discussion proposed novel neural morphological inflection generation us hard attention character alignment separately obtained using bayesian method transliteration substantially different previous state neural task us soft attention character alignment conversion solved jointly probabilistic idea novel sound clearly written experiment comprehensive concern proposed method necessarily state condition suitable task mostly monotonic alignment le context sensitive phenomenon convincing describe practical merit proposed method ease implementation computational cost,2
105.json,strength encoder decoder proposed explicitly take account monotonicity weakness maybe ordinary birnn alignment coupled evaluated morphology monotone seqseq task general discussion propose novel encoder decoder neural network architecture hard monotonic attention evaluate three morphology datasets tough hand well written mostly clear also present novel idea namely including monotonicity morphology task reason including monotonicity pretty obvious unlike machine translation many seqseq task monotone therefore general encoder decoder model used first place still perform reasonably well considered strong argument neural technique general idea explicity enforce monotonic output character generation decoupling alignment transduction first aligning input output sequence monotonically training generate output agreement monotone alignment however unclear point question alignment look like hand alignment seem kind many running example input character aligned zero several output character however seems contrast description given line speak several input character aligned output character many many many many alignment actually quite simple approach monotone seqseq first stage align input output character monotonically many constraint monotone aligner toolkit jiampojamarn kondrak train standard sequence tagger predict exactly many alignment example flog fliege example first align tagger could lstm like predict sequence length sequence length approach suggested multiple paper reference could section question approach differ rather simple idea include baseline issue really pitty tested morphology many interesting monotonic seqseq task could shown system superiority evaluating given explicitly monotonicity also perform better seems general cognitive bias among researcher instance perform worse rest better think wording corrected otherwise fine experimental little linguistic feature infer include take feature possible responsible better performance case rather monotonicity constraint minor point equation please write text similar lower case many ldots know math community recommends write ldots cdots dot level surrounding symbol figure really necessary cyrillic font even address example font inproceedings schnober etal coling author schnober carsten eger steffen dinh erik gurevych iryna title still comparing traditional sequence sequence model encoder decoder neural network monotone string translation task booktitle proceeding coling international conference computational linguistics technical paper month december year address osaka japan publisher coling organizing committee page http aclweb anthology author response thanks clarification think alignment mixed response somehow maybe coding issue think aligning later make many many alignment know compare nicolai cherry kondrak question rather alignment schnober train neural tagger bilstm wonder much differed rather simple baseline tagger monotone start given monotone alignment everything stay monotone contrast start general hard monotonicity constraint note also quite relevant cohn http aclweb anthology architecture also related method like stack lstm similarly predicts sequence action modify annotate input think lose anything using greedy alignment contrast rastogi also hard monotonic attention sum alignment,2
26.json,question answering knowledge base cross attention combining global knowledge present approach factoid question answering knowledge graph freebase using neural attempt learn semantic correlation correspondence various aspect candidate answer answer type relation question entity answer semantic subset word question separate correspondence component learned aspect candidate answer contribution work creation separate component capture different aspect candidate answer rather relying single semantic representation incorporating global context candidate answer interesting aspect work opinion separation candidate answer representation distinct aspect give neural developer little control guiding model towards information beneficial decision making sort harkens traditional algorithm rely feature engineering case feature engineering aspect subtle le onerous encourage continue refining system along line high level idea fairly clear reasonably informed reader devil detail make hard audience immediately grasp insight work part could benefit explanation specifically context aspect candidate answer clearly explained therefore last sentence section seem unclear mention abstract introduction need explanation think current exposition assumes deep understanding prior work reader experiment conducted restrict comparison based system reasoning behind decision reasonable clear work yang described based part comparison including system comparison seem inconsistency compared additionally harm also mentioning comparable performance number best based system observe embeddings learned entirely training data wonder much impact random initialization embeddings performance interesting determine list variance additionally start trained embeddings wordvec instead randomly initialized one impact read possible direction future work occurred possibly include structured query based method part cross attention mechanism word addition using various aspect candidate answer feature could include structured query generate produce candidate answer additional aspect candidate answer attention mechanism could also focus various part structured query semantic match input question additional signal thought note regarding positioning hesitate call proposed attention model admittedly limited understanding attention mechanism apply encoder decoder situation semantics expressed structured form image sentence language natural language question encoded abstract representation generated another structured form caption sentence another language structured query attention mechanism allows encoder jump around attend different part input instead sequentially output generated decoder appear notion confusing broader audience thank clarification author response,3
26.json,strength contributes field knowledge base based question answering tackle problem retrieving structured based natural language question important challenging task clearly identify contribution novelty work provide good overview previous work performance comparison approach related method previous approach based represent question answer fixed length vector merely word limit expressiveness model previous work also leverage unsupervised training potentially help trained generalize make major innovative point question answering problem backbone architecture proposed approach cross attention based neural network attention used capture different part question answer aspect cross attention contains part benefiting attention part try dynamically capture different aspect question thus leading different embedding representation question attention part also offer different attention weight question towards answer aspect computing similarity score answer embeddings learnt task also modeled using transe allows integrate prior knowledge side experimental obtained question proposed approach exhibit better behavior state method contribution made particularly clear ablation experiment cross attention mechanism global information improve performance large margin contains content proposed framework quite impressive novel compared previous work weakness well structured language clear correct minor typo provided page column line read reread page column line pair pair general discussion equation four aspect candidate answer aspect share using separate aspect suggest considering giving name approach instead approach something like lstm something different table general think good idea capture different aspect question answer similarity cross attention based novel solution task experimental also demonstrate effectiveness approach although overall performance weaker based method integrated system think good attempt area encouraged,3
484.json,proposes joint attention utilizes advantage training decoding strength provides solid work hybrid attention framework training decoding experimental showed proposed method could provide improvement japanese mandarin chinese telephone speech recognition task weakness problem sound similar officially published coming ieee international conference acoustic speech signal processing icassp march proposes joint attention using english task proposes joint attention using joint decoding japanese chinese task guess difference joint decoding application japanese chinese task however difference clearly explained took sometimes figure original contribution title title joint attention based speech recognition using multi task learning title joint attention speech recognition think title general first joint attention absolutely remain published arxiv might still acceptable since officially publish ieee conference much earlier specified title represents main contribution contrast existing publication necessary introduction author claim propose take advantage constrained alignment hybrid attention based system training attach objective attention based encoder network regularization proposed taking advantage constrained alignment hybrid attention original idea whole argument attention based versus based necessary attention combination novel furthermore statement propose proposed somewhat weird build upon someone proposal additional extension propose people proposal therefore important state clearly original contribution position proposed method respect existing literature experimental applied proposed method english task applied proposed method japanese mandarin chinese task think interesting could explain detail specific problem japanese mandarin chinese task appear english task example system could address multiple possible output kanji hiragana katakana given japanese speech input without using linguistic resource could important contribution general discussion think better cite official ieee icassp conference rather published arxiv hori watanabe joint attention based speech recognition using multi task learning ieee international conference acoustic speech signal processing icassp march appear,2
715.json,strength focus challenging task answering open domain question wikipedia developed document retriever retrieve relevant wikipedia article question document retriever retrieve exact answer retrieved paragraph used distant supervision fine tune experiment show document reader performs better wikisearch document reader better recent model weakness final inferior model presented also error analysis provided general discussion proposed system interesting however concern document retriever shown better retrieval performance wiki search however described exactly used wikisearch good baseline querying question suit structured retrieval standard baseline distant supervision effective reliable distant supervision clearly avoid using many training example whatever example could fraction actually close correct statistic helpful understand fine tuning distant supervision could helped full wikipedia main said full system give performance correct given correct paragraph given clearly motivation work retrieval aspect webquestions much inferior yodaqa raise question whether wikipedia sufficient answer open domain question think integrated address overall final shown table inferior model wikipedia indicative best strategy point value table different table test table femb error analysis error analysis required various component system specific type question system perform well choose question good candidate answered wikipedia method question webquestions degrades performance,2
216.json,strength idea assigning variable length document segment dependent topic novel prior knowledge worth incorporated based framework whereas full knowledge recent literature find part related work quite convincing method proposed segment sampling complexity impressive crucial efficient computation weakness compared balikas coling work weaker visualization make doubt actual segmenting assigning document could convincing give longer exemplar make color assignment consistent topic listed figure since flexible balikas coling underfitting could please explain general discussion well written structured intuition introduced abstract exemplified introduction quite convincing experiment full range solid achieves better quantitative previous work visualization part stronger explained le powerful visualization confident another concern computation efficiency since seminal work proposed variational inference faster training compared mcmc wish author future development,3
216.json,strength well written well organized incorporate topical segmentation copula enable joint learning segmentation latent model experimental setting well designed show superiority proposed method several different indicator datasets weakness comparison novel segmentation method general discussion present segldacop joint latent topic segment based copula incorporates topical segmentation copula conduct comprehensive experiment using several different datasets evaluation metric show superiority well written well organized proposed reasonable extension copula enable joint inference segmentation topic experimental setting carefully designed superiority proposed fairly validated concern simple segmentation single word segmentation segment previous method noted many work smartly generate segment running though largely affected bias statistical linguistic tool used comparison novel state segment preferable precisely show validity proposed method minor comment line latent radom topic latent random topic,3
67.json,strength tackle important issue building ontology thesaurus method make sense seem well chosen method setup well detailed look like outperform state approach concern weakness main weakness evaluation overall presentation writing list baseline hard understand method really seem justified show mpttern memb apparently previous state mention reference look like method outperforms previous best performing approach convincing enough especially first dataset difference system previous state pretty small seriously lack proofreading could published fixed instance noted error first column page ciline hierarchy shallow level however apparently used past expect deeper difficult branch hyponym hypernym explain high obtained even previous study general discussion approach really original novel applied problem addressed deep learning reason think interesting main flaw first easiest presentation many error typo need corrected started listing help many second issue evaluation opinion technically performance better feel convincing explained memb method shwartz maybe performance recent approach think need reorganize evaluation section order properly list baseline system clearly show benefit approach others fail significance test also seem necessary given slight improvement dataset,1
169.json,strength useful application teacher learner support fine grained comparison system weakness highly superficial description system evaluation satisfying general discussion present approach automatically enriching output system error type useful application teacher learner benefit information many system output corrected version without making type error explicit also allows finer grained comparison system term precision general error type specific figure recall precision unfortunately description system remains highly superficial core system consists manually created rule provide detail rule show example rule specify number rule tell complex ordered could early rule block application later rule instead presenting relevant detail system several page devoted evaluation system participated conll table take entire page list system text repeat many fact figure read table evaluation proposed system satisfying several aspect first annotator independently annotated gold standard test sentence instead simply rating output system given fixed tag possible produce gold standard rather small test sentence highly probable approach taken yield considerably better rating annotation comparison real gold standard marcus comparison agreement reviewing annotated data annotating scratch second said raters individually considered least rule based error type either good acceptable multiple rate considered individually rating averaged common practice score assigned different edits learn distribution edits considered annotator sound much worse average calculated third information test data provided many error category contain error category covered according cateogories rated good annotator forth mean edit boundary might unusual precise description plus example need could problematic application system state system le domain dependent compared system need training data sure true suppose hunspell vocabulary probably cover domain detail manually created rule domain dependent well completely language dependent clear drawback compared machine learning approach moreover test data used test conll domain student essay remains unclear error category designed reason tag given able search easily underspecified category like noun general seems tagset presented nicholls support search well using conll tagset conll gold standard could used evaluation main motivation remains somewhat unclear system important detail left error category hardly motivation discussion provided evaluating conll system presentation remains superficial typo others others superscribed mean check reference incorrect case fleiss kappa,1
169.json,present novel approach evaluating grammatical error correction system approach make possible ass performance system error type term recall also term precision previously possible general since system output usually annotated error category strength proposed evaluation important stepping stone analyzing system behavior includes evaluation variety system approach several advantage previous work computes precision error type independent manual error annotation ass performance multi token error automatically selected error tag computed error span mostly approved human expert weakness part rule derive error type described classifier evaluation lack thorough error analysis based upon lack direction future work improve classifier evaluation performed english unclear difficult approach another language classifier classifier evaluation unclear basis error category devised based previous work although approach general independent alignment algorithm rule probably provide detail error category major part reader least glimpse rule assign error type look like unfortunately apply proposed evaluation language english also elaborate change necessary classifier language assume rule used determining edit boundary well determining error tag depend language processing pipeline certain extent therefore need adapted also error category might need changed provide detail rule assigning error category many overall error type complex estimate effort necessary approach another language error span computed processing step seem inherently continuous also case scorer problematic since error tagged accurately error span discontinuous german example verb separable prefix separated main clause constituent verb constituent verb prefix classifier able discontinuous edit span write human judge rated least automatically assigned error tag appropriate despite degree noise introduced automatic edit extraction cautious judgment since raters might also forgiving boundary noisy addition asked select without knowing system output could case noisy boundary biased towards system output additionally rating option appropriate appropriate might also raters select appropriate make evaluation sound also evaluate human judge rate classifier output boundary manually created without noise introduced faulty boundary classifier evaluation lack thorough error analysis mentioned usually traced back wrong question like addressed raters select appropriate rating expert point possibility improve classifier gold reference auto reference unclear data significance test performed exactly test score think good idea since derived measure weak discriminative power performance term recall precision totally different score also beginning section refer mismatch automatic reference term alignment classification tell comparison gold reference term boundary term classification error type evaluation think surprising team line failed correct unnecessary token error least system straightforward explanation cannot handle superfluous word obvious rule base approach work tag possible determine superfluous word based alone rozovskaya roth provide explanation performs poorly superfluous word analyze comment table respect whether system designed handle error type error type straight forward mapping error type gold standard auto reference example word order error remains unclear whether system failed completely specific error type designed correct cuui example reported precision recall although target word order error cuui case probably similar case also point error classification neither analyzed discussed please report also value appendix table make easier compare system using measure also seems error type system table based instance also made clear reporting value write team iitb achieved best score least category suggests different approach complement different error type nice mention line previous research multi token error analysis helpful future work result need interpretation system probably inherently unable correct error none system trained parallel corpus learner data fluent sense sakaguchi correction mentioned approach impossible provide error annotation system submodules error type admittedly system need adapted include submodule responsible change system output still proposed approach enables compare system producing error tagged output straightforward system unified reference title lack capitalization sakaguchi need wrapped page information missing efron tibshirani author response agree approach fatally flawed think review actually point quite positive aspect approach good ready basis rule classifying error lack description major factor matter additional example rule seen implementation need described replicable adapt generalization language afterthought serious limitation approach worked language design even perform adaption language approach transparent enough others estimate much work adaptation well could reasonably work stating research targeted reinforces problem write error type certain system tackle usually obvious table think simple cuui example mentioned well unnecessary token error five system correct table therefore obvious tackle however write also obvious explanation team difficulty error type,1
66.json,describes several way encode arbitrarily long sequence digit using something called major system major system digit mapped character representing consonantal phoneme possible mapping digit phoneme predefined output encoding typically sequence word constrained digit original sequence correspond character digraph output sequence word added surrounding consonant phoneme form word unconstrained describes several way encode sequence digit output sequence word memorable generally applying syntactic constraint heuristic found application natural language processing concept somewhat interesting read topic however found idea presented rather school feel much focus gram model generation frequent sequence heuristic really could written year sure enough novelty idea warrant publication contribution term modeling search convincing contribution application area instance constrained generation since start sequence output another sequence straightforward monotonic mapping seems like character based sequence sequence encoder decoder sequence sequence learning neural network sutskever work rather well likely fluent output fewer moving part trigram model scoring heuristic postprocessing bigram large amount training arbitrary genre need rely already tagged corpus like worry parser,1
66.json,strength present sentence based approach generating memorable mnemonic number evaluation study presented show sentence based approach indeed produce memorable mnemonic short digit number officiate wasteland overall present problem background literature solution sufficient detail memorizing number phone number account number sufficiently common interesting problem weakness proposed solution seem scale well longer number seems work well digit number though many number people need memorize phone number credit card number longer digit besides number structure phone number country code area code personal number people exploit memorizing number stated address important problem current solution need improved several idea listed section general discussion current presented approach comparison existing approach promising,2
201.json,strength present array accuracy based systematically changing parameter embeddings model context type position sensitive embedding task accuracy context type linear syntactic position sensitive true false embedding skip gram glove task word similarity analogy chunking text classific task experiment investigate variation performance parameter changed goal study interesting community similar paper appeared workshop paper well cited nayak mentioned weakness since essentially present effect systematically changing context type position sensitivity focus execution investigation analysis afraid satisfactory lack hyper parameter tuning worrisome unless otherwise note number word embedding dimension still enlarges context vocabulary time practice hyper parameter levy best configuration worrisome lack hyperparameter tuning make difficult make statement like method better method bound method perform better lower dimensionality unbound model since effective context vocabulary size larger sometimes present strange explanation experimental suggest although hard find universal insight characteristic different context different model concluded according specific task sentence even mean sequence labeling task tend classify word syntax category ignorance syntax word embeddings learned bound representation becomes beneficial sentence contradictory sequence labeling task classified word syntax category syntx becomes valuable feature bound representation ignorance syntax cause drop performance like task happen enough merely mention also done systematic study word embeddings similarly evaluating word embeddings using representative suite practical task nayak angeli manning appeared repeval workshop cited understand focus nayak exactly however provide recommendation hyperparameter tuning experiment design even provide interface automatically running tagging experiment using neural network instead simple linear classifier used current us neural word classifier text classification task simple linear classifier sequence labeling task justification choice classifier simple neural classifier tagging task well raise point since tagging task seems task bound representation consistently beating unbound representation make task general discussion finally make speculative suggestion regarding analysis data said earlier main contribution analysis following table context type position sensitive embedding task accuracy essentially accuracy value want explain term aspect beneficial perform factor analysis pattern mining technique sample data,1
201.json,strength systematically investigated context type linear dependency based representation bound word unbound word affect word embedding learning experimented three model generalized word generalized skip gram glove multiple different task word similarity word analogy sequence labeling text classification overall well written structured experiment thoroughly evaluated analysis could help researcher choose different word embeddings might even motivate model attached software also benefit community weakness novelty limited general discussion dependency based context type dependency parsing affect overall performance fair compare different context type since dependency based rely predicted dependency parsing case corenlp linear,3
256.json,review extends line work model generation dialogue sequence sequence generation problem past utterance dialogue context encoded context vector plus potential hand crafted feature decoded response turn dialogue stand model tend suffer lack diversity specificity local coherence kind response tend produce trained large dialogue datasets containing many topic cornell opensubtitles ubuntu rather attempting produce diverse response using decoder word word beam search shown work well even lose crucial information grammar valid sequence different objective function work introduce latent variable probability distribution induced part network prediction time encoding utterance context sampled decoder greedily used generate response evaluation show small improvement bleu score vanilla seqseq involve learning probability distribution context sampling certainly impressive technical point view application deep learning method specifically conditioned variational auto encoders problem response generation attendant difficulty training model information retrieval technique reference response also interesting conceptual comment introduction motivation behind work architecture evaluation write turn comment introduction motivation seem fully aware long history field various facet whether theoretical perspective applied dialogue manager typically take utterance dialogue context input generates discourse level decision accurate traditionally least dialogue manager select action dialogue act particular dialogue context action chosen passed separate generation module realisation dialogue management usually done context task based system goal driven dialogue manager choose action optimal sense reach goal book restaurant step possible publication lemon pietquin rieser keizer colleague various publication steve young milica gasic colleague overview large literature reinforcement learning model task based dialogue system need make clear distinction task based goal oriented dialogue chatbots social bot latter usually language albeit sophisticated though required type system usually distinct whereas former required complete task latter perhaps required keep user engaged indeed data driven method used build system usually different refer open domain conversation suggest thing open domain conversation conversation always context activity achieving something specific world overarching goal overarching activity overarching genre determines outward shape dialogue determines sort dialogue structure coherent coherence activity context specific indeed human capable open domain dialogue faced conversational topic genre never participated embarrass utterance look incoherent place others already familiar think random person street trying follow conversation coffee break fundamental problem system attempt data extremely diverse open ended conversational genre movie subtitle order train mushing everything together emerges good grammatical structure generic response comment architecture rather generate single encoded context induce distribution possible context sample generate greedily decoder seems general counter intuitive go evidence linguistic psycholinguistic literature dialogue literature show people tend resolve potential problem understanding acceptance locally make sure agree context conversation move rest conversation given point little uncertainty current context conversation massive diversity see diversity conversation actually trying achieve diversity topic context given fixed context multitude possible next action coherent leading conversation different path therefore seems strange least shift burden explaining diversity coherence follow action linguistic verbal surface context uttered though course uncertainty also arise result mismatch vocabulary grammar concept people background probably explain much variation follow response fact least task based dialogue system concerned challenge capture synonymy context dialogue distinct surface lead similar context either virtue interactional syntactic equivalence relation synonymy relation might hold particular domain word sequence word destination like flight booking domain bordes weston kalatzis eshghi lemon latter grammar cluster semantically similar dialogue comment evaluation seek show generate coherent diverse response evaluation method though interesting seems address coherence diversity despite section precision recall metric measure distance ground truth utterance one generates generated utterance unless misunderstanding evaluation method measure diversity counting number distinct gram generated response furthermore sure increase bleu score meaningful small qualitative assessment generated response certainly see diversity contentful utterance example provided frequent case fact also made stronger meaningful compared work different method promote diversity using different objective function fact mention characterise properly despite actually referring,3
256.json,present neural sequence sequence encoding dialog context followed decoding system response open domain conversation introduced conditional variational autoencoder cvae deep neural network based generative learn latent variable describing response conditioning dialog context dialog act proposed model achieved better performance baseline based encoder decoder without latent variable quantitative qualitative evaluation well written clear description theoretically sound idea reasonable comparison also detailed analysis minor comment follows possible provide statistical significance proposed model compared baseline quantitative evaluation difference seem much metric considering importance dialog kgcvae tagging performance affect quality final possibility achieve improvement using better tagger recently deep learning model achieved better performance also tagging think human evaluation part qualitative analysis could costly worth analyze pragmatic perspective future direction could also interesting kgcvae applied task oriented human machine conversation usually much richer linguistic feature available open conversation table blue recall need corrected bleu recall,4
606.json,introduces approach semantic parsing equipped neural sequence sequence seqseq referred programmer encodes natural language question produce program programmer also equipped variable memory component store entity question value intermediate variable formed execution intermediate program variable referred build program also equipped certain discrete operation argmax next edge separate component interpreter computer executes operation store intermediate value explained since programmer inherently seqseq interpreter computer also act syntax type checker allowing decoder generate valid token example second argument operation predicate finally trained weak supervision directly optimizes metric used evaluate performance score discrete operation differentiable reward function trained policy gradient reinforce since gradient obtained reinforce high variance common first pretrain likelihood objective find good sequence action trained auxiliary objective take latter approach find good sequence iterative maximum likelihood approach discussion section presented nice achieves sota webquestions dataset compared weakly supervised written clearly easy follow present exciting direction scope future research direction definitely love presented conference question important one first another alternative training bootstrap parameter theta iterative method instead adding pseudo gold program beam line deleted think work baseline reinforce separate network predicts value function must discussed detail program required multiple operation limited single hop provide example understand bound word limit response give example filter operation used follow motivation behind replacing entity question special symbol minor comment line describe describing line decoder read decoder generates,4
606.json,introduces neural symbolic machine nsms deep neural equipped discrete memory facilitate symbolic execution includes three component manager provides weak supervision learning differentiable programmer based neural sequence sequence encodes input instruction predicts simplified lisp program using partial execution stored external discrete memory symbolic computer executes program provide code assistance programmer prune search space conduct experiment semantic parsing task webquestionssp show able language compositionality saving reusing intermediate execution augmented reinforce superior vanilla reinfroce sequence prediction problem trained weak supervision able outperform existing sate method stagg strength idea using discrete symbolic memory neural execution model novel although implementation simply reduce copying previously executed variable token extra buffer approach still impressive since work well large scale semantic parsing task proposed revised reinforce training schema using imperfect hypothesis derived maximum likelihood training interesting effective could inspire future exploration mixing training neural sequence sequence model scale experiment larger previous work modeling neural execution program induction impressive generally clear well written although point might require clarification key variable token involved computing action probability conflicting notation used refer variable memory key overall like like conference weakness choice dataset webquestionssp testbed using popular webquestions berant benchmark since requires weak supervision using webquestions intuitive straightforward plus could facilitate direct comparison main stream research analysis compositionality contribution work usage symbolic intermediate execution facilitate modeling language compositionality interesting question well question various compositional depth handled simple question easiest solve complex multi one require filtering operation argmax highly trivial present detailed analysis regarding performance question set different compositional depth missing reference find relevant paper field missing example cite previous based method knowledge based semantic parsing berant liang sequence level reinforce training method ranzato closely related augmented reinforce neural enquirer work us continuous differentiable memory modeling neural execution misc reinforce algorithm randomly initialized algo instead using parameter trained iterative server figure,3
554.json,strength trying bridge stochastic gradient mcmc stochastic optimization deep learning context given dropout dropconnect variational inference commonly used reduce overfit systematic introduce analyse bayesian learning based algorithm benefit deep learning community language modeling task proposed mcmc optimizer dropout outperforms rmsprop dropout clearly show uncertainty modeling help reducing fitting hence improving accuracy provided detail experiment setup easily reproduced weakness theory prof show convergence property proposed algorithm show comparison mcmc rmsprop conduct comparison explain relation psgld rmsprop mentioning conterparts family talk training speed impact detail general discussion,3
554.json,strength explores relatively explored area practical application idea behind bayesian neural net task bayesian treatment parameter rnns possible incorporate benefit averaging inference gradient based sampling approximation posterior estimation lead procedure easy implement potentially much cheaper well known technique averaging like ensembling effectiveness approach shown three different task language modeling image captioning sentence classification performance gain observed baseline single optimization weakness exact experimental setup unclear supplementary material contains important detail burn number epoch sample collected main moreover detail inference performed helpful sample taken following certain number epoch burn training data fixed inference every tilda test time sample used according also explicit clarification regarding independence assumption theta theta theta let conditional understand correctly potential theta nice completeness term comparison also greatly benefit discussion experimental comparison ensembling distillation method sequence level knowledge distillation rush distilling ensemble greedy dependency parser parser kuncoro intimately related similar goal incorporating effect averaging discussion related preference related sampling method sampling method variational approximation helpful finally equation hint potential equivalence dropout proposed approach theoretical justification behind combining sgld dropout making equivalence concrete lead better insight effectiveness proposed approach general discussion point addressed,3
104.json,address problem disambiguating linking textual entity mention given background knowledge base case english wikipedia title introduction little overblown misleading since bridging text knowledge task core part overall task nonetheless method perform bridging intermediate layer representation namely mention sens thus following step mention mention sense mention sense entity various embedding representation learned word mention sens entity jointly trained maximize single overall objective function maximizes three type embedding equally technically approach fairly clear conforms current deep processing fashion known best practice regarding embeddings suggest kind alternative clear make material difference rather comment focus basic approach explained however exactly step process involving mention sens better simple direct step mapping word mention entity approach yamada called align algorithm table show step mpme even simplification spme better exactly exact difference additional information mention sens compareed entity understand please check following correct perhaps update make exactly clear going entity profile consist neighboring entity relatedness graph graph built assume looking word level relatedness entity definition page wikipedia profile extended skip gram based embeddings word profile standard distributional semantics approach without sense disambiguation mention sens profile standard distributional semantics approach sense disambiguation sense disambiguation performed using sense based profile language local context word neighboring mention mentioned briefly section without detail problem point approach exactly sens created differentiated defines many sens mention string done looking knowledge base bijective mapping mention sens entity exactly entity mention sense even entity case sense collection definitional profile built starting entity text seed word information used mention sense level used entity level exactly word text reliably associate mention sense occur equivalent entity webpage wikipedia many word average mention sense powerful necessary keep extra differentiation information separate space mention sense space opposed loading additional word entity space adding word wikipedia entity page understanding essentially correct please update section main information true say section first work deal mention ambiguity integration text knowledge representation exact baseline comparison evaluation past year hosted task involving eight nine system performing exactly task albeit freebase considerably larger noisy wikipedia please http positive note really liked idea smoothing parameter section post response read response really satisfied reply evaluation relevant interested goodness embeddings instead fact evaluate goodness application really care conceptually elegant embedding question perform better,3
387.json,strength deep framework proposed extract combine cognitive feature textual feature sentiment analysis sarcasm detection idea interesting novelty weakness replicability important concern researcher cannot replicate system method improvement lack data feature extraction general discussion overall well written organized experiment conducted carefully comparison previous work analysis reasonable offer comment follows suitable sarcastic sarcastic utterance provide detail analysis movement data useful sarcastic sarcastic sentiment classification beyond textual feature provide explanation,2
503.json,tough call think important salvageable technial notably parsing algorithm whole little cohesion united around overarching view formal language language probabilistic treated formal property variety closed intersection opinion mean formal language probabilistic view considered sufficient rigor viewpoint compelling note value formal provided mostly depend flimsiness overarching story research badly written need work find particulary puzzling organization leaf little space elucidating parsing result soundness completeness relegated continuation form supplementary note also find mention probabilistic language title disingenuous fact probabilistic reasoning submission sigificance intersection closure result section also somewhat overstated think unless something understanding restriction right hand side rule case please elaborate merely matter folding finite intersection terminal label,1
503.json,concerned finding family graph language closed intersection made probabilistic strength introduction show relevance overall high level context nice read motivation clear interesting extremely clear requires close reading much formal background nicely take account certain difference terminology interesting hyper edge grammar generalize familiar grammar earley algorithm example predict applies nonterminal edge scan applies terminal edge parsing validation context clarified useful formally correct nice contribution instructive give idea researcher described algorithm used semantic parsing hypergraphs produced another parser restricted method part machinery natural language parsing thus relevant weakness reranking mentioned introduction great news context earley parser linear time grammar unlike special kind formal language grammar unfortunately result involves deep assumption grammar kind input linear complexity parsing input graph seem right deterministic grammar recognise fact input string usually give rise exponential number graph word parsing complexity result must interpreted context graph validation want find derivation graph example purpose graph transduction synchronous derivation clear random reader miss difference semantic parsing string parsing semantic par current work seem control linear order arity edge might useful mention parser extended string input find best hypergraph given external node item representation subgraphs must also keep track covered arity edge make string parser variant exponential easily correctable typo textual problem line misleading intersection probs true distribution cannot refer discussion line think rather talk validation recognition algorithm parsing algorithm parsing mean usually completely different thing much challenging lexical structural ambiguity line unclear element attg sense pairwise distinct compare example extg attg disjoint set move rank definition earlier remove redundancy rather immediately derives perhaps give example nontrivial internal path define subgraph hypergraph since proposition want tell contribute quoted table axiom place introduced axiom link text say trigger general discussion might useful tell msol graph language yield context free string language happens grammar ambiguous deterministic exponential number par even input graph lexical ambiguity reason parser behave given earley recogniser actually strictly polynomial even synchronous derivation semantic graph miss linguistic phenomenon semantic distinction expressed different linguistic mean language affix verb another language express distinction changing object suggesting although increase language independence par cross lingual challenge fully understand role marker subgraphs elided later really used already started miss remark line point seems normal order unique confirm nice cond introduces lexical anchor prediction compare anchor lexicalized grammar sure crossing link occur parsing linearized sentence semantic graph significant question linear complexity parsing input graph seem right deterministic grammar recognise fact input string usually give rise exponential number graph word parsing complexity result must interpreted context graph validation want find derivation graph example purpose graph transduction synchronous derivation parsing complexity case deterministic possibly ambiguous regular tree grammar interested assign tree frontier string like context free grammar adapt given earley algorithm purpose guessing internal node edge although question might seem like confusion relevant context prevents rggs generate hypergraphs whose arity edge word linearised principle determines linearised linear order determined earley path normal order used production consider actual word order string natural language clear connection context free string language set projective dependency graph used semantic parsing written line misleading line mention hrgs used generate context free language graph language string language expert interpret implicit fact rggs generate context free language mean graph noncrossing graph sense kuhlmann jonsson,2
145.json,review multimodal word distribution strength overall strong weakness comparison similar approach could extended general discussion main focus introduction learning multimodal word distribution formed gaussian mixture multiple word meaning representing word many gaussian distribution approach extend introduced vilnis mccallum represented word unimodal gaussian distribution using multimodal current approach attain problem polysemy overall strong well structured clear experimentation correct qualitative analysis made table show expected approach much faulted comment meant help gain additional clarity comment interesting include brief explanation difference approach tian current split single word representation multiple prototype using mixture missing citation could mentioned related work efficient parametric estimation multiple embeddings word vector space neelakantan shankar passos mccallum emnlp multi sense embeddings improve natural language understanding jurafsky emnlp topical word embeddings chua aaai also inclusion result approach table could interesting question attribute loss performance analysis swcs read response,3
145.json,work us gaussian mixture represent word demonstrates potential capturing multiple word meaning polysemy training process done based margin objective expected likelihood kernel used similarity word distribution experiment word similarity entailment task show effectiveness proposed work strength problem clearly motivated defined gaussian mixture much expressive deterministic vector representation potentially capture different word meaning mode along probability mass uncertainty around mode work represents important contribution word embedding work propose margin learning objective closed form similarity measurement efficient training mostly well written weakness question general discussion gaussian mixture model number gaussian component usually important parameter experiment criterion select increase hurt performance learned distribution look like word popular meaning notice spherical case experiment covariance matrix reduces single number purely computation efficiency wonder performance using general diagonal covariance matrix since general case gaussian mixture defines different degree uncertainty along different direction semantic space seems interesting minor comment table referred text reference luong lack publication year read response,3
779.json,proposes novel strategy zero resource translation source pivot pivot target parallel corpus available teacher target pivot first trained pivot target corpus student target source trained minimize relative entropy respect teacher source pivot corpus using word level relative entropy sample teacher approach shown outperform previous variant standard pivoting well zero resource strategy good contribution novel idea clearly explained convincing empirical support unlike previous work make fairly minimal assumption nature system involved hence widely applicable suggestion experiment first interesting robust approach dissimilar source pivot language intuitively true target source target pivot apart second given success introducing word based diversity surprising sentence best sentence sampling experiment costly much since already beam search teacher finally related previous might interesting explore transition word based diversity sentence based student converges longer need signal probability word comment line despite simplicity simplicity target sentence target word assume mean compare probability probable probable word current context current context determined greedily beam section comparison essentially uniform distribution seem informative extremely surprising significantly closer uniform interesting know extent still provides useful signal get better easy measure comparing model trained different amount data different number iteration another useful thing explore section effect mode approximation compared best sentence level score word beam worse word greedy since word beam closer word sampling explanation claimed advantage sent beam look like noise given high variance curve,3
779.json,present method training zero resource system using training data pivot language unlike approach mostly inspired author approach step decoding instead teacher student framework teacher network trained using pivot target language pair student network trained using source pivot data teacher network prediction target language strength present show idea promising also present several set validate assumption weakness however many point need address ready publication crucial information missing flesh clearly training decoding happen training framework found equation completely describe approach might useful couple example make approach clearer also montecarlo sampling done organization well organized example broken several subsection better presented together organization table confusing table referred table made difficult read inconclusive reading section difficult draw conclusion point comparison explained total size corpus involved method useful information appreciate fleshing assumption find dedicating whole section plus experimental space general discussion observe word level model tend lower valid loss compared sentence level method valid compare loss different loss function notation clear script mean never explained deserves explanation better removed approach talk mean nitty gritty import important inline citation style significantly outperform assumption need rewritten target sentence close counterpart,2
684.json,present gated attention mechanism machine reading idea extend attention reader kadlec multi reasoning fine grained gated filter interesting intuitive machine reading like idea along significant improvement benchmark datasets also major concern published proposed mechanism look promising enough convince importance technique state system engineering trick presented boost accuracy blended result incomplete bibliography nearly published work reference section refers arxiv preprint version make future reader suspicious work thoroughly compare prior work please make complete published version available result unpublished work baseline table mentioned previous work unpublished preprint think necessary alternately like author replace vanilla variant proposed baseline make sense result preprint submission presented manuscript fair blind review search arvix archive though conflict table table table table mean actually reader clear implementation assumed table us also glove initialization token attention seem wish proposed method compared prior work related work section differ related work show benefit gated attention translates multi architecture impressive great qualitative example comparison,3
684.json,present interesting reading comprehension depicting multiplicative interaction query local information around word document proposed gated attention strategy characterize relationship work quite solid almost state result whole four cloze style datasets achieved improvement helpful similar task nevertheless concern following aspect referred many paper arxiv think really related work included work caiming xiong http openreview rjekjwvclx work form shuohang wang http openreview qpqxl concentrated enhancing attention operation modeling interaction document query although work evaluated cloze style corpus squad experimental fundamental comparison necessary study adopts attention mechanism variant specially designed reading comprehension task work actually share similar idea suggestion conduct comparison work enhance experiment,2
684.json,strength well written every aspect well motivated clearly explained extensively covered previous work area approach achieves state across several text comprehension data set addition experimental evaluation thorough weakness different variant achieve state performance across various data set however provide explanation size data text anonymization pattern general discussion describes approach text comprehension us gated attention module achieve state performance compared previous attention mechanism gated attention reader us query embedding make multiple pass multi architecture document applies multiplicative update document token vector finally producing classification output regarding answer technique somewhat mirror human solve text comprehension problem show approach performs well large data set daily mail data additional feature engineering needed achieve state performance overall well written novel well motivated furthermore approach achieves state performance several data set minor issue evaluation experimental section mention whether improvement table statistically significant test used value also could find explanation performance data validation performance superior test performance significantly worse,3
562.json,strength zero shot relation extraction interesting problem created large dataset relation extraction question answering likely useful community weakness comparison credit existing work severely lacking contribution seen particularly novel general discussion perform relation extraction reading comprehension order train reading comprehension model perform relation extraction create large dataset querified converted natural language relation asking mechanical turk annotator write natural language query relation schema reading comprehension adding ability return relation original must always return answer main motivation result appears perform zero shot relation extraction extracting relation seen test time well written idea interesting however insufficient experiment comparison previous work convince contribution novel impactful first missing great deal related work neelakantan http arxiv perform zero shot relation extraction using rnns path verga http arxiv perform relation extraction unseen entity cite bordes http arxiv collect similar dataset perform relation extraction using memory network commonly used reading comprehension however merely note data annotated relation level rather triple relation entity pair level bordes done annotation significant difference made clear also naacl http aclweb anthology performs relation extraction using based memory network sure work similar much work really cite establish novelty least early introduction early wondering work differed made clear second neither evaluate another dataset evaluate previously published model dataset make empirical extremely weak given wealth existing work performs task lack novelty work need include experiment demonstrate technique outperforms others task otherwise show dataset superior others since much larger previous allow better generalization,1
562.json,model relation extraction problem reading comprehension extends previously proposed reading comprehension extract unseen relation approach main component queryfication converting relation natural question crowdsourcing part applying generated question sentence answer span extend previously proposed approach accommodate situation correct answer sentence comment read well approach clearly explained opinion though idea using relation extraction interesting novel approach novel part approach crowdsourced part taken directly previous work mention relation extraction well studied problem plenty recently published work problem however compare method previous work raise suspicion effectiveness approach seen table performance number proposed method core task convincing however maybe dataset used hence comparison previous method actually help ass current method stand state slot filling data preparation took first sentence contain answer sentence relation entity first sentence entity wikipedia article please clarify following rule locate answer sentence corresponding entity property wikipedia page daniel weld open information extraction using wikipedia proceeding annual meeting association computational linguistics association computational linguistics overall think present interesting approach however unless effectiveness approach demonstrated comparing recent work relation extraction ready publication,1
108.json,strength well written except place described problem tackle useful proposed approach multigraph based variant empirical result solid weakness clarification needed several place section addition description previous need point issue motivate propose section reason separator introduced additional info convene beyond section seem provide useful info regarding superior discussion section abstract insight better provide example spurious structure general discussion present detecting overlapping entity text improves previous state experiment benchmark datasets clear work better,2
108.json,suggests approach based multigraphs several edge link node detecting potentially overlapping entity strength problem could rather interesting especially crossing entity decide might actually mentioned text technique seems work although empirically show dramatic effect like word spent efficiency compared previous system general well written also need polishing detail minor remark weakness problem really well motivated important detect china entity within entity bank china example introduction point crossing entity case nested entity could much motivated make reader interested approach important detail missing opinion decision criterion include edge line several different option node mentioned never clarified edge present empirical evaluation achieved better previous approach really large margin really call slight improvement outperformed done effect size really matter user improvement percentage point actual effect observe many important entity discovered discovered previous method furthermore performance simplistic dictionary based method achieve could also used find overlapping thing similar direction commercial system like google cloud also able detect link entity achieved datasets also contrast existing commercial system result discussion liked emphasis actual crossing entity performance opinion interesting subset overlapping entity nested one many crossing entity detected possible one missed maybe performance improvement better nested detection also detecting crossing entity general error discussion comparing error made suggested system previous one also strengthen part general discussion like problem related named entity recognition point recognizing crossing entity however interested nested entity hand really motivate scenario also shed light point evaluation discussing error maybe advantage example case emphasis crossing entity compared approach possibly convinced lukewarm maybe slight tendency rejection seems another without really emphasizing opinion important question crossing entity minor remark first mention multigraph reader benefit notion multigraph short description previously noted many previous sound little solving task italic time linear sentence length really matter whether linear cubic spurious structure introduction clear meant regarded chunk chunking noun phrase chunking since pervious previous roth following five type sentence large number spell small one please type state state hyper graph later state seems used analogous node place comma enumeration item page period last child node hypergraph figure obvious first glance hypergraph color visible printing node edge gray also obvious highlighted edge selected others gray entity detected example figure difference knowing long denoting sometimes bracket sometimes please place footnote directly front punctuation mark afterwards footnote missing edge determined missing whether separator defines determined mention hypergraph last paragraph represent entity separator edge chosen algorithmically comma equation find sound little extract entity footnote make sound conduct something like nested crossing remark footnote good favor crossing example clarify combination state alone simple first order assumption previous section demonstrated shown used experiment distinct interpretation published website statistic dataset shown allows make omit tried follow close tried feature suggested previous work close possible following roth please reference noun following roth using bilou scheme highlighted bold effect size significantly better sense effect size genia dataset genia dataset outperforms point call outperform genia dataset recall insufficient table score seems rather similar outperform seems stretch confident increase recall converge mention hypergraph reference title lowercased others,1
333.json,strength propose selective encoding extension sequence sequence framework abstractive sentence summarization well written method clearly described proposed method evaluated standard benchmark comparison state tool presented including significance score weakness detail implementation system compared work need better explained general discussion major review wonder summary obtained using proposed method indeed abstractive understand target vocabulary build word appear summary training data given example shown figure impression summary rather extractive choose better example figure give statistic number word output sentence present input sentence test set page line understand mathematical difference vector still feeling great overlap represent meaning indeed necessary trying using neural network library implementing system detail implementation page section training data used system compare train minor review page line although difference abstractive extractive summarization described section could moved introduction section point user might familiar concept page line please provide reference passage approach achieves huge success task like neural machine translation alignment part input output required page section last paragraph contribution work clear think emphasize selective encoding never proposed true related work section moved method section figure table show example abstractive summarization think enough called figure table section line please provide reference following passage sequence sequence machine translation encoder decoder responsible encoding input sentence information decoding sentence representation generate output sentence previous work apply framework summarization generation task figure seems described page line sigmoid function element wise multiplication defined formula section page first column many element formula defined equation equation equation equation page line readout state depicted figure workflow table mean section parameter training explain achieved value many parameter word embedding size hidden state alpha beta epsilon beam size page line remove word line optimizing algorithm instead optimizing algorithm page beam search please include reference beam search figure typo true sentence council europe slam french prison condition typo supper script superscript time,3
333.json,strength clear well written proposes novel approach abstractive sentence summarization basically sentence compression constrained word output present input excellent comparison many baseline system thorough related work weakness criticism minor best report rouge score three datasets reason reporting recall understandable summary length case could simply report recall score related work come earlier could discussion context work summary compression intended used needed general discussion rouge fine ultimately want human evaluation compression readability coherence metric extrinsic evaluation,3
333.json,present neural approach summarization build standard encoder decoder attention framework network gate every encoded hidden state based summary vector initial encoding stage overall method seems outperform standard seqseq method point three different evaluation set overall technical section reasonably clear equation need explanation could understand notation specific contribution selective mechanism seems novel could potentially used context evaluation extensive demonstrate consistent improvement imagine adding additional encoder layer instead selective layer reasonable baseline given baseline us add expressivity seems implemented luong concern lstm mismatch benefit coming switch quality writing especially intro abstract related work quite make large departure previous work therefore related work nearby introduction seems appropriate related work common good approach highlighting similarity difference work previous work word presented equation simply listing work without relating work useful placement related work near intro allow relieve intro significant background detail instead focus high level,3
276.json,proposes approach sequence labeling multitask learning language modeling us auxiliary objective thus bidirectional neural network architecture learns predict output label well predict previous next word sentence joint objective lead improvement baseline grammatical error detection chunking tagging strength contribution quite well written easy follow part exposed sufficient detail experiment thorough within defined framework benefit introducing auxiliary objective nicely exposed weakness show limited awareness related work extensive across task experiment highlight table show three system proposed contribution baseline dropout lmcost limited comparison sketched textually contribution claiming novelty advancement previous state document improvement properly least reporting relevant score together novel one ideally replication datasets used experiment freely available previous well documented previous system part publicly available view long flaw treat previous work carefully sense find sentence particularly troublesome baseline comparable previous best benchmark reader believe baseline system somehow subsumes previous contribution shady first read factually incorrect quick lookup related work state state error detection conll datasets looking conll shared task report straightforward discern whether latter part claim hold true also yannakoudakis support claim inclusion replication related work general discussion tagging left afterthought comparison plank least partly unfair test across multiple language universal dependency realm showing level performance across language family believe relevant benchmarking proposed system scale multiple language resource language limited training data leaf dimension substantiate claim like idea including language modeling auxiliary task like architecture section general view section one describing experiment suggest nice idea fleshed publication rework include least fair treatment related work replication least reflection multilinguality data system sign field growing maturity view partake reflecting maturity step away faith improvement implemented publication deadline vote borderline,2
276.json,strength article well written done clear straightforward given simple contribution gain substantial least error correction task weakness novelty fairly limited essentially another permutation task multitask learning combining task explored interesting training significantly worse joint training could initialize weight existing trained unlabeled data general discussion hesitating experiment quite reasonable combination task sometimes quite work multitask learning rnns much already cited hard excited work nevertheless recommend acceptance experimental useful others post rebuttal read rebuttal change opinion,3
775.json,proposes approach classifying literal metaphoric adjective noun pair create word context matrix adjective noun element matrix score different method selecting dimension matrix represent noun adjective vector geometric property average noun adjective vector normalized version used feature training regression classifying pair literal metaphor expression approach performs similarly previous work learns vector representation adjective supervision zero shot learning argue approach requires le supervision compared previous work zero shot learning think quite right given seems main point think worth clarifying approach proposed supervised classification task form vector representation occurrence statistic property representation gold standard label pair train classifier similarly supervised classifier tested word occur training data learn example moreover word really unseen need vector representation word interpretation provide good overview previous related work metaphor however sure intuition approach using geometric property vector length identifying metaphor example normalized vector considered seems contribute better performance moreover predictive feature noun vector explain side effect data collected adjective occurs metaphoric literal expression result adjective vector le predictive seems proposed approach might suitable given data shortcoming fold theoretical perspective especially since submitted cognitive track clear learn theory metaphor processing application standpoint sure generalizable approach compared compositional model novelty proposed approach representing noun adjective vector similar agres seems main contribution geometric property classify vector,2
775.json,present method metaphor identification based geometric approach certainly interesting piece work enjoyed learning completely perspective however issue like addressed like read author response following issue strength geometric approach metaphor interpretation research strand altogether well written author claim beauty lie simplicity agree claim implication simplicity addressed simple way please refer weakness section weakness regarding writing doubt well written major issue lucidness indeed poetic language elegance applaud able clarity scientific writing much needed hope agree stuff articulated http chair blog last minute writing advice objection writing providing method effectively zero shot left reader blank notion zero shot introduced figure neutral least metaphoric arrive differentiation talk data otherwise method le intuitive enjoyed reading analysis section clear proposed simple claimed method perform existing technique putting example better believe technicality strength simplicity indeed implication vivid writing mathematical technical definition problem aspect implication definition quite hard understood note able contribution comparing previous research show marginal accuracy gain comparison previous work claiming method capable zero shot slightly overstated method extendable twitter general discussion,2
237.json,strength deal issue finding word polarity orientation unsupervised manner using word embeddings weakness present interesting useful idea however moment applied test case idea based explained intuitive manner thoroughly justified general discussion definitely interesting work benefit experiment carried comparison method example normalized google distance balahur montoyo http ieeexplore ieee abstract document application knowledge obtained real sentiment analysis scenario point work although promising initial phase,1
561.json,introduces general method improving task using embeddings language model context independent word representation useful proposes nice extension using context dependent word representation obtained hidden state neural language model show significant improvement tagging chunking task including embeddings large language model also interesting analysis answer several natural question overall good several suggestion many experiment carried test please change table development data really nice task tagging chunking many interesting long range dependency language might really help case love supertagging claim using task specific necessary language embeddings performs poorly clear backpropagating language experiment certainly seems like potential make task specific unnecessary,3
561.json,proposes approach trained word embeddings trained neural language embeddings leveraged concatenated improve performance english chunking respective conll benchmark domain english test method record state score task strength part well written easy follow method extensively documented discussion broad thorough weakness sequence tagging equal chunking surprised tagging included experiment sequence tagging task welcome grammatical error detection supersense tagging supertagging chunking english sequence tagging general lack multilingual component breadth task welcomed extensive description method think figure overlap sufficed related method rather straightforward simple mean thing seems contribution could better suited short since enjoy extensive discussion section necessarily flaw core method strike particularly exciting focused contribution short description call substantial work long general discussion bottomline concatenates embeddings see improvement english chunking warrant publication long ambivalent score reflect even slightly lean towards negative answer mainly preferred breadth sequence tagging task language also know well method scale resource scenario trained embeddings available sizeable experiment include notion still resource range could learned multi task learning setup substantial view reason vote borderline originality score idea introducing context embeddings nice particular instantiation leaf,2
86.json,summary proposes neural predicting python syntax tree text description guided actual python grammar generates tree node sequentially depth first fashion idea include injecting information parent node part lstm input pointer network copying terminal unary closure collapse chain unary production reduce tree size evaluated three datasets different domain outperforms almost previous work strength overall well written explanation system clear analysis thorough system natural extension various idea similar work include tree based generation parent feeding dong lapata various based semantic parsing copy mechanism liang ling guidance parsing based grammar also explored chen liang http arxiv code assist system used ensure code valid nevertheless stand able generate much longer complex program previous work mentioned weakness evaluation done code accuracy exact match bleu score metric especially bleu might best metric evaluating correctness program instance first example table show first line box different semantics another example variable name different evaluation based code using test case static code analysis convincing another point evaluation system baseline generate code syntactic error possible include result highest scoring well formed code using beam search baseline system generate give fairer comparison since system choose prune malformed code general discussion line approach domain specific language also guided grammar example berant liang us pretty limited grammar logical form table addition comparing line work emphasizing grammar much larger previous work make work stronger line parent feeding mechanism child index used word different generating first child versus second child seqtree dong lapata terminal different hidden state line possible token embedded assumed possible token known beforehand example appendix nice read author response,3
86.json,strength approach proposed seems reasonable experimental make approach seem promising feature approach feature approach general purpose programming language might applicable java however proof still needed another feature data driven syntactic neural described section together section think neural brings around improvement another purpose approach accuracy according experimental data overall nice work clear motivation methodology data analysis well organized presentation weakness line mentioned hypothesis space know mean read supplementary material material included full opinion better give explanation hypothesis space section introduces grammar section describes action probability estimation understanding latter part former section title reflect relation least section explain grammar experimental data wondering train experiment many datasets used true trained accuracy obtained efficiency approach difference neural network based approach used code generation general purpose language domain specific one claim approach scale generation complex program find argument backup conclusion minor comment line underlying syntax syntax language line constraint line decoder us decoder us reference format inconsistent general discussion proposes data driven syntax based neural network code generation general purpose programming langauge python main idea approach first generate best possible using probabilistic grammar given statement natural language ecode surce code using deterministic generation tool generating code relatively easy point first step experimental provided show proposed approach outperform state approach,3
520.json,proposes method generating datasets picture simple building block well corresponding logical form language description goal seems method complexity picture corresponding desciptions controlled parametrized biggest downside seems maximally achievable complexity limited complexity typically faced image captioning multimodal task relative simplicity also difference referenced babi task cover whole qualitative spectrum easy hard reasoning task whereas proposed method qualitatively easy image reconition task quantitatively made harder increasing number object noise unnatural way also reflected experimental section whenever experimental performance satisfying case seem like basic underfitting issue easily tackled restricting extending capacity network using data hard spot qualitative insight introduction stated goal achieve optimal performance find whether architecture able successfully demonstrate desired understanding fundamental contradiction proposed task side meant provide measure whether architecture demontrate understanding hand score supposed taken meaningful seriously general comment general approach made tangible earlier introction rather section,1
520.json,strength introduce software package called shapeworld automatically generating data image captioning problem microworld used generate image caption simple enough make data generated error readily interpretable however demonstrate configuration package produce data challenging enough serve good benchmark ongoing research weakness primary weakness look like demo provides experiment evaluate reasonable baseline image captioning system data generated shapeworld however similar experiment included demo paper includes hyperlink software package github presumably unmasks general discussion scientific progress often involves something analogous vygotsky zone proximal development whereby progress made quickly research focus problem right level difficulty tidigits speech recognition research early exciting since offer simple microworld easy researcher completely comprehend also difficult enough existing model strength work multiplied fact software opensource readily available github generates data format easily used model built using modern deep learning library tensorflow method used software package generate artificial data clearly explained also great experiment different configuration software baseline image caption order demonstrate strength weakness existing technique real concern whether community better served placing demo section publishing demo long track might cause confusion well unfair correctly submitted similar paper demo track,1
222.json,strength introducing task illustrative example well contribution related work section cover state time pointing similarity difference related work proposed method presentation method clear since separate tagging scheme another strong point work baseline used compare proposed method several classical triplet extraction method last presentation example dataset used illustrate advantage disadvantage method important output complement explanation tagging evaluation triplet weakness main contribution tagging scheme described section however already scheme used bilou perform experiment using tagging scheme method regarding dataset line page cite number relation mention number type named entity section evaluation criterion triplet presented criterion based previous work stage entity identification complete consider head entity regarding example shown table output lstm lstm bias considered correct text state relation role wrong although clear relation role considered evaluation general discussion proposes novel tagging scheme investigates model jointly extract entity relation article organized clear well written make easy understand proposed method,4
367.json,strength address long standing problem concerning automatic evaluation output generation translation system analysis available metric thorough comprehensive demonstrate metric higher correlation human judgement bibliography help entrant field weakness written numerical analysis little insight linguistic issue generation method generation difference output different system human generated reference unclear crowd source generated reference serve well context application need language generation general discussion overall could linguistic example description different system risk dropping table help reader intuition,2
760.json,proposes recurrent neural architecture skip irrelevant input unit achieved specifying word read skim jump size jump allowed lstm process word predicts jump size signal stop skip next word continues either number jump reach reach last word differentiable trained standard policy gradient work seems heavily influenced shen apply similar reinforcement learning approach including variance stabilization multi pas machine reading strength work simulates intuitive skimming behavior reader mirroring shen simulate self terminated repeated reading major attribute work simplicity despite simplicity approach yield favorable particular show well designed synthetic experiment indeed able learn skip given oracle jump signal text classification using real world datasets able perform competitively skimming clearly faster proposed potentially meaningful practical implication task skimming suffices sentiment classification suggests obtain equivalent without consuming data completely automated fashion knowledge novel finding weakness mysterious basis determines jumping behavior effectively synthetic dataset thinking case last part given sentence crucial evidence instance movie boring last minute ending blew away example decide skip rest sentence reading boring miss turning point ending blew away mislabel instance negative case solution running skimming direction suggest future work general require sophisticated architecture controlling skimming seems achieve improved skimming combining multi pas reading presumably reverse direction human read understand text digested skim indeed read draft overall work raise interesting problem provides effective intuitive solution,3
326.json,strength established neural network method adversarial network goodfellow nip take advantage different chinese work breaking test set different notion count word chinese could implication many task slightly different notion count correct thinking problem term adaptation possible goodfellow useful thinking problem weakness need name problem mentioned elusive gold standard prefer term multi criterion motivation seems unnecessarily narrow elusive gold standard come sort application chinese word segmentation motivation make unnecessary assumption much reader know chinese know much something think easier many chinese reader like reviewer think chinese simpler easy assume chinese word segmentation easy tokenizing english text string delimited white space guess inter annotator agreement pretty chinese point trying make table considerable room disagreement among native speaker chinese think help could point many task considerable room disagreement task like machine translation information retrieval search much room disagreement metric task designed allow multiple correct answer task like part speech tagging tend sweep elusive gold standard problem hope away fact progress tagging stalled know distinguish difference opinion error annotator return different answer difference opinion machine return different answer machine almost always wrong reader stuck term adversary think nip used modeling noise murphy often wise assume worst think helpful think difference opinion adversarial game like chess chess make sense think opponent sure helpful think difference opinion think clarify applying established method nip us term adversarial deal elusive gold standard problem point elusive gold standard problem common problem study context particular problem chinese problem much general general discussion found much unnecessarily hard going chinese latest nip help even small issue english larger problem exposition consider table line make assertion first block depth network specifically line table support assertion assume refer precision recall explained assume standard measure vocabulary assume thing many number table count significance number even comparable compare number across col performance collection comparable performance another line suggests adversarial method significant take away table line claim significant solution call elusive gold standard problem number table justify claim small quibble english work work many place work mass noun count noun unlike conclusion conclusion conclusion le work work work line dataset datasets line three datasets traditional chinese city ckip five simplified chinese line random randomize,3
326.json,proposes method train model chinese word segmentation datasets multiple segmentation criterion strength multi criterion learning interesting promising proposed also interesting achieves large improvement baseline weakness proposed method compared model baseline lstm proposed however proposed tagging tagging description employ state architecture section misleading purpose experiment section unclear purpose investigating datasets traditional chinese simplified chinese could help however experimental setting separately trained simplified chinese traditional chinese shared parameter fixed training simplified chinese expected fixed shared parameter general discussion interesting detailed discussion datasets adversarial multi criterion learning boost performance zhiheng huang bidirectional lstm model sequence tagging arxiv preprint arxiv xuezhe eduard hovy sequence labeling directional lstm cnns arxiv preprint arxiv,3
12.json,proposes method recognize time expression text simple rule based method strong advantage analysis tool since time expression recognition basic process application experiment show proposed method outperforms state rule based method machine learning based method time expression recognition great concern generality method rule method designed based observation corpus used evaluation well hence afraid rule corpus similarly domain corpus affected rule design statistic discussion show overlap time expression observed corpus shown time expression corpus mostly overlap fact supported generality rule anyway better experiment conducted using corpus distinct rule design process order show proposed method widely effective,2
214.json,proposed macro discourse structure scheme carried pilot study annotating corpus consisting news article chinese treebank built recognize primary secondary relation discourse relation joint elaboration sequence background cause result corpus poorly written difficulty follow strongly suggest find native english speaker carefully proofread regarding content several concern logic clear justifiable logical semantics pragmatic function line prefer define properly macro discourse structure conflict definition macro structure micro structure figure demonstrates combination macro discourse structure micro discourse structure micro discourse structure presented within paragraph however specific example micro discourse structure shown figure micro level discourse structure beyond paragraph boundary capture discourse relation across paragraph kind micro level discourse structure indeed similar macro structure proposed figure also genre independent structure figure advantage macro discourse structure proposed figure genre dependent provide richer information compared figure sentence sentence missing figure subtitle sentence subtitle present corpus construction section informative enough without detailed example hard know meaning discourse topic lead abstract paragraph topic line saying explore relationship micro structure macro structure find correspondent part table agreement study claimed difficult achieve high consistence judgment relation structure subjective measurement data taken layer leaf node first leaf node macro level paragraph micro level edus report agreement study macro level micro level separately second seems take subset data measure agreement reflect overall quality whole corpus high agreement leaf node annotation ensure high agreement leaf node annotation unclear part section table discourse structure discourse relation clear discourse structure discourse relation table amount macro discourse relation still clear mean discourse relation paragraph figure relation exist sentence paragraph experiment since main purpose provide richer discourse structure macro micro level expect initial direction current experiment convincing strong baseline feature clearly described motivated understand discourse relation table chosen perform experiment discourse relation recognition general think need major improvement currently ready acceptance,1
214.json,present unified annotation combine macrostructures structure chinese news article essentially structure adopted paragraph macrostructure adopted paragraph view nuclearity depend relation label also context appealing find major issue annotation experiment detailed notion primary secondary relationship advocated much later became clear essentially notion nuclearity extended macrostructure making context dependent instead relation dependent even status nuclear nuclear nuclear satellite satellite nuclear redefined concept description established theory discourse often incorrect example rich existing work pragmatic function text claimed something little studied error related work section treating chinese dependency discourse treebank different coherence cohesion computational approach subsection lacking reference work performance table nuclearity classification confusing prior work sentence level document level parsing annotation find macro structure annotation description confusing furthermore statistic macro label listed reported agreement calculation also problematic stated measurement data taken layer leaf node think verify validity annotation multiple mention annotation procedure say prelim experiment show good approach finally unclear kappa value calculated since structured task calculation discourse treebank said nuclearity status closely associate relation label baseline performance us relation label note feature explained hierarchical characteristic main contribution combination macro micro structure however experiment relation micro level evaluated even among handpicked one evaluation used verify macro side hence supporting contains numerous grammatical error also text displayed figure illustrate example,0
193.json,introduces ucca target representation semantic parsing also describes quite successful transition based parser inference representation liked believe value simply introduction ucca believe relatively community potential spark thinking semantic representation text also think well thought fairly derivative existing transition based scheme extension introduced make applicable domain reasonable well explained believe appropriate level detail empirical evaluation pretty convincing good compared several credible baseline demonstrated performance multiple domain biggest complaint lack multilingual evaluation especially given formalism experimented exactly supposed fairly universal reasonably sure multilingual ucca corpus exist fact think league corpus used good language english minor point section refer grammarless strike quite correct true ucca representation derived linguistic notion syntax still defines construct compositional abstract symbolic representation text precisely grammar clearly quibble know irked enough feel compelled address edited thanks response,4
557.json,strength clearly written well structured system newly applied several technique including global optimization neural relation extraction direct incorporation parser representation interesting proposed system achieved state performance conll data set include several analysis weakness approach incremental seems like combination existing method improvement performance percent point relatively small significance test provided general discussion major comment employed recent parser glove word embeddings affect relation extraction performance prediction deal illegal prediction minor comment local optimization completely local considers structural correspondence incremental decision explanation introduction misleading point figure connected straight line curve entity represented segment citation incomplete kingma accepted iclr miss page,3
107.json,present several weakly supervised method developing ners method rely form projection english another language overall approach individual method proposed improvement existing method expected novel approach contribution data selection scheme formula used calculate quality score quite straightforward thing however unclear threshold calculated table say different threshold tried done development mention evaluation show clearly data selection important know tune parameter data language pair another contribution combination output system developed tried hard understand work description provided clear present number variant method proposed make sense combine weakly supervised system anything direction good know type text house dataset,2
384.json,strength theoretically solid motivated formal semantics weakness relation extraction majority literature taxonomization referenced inter alia flati tiziano vannella daniele pasini tommaso navigli roberto multiwibi multilingual wikipedia bitaxonomy project soren auer christian bizer georgi kobilarov jens lehmann richard cyganiak zachary dbpedia nucleus open data gerard melo gerhard weikum menta inducing multilingual taxonomy wikipedia zornitsa kozareva eduard hovy semi supervised method learn construct taxonomy using vivi nastase michael strube benjamin boerschinger caecilia zirn ana elghafari wikinet large scale multi lingual concept network simone paolo ponzetto michael strube deriving large scale taxonomy wikipedia simone paolo ponzetto michael strube taxonomy induction based collaboratively built knowledge repository fabian suchanek gjergji kasneci gerhard weikum yago large ontology wikipedia wordnet paola velardi stefano faralli roberto navigli ontolearn reloaded graph based algorithm taxonomy induction experiment poor compare hearst pattern without taking account work previously cited general discussion easy follow supplementary material also well written useful however lack reference relation extraction taxonomization literature apply experiment fact meaningful comparison performed even take account existence system recent hearst pattern read answer still convinced could perform evaluation understand solid theoretical motivation still think comparison important ass theoretical intuition confirmed also practice true work suggested comparison build taxonomy also true comparison possible considering edge taxonomy anyway considering detailed author answer discussion reviewer rise score even still think poor experiment frame correctly relation extraction taxonomy building literature,2
384.json,strength novel approach modeling compositional structure complex category maintains theoretic interpretation common noun modifier also permitting distributional interpretation head modification approach well motivated clearly defined experiment show show decomposed representation improve upon hearst pattern derived relation upon trained term coverage weakness experiment encouraging however nice curve approach alone ensemble hearst pattern table tell modsi increase coverage cost precision figure tell modsi match hearst pattern precision high precision region data however neither tell whether distinguish high precision region curve tell available ensembled model believe unnecessary since already case rangle langle discussion overall nice idea well described evaluated think good addition,3
691.json,overview proposes training sense embeddings grounded lexical semantic resource case wordnet direct evaluation learned sense vector meaningful instead sense vector combined back word embeddings evaluated downstream task attachment prediction strength attachment seem solid weakness whether sense embeddings meaningful remains uninvestigated probabilistic detail hard understand lambdawi hyperparameters trained rank come taken sense rank wordnet related work idea expressing embeddings word convex combination sense embeddings proposed number time previously instance johansson nieto pia embedding semantic network word space naacl decomposed word embeddings ontology grounded sense embeddings based idea also unsupervised sense vector training idea used instance arora linear algebraic structure word sens application polysemy minor comment need define type token standard terminology first lambawi equation needed probability unnormalized general discussion,1
150.json,strength present novel adaptation encoder decoder neural using approach start end character work representation morpheme character release code well final learned model helpful validating work well others looking replicate extends work system reported appears produce translation reasonable quality even first training epoch continued progress future epoch system appears learn reasonable morphological tokenizations appears able handle previously unseen word even nonce word implicitly backing morpheme weakness explicitly state test set reported problematic reader wishing compare reported existing work example matrix statmt reviewer found information look readme code supplement indicates test newstest test newstest explicitly described instruction given software readme great training testing section could enhanced explicit example respective command software respond help flag currently describes level architecture diagram figure appears show fewer layer going caption explicit figure showing layer figure somewhere even appendix showing layer show comparison character based neural system show state type system matrix statmt reported system datasets appears state much higher reported acknowledged ideally discussed handful minor english disfluency misspelling minor latex issue reverse quotation mark corrected general discussion nice contribution existing literature character based neural,3
150.json,strength general well structured clear possible follow explanation idea presented original obtained quite interesting weakness doubt interpretation addition think claim regarding capability method proposed learn morphology propperly backed scientific evidence general discussion explores complex architecture character level neural machine translation proposed architecture extends classical encoder decoder architecture adding deep word encoding layer capable encoding character level input word representation source language sentence deep word decoding layer added output transform target language word representation character sequence final output system objective architecture take advantage benefit character level reduction size vocabulary flexibility deal unseen word time improving performance whole system using intermediate representation word reduce size input sequence character addition claim deep word encoding able learn morphology better state approach concern regarding evaluation compare approach state system taking account parameter training time bleu score however clearly advantage proposed dcnmt front approach bpechar difference approach regard bleu score small hard outperforming without statistical significance information statistical significance evaluated regard training time worth mentioning bpechar take day le dcnmt training time provided bpechar evaluated think complete comparison system carried prove advantage proposed second concern section start claiming investigated ability system learn morphology however section contains example comment even though example well chosen explained didactic worth noting experiment formal evaluation seem carried support claim definitely encourage extend interesting part could even become different hand section seem critical point current work suggest move section appendix soften claim done regarding capability system learn morphology comment doubt suggestion many acronym used defined lstm hgru defined starting even though acronym well known field deep learning encourage defined improve clearness concept energy mentioned first time section even though explanation provided enough point nice refresh idea energy section used several time providing hint interpret high energy character indicating current morpheme split point addition concept peak figure described acronym defined capital letter used rest mention lower cased reason sure necessary monolingual corpus used section seems something wrong figure since colour energy value shown every character table chung taken paper since reported computed seems mention french morphologically poor rather rich slavic language czech link provided training corpus several reference incomplete typo bilingual parallel corpus provided bilingual parallel corpus provided luong manning us luong manning hgru hgru coveres cover consists layer consist layer difference cnmt dcnmt cnmt difference cnmt dcnmt cnmt,2
516.json,strength offer natural useful extension recent effort interactive topic modeling namely allowing human annotator provide multiple anchor word machine induced topic well organized combination synthetic user experiment make strong weakness fairly limited scope term interactive topic approach compare willing accept since make reference explain approach necessarily fast enough interactive experimentation conducive type interaction considered anchoring interface level empirical support claim nice though also nice experiment data newsgroups sort beaten death general discussion general strong appears offer incremental novel practical contribution interactive topic modeling made effort several variant approach simulated experiment conduct fairly exhaustive quantitative analysis simulated user experiment using variety metric measure different facet topic quality,3
516.json,strength clear description method evaluation successfully employ interprets variety evaluation solid demonstration practicality technique real world interactive topic modeling weakness missing related work anchor word evaluation newsgroups ideal theoretical contribution small general discussion propose method interactive user specification topic called tandem anchor approach leverage anchor word algorithm matrix factorization approach learning topic model replacing individual anchor inferred gram schmidt algorithm constructed anchor pseudowords created combining sparse vector representation multiple word topic facet determine harmonic mean function construct pseudowords optimal demonstrating classification accuracy document topic distribution vector using anchor produce improvement gram schmidt also demonstrate work faster existing interactive method allowing interactive iteration show user study multiword anchor easier effective user generally like contribution straightforward modification existing algorithm actually produce sizable benefit interactive setting appreciated effort evaluate method variety scale think technical contribution relatively small strategy assemble pseudowords based topic facet thoroughness evaluation full instead short nice idea build facet absence convenient source like category title newsgroups initializing topic interactive learning frustration find evaluation newsgroups great topic modeling document widely different length preprocessing matter user trouble making sense many message naive word model beat topic model substantial margin classification task useful shorthand well topic corresponds meaningful distinction text topic task like classifying news article section review class subject review might appropriate also nice case better appealed common expressed application topic model exploration corpus number comparison think missing contains little reference work since original proposal anchor word addition comparing standard gram schmidt good method dimensional embeddings interpretable anchor based topic inference also liked seen reference nguyen evaluating regularized anchor word nguyen anchor going fast accurate supervised topic model provide useful insight anchor selection process smaller note entire dataset quite sure mean think claiming take long pas assumption subset data retrain instead full sweep good clarify mean reason consider operator element wise seem correspond idea union intersection operator element wise clear one chose better option usenet capitalized fewer pretty aggressive boundary also remove header footer quote message liked explanation tell confusion using tandem anchor overall think meaningful contribution interactive topic modeling like available people outside machine learning community investigate classify test hypothesis corpus post response appreciate thoughtful response question maintain complimentary related work useful compare interactive work even something different,3
239.json,strength proposed interesting important metric evaluating quality word embeddings data efficiency used supervised task another interesting point separated three question whether supervised task offer insight evaluate embedding quality stable ranking labeled data size benefit linear linear model overall presented comprehensive experiment answer question quite interesting know research community weakness overall result useful practioners field merely confirms known suspected depends task hand labeled data size type result actionable reviewer noted comprehensive analysis deepens understanding topic general discussion presentation improved specifically order figure table match order mentioned paper right order seems quite random several typo please spell checker equation useful exposition look strange removed leave text explanation mention appendix available missing citation public skip gram data claim strong must explained clearly useful observation interesting important good follow provide concrete evidence example embedding visualization help provide example specialized word embeddings different general purpose embedding figuer small read,2
444.json,study properly evaluate system produce ghostwriting lyric present manual evaluation along three aspect fluency coherence style matching also introduce automatic metric consider uniqueness maximum training similarity stylistic similarity rhyme density find interesting analysis discussion manually evaluating style matching especially make sense also exist important concern convinced appropriateness fluency coherence rating line level mention following find work actually studying different setting lyrical challenge response treated line level nature work full verse consists multiple line normally topically structurally coherent currently cannot reason evaluate fluency coherence verse whole also reckon count much automatic metric main goal generate similar unique lyric uniqueness evaluation calculation performed verse level however many rapper produce lyric within specific topic theme system extract line different verse presumably might also fluent coherent verse verse level similarity score hardly claim system generalizes well stylistic similarity specified artist think rhyme density position independent therefore enough reflect full information style artist seem automatic metric verified well correlated corresponding real manual rating uniqueness stylistic matching also wonder need evaluate semantic information commonly expressed specified rapper well caring rhythm meanwhile understand motivation study lack sound evaluation methodology however still find statement particularly weird methodology produce continuous numeric score whole verse enabling better comparison enabling comparison really important making slightly vague reliable convincing judgement minor issue incorrect quotation mark line,1
501.json,proposes task selecting appropriate textual description given scene image list similar option also proposes couple baseline model evaluation metric human evaluation score strength well written well structured clear contribution well support empirical evidence easy read well motivated method selecting appropriate caption given list misleading candidate benefit image caption understanding model acting post generation ranking method weakness sure proposed algorithm decoy generation effective consequence put question target caption algorithm basically pick similar representation surface form belong image fundamentally issue approach belonging image mean appropriate describe image especially representation surface form close ground truth label might valid figure generated decoy either target good decoy giraffe elephant fair substitute target small playing kite fly kite thus afraid dataset generated algorithm train really beyond word recognition claimed contribution shown figure decoy filtered word mismatch giraffe elephant bread frisbee kite separated word match look tempting correct option furthermore interesting human correctly sampled test mean example really hard even human correctly classify decoy fact good enough target substitute even better human choose ground truth target general discussion think well written clear motivation substantial experiment major issue data generating algorithm generated dataset seem helpful motivation turn make experimental conclusion le convincing tend reject unless concern fully addressed rebuttal,1
501.json,strength task seems like good test understanding language vision like task clear evaluation metric failure caption generation task quite interesting result demonstrates model good language model good capturing semantics image weakness experiment missing baseline state trained label vocabulary liked detail human performance experiment many incorrectly predicted image caption genuinely ambiguous could data cleaned yield even higher human accuracy general discussion concern data prove easy gameable address concern running suite strong baseline data demonstrating accuracy convinced current experiment chosen neural network architecture appear quite different state architecture similar task typically rely attention mechanism image another nice addition analysis data many token correct caption share distractors average kind understanding necessary distinguish correct incorrect caption think kind analysis really help reader understand task worthwhile relative many similar task data generation technique quite simple really qualify significant contribution unless worked surprisingly well note could find description ffnn architecture either supplementary material look like kind convolutional network token detail unclear also confused veqseq ffnn applied classification caption generation loglikelihood caption combined ffnn prediction classification ffnn score incorporated caption generation fact caption generation performs statistically significantly worse random chance need explanation possible description neural network hard understand final paragraph section make clear however consider starting section,1
805.json,strength originality core evaluation measure good accuracy proposed similarity measure large number diversity datasets evaluation weakness typo line design design line figure figure line among among line figure introduced within article body line dataset contains dataset contains line table table tensorflow replaced textflow imprecision feature computation accuracy lemma wordnet synset detailed discussed impact general similarity accuracy evaluation neural network said implemented python code said available able repeat experiment training evaluation set said shared said demand license able repeat experiment general discussion,3
741.json,present graph based approach producing sense disambiguated synonym set collection undisambiguated synonym set evaluate approach inducing synonym set wiktionary collection russian dictionary comparing pairwise synonymy relation using precision recall wordnet babelnet english synonym set ruthes another russnet russian synonym set well written structured experiment evaluation least prose part easy follow methodology sensible analysis cogent happy observe objection reading mismatch vocabulary synonym dictionary gold standard ended resolved least addressed final page thing concern seem properly understood previous work undercut stated motivation first instance misunderstanding paragraph beginning line omegawiki lumped wiktionary wikipedia discussion resource formally structured contain undisambiguated synonym reality omegawiki distinguished resource using formal structure relational database based word sens rather orthographic form translation synonym semantic annotation omegawiki therefore unambiguous second serious misunderstanding come three paragraph beginning line claim babelnet rely english wordnet pivot mapping existing resource criticizes mapping error prone though true babelnet us wordnet pivot basically general purpose specification representation lexical semantic resource link exists independently given lexical semantic resource including wordnet given alignment resource including one based similarity dictionary definition cross lingual link maintainer made available various database adhering spec contain variety lexical semantic resource aligned variety different method given database queried synset generate synset user free produce database importing whatever lexical semantic resource alignment thereof best suited purpose three criticism line therefore entirely misplaced fact think least criticism appropriate even respect babelnet claim watset superior babelnet babelnet mapping machine translation error prone implication watset method error free least significantly le error prone grandiose claim believe supported ought known advance similarity based sense linking algorithm graph clustering algorithm alone study think criticism ought moderated also think third criticism babelnet reliance wordnet pivot somewhat miss point surely important issue highlight fact pivot english rather synset already manually sense annotated think last paragraph first paragraph extensively revised focus general problem generating synset sense level alignment translation lsrs gurevych survey rather particularly babelnet us certain particular method particular method aggregate existing one helpful point somewhere although alignment translation method used produce synset enrich existing one always explicit goal process sometimes serendipitous noisy side effect aligning translating resource differing granularity finally several point line synset twsi jobimtext criticized including many word hypernym hypnomyms instead synonym problem really unique twsi jobimtext often hypernym hypernym appear output watset vague idea comparing table analyze synonym relation watset really better filtering word semantic relation nice quantitative evidence relatively minor point nonetheless fixed line sentence kiselev seems rather useless bother mentioning analysis going tell found line took long time figure relation discover correct word sense suppose supposed maybe better call approach whatset least consider rewording sentence better explain figure practically illegible owing microscopic font please increase text size similarly table small read comfortably please larger font save space consider abbreviating header maybe reporting score range instead eliminate leading column line wiktionary moving target help others replicate compare work please indicate date wiktionary database dump used throughout constant switching time computer modern distracting root problem longstanding design flaw latex style file exacerbated decision occasionally number math mode even running text please removing usepackage time preamble replacing either usepackage newtxtext usepackage newtxmath usepackage mathptmx reference gurevych eckle kohler matuschek linked lexical knowledge base foundation application volume synthesis lecture human language technology chapter linking algorithm page morgan claypool read author response,3
741.json,strength proposes method word sense induction synonymy dictionary method present conceptual improvement existing one demonstrates robust performance empirical evaluation evaluation done thoroughly using number benchmark strong baseline method weakness couple small point like discussion nature evaluation first observes model score relatively room much improvement natural ceiling performance nature task discus lexical sparsity input data wonder much performance sparsity account second also like discussion evaluation metric chosen known word sens analyzed different level granularity naturally affect score system another point clear obtained vector word sens used sens determined step anyway sens marked input corpus general discussion recommend presentation meeting solid work,3
350.json,strength improves state method might applicable domain weakness much novelty method quite clear data general enough domain general discussion describes rule based method generating additional weakly labeled data event extraction method three main stage first us freebase find important slot filler matching sentence wikipedia using slot filler stringent resulting match next us framenet improve reliability labeling trigger verb find nominal trigger lastly us multi instance learning deal noisily generated training data like improves state trival benchmark rule involved seem obfuscated think might useful practitioner interested improve system domain hand manual effort still needed example mapping freebase event type event type written section line also make difficult future work calibrate apple apple apart method also seem novel comment also concern generalizability method domain section line say event type selected freebase selected coverage event type data generally well written although suggestion improvement section line us argument liked time location mean role argument maybe want actual realization time location example minor typo line missing major concern,2
33.json,strength innovative idea sentiment regularization experiment appear done well technical point view useful depth analysis weakness close distant supervision mostly poorly informed baseline general discussion present extension vanilla lstm incorporates sentiment information regularization introduction present claim previous approach phrase level supervision present phrase level annotation expensive contribution instead simple using linguistic resource related work section provides good review sentiment literature however mention previous attempt linguistic regularization explanation regularizers section rather lengthy repetitive listing could well merged respective subsection notation section inconsistent generally hard follow notably sometimes used subscript sometimes superscript parameter beta never explicitly mentioned text entirely clear constitutes position terminology parameter lstm output seems index sentence thus preceding sentence prediction sentence however description regularizers talk preceding word sentence still us assumption actually overloaded either mean sentiment sentence word however made clearer text dangerous issue tread fine line regularization distant supervision work problem many way integrate lexical information polarity negation information putting information feature compare implementation teng nscl important know whether used lexicon work case comparison fair also understand cannot nscl dataset access implementation matter swapping datasets remaining baseline appear using lexical information make rather poor much like vanilla lstm lexical information simply appended word vector helpful analysis model experiment show indeed learns intensification negation extent experiment interesting know behaves vocabulary word respect lexicon learn beyond memorization generalization happen word seen training minor remark figure table small read print mostly well written apart point noted could benefit proofreading grammatical error typo left particular beginning abstract hard read overall pursues reasonable line research largest potential issue somewhat shaky comparison related work could fixed including stronger baseline final crucial establish whether comparability given experiment hope shed light response http aclweb anthology update author response thank clarifying concern experimental setup nscl believe comparison teng fair lstm good know however crucial part stand baseline weak marginal improvement still vague better open comparison including significance test understand defined effect word make much interesting additional experiment current regularization experiment,2
777.json,strength deviation vocal user average user interesting discovery could applied identify different type user weakness initial work topic expanded future possible comparison matrix factorization similar topic distributional semantics latent semantic analysis useful general discussion describe approach modeling stance sentiment twitter user topic particular address task inter topic preference modeling task consists measuring degree stance different topic mutually related work claimed advance state task since previous work case study proposed unlimited topic real world data adopted approach consists following step linguistic pattern manually created large number tweet expressing stance towards various topic collected next text expressed triple containing user topic evaluation relationship represented tuples arranged sparse matrix matrix factorization rank approximation performed optimal rank identified definition cosine similarity used measure similarity topic thus detect latent preference represented original sparse matrix finally cosine similarity also used detect inter topic preference preliminary empirical evaluation show predicts missing topic preference moreover predicted inter topic preference moderately correlate corresponding value crowdsourced gold standard collection preference according overview discussed related work section previous system compared latter task prediction inter topic preference reason promising listed specific comment row high quality make high quality properly defined remove occurrence high quality caption figure remove term generic section collect collected section explains collected irony irony support since procedure detect various pattern maybe author explain possible pattern containing topic collected next manually filtered row unuseful useless including including topic guess term retrieved mistake topic row remove first sentence start twitter user row like procedure used find optimal previous work number often assumed useful find empirically call,2
331.json,strength present approach creating concept map using crowdsourcing general idea interesting main contribution lie collection dataset imagine dataset valuable resource research field clearly effort gone work weakness overall felt overstated placed example claim crowdsourcing scheme contribution claim quite strong though read like applying best practice crowdsourcing work novel method rather well thought sound application existing knowledge similarly claim develop present corpus seems true effort invested preparation section reveals actually based existing dataset criticism presentation work though general discussion summary sentence come crowdsource task still quite subjective cluster come part tacb dataset expert annotator used create gold standard concept map information needed section seems quite crucial trained made expert,2
433.json,strength nice nice data much work creole like language especially english weakness global feeling deja similar technique applied domain ressource language replace word embeddings cluster neural model whatever fashion year find le applied urdu domain parsing liked though appreciated highlight contribution position work better within literature general discussion present experiment designed show effectiveness neural parser scarce resource scenario introduce data creole english singapour called singlish data relatively small annotated sentence used unlabeled sentence word embeddings induction manage present respectable interesting approach even though using feature relatively close language unknown parsing community line work parsing urdu hindi arabic dialect using based parser assuming singlish extreme domain english given experiment wonder classical technique domain adaptation namely training uden singlish within cross fold experiment another interesting baseline without word embeddings lingual embeddings enough parallel data available think interesting really appreciated positioning regarding previous work parsing ressources language extreme domain adaptation table presenting irish small treebanks nice also come regarding labeled relation note reading answer thanks clarification especially redoing evaluation raised recommendation hope accepted,3
433.json,construct dataset singaporean english singlish sentence annotated universal dependency show improve performance tagger dependency parser singlish corpus integrating english syntactic knowledge neural stacking strength singlish resource language community need data resource language dataset accompanying useful contribution also relatively little research creole potential using transfer learning analyze creole make nice contribution area experimental setup used clear provide convincing evidence incorporating knowledge english trained parser singlish parser outperforms english parser singlish parser singlish data also provide good overview relevant difference english singlish purpose syntactic parser useful analysis different parsing model handle singlish specific construction weakness three main issue insufficient comparison annotation english language many construction bring specific singlish also present language annotation ideally consistent singlish language like analysis impact training data size central claim using english data improve performance resource language like singlish much singlish data needed english data became unnecessary happens train single parsing concatenated singlish datasets much simpler incorporating neural stacking case neural stacking stronger outperform baseline general discussion line tagger dependency parser perform poorly singlish text based observation clear quantify later seems hand wavy line comparison neural network model multi lingual parsing tell directly approach mapping singlish english word embeddings embedding space line introduction point appropriate point singlish data also data domain match line borrowed word annotated according original meaning mean language borrowed usage singlish figure standard english gloss useful understanding construction checking correctness relation used line topic prominence compare dislocated label dislocated relation capture preposed topic postposed element syntax describing sound similar topic comment style syntax different make clear line second noun phrase used modify predicate presence preposition regarded nsubj nominal subject need gloss determine analysis make sense phrase really used modify predicate nsubj make distinction core argument nsubj dobj modifier case modification modification relation core argument relation clarify language line standard predicative verb used copula often depends complement avoid copular head explicit decision made increase parallelism copular language singlish call think rest discussion copula handling necessary line deletion noun phrase deletion often null subject object common language zero anaphora spanish italian russian japanese good point also point dealt language believe handle ling subj verb inversion common interrogative language marta supermercado marta supermarket question present english though perhaps frequent make sure analysis consistent language data selection annotation chose singlish sentence course english parser poorly chosen disimilar sentence english parser seen sense standard english parser overall singlish filtered common sentence vocabulary term construction discussed language necessarily capture unusual sentence structure particularly around long distance dependency investigate whether method good capturing sentence grammatical difference english discussed section line inter annotator agreement unlabeled attachment score labeled attachment score agreement tag integrated note silveira produced measured inter annotator agreement token basis discrepancy tagging parsing section tagging parsing like analysis effect training size much singlish data needed train tagger parser entirely singlish accuracy stacked happens concatenate datasets train hybrid dataset singlish result line typo rained trained neural stacking lead biggest improvement nearly category except slightly lower competitive performance deletion case seems english data strongly bias parser expect explicit subj could deleting subj english sentence improve performance construction,3
68.json,strength broadens applicability readability score additional language produce well validated applicability score vietnamese weakness greatest weakness respect readability score limited interest within field computational linguistics somewhat useful educational public communication field impact progress computational linguistics limited minor weakness writing numerous minor grammatical error although discussion compare performance feature previous work unclear poorly previous readability measure perform relevant developed practical purpose general discussion stronger candidate inclusion corpus importantly label developed released could used widely development scalar readability metric enable investigation application powerful feature selection method,1
68.json,strength dataset resource poor language weakness incomplete related work reference comparison recent method approach lack technical contribution weak experiment general discussion present simple formula readability assessment vietnamese text using combination feature word count sentence length train simple regression estimate readability document major weakness lack technical contribution early work readability assessment employed simple method like outlined recent work predicting readability us robust method rely language model instance http readabilitybea http personal umich kevynct pub readability invited article camera comparison method could useful contribution make stronger especially simple method outlined compete complicated model baseline experiment smog gunning index also presented well vietnamese metric datasets cite another problem previous readability index selective classified content granular level corresponding grade level instance coarse classification scheme label document easy medium hard make metric uninteresting also classifier work probably mature suffers significant weakness accepted stage encourage incorporate suggested feedback make better also quite grammatical error addressed future submission,0
68.json,present formula assessing readability vietnamese text formula developed based multiple regression analysis three feature furthermore developed annotated text corpus three readability class easy middle hard research language english interesting important especially come resource language therefore corpus might nice additional resource research seems publish right however think convincing current shape influence future research reason provide reason need delevoping formula readability assessment given already exist formula vietnamese almost feature disadvantage formula formula presented better general experimental section lack comparison previous work analysis claim accuracy formula corpus good applied practice accuracy formula already exist pro con existing formula compared mentioned analysis missing word sentence length number difficult word considered easy middle hard example formula could applied practical application nice well related work section rather background section since present previously published formula missing general discussion related work paper might interesting dubay principle readability rabin determining difficulty level text written language english since vietnamese syllable based word based wondering word study particular approach merging syllable approach accuracy approach content experiment comparison analysis discussion related work enough long additional remark language need improvement equation usage parenthesis multiplying operator inconsistent related work section usage capitalized first letter inconsistent,0
130.json,strength proposes apply speech transcript narrative description order identify patient mild cognitive impairment code claim able distinguish healthy control participant patient line however conclusion line accuracy ranging mean easy distinguish healthy subject cognitive impairment beginning optimistic conclusion anyway message encouraging reader becomes curious detail actually done corpus submitted dataset constructed healthy patient control participant understandable people speak portuguese good incorporate technological detail article probably include least example short transcript translated english eventually part sample network embeddings transcript weakness start detailed introduction review relevant work cited reference le background omitted salton section reference directly related topic sentiment classification pedestrian detection image line omitted general line section shortened well suggestion compress first page focusing review strictly topic consider technological innovation detail incl sample english translation abcd cindarela narrative relatively short narrative portuguese abcd dataset open question similarity word found order construct word embeddings line explain generate word level network continuous word representation source learning continuous word representation datasets abcd cinderella external corpus used line written word level gram network used generate word embeddings source training sure kind network together provide better accuracy vocabulary word line come general discussion important study help discover cognitive impairment perspective interesting another interesting aspect deal portuguese important explain computes embeddings language relatively fewer resource compared english text need revision shortening section compressing adding explanation experiment clarification nurc transcription norm given technical comment line lightweight lightweight line line cookie cookie word repeated twice table column line number line link behind title line point next list,2
130.json,strength explores problem identifying patient mild cognitive impairment analyzing speech transcript available three different datasets graph based method leveraging occurrence information word found transcript described feature encoded using different characteristic graph lexical syntactic property many others reported using fold cross validation using number classifier different model exhibit different performance across three datasets work target well defined problem us appropriate datasets weakness suffers several drawback hard read incorrect usage english current manuscript benefit review grammar spelling main machine learning problem addressed poorly described single instance classification seems every transcript classified case dataset description describe number transcript level table describe data study produced transcript patient irrelevant classification task text page consumed simply describing datasets detail affect classification task also unsure number section text say people involved total number male female table le motivation behind enriching graph represent word node graph connect similarity vector irrespective occurrence datsets biomedical domain domain specific tool leveraged since dataset class distribution unclear unclear determine accuracy good measure evaluation either case since binary classification task desirable metric reported unto decimal place small datasets transcript without statistical test increment therefore unclear gain significant,1
130.json,describes novel application mostly existing representation feature set method namely detecting mild cognitive impairment speech narrative nature problem datasets domain thoroughly described missing detail proposed solution experiment sound reasonable overall found study interesting informative term drawback need considerable editing improve readability detail concept appear missing example detail multi view learning used omitted linguistic feature need clarified entirely clear datasets used generate word embeddings presumably datasets described appear small purpose also clear disfluency filled pause false start repetition removed dataset might suggest important feature context also clear popular weighting scheme used classification addition test significance provided substantiate conclusion experiment lastly related work described superficially detailed comment provided abstract abstract need shortened detailed note line need rephrasing however disfluency produce agrammatical speech impacting parsing impacting parsing line mean correct grammatical error transcript manually clear performed fact grammatical error present indicate reading introduction related work section becomes clear mean perhaps include example disfluency line need rephrasing lightweight language independent representation line need rephrasing immediately clear exactly datasets maybe cinderella line year sure exactly year mean line need rephrasing line obvious also problem disfluency explanation helpful line mean best scenario line public corpus dementia bank link citation dementia bank helpful line link citation describing picnic picture western aphasia battery helpful line explanation subtest helpful line missing citation line appears core related work described superficially example helpful know precisely method used achieve task compare study line please refer conference citation guideline believe something along line aluisio used line definition appears missing line could rephrasing lemmatization necessarily last step text processing normalization fact also additional common normalization preprocessing step omitted line create word embeddings using datasets external datasets line consisted consist line need rewritten manually segmented dementiabank cinderella mean segmented segmented sentence datasets automatically segmented abcd defined itemized datasets subsequently refer dataset confusing maybe could explicitly name datasets opposed referring first second third table caption demographic information present additional statistic dataset described line clear filled pause false start repetition removed might suggest important feature context line multidisciplinary team psychiatrist consisting psychiatrist line link citation describing transcription norm helpful section clear dataset used generate word embeddings line shortest path defined feature section linguistic feature need significantly expanded clarity also please check conference guideline regarding additional page supplementary material line work term frequency work term frequency also seems common weighting scheme sentence line need rewritten line mean threshold parameter threshold word embedding cosine distance line missing period section classification algorithm detail exactly scheme multi view learning used entirely omitted statistical significance result difference provided,2
87.json,strength nicely written understandable clearly organized targeted answering research question based different experiment weakness minimal novelty first sentence heuristic summarization literature many year work essentially applies heuristic evolved keyword extraction setting work trivial really novel lack state recent method experiment system evaluation state system simply us strong baseline even though experiment answer question perform better baseline confident illustrates system performs better current state somewhat reduces value general discussion overall good propose published presented hand propose position system performance respect martinez romo juan lourdes araujo andres duque fernandez semgraph extracting keyphrases following novel semantic graph based approach journal association information science technology work hold remarkable resemblance point ngoc minh nguyen akira shimazu unsupervised keyphrase extraction introducing kind word keyphrases australasian joint conference artificial intelligence springer international publishing,3
649.json,strength proposes evaluation metric automatically evaluating quality dialogue response task oriented dialogue metric operates continuous vector space representation obtained using rnns comprises component compare context given response compare reference response given response comparison conducted mean product projecting response corresponding context reference response space projection matrix learned minimizing squared error prediction human annotation think work give remarkable step forward towards evaluation task oriented dialogue system different previous work area pure semantic similarity pursued going beyond pure semantic similarity elegant manner learning projection matrix transform response vector context reference space representation curious projection matrix differ original identity initialization training model think valuable discussion introduced rather focusing much resulting correlation weakness also leaf lot question related implementation instance clear whether human score used train evaluate system single annotation resulting average annotation also clear dataset split train test whether fold cross validation conducted also nice better explain table correlation adem related score presented validation test set score presented full dataset test section training vhred also clumsy confusing probably better give le technical detail better high level explanation training strategy advantage general discussion many obvious case metric fail often incapable considering semantic similarity response figure careful statement like problem semantic similarity opposite problem completely different semantic cue might constitute pragmatically valid response semantic similarity enough evaluate dialogue system response dialogue system response evaluation must beyond semantics actually matrix helping accurate evaluate dialogue response quality automatically could considered automatic turing test original intention turing test proxy identify define intelligent behaviour actually proposes test intelligence based intelligent machine capability imitate human behaviour difficult common human distinguish machine response actual human response course related dialogue system performance think correct automatically evaluating dialogue response quality automatic turing test actually title towards automatic turing test somehow misleading simplifying assumption good chatbot whose response scored highly appropriateness human evaluator certainly correct angle introduce problem task oriented dialogue system rather turing test regarding related work might like take look well make reference wochat workshop series shared task description corresponding annotation guideline discussion session used used,3
654.json,general discussion extends zhou approach semantic role labeling based deep bilstms addition applying recent best practice technique leading quantitative improvement provide insightful qualitative analysis well written clear structure provide comprehensive overview related work compare representative model hace applied data set found interesting convincing welcome research contribution show work well also analyzes merit shortcoming learning approach strength strong insightful discussion error analysis weakness little insight regarding task,3
654.json,present state deep learning semantic role labeling natural extension previous state system zhou recent best practice initialization regularization deep learning literature give relative error reduction gain task also give depth empirical analysis reveal strength remaining issue give quite valuable information researcher field even though understand improvement point measure quite meaningful result engineering point view think main contribution extensive analysis experiment section depth investigation analysis section detailed analysis shown section performed quite reasonable give comparable literature novel information relation accuracy syntactic parsing type analysis often omitted recent paper however definitely important improvement well written well structured really enjoyed like accepted,3
382.json,strength present step direction developing challenging corpus training sentence planner data text important timely direction weakness unclear whether work reported represents substantial advance perez beltrachini method selecting content directly compare present appears main novelty additional analysis however rather superficial good report comparison nnlg baseline fare corpus comparison however bleu score appear much much higher suggesting nnlg baseline sufficient informative comparison general discussion need clearly articulate count substantial advance published already perez beltrachini nnlg baseline taken seriously contrast lrec common publish main session corpus development methodology absence system making corpus also stronger included analysis syntactic construction corpus thereby directly bolstering case corpus complex exact detail number different path shape determined also included ideally associated syntactic construction finally note limitation method nothing include richer discourse relation contrast consequence background long central respect corpus described walker jair isard lrec interesting discussed comparison method reference marilyn walker amanda stent franois mairesse rashmi prasad individual domain adaptation sentence planning dialogue journal artificial intelligence research jair isard methodius corpus rhetorical discourse structure generated text proceeding tenth conference language resource evaluation lrec portoro slovenia addendum following author response thank informative response response offer crucial clarification raised overall rating comparison perez beltrachini perhaps important eventual reader still seems reviewer advance could made much clearer true perez beltrachini cover content selection dataset differs really seem much complete methodology constructing data text dataset beyond obvious crowd sourcing step extent step innovative especially crucial highlighted interesting crowd sourced text rejected verification step related reviewer concern interesting example rejected extent indicates higher quality text dataset beyond main point really collecting crowd sourced text make possible make comparison corpus data text level reviewer crucial whole picture nnlg baseline issue relative difference performance baseline corpus could disappear substantially higher scoring method employed assumption relative difference remain even fancier method made explicit acknowledging issue footnote even limitation comparison still strike reviewer useful component overall comparison datasets whether dataset creation able without system though indeed unprecedented issue perhaps novel important dataset likely reviewer acknowledges importance dataset comparison existing one even advance already published content selection work finally reviewer concurs reviewer need clarify role domain dependence mean wide coverage final version accepted,3
382.json,strength potentially valuable resource make good point weakness awareness related work trying domain independent microplanning even possible crowdsourced text appropriate general discussion interesting present potentially valuable resource many way sympathetic however high level concern addressed perhaps address response surprised constant reference comparison fairly obscure previously heard better justified work comparison well known corpus one list section also many project looked microplanning issue verbalising dbpedia indeed workshop many paper dbpedia http webnlg sciencesconf http aclweb anthology also previous work duboue kutlak like le fixation awareness work dbpedia microplanning tends domain genre dependent example pronoun used much often novel aircraft maintenance manual much work focused domain dependent resource real question whether possible even theory train wide coverage microplanner discus need show aware concern concerned quality text obtained crowdsourcing people dont write well clear gathering example text random crowdsourcers going produce good corpus training microplanners remember ultimate goal microplanning produce text easy read imitating human writer along learning approach microplanning make sense confident human writer produced well written easy read text reasonable assumption writer professional journalist example dubious writer random crowdsourcers presentational perspective ensure text meet font size criterion text especially tiny difficult read text font size text body initially rate borderline look forward seeing author response adjust rating accordingly,2
117.json,strength address relevant topic learning mapping natural language relation context partial information argument case large number possible target relation proposal consists method combine different representation input text word level representation segmentation target relation name also input text relation single token without segmentation relation name input text seems main contribution ability rank entity entity linking step show improvement compared state weakness approach evaluated limited dataset general discussion think section fit better inside related work become section proposal thus section splitted properly,3
588.json,content proposes task provides dataset task predict blanked named entity text help external definitional resource particular freebase named entity typically rare appear often corpus possible train model specifically entity argues convincingly important setting explore along multiple baseline neural network model problem presented make external resource also accumulates evidence across context text strength collection desideratum task well chosen advance field predicting blanked named entity task already shown interesting daily mail dataset make task hard language model focus rare entity drive field towards interesting model collection baseline well chosen show neither without external knowledge simple cosine similarity based external knowledge task well main model chosen well text clear well argued weakness puzzled fact using larger context beyond sentence blank help model using additional context hierenc accumulates knowledge context possible explanation either sentence blank across board informative task sentence without explanation suggested seems unintuitive case another possible explanation using additional context hierenc using temporal network much useful enlarging individual context feeding larger recurrent network think could going general discussion particularly like task data proposes setup really drive field forward think mind main contribution,3
588.json,general discussion deal task predicting missing entity given context using freebase definition entity highlight importance problem given entity come long tailed distribution popular sequence encoders encode context definition candidate entity score based similarity context clear task indeed important dataset useful benchmark approach serious weakness evaluation leaf question unanswered strength proposed task requires encoding external knowledge associated dataset serve good benchmark evaluating hybrid system weakness model evaluated except best performing hierenc access contextual information beyond sentence seem sufficient predict missing entity unclear whether attempt coreference anaphora resolution made generally help well human perform task choice predictor used model unusual unclear similarity context embedding definition entity good indicator goodness entity filler description hierenc unclear understand input temporal network average representation instantiation context filled every possible entity vocabulary seem good idea since presumably instantiation correct likely introduce noise informative given rare entity prediction problem help look type level accuracy analyze accuracy proposed model vary frequency entity question important assumption made good entity embeddings assumption tested tried building classifier take input read response still think task dataset could benefit human evaluation task potentially good benchmark system know difficult task presented indicative reason stated hence changing score,1
79.json,considers problem completion proposes itransf purpose unlike stranse assigns relation independent matrix proposes share parameter different relation proposed tensor constructed contains various relational matrix slice selectional vector alpha used select subset relevant relational matrix composing particular semantic relation discus technique make alpha sparse experimental standard benchmark datasets show superiority itransf prior proposal overall well written experimental good however several concern regarding work hope answer response arranging relational matrix tensor selecting appropriately considering linearly weighted relational matrix ensure information sharing different relational matrix case performed tensor decomposition projected different slice relational matrix common lower dimensional core tensor clear approach taken despite motivation share information different relational matrix requirement share information across different relational matrix make attention vector sparse contradictory attention vector truly sparse many zero information flow slice optimisation spend space discussing technique computing sparse attention vector mention page regularisation work preliminary experiment however experimental shown regularisation explain suitable task reviewer appears obvious baseline especially given ease optimisation instead hard optimisation propose technique rather crude approximation solve trouble could spared used vector alpha performing selection weighing slice slightly misleading call attention term used different type model attention used machine translation clear need initialise optimisation trained embeddings transe cannot simply randomly initialise embeddings done transe update fair compare transe transe initial point learning association semantic relation idea used related problem relational similarity measurement turney jair relation adaptation bollegala ijcai good current work respect prior proposal modelling inter relational correlation similarity thanks providing feedback read,2
96.json,strength dataset useful researcher area algorithm sentiment word based machine translation proposed interpret sarcasm tweet weakness provide detailed statistic constructed dataset integrating sentiment word clustering machine translation technique simple straightforward novelty challenging issue general discussion overall well written experiment conducted carefully analysis reasonable offer comment follows according data collection process tweet annotated five time determine regarded gold standard measure performance technique moses well known good baseline another technique together comparison differ work focus sarcasm detection research topic interesting attempt interpret sarcasm reflecting semantics,2
96.json,focus interpreting sarcasm written twitter identifying sentiment word using machine translation engine find equivalent sarcastic tweet edit thank answer appreaciate added line commenting strength among positive aspect work like mention parallel corpus presented think useful researcher area identifying interpreting sarcasm social medium important contribution also attempt evaluate parallel corpus using existing measure one used task also used human judgement evaluate corpus aspect fluency adequacy equivalent sentiment room improvement tackling problem interpretation monolingual machine translation task interesting appreciate intent compare architecture think relatively small dataset needed used predictable neural interpretation performing worse moses interpretation came conclusion seeing table addition comparing architecture liked configuration used moses least provide explanation configuration described line choice justified thank response understand difficult write detail hope include line answer believe could valuable information presented sing clear evaluate component beforehand important component evaluated particularly clustering used positive negative word said used mean clustering algorithm clear wanted create cluster word test number instead positive negative word respectively also could another algorithm beside kmeans instance star clustering algorithm aslam require parameter thanks clarifying sign search tweet sentiment word found change cluster contain word assuming limit number sentiment word found decides many sentiment word change example tweet provided section constantly irritated anxious depressed great feeling clustering stage sign something like constantly cluster cluster cluster cluster feeling correct please explain sign thanks clarifying minor comment line section said sign context interpretation differ original sarcastic tweet case come closer gold standard human interpretation mean human interpretation original tweet idea section line could eliminate footnote adding cluster cluster number reference aslam javed pelekhov ekaterina daniela star clustering algorithm static dynamic information organization journal graph algorithm application http eudml,3
727.json,strength interesting task clearly written easy follow created data useful researcher detailed analysis performance weakness method adapted related work result comparison explanation uniqueness task discussion limitation previous research solving problem added emphasize research contribution general discussion present supervised weakly supervised model frame classification tweet predicate rule generated exploiting language based twitter behavior based signal supplied probabilistic soft logic framework build classification model political frame classified tweet multi label classification task experimental demonstrate benefit predicate created using behavior based signal please find specific comment discussion frame classification differs stance classification umbrella different level granularity benefit adding brief discussion exactly transition long congressional speech short tweet add challenge task example past research rely specific cross sentential feature apply tweet consider adapting method frame classification work congressional speech stance classification work text extent possible limitation twitter data compare work seems weakly supervised unsupervised term interchangeably used case please clarify author response believe weakly supervised technically correct terminology setup work used consistently throughout initial unlabeled data labeled human annotator classification weak noisy label sort keywords come expert presented method completely unsupervised data traditional unsupervised method clustering topic model word embeddings calculated kappa straightforward reflection difficulty frame classification tweet line viewing proof rather strong claim kappa merely represents annotation difficulty disagreement many factor contribute value poorly written annotation guideline selection biased annotator lack annotator training difficulty frame classification tweet human annotator actually intend relate cohen kappa strong enough task opinion rely annotated label line ignore contextual information negation conditional hypothetical statement impacting contributing word calculating similarity frame tweet effect frame prediction consider using model determine similarity larger text unit perhaps using skip thought vector vector compositionality method ideal exclude annotated data calculating statistic used select gram line mention entire tweet data used otherwise statistic test fold labeled data weakly supervised setup still leak selection process think made difference current selection gram size unlabeled data much larger constituted cleaner experimental setup please precision recall table minor please double check rule footnote placement concerning placement punctuation,3
727.json,strength address challenging nuanced problem political discourse reporting relatively high degree success task political framing detection interest community well written weakness quantitative given author compared traditional baseline classification algorithm making unclear degree necessary poor comparison alternative approach make difficult know take away qualitative investigation interesting chosen visualization difficult make sense little discussion perhaps make sense collapse across individual politician create clearer visual general discussion submission well written cover topic interest community time lack proper quantitative baseline comparison minor comment line year provided boydstun citation unclear similar behavior time tweeting necessarily indicative similar framing citation given support assumption related work go quite number area gloss work clearly related model political discourse work spending much time mentioning work tangential unsupervised model using twitter data section unclear whether wordvec trained dataset used trained embeddings give intuition behind unigrams used predict frame bigram trigram used predict party note temporal similarity worked best hour chunk make mention important assumption unable provide full work still worthwhile give reader sense performance look like time window widened table caption make clear score well clarifying score weighted micro macro also made clear evaluation metric section page,2
818.json,summary aim learn common sense relationship object category comparative size weight strength rigidness speed unstructured text insight leverage correlation action verb comparative relation throw larger strength proposes novel method address important problem mining common sense attribute relation text weakness liked example object pair action verb predicted attribute relation interesting action verb corresponding attribute relation also lack analysis discussion kind mistake make number object pair dataset small many distinct object category scalable approach larger number object pair unclear frame similarity factor attribute similarity factor selected general discussion suggestion discus following work compare mining attribute attribute distribution directly getting comparative measure advantage offered proposed method compared direct approach extraction approximation numerical attribute dmitry davidov rappoport minor typo abstract line mention dimension five line line object first first line skimp smaller line selctional selectional,3
818.json,study interesting problem extracting relative physical knowledge action object unstructured text inference factor graph consists type subgraphs action graph object graph stem insight common knowledge physical world influence people talk even though rarely explicitly stated strength try solve interesting challenging problem problem hard reporting bias insight approach inspiring innovative clearly described idea handling text sparsity semantic similarity factor also appropriate empirical evidence well support effectiveness compared baseline well written informative visualization except minor error like dimension abstract five everywhere else weakness benefit drawback component still somehow discussed hard tell limited quantitative example inherent discrepancy cross verb frame similarity within verb frame similarity action object compatibility frame throw thrown share verb primitive throw infer within verb given side frame thrown kicked share frame known inferred current deal discrepancy might better qualitative analysis evidence also need provided gauge difficult task dataset example incorrectly classified action object also ambiguous human type action object tend make mistake verb frame type usually harder others interestingly mistake made incorrectly enforced proposed semantic similarity think analysis component qualitative inspire general framework task general discussion author response reading response tend keep current rating accept response well address concern tend believe necessary background experimental analysis added given organization extra page hard author response think general solid interesting tend accept better question answered,3
376.json,strength useful modeling contribution potentially useful annotated data important problem event extraction relationship country expressed news text weakness many point explained well general discussion work tackle important interesting event extraction problem identifying positive negative interaction pair country world rather actor country primary contribution application supervised structured neural network model sentence level event relation extraction previous work examined task overall area knowledge publicly availble sentence level annotated data problem make contribution well annotating data included submission released could useful future researcher area proposed model seem application various tree structured recursive neural network model demonstrate nice performance increase compared fairly convincing broad baseline able trust also present manual evaluation inferred time series news corpus nice torn problem terrific application recursive model seems like contribution problem unfortunately many aspect model experimentation evaluation explained well work carefully written could really great note baseline need explanation example sentiment lexicon explained lstm classifier left highly unspecified multiple different architecture lstm classification trained reference approach using shelf code case please refer cite also make easier reader understand replicate necessary impossible replicate based line explanation supplied code seem include baseline recursive model great supplied code part system want penalize missing relevant since detail baseline could really replicated based explanation recursive model trained visualization section minor contribution really innovation finding work work line line unclear problem difficult compared also sentence somewhat ungrammatical tree binarized footnote tensor version need citation explain referred state verb defined definition event word come particular previous work motivates please refer something appropriate related footnote course collapsed form work using dependency label point stanford collapsed form remove preposition dependeny path instead incorporate label cameo tabari category mapped positive negative entry performance sensitive mapping seems like hard task hundred cameo category consider using goldstein scaling used political science well cited work connor reason sentiment lexicon appropriate task clear failed finding alpha meeting requirement mean requirement attempt find precision recall value based class mean contingency table gold predicted class sentence leaf ambiguous precision recall calculated information aggregation seems fine though fairly temporal smoothing function standard much justification especially given something simpler like fixed window average could used visualization seems pretty without much justification choice graph visualization shown seem illustrate much also discus related work spatial visualization country country relationship peter hoff michael ward union country well defined concept mmybe mean international organization strong weak peak selected particular chosen peak need example explanation mean judge polarity peak look like algorithm wrong hard ass agreement rate judged claim gerrish connor different purpose output work right work extract time series statistical information polarity relationship country also extract topical keywords explain aspect relationship concerned le concerned certainly previous work address fine address last sentence seems like pretty statement raise question gerrish connor conduct evaluation external database country relation developed political science military interstate dispute work evaluation well various weakness data evaluation approach need discussed justified,1
726.json,proposes approach learning semantic parser using encoder decoder neural architecture distinguishing feature semantic output full query method evaluated standard datasets atis well novel dataset relating document search solid well executed take relatively well established technique form encoder decoder trimming data augmentation paraphrasing us generate query purported advantage query expressive semantic formalism commonly used literature edited untrained crowd worker familiar semantic parsing expressive standard semantic formalism really query three datasets standard formalism unable capture full semantics query really best datasets showcase expressivity also term learns fraction actually much extra expressivity able capture also bias term style query tends generate wanted better sense potential actuality able capture need extra expressivity relative datasets experiment somewhat related start section assert harder directly produce never actually show seems statement expressivity anything else return question much actually generating next really liked seen discussion type query generates second part evaluation scholar dataset specifically query formed way formed crowd worker required post edit query much effort take equally correct crowd worker constructing query always able construct perfect query experience suggest similar vein error analysis liked seen agreement number annotator incomplete result query seems rely heavily existing knowledge part annotator therefore highly subjective overall achieves impressive well executed wanted insight true ability generate better sense subset language generates couple minor thing linguist write refer linguist linguist able write query either think point trying make simply annotator without specific training semantic translation query able perform task clear anonymized utterance point right saying paraphrase single word time presumably exclude entity paraphrasing introduce visual variable term line type differentiate three line viewing grayscale various inconsistency reference casing issue freebase wang missing critical publication detail wong mooney,3
419.json,introduces simple effective method morphological paradigm completion resource setting method us character based seqseq trained example language resource poor language closely related resource rich language training example annotated paradigm property language thus enables transfer learning across language language share common character common paradigm proposed multi lingual solution novel similar architecture explored syntax language modeling novelty apply approach morphology experimental show substantial improvement monolingual baseline include thorough analysis impact language similarity quality interesting clearly written think nice contribution conference program detailed comment main question proposed general multilingual methodology limited pair language rather set similar language example romance language could included training improve spanish paradigm completion slavic language cyrillic script could mixed improve ukrainian interesting extension model lingual multilingual setting think arabic fair fairly meaningless baseline given different script morphology target language interesting baseline language partially shared alphabet different typology example slavic language latin script could used baseline language romance language arabic excluded consider distant language family baseline experimental still strong half page discussion contribution arabic regularizer also add little remove arabic experiment regularizer according footnote work even better adding arabic transfer language related work missing line work language universal model basically approach learn shared parameter input multiple language language input mediate language related study include multilingual parser ammar language model tsvetkov machine translation johnson minor think claim correct line tag easy transfer across language transfer annotation also challenging task reference waleed ammar george mulcaire miguel ballesteros chris dyer noah smith many language parser tacl yulia tsvetkov sunayana sitaram manaal faruqui guillaume lample patrick littell david mortensen alan black lori levin chris dyer polyglot neural language model case study cross lingual phonetic representation learning naacl melvin johnson mike schuster quoc maxim krikun yonghui zhifeng chen nikhil thorat google multilingual neural machine translation system enabling zero shot translation arxiv preprint arxiv response author response thanks response looking forward reading final version,3
462.json,strength approach described manuscript outperformed previous approach achieved state result regarding data method used combination market text data approach used word embeddings define weight lexicon term extending similar term document weakness deep learning based method known able achieve relatively good performance without much feature engineering sentimental analysis literature search needed compare related work better approach generally improved performance feature based method without much novelty proposal feature general discussion manuscript described approach sentimental analysis method used relatively method using word embeddings define weight lexicon term however novelty significant enough,1
462.json,strength weakness general discussion investigates sentiment signal company annual filing report forecast volatility evaluate information retrieval term weighting model seeded finance oriented sentiment lexicon expanded word embeddings used reduce dimensionality support vector regression applied similarity estimation addition text based feature also text based market feature sector information volatility estimate multiple fusion method combine text feature market feature evaluated comment interesting include experimental condition namely simple trigram prior sentiment feature reflect delta idf score individual feature additional baseline good binary feature could corroborate reference http pdfs semanticscholar ccaaaeeefeebebea,1
97.json,strength try tackle practical problem automated short answer scoring particular japanese gotten much attention english language weakness simply read like system description light experiment insight show lack familiarity recent related work aimed english term methodology evaluation couple http aclweb anthology page http aclweb anthology also recent kaggle competition generated several methodology http kaggle asap general discussion meet standard preferred experiment feature ablation study algorithm comparison motivated final system design well sort qualitative evaluation user study mixed initiative user interface feature improved score feel like work progress without actionable method insight also pearson spearman correlation kappa score considered appropriate accuracy sort ordinal human score,1
97.json,present text classification method based training technique using labeled unlabeled data reported experimental several benchmark data set including trec data showed method improved overall performance compared comparative method think approach using training fine tuning novel originality labeled unlabeled data training step compare three baseline without training deep learning unsupervised training using deep autoencoders think interesting compare method method presented introduction section,2
395.json,outline method learn sense embeddings unannotated corpus using modular sense selection representation process learning achieved message passing scheme module cast reinforcement learning problem strength generally well written present idea clearly make comparison related work required experiment well structured overall good though outstanding however several problem prevent endorsing completely weakness main concern magnification central claim beyond actual worth term deep title several time skip gram architecture deep misrepresentation also reinforcement learning central claim however best understanding motivation implementation lack clarity section try cast task reinforcement learning problem go major drawback learning algorithm used algorithm relate originally claimed policy furthermore remains unclear novel modular approach work seems similar learning approach optimal sense selected step objective optimized step yield better sense representation properly distinguish approach motivative preferred first place make term pure sense representation multiple time claim central contribution sure mean beneficial claim linear time sense selection clear case highlighting fact relevant part helpful finally claim state however single maxsimc metric work achieved overall better using avgsimc metric state everything claim achieves abstract intro least little misleading,2
395.json,describes novel approach learning multi sense word representation using reinforcement learning cbow like architecture used sense selection computing score sense based product word embeddings current context corresponding sense vector second module based skip gram used train sense representation given sense selection module order train module apply learning value provided cbow based sense selection module reward given skip gram negative sampling likelihood additionally propose approach determining number sens word parametrically creating sens value existing score score resulting approach achieves good maxsimc metric comparable previous approach avgsimc suggest approach could used improve performance downstream task replacing word embeddings probable sense embedding nice claim explored perhaps sequential labeling task tagging especially light previous work questioning usefulness multi sense representation downstream task found somewhat misleading suggest relying maxsimc could reduce overhead real world application sense disambiguation step associated parameter still required addition sense embeddings clustering based approach using weighted average sense representation similar overhead claim improving wordvec using data also particularly surprising scws misleading contribution advance differ much previous work modular quality approach flexibility think could explored sense disambiguation module us vector averaging cbow approach positive aspect able substitute context composition approach using alternative neural architecture composition technique relatively easily applies interesting approach problem explored many way standard benchmark comparable previous work particularly surprising interesting however approach go beyond simple extension skip gram multi sense representation learning providing modular framework based reinforcement learning ideally aspect explored overall approach interesting enough considered acceptance could help move research area forward number typo addressed line representation selects note thank response,3
792.json,strength presentation final section excellent read well start clear structure argumentation part good address important problem attempting incorporate word order information word sense embeddings proposed solution interesting weakness unfortunately rather inconsistent left entirely convinced proposed model better alternative especially given added complexity negative fine insufficient analysis learn moreover reported word analogy task besides told proposed model competitive could interesting analyzed aspect experimental setup unclear poorly motivated instance corpus datasets detail unfortunately quality deteriorates towards reader left little disappointed quality presentation argumentation general discussion learn representation word sens shared emerging space done lstmembedsw version rather consisently performs worse alternative case motivation learning representation word sens shared semantic space entirely clear never really discussed motivation intuition behind predicting trained embeddings explicitly stated also trained embeddings lstmembedsw representation word sens used different alternative possible setup used experiment importance learning sense embeddings well recognized also stressed unfortunately however seems never really evaluated remains unclear word similarity datasets considers word independent context size training corpus instance using different proportion babelwiki shown figure however comparison somewhat problematic size substantially different size semcor moreover really small typically small corpus learning embeddings wordvec proposed model favor small corpus stated evaluated test set independent wssim wsrel make comparison problematic case giving three win opposed proposed model said faster train using trained embeddings output layer however evidence support claim provided strengthen table dimensionality fair comparison section synonym identification missing similarity measurement describe multiple choice task approached reference table missing description training word analogy task mentioned describing corresponding dataset,1
19.json,strength approach novel promising beating state weakness linguistic motivation behind troublesome feel benefit thoughtful interpretation general discussion present approach zero pronoun resolution chinese advocate novel procedure generating large amount relevant data unlabeled document data integrated smartly based architecture training step improve state mixed feeling study hand approach seems sound show promising beating recent system chen hand main contribution framed disturbing linguistic point view particular zero pronoun resolution linguistically speaking context modeling task requiring accurate interpretation discourse salience semantic syntactic clue start assumption zero pronoun used specific context full normally possible perspective generating data replacing nominal zero blank sound convincing indeed show training module alone achieve reasonable performance think generated pseudo data called data seems likely encode form selectional preference nice could invest effort better understanding exactly training module learns reformulate corresponding section benefit proofreading native speaker english example sentence line grammatical point line restriction noun especially pronoun example strategy common pronoun english guarantee occurrence token indeed coreferent line term antecedent typically used denote preceding mention coreferent anaphor mean line ontonotes typo line shown evaluation gold annotated data provide reliable estimation performance indeed recent study coreference evaluate system mention example study chen citing provide different type evaluation including system mention please consider rerunning experiment realistic evaluation setup line understand dagger system name mean improvement statistically significant domain including line learn typo section section abbreviation instead without introducing please unify terminology reference please double check capitalization,3
481.json,work extend coco adding incorrect caption existing caption word difference demonstrate state method captioning perform extremely poorly determining caption fake determining word fake caption wrong selecting replacement word given fake word work build upon wealth literature regarding underperformance vision language model relative apparent capacity think work make concrete fundamental question area vision language model interesting thing consider nice task model shed light broken ness setting perform insightful analysis factor associated failure figure biggest concern similarity ding said think make really good point ding generate similar caption one differ word still break model think justifiably fundamental difference observation demonstrates ding engineering requirement simple approach still break thing catastrophically another concern neuraltalk select hardest foil clever idea worried creates risk self reinforcement bias neuraltalk bias fundamentally baked foil coco think section could longer relative rest liked paragraph liked part overall like nicely build upon highlight defficiencies vision language integration ding similarity game breaker think anything work show vision language model easy fool ding method even required small thing liked seen another baseline simply concatenates extracted feature train softmax classifier blind nice touch dumb vision langauge baseline close well lstm attention could made point even stronger supercategory wordnet coco understand idea specific artefact undesirable artifact included chance table line constant prediction baseline flip blind prediction better baseline entirely clear think chance line confusion ariplane reading author response think author response spot concern neuraltalk bias additional baseline addressed confident addressed final version keep score,3
706.json,thanks response look forward reading effect incentive ambiguity language domain review author response proposes build natural language interface allowing user define concept syntax trivial extension wang liang manning learning language game interaction question size vocabulary used possible position respect previous work inverse reinforcement learning imitation learning strength well written provides compelling direction solution problem dealing large possible program learning natural language interface weakness discus effect incentive final performance alternative considered claim method extended practical domain clear straightforward going sensitive method size vocabulary required domain increased ambiguity natural language create problem question discussed current experiment real world application definitely strengthen even,3
706.json,strength report interesting project enable people design language interacting computer program place using programming language specific construction focus ability people make definition nicely make recursive definition arrive general giving command example showing user could generate definition create palm tree motivating approach using learning grammar capture case seems like good weakness seems extension similar topic helpful explicit much comparison previous work related work section feature learning interesting always clear come play example good example social feature influenced outcome otherwise people work together create language general discussion,3
614.json,proposes integrating word sense inventory existing approach lexical substitution task using inventory filter candidate first propose metric measure mutual substitutability sense inventory human judgment lexsub task empirically measure substitutability inventory various source wordnet ppdb next propose clustering different paraphrase word ppdb using multi view clustering approach automatically generate sense inventory instead using aforementioned inventory finally cluster naive majority technique filter existing ranked list substitution candidate strength idea marrying vector space based approach sense inventory lexsub task useful since technique seem complementary information especially since vector space model typically unaware sense polysemy oracle evaluation interesting give clear indication much gain expect best case still large oracle actual score still argue usefulness proposed approach large difference unfiltered oracle weakness understand effectiveness multi view clustering approach almost across board paraphrase similarity view significantly better view combination learn usefulness view empirical example different view help clustering paraphrase word analysis different clustering technique differ except task directly without detailed analysis difference similarity view hard draw solid conclusion different view fully clear first read specifically immediately clear section connect reading like disjoint piece work instance understand connection section section adding forward backward pointer reference section useful clearing thing relatedly multi view clustering section need editing since subsection seem order citation seem missing line relatively poor performance noun make uneasy expect twsi really well nature fact oracle ppdbclus higher clustering approach disconcerting like understand better also directly contradicts claim clustering approach generalizable part speech since performance clearly uniform general discussion mostly straightforward term technique used experiment even show clear gain lexsub task pronged approach potentially gained using stronger algorithm additional question line hypernym hyponym line need symmetric line weighting scheme seems kind arbitrary indeed arbitrary principled choice high performance substclus ascribable fact number cluster tuned based view tuning number cluster based matrix affect conclusion related task could approach possibly generalize specific lexsub,3
614.json,strength present method exploit word sens improve task lexical substitutability show improvement prior method weakness reader usually important insight take away picture point view field natural language processing computational linguistics task lexical substitutability general particular help either improving system provide insight language find good answer answer either question reading practitioner want improve natural language understanding system focused first question lexical substitutability task improved compared prior work presented help application given current state high performing system discrete clustering word longer utterance often break compared continuous representation word paper discrete lexical semantics achieve task versus word distributed representation used input task machine translation question answering sentiment analysis text classification forth motivate work lexical substitutability given discrete lexical semantic representation often work well introduction cite paper several year back mostly small data scenario given word based english method task eager response general question mine minor point motivate consider substitute presented table tasha snatched away tasha snatched away sheet sentence varying meaning holding scenario word substituted sheet least understanding cannot hence much subjectivity task lexical substitute completely alter semantics original sentence minor point citation section missing addition read author response sticking earlier evaluation,1
182.json,dear thanks replying review comment clarifies detail question appreciate promise publish code helpful researcher based increased overall score strength well written extensive experiment good weakness nothing ground breaking application existing technology code available could expected general discussion established audio feature mfccs minor detail place lstm lstm place hyphen text explanation convolution clear table appear earlier page already cited standard approach video processing alternative probably positioned check overfitting mean avoid name write italic make easier recognize output wrong numerus either output superflous whitespace layer concatenation line know exact number person remove comma since doesnt insert hand reference need cleanup superflous whitespace munich superflous volume superflous line indent linguistics properly,3
660.json,present approach generating english poetry first approach combine neural phonetic encoder predicting next phoneme phonetic orthographic decoder computing likely word corresponding sequence phoneme second approach combine character language weigthed impose rythm constraint output language second approach also present heuristic approach permit constraining generated poem according theme love poetic device alliteration generated poem evaluated instrinsically comparing rythm generated line gold standard extrinsically asking human evaluator determine whether poem written human machine rate poem readability form evocation indicate second performs best human evaluator find difficult distinguish human written machine generated poem interesting clearly written article novel idea different model poetry generation based phonetic language character convincing evaluation precision evaluator protocol good evaluator evaluate poem many judgment collected poem task mention english native speaker poem notoriously hard read fluent second character based perhaps missed mechanism avoid generating word frequent word generated poem first transliterate phonetic orhographic representation rather since overall rule first good generic generating poetry might interesting spend le space evaluation second particular interested detailed discussion impact heuristic constrain theme poetic device impact evaluation could combined jointly constrain theme poetic device combination neural mode wfst reminiscent following combine character based neural generate dialog act wfst avoid generating word relate work cite natural language generation character based rnns finite state prior knowledge goyal raghav dymetman marc gaussier eric coling,3
660.json,describes methodology automatic generation rhythmic poetry rely neural network second allows better control form strength good procedure generating rhythmic poetry proposal adding control theme poetic device alliteration consonance asonance strong evaluation rhythm weakness poor coverage existing literature poetry generation comparison existing approach poetry generation evaluation theme poetic device general discussion introduction describes problem poetry generation divided subtasks problem content poem semantics problem form aesthetic rule poem follows solution proposed address subtasks limited fashion rely neural network trained corpus poetry represented phonetic character level depending solution encode linguistic continuity output indeed ensure output resemble meaningful text equivalent found providing poem appropriate semantics overstatement problem form said addressed case rhythm partial solution proposed poetic device aspect form concerned structure larger scale stanza rhyme scheme remain beyond proposed solution nevertheless constitutes valuable effort advancement poetry generation review related work provided section poor even cover previous effort consider worth mentioning work manurung misztal indurkhya cited later page placed section respect mentioned related research effort particular relevance consider gabriele barbieri franois mirko degli esposti markov constraint generating lyric style proceeding european conference artificial intelligence ecai raedt christian bessiere didier dubois patrick doherty paolo frasconi press amsterdam netherlands netherlands http work address similar problem discussed present gram based generation problem driving generation process additional constraint include review work discus similarity difference another research effort related attempting bearing evaluation process stephen mcgregor matthew purver geraint wiggins process based evaluation computer generated poetry proceeding inlg workshop computational creativity natural language generation page edinburgh september association computational linguistics work also similar current effort model language initially phonological level considers word gram level superimposed also feature layer representint sentiment consideration mcgregor make evaluation computer generated poetry also relevant extrinsic evaluation described present another work believe considered generating topical poetry ghazvininejad choi knight proc emnlp work generates iambic pentameter combining finite state machinery deep learning interesting proposal current constrasts particular approach although le relevant present consider extending classification poetry generation system mention rule based expert system statistical approach include evolutionary solution already mention work manurung evolutionary nature operating grammar case stand hold little effort comparison prior approach poetry generation make effort contextualise work respect previous effort specially case similar problem addressed barbieri similar method applied ghazvininejad,2
94.json,strength make several novel contribution transition based dependency parsing extending notion monotonic transition system dynamic oracle unrestricted projective dependency parsing theoretical algorithmic analysis clear insightful admirably clear weakness given main motivation using covington algorithm able recover projective arc empirical error analysis focusing projective structure strengthened even though main contribution theoretical side relevant include comparison state conll data set monotonic baseline version parser general discussion extends transition based formulation covington dependency parsing algorithm unrestricted projective structure allowing monotonicity sense later transition change structure built earlier transition addition show approximate dynamic oracle formulated system finally show experimentally oracle provide tight approximation monotonic system lead improved parsing accuracy monotonic counterpart majority language included study theoretical contribution view significant enough merit publication also think could strengthened empirical side particular relevant investigate error analysis whether monotonic system improves accuracy specifically projective structure analysis motivated ground ability recover projective structure main motivation using covington algorithm first place projective structure often involved long distance dependency hard predict greedy transition based parser plausible system improve situation another point worth discussion empirical relate state light recent improvement thanks word embeddings neural network technique example monotonicity claimed mitigate error propagation typical classical greedy transition based parser another mitigating problem recurrent neural network preprocessors parser order capture global sentence context word representation technique competing complementary full investigation issue clearly outside scope discussion highly relevant specific question data set conll shared task used sure legitimate reason stating explicitly prevent reader becoming suspicious hypothesis accuracy decrease basque monotonic system similar weaker trend seen also turkish catalan hungarian perhaps german compare state data set relevant contextualising allowing reader estimate significance improvement author response satisfied author response reason change previous review,3
37.json,strength relatively clear description context structure proposed approach relatively complete description math comparison extensive alternative system weakness weak summary side side human comparison section disfluency agrammaticality general discussion article proposes principled mean modeling utterance context consisting sequence previous utterance minor issue past turn table could making text associated table line le difficult ingest currently reader need count turn identifying reference description wonder whether second third last imply side specific global enumeration reader confusion eliminated explicitly defining segment mean segment level occurring line previously line seemingly thing referred sequence sequence similarity matrix term appear used interchangeably clear actually mean despite text section seems mean word subsequence word subsequence word subsequence implies whole utterance sure currently variable symbol appears used enumerate word utterance line well utterance dialogue line choose different letter different purpose avoid confusing reader going equation statement indicates retrieval based chatbot provide better experience state generation practice section appears unsupported approach referred deemed comparable case baseline better proposed method remaining case encouraged ass present statistical significance comparison weak comparison permit best claim proposed method worse rather better vhred baseline choose insert figure explicit first layer second layer third layer label accompanying text pervasive meet response candidate meet utterace line difficult understand spelling gated recurrent unites respectively line removed punctuation line exchanged baseline baseline cannot neglect,3
352.json,introduces configuration training objective neural sequence model multi task setting describe well multi task setting important task shared information scenario learning many task improve overall performance method section relatively clear logical like ended though could slightly better organized organization realized reading problem shared feature private feature space private feature shared space novel method problem organization front make method cohesive case introduce method keep task specific feature shared representation adversarial loss another keep shared feature task specific representation orthogonality constraint point confusion adversarial system lstm output another layer thetad relying parameter output considered probability distribution compared actual mean possible learn effectively mask task specific information lstm output seem like guarantee task specific information removed read evaluation section wrote hoped experiment look like interesting idea experiment imagine think basic show validity method helpful best known task primary concern lack deeper motivation approach think easy understand totally shared problem conflict feature space extension partially shared feature seems like reaction issue expect useful shared information shared latent space task specific space learn feature space maybe work maybe logic clear contrast seem start assumption shared private issue expected argument flow fully shared obviously problem shared private seems address practice shared private fully address issue reason introduce method effectively constrains space table helped partially understand going wrong shared private method term usually connotation another general trend probably shared feature space simple explanation example logical argument flow help introduction make really nice reading finally think research tie uncited work deep hierarchical supervised tagging lower level chunking next level tagging higher discus quality make possible conclude work task sufficiently similar made think previous work potentially could learn sufficiently similar task sufficiently similar shared learn nothing fall back learning independent system compared shared private baseline might overfit perform poorly inproceedings sogaarddeep title deep multi task learning level task supervised lower layer author gaard anders goldberg yoav booktitle proceeding annual meeting association computational linguistics volume page year organization association computational linguistics,3
352.json,summary present method learning well partitioned shared task specific feature space lstm text classifier multiclass adversarial training encourages shared space representation discriminative classifier cannot identify task source thus generic model evaluates fully shared shared private adversarial shared private lattermost main contribution also orthogonality constraint help reward shared private space distinct lower error rate single task multi task neural model also experiment task level cross validation explore whether shared representation transfer across task seems favourably finally analysis shared layer activation suggesting misled strong weight learned specific inappropriate task review summary good idea well expressed tested minor comment strength nice idea working well together particularly like focus explicitly trying create useful shared representation quite successful community appears need work quite hard create section clearly expressed task level cross validation section good evaluate transfer implementation data weakness minor typographic phrasing error individually fine enough warrant fixing infantile cart slightly real example data different differ working adversarially towards working competing matric matrix hyperparameter hyperparameters section number number agreement error closely edited shading final table print strangely mention unlabelled data table semi supervised learning section experiment omitted misunderstood error rate difference promising table statistical significance testing help make really convincing especially highlight benefit adversarial training pretty straightforward adapt parametric approximate randomisation test http titech takamura pub randtest promising note reference chinchor produce colour inconsistent caption figure blue used seems swapped worth checking misunderstood caption general discussion wonder connection regularisation effect adversarial training orthogonal training help limit shared feature space might worth drawing connection regularisation literature,3
489.json,comment author response thanks response particularly clarification hypothesis agree comment cross modal mapping share kind equation visual referential seem assume referent visually presented visual information usefully added word representation aggregate form encode perceptual aspect word meaning done textual information instance fact banana yellow frequently mentioned text adding visual information extracted image account aspect semantic representation word kind technical specific build distributional model also relevant think human cognition probably representation banana aggregate information banana seen touched tasted useful could discus issue explicitly differentiating multi modal distributional semantics general cross modal mapping particular also model perform similarly comment really urge accepted state form even completely align hypothesis goal enough better description useful community clinging digit difference large extent independent whether difference actually statistical significant bridge chance collapsing another difference statistically significant really make first bridge better bridge walk small quibble could find kind compact point title geared towards either generally explore find tackle extremely interesting issue label referential word meaning namely connection word meaning referent object external world applied understood correctly argue different typical word meaning representation obtained distributional method thing abstract lexical meaning word label appropriate given referent specific property specific context although context something explicitly leave aside hypothesis previously explored work schlangen colleague cited explores referential word meaning empirically specific version task referential expression generation namely generating appropriate noun given visually represented object strength problem tackle find extremely interesting argue problem previously addressed mainly using symbolic method easily allow exploration speaker choose name object scope research go beyond address link semantic representation reference broadly also like current technique datasets cross modal mapping word classifier referit dataset containing large amount image human generated referring expression address problem hand substantial number experiment well analysis weakness main weakness statement specific hypothesis within general research line probing found confusing result also hard make sense kind feedback give initial hypothesis especially point direction say pursues hypothesis accurate referential word meaning need fully integrate visual lexical knowledge expressed distributional vector space time beyond treating word independent label first part hypothesis understand fully integrate fully integrate visual lexical knowledge goal simply show using generic distributional representation yield worse using specific word adapted classifier trained dataset explicitly discus bound showing specifically word classifier must trained dataset word classifier sufficient amount item dataset obtained whereas word vector available many word obtained independent source even cross modal mapping trained dataset moreover simplest ridge regression instead best method lazaridou conclusion method better taken grain salt however hoping research goal constructive broader please clarify us three previously developed method previously available dataset problem defined schlangen sense originality high also point select limited subset referit dataset quite small vocabulary word even sure limited detailed comment aspect could clearer detailed comment contains many empirical analysis make concerted effort together still found difficult whole picture exactly experiment tell underlying research question general specific hypothesis tested particular different piece puzzle present together general discussion added author response despite weakness find topic relevant also novel enough interesting current technique address problem reference generally allows aspect explored received enough attention experiment analysis substantial contribution even though mentioned like present coherent overall picture many experiment analysis together address question pursued detailed comment section missing following work computational semantic approach reference abhijeet gupta gemma boleda marco baroni sebastian pado distributional vector encode referential attribute proceeding emnlp aurelie herbelot maria vecchi building shared world mapping distributional theoretic semantic space proceeding emnlp work beyond early work focus link flat metric flat section please number related dataset table specifying image region number overall number word number object name original referit dataset version release data data reviewing form marked data find information cannot considered name image object name semantically annotated portion referit keep girl example generally head noun relational generally could motivate choice understand ended restricted subset referit feature list extract suggest lexical least distributional knowledge detrimental learning word refers world follow frome norouzi cross modal projection give better different type task setup object labeling number belong data section table difference method statistically significant really numerically small conclusion method perform similarly seems unwarranted especially suggests part table also method highest accuracy almost identical counter intuitive given idea going section define ensemble classifier hand instead learning also method amount majority voting right table order model table text table report cosine distance discus term similarity clearer accordance standard practice reported cosine similarity table comment reported right column found curious gold data similarity higher transfer whereas task think could squeeze information phenomenon model format section like idea task confused understand line task exactly example help testsets train example hypernym hypernym even previous cross modal mapping model force understand claim larger test set think could even exploit referit using data moving datasets,3
489.json,author response accept response emphasizing novelty task comparison previous work also increase rating dataset software promised become public article publishing general present interesting empirical comparison referring expression generation model main novelty lie comparison unpublished called press anonymous described section clear whether extended modified anyhow current novelty considered comparison unpublished existing model complicates evaluation novelty similar experiment already performed model unclear comparison performed presented significant novelty might combined stated clearly combination described enough detail contribution considered following side side comparison method analysis zero shot experiment mostly confirms similar observation previous work analysis complementarity combined weakness unclear novelty significance contribution work seems like experimental extension cited anonymous main method introduced another weakness limited size vocabulary zero shot experiment seem contributive part additionally never presented significance score accuracy solidified empirical contribution work main value general feeling appropriate conference empirical method emnlp lastly found link usable software existing datasets used work observation section abstract compare three recent model abstract write also experiment combination approach section write present exploit distributional knowledge learning referential word meaning well explore compare different way combining visual lexical aspect referential word meaning eventually might better summarization novelty introduced give credit value work suggestion write abstract eventually even section focusing novel stating compare model others introduction determining name typo concerning concerning disjunct extension specify exemplify please building figure building figure section following lazaridou omitted section associate word corresponding distributional vector value used experiment finding optimal value please describe original work obvious obvious optimal finding similar vector section annotate training instance fine grained similarity signal according object name please exemplify language quite typo draft generally language cleaned well also believe american english spelling standard preferable summarise summarize please double check conference track chair,3
173.json,strength weakness many grammar error abstract general discussion,3
371.json,describes idea learn phrasal representation facilitate based language model neural machine translation strength idea incorporate phrasal information task interesting weakness description hard follow proof reading english native speaker benefit understanding evaluation approach several weakness general discussion equation mention phrase representation give length word embedding vector used representation generated based propose description using pyramid lstm sequential part combination architecture reason improvement simplified version performing better performing large data difference rnnsearch groundhog rnnsearch baseline table motivation using ending phrase using starting phrase pyramid encoder performing fair comparison since normally help make complex rnnsearch several time pbnmt section intent section,1
371.json,proposed phrasal architecture sequence sequence generation evaluated architecture based language modelling test evaluated fbi chinese english machine translation task nist evaluation set phrasal prnn architecture achieved generating subnetworks phrase strength phrasal architecture weakness technical unclear whether limit phrase length prnn maybe missed please explicit affect quite drastically every sentence largest phrase length sentence length largest phrase length sentence length simplified sort convolution state go convolution layer final softmax attention limit phrase length prnn make system tractable also mean phrase determined token ngrams produce sliding window pyramid encoders sentence instance parameter phrase close zero disable phrase phrase good intrinsic evaluation prnn addition evaluating purely perplexity bleu extrinsically usage attention mechanism without sort pruning might problematic phrasal level author opted sort greedy pruning described caption figure support given fixed phrase pair train time attention mechanism phrasal level computed inference apply attention data test time might kind problematic architecture scaled larger dataset empirical issue language modelling experiment choice evaluation train possibly dataset like common crawl enwiki appropriate language modelling experiment main issue experiment reporting need quite reworking evaluation table fair since trained larger corpus fbi tested fact previous study reported perplexity baseline using lstm lstm perplexity provided author showed fbi give advantage computing language perplexity tested also regarding section please cite appropriate publication previous work presented table previous work using training additionally version prnnv reported fbi evaluation table result section cannot simply presenting table without explanation still result section although clear bleu perplexity objective automatic measure evaluate architecture really okay table show perplexity bleu score without explanation table necessary explain lstm perplexity previous work higher author implementation table result presented table match description section true prnn outperforms pbsmt make clear different evaluation set score differs averaged test score prnn performs better please also make clear whether test micro average testsets concatenated evaluated macro average average taken across score individual test set score table please also include significance bleu improvement made prnn respect baseline http github jhclark multeval general discussion main contribution work phrasal effect architecture rather important show phrase coherent vanilla lstm thus bleu evaluation insufficient closer look evaluating phrase subset evaluation necessary support claim baseline system groundhog contains attention mechanism please specific describing section table please remove attention layer encoder figure also lack attention mechanism provides disadvantage baseline system unclear whether prnn outperform additive feature system attention mechanism unfair disadvantage even prevalent prnn us multiple phrasal attention layer within single sentence simple system without attention used benchmark question simpler phrasal pyramid rnns phrase soft average pooling layer minor issue figure little redundant think figure enough compare prnn figure also possibly figure combined pyramid part figure space freed explain section please abuse figure table caption whenever possible please keep description table figure text please verbose caption description main text figure table spacing equation also reduced latex vspace,1
768.json,strength well written examining context lexical entailment task great idea well defined approach experimental good analysis weakness information missing insufficient table caption descriptive clear description word type feature given general discussion present proposal consideration context lexical entailment task experiment demonstrate context informed model better context agnostic model entailment task liked idea creating negative example negative annotation automatically way described based wordnet positive example dataset interesting method develop dataset also liked idea transforming already used context agnostic representation contextualized representation experimenting different way contextualized representation mask contetxvec testing different datasets generalizability across different datasets also cross linguistically motivation various decision experimental design good used split used context ppdb showed thought clearly exactly line might want state briefly class weight determined added account unbalanced data context experiment affect direct comparison previous work way change line directionality directionality table suggested change line hierarchy wordnet hierarchy wordnet sake completeness represent mask also figure read author response,3
768.json,proposes method recognizing lexical entailment specifically hypernymy context proposed method represents context averaging pooling pooling word embeddings representation combined target word embedding element wise multiplication context representation left hand side argument concatenated right hand side argument creating single vectorial representation input input logistic regression classifier view major weakness first classification used concat linear classifier shown inherently unable learn relation supervised distributional method really learn lexical inference relation levy second make superiority claim text simply substantiated quantitative addition several clarity experiment setup issue give overall feeling still half baked classification concatenating word vector input linear classifier mathematically proven incapable learning relation word levy motivation behind using contextual setting handicap might somewhat mitigated adding similarity feature feature symmetric including euclidean distance since expect feature detect entailment convinced reasonable classification task superiority claim claim contextual representation superior contextvec evident best result table table excluding ppdb feature understanding variant proposed contextual representation fact us contextvec representation word type experiment us ready made embeddings glove parameter contextvec tuned completely different datasets different size comparing empirically flawed probably biased towards method using glove trained much larger corpus addition seems biggest boost performance come adding similarity feature proposed context representation discussed miscellaneous comment liked wordnet dataset using example sentence nice trick quite understand task cross lingual lexical entailment interesting even reasonable basic baseline really missing instead random baseline well true baseline perform context agnostic symmetric cosine similarity target word general table difficult read caption make table self explanatory also unclear variant mean perhaps precise description text variant could help reader understand ppdb specific feature really unclear could understand table overfull table random typo line table author response thank addressing comment unfortunately still standing issue prevent accepting problem base learning prototypical hypernym mathematically able learn relation appears different reading table maybe clarity issue prevents understanding claim contextual representation substantially improve performance supported furthermore seems like factor similarity feature greater effect,1
355.json,proposes joint neural modelling approach analysis japanese based grid rnns compare variously conventional single sequence approach solidly executed targeting well established task japanese achieving state task presenting task mostly accessible manner versed japanese said felt could talked complexity task example figure talking inherent ambiguity argument first predicate argument second predicate better describing task contrast largely ambiguity zero pronoun also liked seen stats proportion zero pronoun actually intra sententially resolvable complicates task defined needing implicitly distinguish intra inter sentential zero anaphor thing sure case inter sentential zero pronoun argument given predicate representation simply marking argument marked empty argument reading former case explicit representation fact zero pronoun seems like slightly defective representation potentially impact ability capture zero pronoun discussion appreciated constraint seem captured based method explicitly given predicate generally argument given type given argument generally fill argument slot given predicate liked seen analysis output well able learn sort constraint generally given number table single multi really improvement multi liked seen discussion relative difference output model largely identical different aggregate context observe difference model analysis like shed light internals model made difference solid strong main area believe could improved including take quite work presentation good figure aiding understanding level language issue nothing major error propagation error propagation solution solution figure bread bread independence independence good good significent significance watch casing reference japanese lstm conll,3
323.json,introduces extension entity grid convolutional neural network used learn sequence entity transition indicating coherence permitting better generalisation longer sequence entity direct estimate transition probability original nice well written instead proposing fully neural approach build existing work neural network overcome specific issue step valid approach useful expand comparison existing neural coherence hovy admit surprised score hovy achieves task make reader wonder error experimental setup performance corpus dependent proposed achieve corpus task successful deeper investigation factor strengthen argument considerably general fluent readable many place definite article missing line probably suggest proofreading specifically article usage mind expression limit used repeatedly sound unusual maybe limit capacity stop clearer final recommendation adjusted considering author response agree objective difficulty running people software held present effort made test hovy system problem encountered documented also suggest reproduce hovy original data set sanity check unless already done work,3
323.json,proposes convolutional neural network approach coherence text based well known entity grid representation coherence put approach well motivated described especially appreciate clear discussion intuition behind certain design decision section titled work extensive evaluation several task show proposed approach beat previous method however strange previous result could reproduced hovy suggest implementation modelling error addressed still relatively simple neuralization entity grid understand dimensional vector necessary represent four dimensional grid entry case extended grid help optimizing directly coherence ranking help learn better difference transition chain sentence might make difference especially since many article short writing seemed lengthy repeat certain part several place example introduction entity grid particular section also present related work thus first section repetition deleted worked section necessary rest section probably added section subsection rename section related work overall seems like solid implementation applying neural network entity grid based coherence considering proposed consolidation previous work expect full innovation representation feature task minor point benefit proof reading native speaker article missing many place corpus brown toolkit bottom left column figure figure firstly secondly first second limit prevents considering consider removing tandard final paragraph section since necessary follow short,2
148.json,strength article put field together text readability human machine comprehension text weakness goal entirely clear read time still understand talking article highly ambiguous talk machine comprehension text readability human miss important work readability field section completely unrelated discussion theoretical topic feeling trying answer many question time making quite weak question text readability impact datasets analyzed separately prerequisite skill general discussion title ambiguous good clarify referring machine comprehension text human reading comprehension reading comprehension readability usually mean dataset analysis suggested readability datasets directly affect question difficulty depends method feature used answer detection dependency parse feature need proofread english important omission like question easy solve simply look page annotate datasets metric mixing machine reading comprehension text human reading comprehension text although somewhat similar also quite different also large area readability text difficulty reading content check dubay principle readability costa mesa impact information good pointer distinguishing work readability question human article highly ambiguous page example show readability text necessarily correlate difficulty question machine comprehension section referring skill human machine machine citing paper human sure referring machine many question annotator annotate annotator clear annotate question keeping mind machine people,3
49.json,strength present interesting extension attention based neural approach leverage source sentence chunking additional piece information source sentence modified chunking information used differently recurrent layer focus generating chunk time focus generating word within chunk interesting believe reader enjoy getting know approach performs clearly written alternative approach clearly contrasted evaluation well conducted direct contrast paper evaluation table even though could strengthened comment convincing weakness always could done experiment section strengthen case chunk based model example table indicates good compared previous paper careful reader wonder whether improvement come switching lstms grus word good tree sequence result verify chunk based approach still best another important aspect lack ensembling emphasis claiming best single ever published probably true best system reading table correctly ensemble able report chunk based ensemble come table could much stronger impact finally table interesting included decoding time mention briefly character based le time consuming presumably based eriguchi cite provided number chunk based decoding reported either chunk based faster slower word based similar know adding column table decoding time give value general discussion overall think interesting worth publishing minor comment suggestion improve presentation opinion course think clearly state early chunk supplied externally word learn chunk became apparent reading cabocha page think mentioned earlier important contrast char based baseline often text least couple time boast bleu gain think reader bothered reader interested gain best baseline good detail unks handled neural decoder least citation dictionary based replacement strategy used sentence line train encodes source sentence single vector strictly correct correct bidirectional encoder encodes source sentence vector least figure motivating example line weird depend bite depend source side depend bite argument long dependency problem really apply,3
49.json,summary introduces chunk level architecture existing model three model proposed correlation word chunk modelling target side existing model strength well written clear proposed model contribution proposed model incorporating chunk information model novel well motivated think model generally applicable many language pair weakness minor point listed follows figure surprised function word dominate content one japanese sentence sorry understand japanese equation sequence vector like matrix represented bold text distinguish scalar equation instead line encoder state referred bidirectional state line confused phrase sequential information chunk chunk still sequential information equation confused perhaps insert like indicate word chunk question experiment table source language statistic baseline running baseline without using chunk information instead using baseline vsrc different easy effect chunk based model baseline processing post processing step baseline comparable response still think baseline reference baseline existing shown figure baseline result useful comparison chunk translated example generated automatically manually possible compare chunk generated bunsetsu chunking toolkit case chunk information test table required response address point surprised beam size used decoding process suppose large beam size likely make prefer shorter generated sentence past tense used experiment line used line perform performed used general discussion overall solid work first tackling chunk based well deserves slot,3
496.json,strength nice coverage different range language setting isolate relatedness amount morphology interact translating closely related morphologically rich language distant one affecting system learns morphology include illuminating analysis part architecture responsible learning morphology particularly examining attention mechanism lead impoverished target side representation finding high interest practical usefulness user weakness gloss detail character based encoder many different way learn character based representation omitting discussion leaf open question generality finding also analysis could made interesting chosen language richer challenging morphology turkish finnish accompanied finer grained morphology prediction analysis general discussion brings insight model learn morphology training system using encoder decoder representation respectively input feature representation morphology tagging classification task straightforward extension string based neural learn source syntax using methodology time applied morphology finding offer useful insight system learn,3
496.json,strength describes experiment address crucial problem understanding learn morphology syntax clear objective experiment effectively laid good state review comparison general pleasure read sound experimentation framework encoder decoder recurrent layer output used train morphological classifier show effect certain change framework classifier accuracy character instead word experimentation carried many language pair interesting conclusion derived work agree intuition weakness contrast character based word based representation slightly lacking byte pair encoding showing strong performance literature relevant replace word based representation three many section higher layer focused word meaning similar sentence section ready agree intuition think experiment support particular sentence therefore included clearly stressed reasonable hypothesis based indirect evidence translation performance improves morphology higher layer discussion fine present thorough systematic analysis derives several interesting conclusion based many data point across several language pair find particularly interesting target language affect quality encoding source side particular target side morphologically poor language english tagger accuracy encoder improves increasing depth encoder improve accuracy experiment needed determine improve attention layer hurt quality decoder representation wonder actually related attention hurt decoder representation difficult learn morphologically rich language turn encoders learn based global objective backpropagates decoder strong indication need separate objective govern encoder decoder module,3
435.json,proposes method detecting causal relation clause using neural network deep learning although many study network particularly deep indeed certain discourse connective unambiguous regarding relation signal causal take advantage recent dataset called altlex hidey mckeown solve task identifying causal causal relation relation explicitly marked arguing convolutional network adept representing relevant feature clause lstms propose classification architecture us glove based representation clause input lstm layer followed three densely connected layer tanh final decision layer softmax best configuration system improves hidey mckeown classifier several example generalization system performs well shown indicator word always causal training data found correctly causal test data therefore appreciate system analyzed qualitatively quantitatively well written description problem particularly clear however clarification difference task task implicit connective recognition welcome could possibly include discussion previous method implicit connective recognition cannot used case appreciable uploaded code submission site inspected briefly execute uploading older data code also useful provides many example clear meaning coding given mention binary classification wonder also given data hidey mckeown right repost point clarify meaning bootstrapping apparently extends corpus construction corpus briefly clearly explained additional bootstrapping certainly interesting experiment neural network task merit proposed system entirely convincing seems indeed best configuration among option found test data best configuration announced improving hidey however fair comparison involve selecting best configuration devset moreover entirely clear significant improvement hand possible given size dataset compute statistical significance indicator hand consider also reliability gold standard annotation possibly creator dataset upon inspection annotation obtained english simpleenglish wikipedia perfect therefore score might need considered grain salt finally neural method previously shown outperform human engineered feature binary classification task sense rather confirmation known property interesting experiment simpler network used baseline layer lstm analysis could explain neural method seems favor precision recall,2
103.json,proposes method evaluating topic quality based using word embeddings calculate similarity either directly indirectly matrix factorisation achieving impressive standard datasets proposed method represents natural important next step evolutionary path research topic evaluation thing troubled achieve state three datasets large inconsistency method perform method perform le well state practice none proposed method consistently beat state based method perform notably badly genomics dataset someone want take method shelf arbitrary dataset considerable worry suspect lower genomics relate proportion term comment possible automatically predict method perform best based vocab match glove discussion issue proposed method strong similarity method proposed lexical chaining literature encourage read include future version emphasis method parameter word embedding method large number parameter implicit method huge deal worth acknowledging method deal term genomics dataset term present pretrained glove embeddings simply ignored impact method level issue description word embeddings section implicitly assume length vector unimportant saying cosine similarity used measure similarity vector vector unit length unproblematic wordvec actually return unit length vector trained vector normalised post wordvec vector length certainly uniform small detail important graph figure small readable,1
103.json,proposes method evaluation topic model partition word topic cluster bucket based cosine similarity associated word embeddings simplest setup word considered either existing bucket cosine similarity word bucket certain threshold bucket created word complicated method based eigenvectors reorganisation also suggested method evaluated three standard data set weakly supervised text classification setting outperforms state basic idea behind rather simple certain flavour offer explanation topic quality measurable term word word similarity obvious given topic word embeddings defined respect rather different notion context document sequential context time proposed method seems work quite well like significance test table though overall clearly written even though language issue also found description technique section hard follow believe mostly using passive voice threshold computed place actually making design choice find explain different method clearly subsection method seems space completely fill page content could easily downsize rather uninformative trace method page question sensitive proposed technique different word embeddings example score used wordvec instead glove,2
98.json,present treebanks test delexicalized transfer parser unsupervised parser enriched external probability interesting think could improved mcdonald presented averaged accuracy language language transfer parser reached mcdonald could treebanks since available definitely state case footnote used malt parser default feature tuning specific delexicalized task probably bring better using maltparser default setting maltoptimizer optimizing easy mstparser could optimized line recent parser produce better parser already applied universal dependency leave setup reference instance unsupervised parser performs better language le resourced language family indo european interesting whether hold recent cross lingual parser probability probability seems like random decision table least need detail experiment whether make sense paper take account cross lingual dependency parsing universal dependency predicted label tiedemann language training bilingual parser harmonized treebanks vilares alonso gmez rodr guez present maltparser recent parser also delexicalized parser crosslingual dependency parsing based distributed representation jiang wanxiang david yarowsky haifeng wang ting proc many language parser ammar mulcaire ballesteros dyer smith minor point think need table table could solved footnote website perhaps table included probability table definitely,1
98.json,evaluates minimally supervised dependency parser version manually prior probability treebanks universal dependency report average slightly lower couple delexicalized transfer parser sometimes substantially better indo european language idea biasing otherwise unsupervised parser basic universal rule used number time literature main value present empirical evaluation approach treebanks however approach evaluation leaf question unanswered first want know unlabeled parsing considered appropriate least necessary dependency label standardised whole point give uniform analysis term typed dependency parsing approach take account seems misguided since approach based manually defined universal rule easy enough formulate rule labeled dependency second like know prior probability word universal grammar meant encode alternative tested evaluated present version presented bunch number without explanation justification except based annotation style third main claim unsupervised system work better indo european language seems supported number exactly going type dependency handled better unsupervised system even though full error analysis scope short analysis small sample could really interesting finally comparison delexicalized transfer parser seems biased number factor restricting unlabeled dependency thing since delexicalized parser could easily produced labeled dependency another thing amount training data arbitrarily restricted token treebank finally seems delexicalized parser properly tuned replacing word form lemma underscore without revising feature model likely produce optimal,2
163.json,show distributional information stored word vector model contain information label version annotated word replaced lemma train word embeddings corpus resulting vector train logistic classifier predict word evaluation performed corpus using cross validation well corpus clearly presented discussed analyzed length clear well written main issue contain anything term describe straightforward experiment without idea method interesting indeed provide empirical grounding notion regard certainly worth published quantitative emprirical linguistic venue another note literature tagging induction using word embeddings cited extensively instance ammar duer levin ling emnlp plank gaard goldberg,1
163.json,general comment present exploration connection part speech tag word embeddings specifically word embeddings draw interesting somewhat straightforward conclusion consistency tag clear connection word vector representation detailed error analysis outlier classification definitely strong point however seems missing critical main point reason corpus tagged first place unlike purely linguistic exploration morphosyntactic category underlined semantic prototype theory croft corpus created tagged facilitate task mostly parsing whole discussion could reframed whether distinction made distributional vector beneficial parsing compared original tag upos matter also missing related work context distributional induction recommend starting review christodoulopoulos adding recent work including blunsom cohn yatbaz light body work section barely novel system restriction term external knowledge achieve comparable specific issue abstract contributed distributional vector contain information affiliation unless misunderstanding sentence hardly result especially english every distributionally based induction system past year present many cluster purity number show result assertion line relation vector mostly semantic correct mikolov colobert subsequent work show syntactic information vector also previous comment cluster purity score fact revert statement beginning section line move upos surely fine grained distinction original tagset interesting understand footnote failed attempt performed work criterion fail brown cluster vector almost perfectly align upos tag observation proper noun much similar common noun line interesting existence frequent function word almost singlehandedly explain difference understand practical reason analysing frequent word pair interesting happens tail term vector also type error classifier make could imagine alternative pure distributional morphological since lemmatizing feature allow better generalization tag frequency word minor issue change sentential reference newcite mikolov showed,1
7.json,buying motivation proposed method much faster train train neural network also keep property distribution going lower dimensionality however convinced important vector transformable ppmi importantly direct comparison related work detailed comment definition kendall strange original formula sure come spearman correlation standard semantic task evaluation time datasets chosen evaluation standard one measuring semantic relatedness community prefers nice set recommend also include standard one line figure third line direct comparison related work statement typo large extend extent,1
7.json,present positive projection word embedding method random projection method random projection matrix whose expected value positive argue enables application ppmi possible expected value random projection method computation efficient main reservation clarity particularly could understand core difference method proposed previous random projection method hence could understand whether advantage argue achieve hold hard follow argument starting introduction argument supported line sentence start addition line sentence start since line sentence start thus worked vector space modeling expert random projection used research hard understand logic behind research avenue believe self contained possible follow people experience field lot english mistake large extend ppmi addition cannot evaluating couple standard benchmark wordseim simlex couple others present method feel insufficient evaluate dataset unless provide good justification recommend substantially improve presentation resubmit another conference,1
143.json,describes four method obtaining multilingual word embeddings modified qvec metric evaluating efficacy embeddings embedding method multicluster us dictionary word multilingual cluster cluster embeddings obtained serve embeddings word reside cluster multicca extends approach presented faruqui dyer embedding bilingual word multilingual word using english embeddings anchor space bilingual dictionary otherlanguage english used obtain projection monolingual embeddings word language anchor space multiskip extends approach presented luong embedding using source target context alignment multilingual case extending objective function include component available parallel corpus translation invariance us rank decomposition word matrix objective includes bilingual alignment frequency component work bilingual embeddings evaluation method us maximize correlation word embeddings possibly hand crafted linguistic data basis vector obtained aligned dimension produce score invariant rotation linear transformation proposed method also extends multilingual evaluation general well written describes work clearly major issue contribution respect translation invariance embedding approach gardner extension multilingual embeddings line explaining novelty help super sense annotation across multiple language problem number feature intersection multiple language become really small propose address problem beyond footnote much coverage affect score table example dependency parsing multi cluster multicca significantly different coverage number score close general table tell consistent story mainly intrinsic metric multilingual embedding technique seem perform best given primary goal create embeddings perform well word translation metric intra language disappointing method performs best invariance approach also strange multi cluster approach discard inter cluster word language semantic information performs best respect extrinsic metric question loss performance fixing word embeddings dependency parsing task gain simply using embeddings alternative random embeddings lstm stack parser table average embeddings described section advantage using multi skip approach instead learning bilingual embeddings performing multi learning projection across distinct space dictionary extraction approach parallel corpus alignment google translate reflect challenge using real lexicon explore real multi lingual dictionary,2
143.json,proposes dictionary based method estimating multilingual word embeddings motivated clustering multicluster another canonical correlation analysis multicca addition supersense similarity measure proposed improves qvec substituting correlation component taking account multilingual evaluation evaluation performed wide range task using portal developed shown case proposed representation method outperform baseline think well written represents substantial amount work done presented representation learning evaluation method certainly timely also applaud documentation general feel however go perhaps much breadth expense depth prefer thorougher discussion regarding conflicting outcome multicluster language regarding effect estimation parameter decision multicluster think high practical value research community improved qvec measure portal frankly learned much reading term research question addressed answered concrete remark make sense include correlation table monolingual qvec qvec well stated proposed qvec improvement qvec minor combination several cross lingual word similarity datasets sound though different nature whereas really kind different language right equation exceed column margin line mention coulmance referring multiskip baseline section mention luong correspondence work think reasonable justice citing related work relevant could included multilingual embeddings clustering chandar lauly larochelle khapra ravindran raykar saha autoencoder approach learning bilingual word representation nip hill jean devin bengio embedding word similarity neural machine translation arxiv preprint arxiv wang bansal gimpel livescu deep multilingual correlation improved word embeddings naacl faruqui dyer information theoretic approach bilingual word clustering multilingual training embeddings sake better source language embeddings suster titov noord bilingual learning multi sense embeddings discrete autoencoders naacl wang learning sense specific word embeddings exploiting bilingual resource coling broadly translational context explored diab resnik unsupervised method word sense tagging using parallel corpus,2
129.json,describes method domain data selection convolutional neural network classifier applying framework johnson zhang method performs bleu point better language based data selection unlike method robust even small domain data provided claim improvement bleu point however improvement magnitude achieved domain data training training domain data already produce bleu might interesting also compare system interpolates separate domain model impressive result opinion come second experiment demonstrates classifier still effective little domain data however second experiment zhen task includes actual domain data training possibly making selection easier result also hold task domain data training ene enzh task already point direction since development set contain hundred sentence pair think claim better supported reported task sentence pair used training translating social medium text often face different problem domain striking high rate conventional spelling latin script least text also contain special character sequence usernames hashtags emoticon special preprocessing filtering step applied data since data selection cannot address problem interesting know detail kind improvement made adaptation data selection maybe providing example following remark concern specific section section could made clearer different vector word embeddings segment vector vector combined illustration architecture helpful designated loss function section completeness sake could mentioned system weight tuned,3
66.json,present stack lstm parser based work henderson joint syntactic semantic transition based parsing dyer stack lstm syntactic parsing transition system former stack lstm latter show interesting compared joint system conll shared task like well written well explained related work good interesting methodology sound minor concern regarding chinese embeddings leading believe good embeddings informative clever moreover description system clear hyperparameters justified discussion interesting thing proposed system lack originality sense work henderson put basis semi synchronised joint syntax semantic transition based parsing several year dyer came stack lstm last year method opinion waiting parser designed glad done,4
66.json,general comment present joint syntactic semantic transition based dependency parser inspired joint parser henderson claim main difference vectorial representation used whole parser state instead element stack last parser configuration algorithm plain greedy search idea take advantage stack lstms vector representing state parser keep memory potentially large scoped syntactic feature known decisive feature semantic role labeling path predicate candidate role filler head system tested conll data english multilingual conll data compare system performance previously reported performance showing system well compared system le compared recent proposal bottom table emphasized though proposed system require hand craft feature fast simple greedy algorithm well written describes substantial amount work building recently popular lstms applied henderson algorithm appears somewhat visionary reservation concerning choice simple greedy algorithm render comparable cited work much additional work space provide instance beam searched performance detailed comment question section comment presence link help understanding better target task summary difference transition used work henderson provided current form difficult tell directly reused henderson slightly modified section need representation concatenating word predicate disambiguated sense seems redundant since disambiguated sense specific predicate section organization section confusing concerning multilinguality conll focused english conll shared task extended language,3
66.json,performs overdue back problem joint semantic syntactic dependency parsing applying recent insight neural network model joint model promising thing success transition based neural network parser contribution first present transition system seems better hendersen system based contribution show neural network succeeds problem linear model previously struggled attribute success ability neural network automatically learn feature extract however think another advantage neural network might worth mentioning linear need learn weight feature class pair mean jointly learn problem learn many parameter neural network much economical respect suspect transition system work well variety neural network model global beam search andor many orthogonal improvement could made expect extension method produce state nice attempt derive dynamic oracle transition system even appendix follow work first glance seems similar eager oracle action excludes semantic arc word start buffer word semantic stack action excludes semantic arc word stack word buffer action seem exclude reverse,4
165.json,proposes method discovering correspondence language based author correspondence word sharing meaning number slavic language develop code rule match substring language formulate objective balance description data given trained tested slavic language shown several distance measure phylogenetic tree example found correspondence motivation formulation approach make sense seems like reasonable tool attack problem motivation employing presented nicely must admit though derivation entirely clear point resemblance objective bayesian inference think application bayesian inference biological phylogenetic inference using mrbayes tool empirical comparison could insightful related work lacking comparison method borrowing cognate detection computational method historical linguistics example study alexandre bouchard cote tandy warnow luay nakhleh andrew kitchen available tool apply given dataset mention list moran also relevant tool biological phylogeny inference applied paup mrbayes approach methodology alignment procedure memory runtime bottleneck appears major drawback allowing comparison language long multiple language involved phylogenetic tree interesting language curious idea dealing issue phylogenetic tree using neighbor joining creating phylogenetic tree known disadvantage like specify root manually sophisticated method convergence stopping criterion data datasets mixed cognate necessarily swadesh list considered might impact data orthographic form might hide many correspondence especially apparent language different script therefore learned rule might indicate change script real linguistic correspondence seems like shortcoming could avoided working level phonetic transcription unclear point optimal unigram symbol usage rule line merging done maximization step entirely clear minor issue focus focus line ref johann mattis list steven moran open source toolkit quantitative historical linguistics proceeding annual meeting association computational linguistics system demonstration page sofia bulgaria association computational linguistics http aclweb anthology andrew kitchen christopher ehret shiferaw assefa connie mulligan bayesian phylogenetic analysis semitic language identifies early bronze origin semitic near east,2
132.json,combination wordvec could potentially interesting main problem current technical detail incomprehensible section need complete rewrite reader familiar wordvec could relatively easily high level picture model combined current presentation achieve detailed comment third paragraph introduction make sense requires deriving approximation approximation time consuming develop prototype easier evaluate feature word vector pivot target unlike wordvec motivation decision mean separate word marginal distribution adaptation included structure point kind structure similarity footnote break anonymity appear evaluation day give example cluster long gone figure look like might quantitative evaluation described overly long caption statement conclusion solves word analogy overstating shown cherry picked example king queen sort chang conference journal name advance like guess venue,1
132.json,proposes neural styled topic extending objective wordvec also learn document embeddings constrains sparsification hence mimicking output topic really liked proposed found example presented highly promising really missing however empirical evaluation evaluation entirely fall back table example without indication representative example attempt directly compare standard neural topic model without empirical evaluation impossible sense true worth making hard accept idea could achieved topic representation document supervised document categorisation setup compare topic topic cardinality indirect evaluation quality representation direct evaluation dataset document similarity annotation based pairwise comparison topic vector fantastic releasing code compromised anonymity publishing github link submitted version strictly speaking sufficient rejected outright leave issue select example figure presenting subset actual topic potentially reek cherry picking section discus possibility calculating word representation topic based pairwise comparison word vocabulary going extremely expensive process reasonable vocab size number topic really feasible identify token using spacy section extract noun chunk chunk type similarly section something else given wordvec trained embeddings include small number multiword term clear deal term experiment report appear train entire document collection perhaps immediate problem context want apply trained novel document case updating wordvec token embeddings going mean updated relative training collection wordvec embeddings going directly comparable tuned embeddings finding topic worked best newsgroups corpus surprising given composition possibly another simple form evaluation could based information theoretic comparison relative true document label able perform direct comparison couple neural topic model meed compare ziqiang sujian yang wenjie heng novel neural topic supervised extension aaai nguyen quoc richard billingsley mark johnson improving topic model latent feature word representation transaction association computational linguistics shamanta debakar sheikh motahar naim parang saraf naren ramakrishnan shahriar hossain concurrent inference topic model distributed vector representation machine learning knowledge discovery database springer international publishing level thing line similarity similarity line mean topic basis affected awkward caption figure example perhaps term rather word reference formatting place advance advance neural roder missing conference name,1
11.json,present version coreference task tailored wikipedia task identify coreference chain specifically corresponding entity wikipedia article annotate document coreference chain roughly mention refer main concept article describe simple baseline basic classifier outperforms moreover integrate classifier stanford rule based coreference system substantial benefit state system wikipedia think proposes interesting twist coreference make good sense information extraction perspective potential somewhat revitalize shake coreference research might bridge interesting coreference literature entity linking literature sometimes unimpressed paper dredge task standard system perform poorly propose tweak system better however case actual task quite motivating rather fishing domain thing really feel like wait standard system perform poorly setting actually pretty important task main concept resolution intriguing task perspective imagine many time document revolve primarily around particular entity biographical document dossier briefing person event clinical record information care extracting specific entity standard coreference task always issue large number mention seemingly pretty irrelevant problem like generic mention task unquestionably composed mention actually matter methodology standpoint notion main concept provides discourse anchor useful coreference appears still substantial overhead improve beyond baseline particularly pronominal mention coreference directly wikipedia also open door interesting knowledge illustrate think domain likely interesting testbed idea improve coreference overall general setting difficult robust improvement dwarfed amount work dealing aspect problem moreover unlike past work carved slice coreference winograd schema work make impact metric overall coreference problem domain wikipedia many community pretty interested technique overall technique strong point though seem effective feature seem pretty sensible seems like additional conjunction help unclear whether experimentation vein also state earlier work primary resolution system binary classifier explicitly stated early enough left undefined throughout description featurization minor detail organization perhaps introduce dataset immediately related work section concrete given baseline motivating approach section refers dcoref scoref cite stanford paper make clear stanford coreference system many unfamiliar dcoref scoref name term candidate list unclear especially following leverage hyperlink structure article order enrich list mention shallow semantic attribute link found within article consideration look candidate list mention match surface string link please make clear candidate list mention article possible candidate coreferent think reader understand module supposed import semantic information link structure wikipedia mention hyperlinked article female freebase mention female keep terminology clear section say consider union mention mention predicted method described raghunathan however section implies missing additional mention extracted,4
86.json,detail provided method used produce issue disclosure restriction reader know learning algorithm training data resource made approach nothing help reader sentiment analysis method share research research hence belong conference perhaps submission demo session somewhere good idea even demo however need share detail method used,0
12.json,reviewed earlier short draft point flaw experiment setup corrected since back suggested willing accept draft another event provided flaw corrected obstacle another reviewer point setup somewhat artificial focus real resource language relating cost finding paying annotator believe exposed writeup oversell method relevant line work annotation projection extremely resource language johannsen agic nice reflect related work discussion completeness summary think nice contribution vote accept indicated whether data made available evaluate part good faith presuming public availability research,3
91.json,applies idea translation pruning neural explore three simple threshold histogram pruning scheme applied separately weight class third applied entire also show retraining model produce performance equal full even weight pruned extensive analysis explains superiority class blind pruning scheme well performance boost retraining main idea simple seems quite useful memory restricted application particularly liked analysis section give insight component usually treated like black box insight interesting main motivation compression argument stronger included number actual memory consumption compressed comparison uncompressed minor remark substantial amount work pruning translation model phrase based could referenced related work johnson martin foster kuhn improving translation quality discarding phrasetable emnlp zen stanton peng systematic comparison phrase table pruning technique emnlp took understand figure find informative additional barplot figure showing highest discarded weight magnitude class also allow comparison across pruning method,3
18.json,present first broad coverage semantic parser ucca specific approach graph based semantic representation unlike conll semantic dependency graph ucca graph contain nonterminal node represent word string unlike amrs ucca graph grounded take mean text token appear node semantic representation present number parsing method including transition based parser directly construct ucca par evaluate given ucca ucca annotated data exist seems reasonable develop semantic parser ucca however introduction background section wrong note seem argue ucca graph based semantic representation formalism make sense studied work also seems unnecessary good ucca parser could nice contribution entirely agree three criterion semantic representation formalism introduction instance clear nonterminal node contribute expressive capacity sure inconvenient decide word head coordinated structure exactly information could represented nonterminal informative edge label also question discontinuity even arise grounded advantage grounded representation style one become clear also think word grounded used enough different concept semantics past year encourage find different anchored lexicalized thus feel entire introductory part phrased argued much carefully parser seems fine although check detail however find evaluation impressive primary edge even straightforward maltparser outperforms parser presented score remote edge dependency tree parser like malt cannot compute directly high either furthermore conversion dependency graph dependency tree studied quite name tree approximation context conll shared task semantic dependency parsing albeit without nonterminal node several proposed method reconstructing edge deleted graph tree conversion instance agic semantic dependency graph parsing using tree approximation discus issue involved reconstruction detail incorporating method likely score maltparser lstm based maltparser could improved strength parser becomes even le clear,1
137.json,general comment report experiment predicting level compositionality compound english dataset used previously existing compound whose compositionality ranked specified number judge general form experiment compute cosine similarity vector compound treated token composition vector component evaluation performed using spearman correlation cosine similarity human judgment experiment vary vector used neural embeddings versus syntactic context count vector latter case whether plain aligned vector used dependent component compound alignment try capture shift dependent head alignment proposed previous suppressed reference indicate syntactic context count vector outperform embeddings aligned alone performs le well modified vector highly tuned combination aligned unaligned vector provides slight improvement regarding form found introduction quite well written part like section difficult read although underlying notion complicated rephrasing running example could help regarding substance several concern innovation respect reddy seems aligned vector published previous suppressed reference dataset small enough described particular range frequence quite likely impact since improvement using aligned vector marginal small dataset unclear choice compound performed find finding quite fragile detailed comment question section understand need name packed anchored tree seems plain extraction path lexical item dependency tree namely plain extension traditionally done syntactic distributional representation word typically path length length collapsed preposition called tree elementary apts section table forget mention discard feature order instance nmod overline nsubj dobj appear leftmost bottom cell table elimination incompatible type mention example provided find clear section since reddy dataset central work seems necessary explain compound selected frequency range compound component chance vary depending frequency range many judgment provided given compound many compound final compositionality score problem ranking compute spearman correlation apparently constituent component sequence suggest component constituent also sense phrase syntagm intuition constituent used literally within phrase highly likely compound constituent share occurrence note intuition certainly true constituent head phrase otherwise much le true spelling distribution spelling section note elementary representation constituent compound phrase contain contextual feature associated compound phrase token unless occurred constituent context please provide running example order help reader follow object talking compound phrase token refer merged component compound section guess elementary apts triplet target word dependency path word find name confusing clarify whether shifted refer defined equation removing feature tend lot thing positive mean phrase appear observed small highly informative context phrase refer thing whole sentence seems contradictory please clarify general expect little overlap apts prop erly aligned properly aligned mean mean aligned understand paragraph potential overlap considerable particular case nmod relation component paragraph quite puzzling whole make higher order dependency feature suddenly critical point actually measuring crucial metric similarity composed observed phrasal vector first order feature note supposed provide answer understand explanation order path composed representation reliable please clarify section smoothing ppmi calculation value generally small positive effect seem obvious table optimal value equation important order estimate much hybridity provides slight gain respect unaligned seems table correspond using combination could help legend also could provide wordvec vector compound phrase understand intuition behind freq baseline frequent compound tend compositional suggests maybe bias dataset,2
176.json,proposes knowledge step proposing treat number sentence pair scoring task answer scoring paraphrasing among others instance general task understanding semantic relation sentence furthermore investigate potential learning generally applicable neural network model family task find exciting proposal worthy presentation conll discussion investigation main problem fact feel unfinished accepted publication proviso number update made final version first table need completed given large number individual written discussion terribly short much interpretation discussion sorely needed abstract promise presentation challenging dataset seem deliver incongruity need resolved vary quite across different task could investigation made model fail task succeed others even solid answer found interesting hear position regarding whether question modeling rather dissimilarity task really work group unified whole please include example instance various datasets used including prototypical sentence pair pair pose problem classification transfer learning recommended task nature data general task rather size dataset determine answer question despite unpolished nature though exciting approach could generate much interesting discussion happy published finished form recognize view shared reviewer minor point language weigh weighed consistently used context rather require weight weighted several misspelling sentence sentene interpunction instance world overlap instead word overlap,3
151.json,proposed unsupervised algorithm universal dependency require training tagging based pagerank word small amount hard coded rule article well written detailed intuition behind prior information added explained clearly think contribution substantial field unsupervised parsing possibility future work presented give rise additional research,3
13.json,model event linking using cnns given event mention generate vector representation based word embeddings passed followed pooling also concatenate resulting representation several word embeddings around mention together certain pairwise feature produce vector similarity using single layer neural network compute coreference score tested dataset expanded version performance comparable previous feature rich system main contribution opinion developing neural approach entity linking combine word embeddings several linguistic feature interesting find using word embeddings sufficient good performance fortunately linguistic feature used limited require manually crafted external resource experimental setting appears gold trigger word used rather predicted one make argument reasonable although still liked performance predicted trigger especially problematic competitor system used predicted trigger comparison fair fact different paper different train test split worrisome encourage stick previous split much possible unclear point number indicating cross sentential information needed convincing however last statement second paragraph line clear embeddings position said generaties similar word embeddings exactly randomly initialized lexicalized clear relative position next word embedding relative position next different word exactly left right neighbor used create representation line affect pooling operation word embeddings word word trigger word appended seems arbitrary word choice clear event mention representation line used following section sent appear used pairwise feature used section feature binary assume encoded binary vector distance feature example kept fixed training issue suggestion approach applied entity coreference resolution well allow comparing previous work popular datasets like ontonotes square function nonlinearity interesting novel think applicability task datasets dataset publicly available also presented plan release help researcher compare method least good comparison feature rich system also dataset number reported quite close significance testing help substantiating comparison related work among work entity coreference resolution might mention neural network approach wiseman minor issue line redundant baseline referred type table event text line ref learning anaphoricity antecedent ranking feature coreference resolution wiseman alexander rush jason weston stuart shieber,3
683.json,mention aiming sota however ensemble resnets lower performance single network indicates experimentation preferably larger datasets necessary literature review could least mention existing work wide resnets,3
683.json,consideration proposes procedure incrementally expanding residual network adding layer boosting criterion main barrier publication weak empirical validation task considered quite small scale mnist convolutional basically uninteresting test point compare literature cifar fail improve upon rather simple single network published baseline springenberg example obtains without data augmentation pretty sure simple resnet result somewhere outshines cifar little interesting better used seeing done recent literature crawl unsurprising expect ensemble well dearth labeled training data hundred label typical cifar cifar simple data augmentation scheme employed form regularization simpler alternative complicated iterative augmentation scheme like easier sell method either option scarce labeled datasets data augmentation trivial image related application random crop reflection easy valid necessitate different benchmark comparison simpler method like said data augmentation dropout especially ensemble interpretation,2
379.json,describe implementation tensorflow fold allows various computation without modifying computation graph achieve creating generic scheduler tensorflow computation graph accept graph description input execute show clear benefit approach task computation change datapoint case treernn experiment compare static batch graph structure repeated many time batch size reason score higher provide comparison main alternative method someone could create tensorflow graph dynamic batch word instead using graph scheduling algorithm could explicitly create uniform batch tensorflow graph using standard tensorflow,6
379.json,present novel strategy deal dynamic computation graph arise computation dynamically influenced input data lstms propose unrolling strategy operation done every step allows kind batching input presented idea novel clearly indicate potential approach sake clarity presentation drop part section combinator library neural network present technical detail general interesting help understanding core idea presented experimental stanford sentiment treebank opinion supporting claim towards speed little confusing important point even though presented ensemble variant set state subtasks framework even line probably speculated ensemble averaging appreciate clearer argumentation respect update update newest revision increase rating improved clear argumentation,7
396.json,present interesting framework image generation stitch foreground background form image obviously reasonable approach clearly foreground object however real world image often quite complicated contain multiple layer composition instead simple foreground background layer proposed method deal situation overall reasonable work approach important problem angle think sizable effort remain needed make generic methodology,5
396.json,propose method generates naturally looking image first generating background conditioned previous layer multiple foreground object additionally image transformer layer allows easily different appearance like discussion choice foreground mask rather predicting foreground directly mnist example foreground seems completely irrelevant cifar course add texture color mask ensures crisp boundary mask binary mask alpha blending mask find fact learns decompose image nicely learns produce crisp foreground mask much spurious element though cifar pretty fascinating proposed evaluation metric make sense seems reasonable however afaict theoretically possible high score even though produce image recognizable human classifier network produce generator encodes class subtle though happen given training adversarial network show indeed nicely decomposition much nicer spatial transformer used however also seems indicate foreground prediction foreground mask largely redundant final niceness decomposition appears largely irrelevant furthermore transformation layer seems small effect judging transformed masked foreground object mainly scaled column clear final composed image really advertised regarding eval experiment using clear better provide user minimized match rather random pair assume adversarial divergence real image actually evaluated interesting close multiple differently initialized network actually also please mention confidence generated different training set initialisation eval set many run,6
545.json,thank interesting read idea presented good basis true experiment rather simple interesting empirical evidence pro approach seems decrease training time prime importance deep learning although come price slightly complex grounded theory product function basis compositional architecture described theoretically semiring kernel could used decrease need handcrafting structure problem existing convolutional neural network con experiment simple dataset norb although great understand dynamic simpler dataset analysis complex datasets important empirical evidence compositional kernel approach compared convolutional neural network hence fair compare said large datasets imagenet minor section claim ckms symmetry object felt ample justification provided claim,5
512.json,provides principled framework learning activation function deep neural network theoretical justification choice nonparametric activation function given theoretical satisfactory particularly like experimental setup method tested image recognition datasets achieve relative increase test performance compared baseline well written novel theoretical technique intuition behind proof theorem given little clear main body appendix clarifies everything,6
512.json,summary introduces parametric class linearity used neural network suggests stage optimization learn weight network linearity weight significance introduces nice idea present nice experimental however find theoretical analysis informative distractive main central idea thorough experimentation idea using different basis comparing wider network equivalent number cosine basis used leaned help supporting comment weight linearity learned shared across unit layer unit linearity weight tied across unit layer question interesting study optimal linearity different linearity learned hidden unit normalized normalized word linearity change batch normalization normalization affect conclusion polynomial basis fail,5
768.json,proposes learn group orthogonal feature convnet penalizing correlation among feature group technique applied setting image classification privileged information form foreground segmentation mask trained learn orthogonal group foreground background feature using correlation penalty additional background suppression term pro proposes group wise diversity loss term novel knowledge foreground segmentation mask improve image classification also novel method evaluated standard relatively large scale vision datasets imagenet pascal con evaluation lacking baseline leaf background suppression term reader know much term contributing performance group orthogonal term background suppression term also confusing seems redundant group orthogonality term already serve suppress background feature foreground feature extractor nice incomplete privileged information full imagenet dataset rather privileged information included image available verify method segmentation mask remains useful even regime labeled classification data presentation overall confusing difficult follow example section titled unified architecture gocnn overview method whole list specific implementation detail even first sentence minor calling regression loss writing rather necessary make understanding difficult never seen norm regularization term written described regression minor think suppression label swapped suppress foreground mask suppress additional question table privileged information different table setting idea presented novel show promise currently sufficiently ablated reader understand aspect method important besides additional experiment could also reorganization revision clarity edit considering latest revision particularly full imagenet evaluation reported table demonstrating background segmentation privileged information beneficial even full labeled imagenet dataset upgraded rating reiterate minor point figure though still think label part figure swapped match label topmost path figure text suppress foreground currently background foreground want reverse suppress foreground,5
338.json,thank interesting angle highway residual network show angle kind representation learnt layer aforementioned model residual information provided periodic number step layer preserve feature identity prevents lesioning unlike convolutional neural net pro iterative unrolling view extremely simple intuitive supported theoretical reasonable assumption figure gave clear visualization iterative unrolling view con even though perspective interesting empirical shown support argument major experiment image classification language model trained mutation character aware neural language model figure could combined enlarged show effect batch normalization,6
338.json,describes alternative view hierarchical feature representation deep neural network viewpoint refining representation well motivated agreement success recent structure like resnets pro good motivation effectiveness resnets highway network convincing analysis evaluation con effect finding interpretation batch normalization captured briefly seems significant explanation finding zeiler fergus using viewpoint missing remark missing word line valid,7
338.json,provides perspective understanding resnet highway perspective assumes block inside network residual skip connection group successive layer hidden size performs iteratively refine estimate feature instead generate representation perspective contradictory traditional representation view induced resnet highway network well explained pro novel perspective understand recent progress neural network proposed provides quantitatively experimentals compare resnet highway show contradict several claim previous work also give discussion explanation contradictory provides good insight disadvantage advantage kind network main con experiment sufficient example since main contribution propose unrolled iterative estimation stage figure seems follow assumption unrolled iterative estimation say note stage four three block appears underestimating representation value indicating probable weak link architecture thus much better experiment show condition performance stage follow assumption moreover provide experiment show evidence unrolled iterative estimation comparing resnet highway lack experiment point main concern,5
504.json,try present first step towards solving difficult problem learning limited number demonstration try present contribution towards effort unsupervised segmentation video identify intermediate step process define reward function based feature selection task pro first attempt solve challenging problem robot taught real world task visual demonstration without retraining method well motivated try transfer prior learned object classification task deep network feature address problem limited training example demonstrated reward function could interpretable correlate transition subtasks breaking video subtasks help video demonstration based method achieve comparable performance method requires full instrumentation complex real world task like door opening con unsupervised video segmentation serve good starting point identify subtasks however multiple prior work domain need referenced compared particularly video shot detection shot segmentation work identify abrupt change video break visually diverse shot method could easily augmented feature note multiple paper domain refer survey yuan trans circuit system video tech claim find necessary identify commonality across demonstration limit scope problem drastically requires demonstration follow specific constraint noted past literature video segmentation tang eccv us commonality perform unsupervised video segmentation unsupervised temporal video segmentation approach compared simple random baseline sample video however given large amount literature domain difficult judge novelty significance proposed approach experiment hypothesize sparse independent feature exists discriminate wide range unseen input encode intuition feature selection strategy validity hypothesis experimentally well demonstrated instance comparison simple linear classifier subtasks useful overall present simple approach based idea recognizing goal unsupervised fashion help learning visual demonstration well motivated first step towards difficult task however method claim presented need analyzed compared better baseline,3
504.json,proposes novel method learn vision feature intermediate reward guide robot training real world since sequence human demonstration first segment sequence fragment feature roughly invariant corresponding fragment across sequence cluster find discriminative feature fragment us reward function feature trained deep model idea simple seems quite effective picking right reward function good comparison although could better error bar however baseline strong particular vision related baseline example random reward simply output true false seems quite arbitrary serve good baseline performance still surprisingly better baseline random simpler feature extraction image binning feature simply picking frequent one might discriminative proposed feature wonder whether simpler vision based approach lead similarly performed reward function delicate step segment altogether,5
504.json,explores simple approach learning reward function reinforcement learning visual observation expert trajectory case little training data available obtain descriptive reward even challenging condition method us trained neural network feature extractor similar large body work task transfer neural net area computer vision represents reward function weighted distance feature automatically extracted frame provided expert trajectory well written explains involved concept clearly also embedding presented approach literature inverse reinforcement learning resulting algorithm appealing simplicity could prove useful many real world robotic application three main issue current form addressed believe significantly strengthened although recursive splitting approach extracting frame seems reasonable feature selection well motivated missing baseline experiment happens feature selection disabled distance feature used immediately break procedure trade even simpler baseline proposed following procedure simply frame recorded trajectory calculate distance feature space weight according time approach proposed well work understand desire combine extracted reward function simple method believe used simple controller could potentially introduce significant bias experiment since requires initialization expert trajectory direct consequence initialization procedure already started close good solution extracted reward function potentially queried small region around observed initial image perhaps exception human demonstration without additional experiment thus unclear well presented approach work combination method training controller understand number available image excludes training deep neural directly task hand wonder baseline happens us random projection image form feature vector well distance measure using image norm image difference distance measure based first principal component work seems occlusion exclude working well without empirical evidence hard confirm minor issue page make idea imitation read awkwardly page inception network trained imagenet trained imagenet classification page definition transition function stochastic case seems broken page efficient enough evaluate strangely written sentence additional comment rather real issue mainly empirical nature little actual learning performed obtain reward function theoretical advance needed necessarily make empirical evaluation important liked clear exposition approach boil computing quadratic distance feature extracted frame nice make connection standard approach section could argue derivation strictly necessary,5
441.json,tldr present variable computation recurrent neural network vcrnn vcrnn similar nature adaptive computation time graf imagine vanilla timestep subset variable computation state updated experimental convincing limited comparison cited work basic lstm baseline gating mechanism timestep vcrnn generates vector seen gating mechanism based vector first first literally first state subset vanilla state gated updated extra hyperparams epsilon needed give value explain selected sensitive critical hyperparms mechanism novel feel clunky awkward feel well principled first state updated rather generalized solution subset state updated short section text comparing soft gating mechanism grus lstms multiplicative rnns nice well variable computation argument made vcrnn save computation versus vanilla rnns technically true practice probably case size rnns compare saturate modern core theory computation might saved practice probably difference wallclock time also report wallclock number make argument hard sell evaluation reviewer wished citation work comparison stronger baseline vanilla first lstms simple quite standard nowadays lack comparison basic stacked lstm architecture experiment number quite discouraging well compared state vcrnn beat basic vanilla baseline also cite compare basic architecture however many contribution since basic architecture performs vastly better please chung table chung also experimented cite compare large number important contribution cool experiment graph character computation vcrnn figure show space word boundary computation cool however make wonder lstm well magnitude change state vector space lstm suspect something similar minor please equation number hard refer review discussion reference chung hierarchical multiscale recurrent neural network graf adaptive computation time recurrent neural network multiplicative integration recurrent neural network,3
441.json,high novelty work enjoyable read concern le mirror review question certainly agree learned variable computation mechanism obviously something interesting empirical really need grounded respect state lstms still elephant room note consider beating lstms grus method particular prerequisite acceptance comparison nevertheless made review response brought lstms perform computation timestep elman network true axis along compared factor controlled least expectation varying number lstm cell brief discussion proposed gating mechanism light currently popular one strengthen presentation light concern addressed modifying review understanding manuscript amended include comparison posted comment,6
441.json,describes simple clever method allowing variable amount computation time step rnns architecture seems outperform vanilla rnns various sequence modelling task visualization assignment computational resource time support hypothesis able learn assign computation whenever longer longer term dependency need taken account proposed evaluated multitude task ability outperform similar architecture seems consistent task allow interesting analysis amount computation request time step interesting seems resource start word ascii character also like investigation effect imposing pattern computational budget assignment us prior knowledge task superior performance architecture impressive convinced baseline model equal number hyperparameters tune come back point next paragraph mainly clarity issue abstract claim computationally efficient regular rnns wall time measurement supporting claim theoretically able save computation point made clearly conceptual ability choose allocate resource make interesting enough claim computational gain misleading without actual back also find unfortunate clear text hyperparameter chosen whether chosen randomly using hyperparameter search held data influence fairness comparison rnns similar type hyperparameter controlling regularization like example dropout weight noise even regularization rnns tricky consider serious flaw impressed enough fact architecture achieves roughly similar performance learning allocate resource think detail type important absent text even superior performance extra regularization controlling parameter actually seen useful part architecture nice know sensitive precise value knowledge proposed architecture novel amount computation determined unlike method variable computation seen quite inventive originality strongest point currently hard predict whether method variable computation used practice given also depends feasible obtain actual computational gain hardware level said architecture turn useful learning long term dependency also think interpretability value nice property method visualization interesting might shed light make certain task difficult rnns pro original clever idea nice interesting visualization interesting experiment con experimental detail clear convinced strength baseline claim actual computational saving without reporting wall clock time edit positively impressed ended addressing biggest concern raised score adding lstm baseline version significantly improves empirical quality addressed question experimental detail found important promised change wording remove confusion whether computational saving conceptual actual wall time think fine conceptual long clear abstract want make clear since change currently still promise score assumed apply updated version aforementioned concern indeed addressed edit since know difference sota task large lower score learning still think good cannot stand,6
695.json,summary introduce noiseout reduce parameter pruning neuron network identifying pair neuron produce correlated output replacing pair neuron appropriately adjusting weight technique relies neuron high correlation however introduce additional output neuron noise output network trying predict mean noise distribution constant increase correlation neuron experiment test mnist svhn comment interesting suggestion prune neuron experiment larger datasets probably need convincing approach guaranteed work well equation seems straightforwards seems like larger datasets noise output might added ensure higher correlation downside term overall accuracy presented clearly definitely interesting read encourage continue line work,4
695.json,proposes test idea method pruning network identifying highly correlated neuron pair pruning pair modifying downstream weight compensate removal work well removed neuron highly correlated method dubbed noiseout increasing neuron correlation adding auxiliary noise target output network training first idea fairly straightforward clear tried seem work second idea unclear value seems reviewer merely regularizing effect comment direction right constant gaussian treatment seem produce effect network right binomial effect seems nonoise true conclude noiseout target simply serving regularize network reduce capacity slightly show whether effect true need compare method reducing network capacity example reducing number neuron applying regularization various value applying dropout various strength make attempt direction critically miss several comparison treatment pruned without regularization pruned pruned dropout experiment included used produce plot like without comparison seems impossible conclude noiseout anything provide similar regularization dropout combined idea produce considerable reduction parameter sadly experiment exposition somewhat lacking really understand going little work could quite interesting probably accepted additional comment section state experiment stop criterion accuracy decay threshold criterion match original accuracy therefore compressed network accuracy original network accuracy train accuracy test accuracy train test accuracy need shown much test performance lost pruning test typically referred cheating choice need clearly stated defended lowercase used indicate correlation never actually specified confusing state indicates correlation compare pruning method numerical comparison attempted,4
695.json,proposes prune neural network removing neuron whose operation highly correlated neuron idea nice somewhat novel pruning method concentrate removal individual weight however done research topic however experimental theoretical justification method need improved publication experiment report accuracy degradation pruning table laconically stating network degrade convincing detail given figure however figure disagrees table table number parameter range figure picture range unless detail provided simply claiming network remove neuron number degradation accuracy convincing theory proof match experimental condition make unreasonable assumption proof show absence bias network constant output correlated neuron generate output offset however exactly network bias explain noise injection help proof suggests fine deterministic auxiliary neuron interpretation noisy output injects gradient noise concurrent iclr submission,2
380.json,present method changing objective generative adversarial network discriminator accurately recovers density information underlying data distribution course deriving changed objective prove stability discriminator guaranteed standard setup recovered additional entropy regularization term clearly written including theoretical derivation derivation additional regularization term seems valid well explained experiment also empirically seem support claim proposed changed objective better discriminator issue current form presentation albeit fairly clear detail following initial exposition beginning fails accurately convey difference energy based view training gans standard result took several pass understand hold standard think clearer state connection front perhaps without additional perspective perhaps additional explanation implemented right experiment want detail appendix also comment proposed procedure construction result improved generator unless misunderstand something result improved stability training also make claim uninformed reader might wrong impression especially since mention improved performance compared salimans inception score experiment might worth mentioning early experiment although well designed mainly convey qualitative exception table appendix datasets know evaluating gans easy task wonder whether additional quantitative experiment could performed evaluate discriminator performance example could evaluate well final discriminator separate real fake example robust classification injected noise classification accuracy change noised training data might wonder whether last layer feature learned discriminator using changed objective better suited auxiliary task classifying object category main complaint completely unclear generator discriminator look like experiment mention code available soon feel like short description least form energy used also appear somewhere perhaps appendix,6
380.json,submission explores several alternative provide generator function generative adversarial training additional gradient information exposition start describing general formulation additional gradient information termed pgen could added generative adversarial training objective function equation next prove shape optimal discriminator indeed depend added gradient information proposition unsurprising finally propose three particular alternative construct pgen negative entropy generator distribution norm generator distribution constant function resembles ebgan objective zhao exposition move experimental evaluation method set pgen approximate entropy generator distribution point intuition objective function study vanilla objective plus regularization term encourages diversity high entropy generator distribution hope regularization transform discriminator estimate energy landscape data distribution experimental evaluation proceeds showing contour plot obtained generator distribution problem studying generation diversity mnist digit showing sample cifar celeba problem convincing since clearly observe discriminator score translate unnormalized value density function mnist offer good intuition also prototypical digit assigned larger score unnormalized density discriminator le prototypical digit assigned smaller score sample experiment section le convincing since sample baseline model provided comparison recommend clarify three aspect first seen entropy regularization lead discriminator estimate energy landscape data distribution regularization reshape generator function nice mean mnist digit according generator statistic possible second sample produced proposed method compare visually speaking state third shortcoming method versus vanilla much computational overhead qualitative quantitative difference entropy estimator proposed manuscript overall clearly written vote acceptance open question breakthrough pursue derive objective discriminator estimate data density function training,7
553.json,decade near data processing requirement large scale linear learning platform time load data exceeds learning time justified introduction approach spark deep learning usually deal data contained single machine bottleneck often method overcomes bottleneck could relevant unfortunately work still preliminary limited linear training algorithm little interest iclr readership recommend publication conference reach large scale linear audience first icml clear well written present form probably mostly need proper benchmark large scale linear task obviously convincing learning simulation welcome target iclr flash memory fpga handle experiment choice mnist somewhat task small performance notoriously terrible using linear approach even report,3
416.json,much existing deep learning literature focus likelihood based model however maximum entropy approach equally valid modelling scenario information given term constraint rather data limited work flexible maximum entropy neural model surprising fact optimizing maximum entropy requires establishing effect constraint distribution formulating entropy complex distribution unbiased estimator entropy sample alone explicit density needed challenge limit approach identified invertible neural model provide powerful class model solving maximum entropy network problem go establish approach contribution recognising normalising flow provide explicit density used provide unbiased estimator entropy resulting lagrangian implemented relaxation augmented lagrangian establishing practical issue augmented lagrangian optimization reviewer aware work novel approach natural sensible demonstrated number model clear evaluation done enough experiment done establish appropriate method though entirely necessary good example benefit flexible flow transformation much clearer discussion computational scaling aspect valuable guessing approach probably appropriate learning le appropriate inferential setting known conditioned particular instance based constraint discussion appropriate case good issue match theory regularity condition brought clear described well exceeds theoretical discussion occur regarding numerical method paper field quality good sound providing novel basis flexible maximum entropy model clarity good originality refreshing significance significant development term whether used method clear stage minor issue please label equation others might wish refer even page algorithm algorithm update overcome stability appears slightly opaque mildly worrying assume still residual stability issue comment solves problem issue support glossed little support additional condition support seems hard encode indeed turn guess gaussian invertible unbounded transformation support happens trivial general setting seems issue dealt indeed dirichlet example explicitly required support complex constraint trivial invertible model known jacobian nice include general treatment rather relegating specific example overall pleased someone tackling question natural approach,8
416.json,propose approach estimating maximum entropy distribution subject expectation constraint approach based using normalizing flow network linearly transform sample tractable density function using invertible transformation allows access density resulting distribution parameter normalizing flow network learned maximizing stochastic estimate entropy obtained sampling evaluating density obtained sample stochastic optimization problem includes constraint expectation respect sample normalizing flow network constraint approximated practice sampling therefore stochastic optimization problem solved using augmented lagrangian method proposed method validated problem dirichlet distribution financial problem involving estimation price change option price data quality seems technically sound concern approach followed apply augmented lagrangian method objective constraint stochastic propose solution problem based hypothesis test think likely already addressed literature good could comment experiment performed show proposed approach outperform gibbs sampling exact optimal distribution least equivalent advantage closed form solution density concern difficulty problem considered dirichlet distribution relatively smooth distribution financial problem dimensional case numerical method compute normalization constant plot exact density seem easy show method perform challenging setting high dimension complicated linear constraint clarity clearly written easy follow originality proposed method original since based applying existing technique normalizing flow network specific problem finding maximum entropy distribution methodological contribution almost existing could mention combination normalizing flow network augmented lagrangian method significance seem significant sense able find density maximum entropy distribution something seem possible however clearly useful practice problem address real world data financial data could solved well using dimensional quadrature consider challenging problem clear practical interest minor comment detail given plot bottom right figure obtained dirichlet whose true small mean give detail choose dirichlet changed updated review score look last version submitted includes experiment,5
601.json,summary present comprehension dataset called newsqa dataset containing question answer pair news article dataset collected four stage process article filtering question collection answer collection answer validation example dataset divided different type based answer type reasoning required answer question human machine performance newsqa reported compared squad strength agree model benefit diverse datasets dataset collected news article hence might pose different set problem current popular datasets squad proposed dataset sufficiently large data hungry deep learning model train inclusion question null answer nice property good amount thought gone formulating four stage data collection process proposed barb performing good published state much faster weakness human evaluation weak near native english speaker performance example hardly representative complete dataset also performance example necessary clearly demonstrate dataset harder squad either calculate human performance squad calculate human performance newsqa squad consistent manner large enough subset good representative complete datasets dataset community dataset antol iccv also method squad compute human performance section say question answer agreed upon atleast worker number inconsistent question answer without agreement validation last line section article shown multiple questioner ensured questioner asking question article asking similar question mention keep hyperparameters squad accuracy hyperparameters tuned using validation newsqa example labeled reasoning type seem enough represent complete dataset also performance example performance shown figure student graduate undergraduate student researcher test seems small suggestion answer validation step nice maybe dataset released version answer collected stage without validation step current format validation step preliminary evaluation proposed dataset large scale machine comprehension dataset collected news article suggestion diverse enough existing datasets state model definitely benefit better human evaluation think make good poster,5
601.json,summary proposes novel machine comprehension dataset called newsqa dataset consists question answer pair based news article analyzes different type answer different type reasoning required answer question dataset evaluates human performance performance baseline dataset compare performance squad dataset strength present large scale dataset machine comprehension question collection method seems reasonable collect exploratory question answer validation step desirable proposes novel computationally efficient implementation match lstm weakness human evaluation presented satisfactory human performance reported small subset question seems unlikely question provide reliable measure human performance entire dataset consists thousand question newsqa dataset similar squad dataset term size dataset type dataset natural language question posed crowdworkers answer comprising span text related paragraph present empirical way show newsqa challenging squad human machine performance newsqa larger squad however since human performance number reported small subset trend might carry human performance computed dataset sentence level accuracy squad higher newsqa however mention difference accuracy could likely different length document datasets even measure truly reflect squad le challenging newsqa clear newsqa truly challenging squad mention barb computationally efficient faster compared match lstm however report much faster barb compared match lstm page boundary pointing paragraph clarify refers review summary dataset collection method seems interesting promising convinced following human performance significant percentage dataset empirical study fairly show newsqa challenging better squad,5
744.json,proposes network called gated residual network layer design add gating shortcut connection scalar regulate gate claim approach improve training residual network seems could competitive performance cifar state model wide net wide gated resnet requires much parameter densenet variant obtaining little improvement dense importantly state obtained best cifar cifar updated version densenet huang version called densenet outperforms reported cifar cifar parameter densenet still outperforms parameter much le variant paper state report result image therefore empirical need also image demonstrate improvement claimed achieved proposed trick adopts highway neural network residual network intuitive motivation sufficiently novel empirical prove sufficient effectiveness incremental approach,4
744.json,proposes learn single scalar gating parameter instead full gating tensor highway network claim gating easier learn allows network flexibly utilize computation basic idea simple clearly presented natural simplification highway network allow easily shutting layer keeping number additional parameter however regard leaf point firstly mention gate highway network data dependent potentially powerful learning fixed gate unit independent data secondly fair comparison highway network show simpler formulation indeed easier learn original design plain layer instead residual layer based argument made work fine tested work argument incorrect incomplete mnist experiment since hyperparameters fixed plot misleading dependence hyperparameters exists different model experiment appears based srivastava indeed designed test optimization aggressive depth apart hyperparameter search regularization dropout batch norm appear theoretical argument architecture cifar experiment obtained improvement compared baseline wide resnets small therefore important report standard deviation case clear difference significant question regarding always relu potential problem becoming never recovering also mean wide resnet residual block zeroed since,4
528.json,talk design choice recommendation performing program induction gradient descent basically advocating reasonable programming language practice immutable data higher order language construct mentioned comment feel fairly strongly marginal best contribution beyond terpret already published system extensive experimentation theoretical grounding clear think terpret deserves large amount attention truly inspiring contradicts finding original provide convincing evidence gradient based evaluator terpret superior even frankly appropriate program induction uncomfortable make wonder gradient based method carefully vetted first place extensive comparison already implemented alternative included opinion want give original terpret attention think deserves threshold hand basically unreadable actually contradicts mother well defended way irreproducible without think unfortunately threshold,4
528.json,discus range modelling choice designing differentiable programming language propose recommendation tested algorithmic task list length list return element list solution learnt input output example pair training test main difference work differentiable architecture like neural nram fact automatically producing code solves given task main concern experiment nice comparison neural network mentioned related work also useful typical problem used mentioned neural architecture problem sorting merging adding wondering going generalize type program solved prefix loop suffix structure also concerning although task simple structure solution restricted using extension work proposed still fails find solution example loop fails solve list length task run generates code rather black neural architecture nice learn example con weak work simple task missing comparison neural architecture,3
528.json,proposes recommendation design differentiable programming language based made gradient descent successful experiment must expert program induction understand value exploring explore making program learning easier find engaging first everything built terpret publicly available also discussion detailed programming language side le learning side conceivable best received programming language conference comparison alternative generating code valuable opinion motivate overall setup pro useful well executed novel study con learning specific contribution domain related constraint sure great iclr,4
528.json,present small important modification made differentiable program improve learning overall modification seem substantially improve convergence optimization problem involved learning program gradient descent said program learned still small unlikely directly useful,5
656.json,investigates hybrid network consisting scattering network followed convolutional network using scattering layer number parameter reduced first layer guaranteed stable deformation experiment show hybrid network achieves reasonable performance outperforms network network architecture small data regime often heard researcher necessary learn level feature convolutional network every time trained theory using fixed feature could save parameter training time aware first investigate question view show using scattering feature bottom layer work well learned feature completely obvious priori interesting disagree framing hybrid network superior term generalization data regime hybrid network sometimes give better accuracy quite architecture capacity tuned dataset size full dataset hybrid network clearly outperformed fully learned model understood correctly simply compared identical architecture without scattering first layer complicates drawing conclusion claim hybrid network theoretical advantage stability however first layer hybrid network stable learned one still create instability furthermore potentially unstable deep network outperform stable scattering net partially stable hybrid net question importance stability defined theory scattering network conclusion think investigates relevant question convinced hybrid network really generalizes better standard deep net faster computation test time could useful power mobile device aspect really fleshed minor comment section learni,4
656.json,thanks detailed response clarification proposes scattering transform lower layer deep network fixed representation enjoys good geometric property local invariance deformation thought form regularization prior layer network trained perform given supervised task final thought plugging standard deep convolutional network scattering transform evaluation cifar show proposed approach achieves performance competitive high performing baseline find interesting idea cascading representation seems natural thing best knowledge first work combine predefined generic representation modern architecture achieving competitive performance high performing approach state resnets variant achieves significantly higher performance believe work strongly delivers point convincingly show lower level invariance obtained analytic representation scattering transform simplifying training process using le parameter allowing faster evaluation hybrid approach become crucial data regime author argues scattering initialisation instability cannot occur first layer contrary operator expansive naturally suggests robust adversarial example extremely interesting present empirical evaluation task practical impact hybrid network fooled adversarial case render scattering initialization attractive,6
713.json,present linear function deep neural network linearity report gain datasets interest used production network minimal increase computation,6
713.json,present parameterized variant show proposed function help deal vanishing gradient deep network better existing linearity present theoretical analysis practical validation presented approach interesting observation statistic pelu parameter reported perhaps explanation observed evolution parameter help better understand linearity hard evaluate experimental validation presented given difference number parameter compared approach,5
343.json,proposes approach character language modeling clms based developing domain specific language represent clms experiment show mixed performance versus neural approach modeling linux kernel data wikipedia text however proposed model slightly compact fast query compared neural clms proposed approach difficult understand overall perhaps aimed towards community already working sort approach lack sufficient explanation iclr audience critically gloss major issue demonstrating proposed valid probabilistic training performed data clearly gradient based training approach used finally experiment feel incomplete without showing sample drawn generative analyzing learned determine learned overall feel describe approach enough depth reader understand implement almost section devoted exposition without specifying probability computed using training performed probability actually encoded description seems discrete decision rather probability training perhaps covered previous paper need discussion work section enough explain training work measure optimality achieved given quite different hypothesis space neural model gram looking sample drawn seems critical current experiment show score utterance relatively well interesting sample structured sample neural approach example long range syntax constraint like bracket,4
343.json,propose method language modeling first generating program learning count based parameter program pro include proposed method innovative highly different standard lstm based approach late also much quicker apply query time strong empirical obtained modeling code though synthesis method neural method hutter task detailed description language syntax provided con suggestion synthesis procedure using mcmc left vague even though able make procedure efficient question work build work literature surely related work could also expanded work better context compact convincing example human interpretability helpful comment training time evaluation table give basic information whether training done spec,7
640.json,discus multi sense embedddings proposes learning using aligned text across language suggests adding language help improve word sense disambiguation ambiguity carried across language pair idea propose particular setup learning multi sense embeddings exploiting multilingual data broadly fine unfortunately fall short number way section unnecessarily convoluted nice idea could described concise fashion next importantly comparison work lacking extent impossible evaluate merit proposed objective fashion could stronger learned embeddings evaluated downstream task evaluated published method current version little leaving mostly relative variant plot really anything story,3
640.json,work aim address representation multi sense word exploiting multilingual context experiment word sense induction word similarity context show proposed solution improves baseline computational linguistics perspective fact language le similar english help intriguing following problem work hard follow hard compared baseline paragraph discussion clearly compare contrast work proposed slight variation previous work thus experimental setup designed compare part help improvement much thus mono exposed training data sure proposed better mono observe data lack computational power suggest following baseline turning multilingual data monolingual using alignment train baseline pseudo monolingual data provides good benchmark intrinsic evaluation message could conveyed strongly improvement downstream task,3
705.json,follow nip unsupervised learning spoken language visual context exactly proposes future work section perform acoustic segmentation clustering effectively learning lexicon word like unit using embeddings system learns analysis interesting really like going main concern novelty feel like work rather trivial follow existing fine analysis satisfying currently feel like illustrating thing nip minor improvement learns interesting analysis liked thing like comparison different segmentation approach audio image suppose access perfect segmentation modality happens also interesting look learned grounded representation evaluate multi modal semantics task apart well written really like research direction important analyze model learn good example type question afraid however novel enough question deep enough make better borderline iclr,5
705.json,work proposes joint classification image audio caption task word like discovery acoustic unit correlate semantically visual object general interesting direction research allows richer representation data regularizing visual signal audio visa versa allows training visual model video major concern amount novelty work author previous publication nip claim sophisticated architecture indeed show improvement recall however improvement marginal added complexity architecture clustering grouping section hacky instead gridding image could actually object detector yolo fasterrcnn estimate accurate object proposal rather using mean spectral clustering approach alleviate gaussian assumption distribution assigning visual hypothesis acoustic segment form partite matching used overall really like direction research encourage continue developing algorithm train multimodal datasets however work quite novel enough nip,4
705.json,contribution introduces method learning semantic word like unit jointly audio visual data multimodal neural network architecture accepts image audio spectrogram input joint training allows embed image spoken language caption shared representation space audio visual grounding generated measuring affinity image patch audio clip allows relate specific visual region specific audio segment experiment cover image search audio image annotation image audio task acoustic word discovery novelty significance correctly mentioned section computer vision natural language community studied multimodal learning image captioning retrieval regard multimodal learning offer incremental advancement since primarily us novel combination input modality audio image however bidirectional image audio retrieval already explored prior work harwath nip apart minor difference data architecture training procedure submission identical prior work novelty submission therefore procedure using trained associating image region audio subsequence method employed association relatively straightforward combination standard technique limited novelty trained used compute alignment score densely sampled image region audio subsequence alignment score number heuristic applied associate cluster image region cluster audio subsequence missing citation work area spanning computer vision natural language speech recognition missing reference ngiam multimodal deep learning icml positive point using data improved architecture improves prior work bidirectional image audio retrieval presented method performs efficient acoustic pattern discovery audio visual grounding combined image acoustic cluster analysis successful discovering audio visual cluster pair negative point limited novelty especially compared harwath nip although give good clustering method limited novelty feel heuristic proposed method includes many hyperparameters patch size acoustic duration threshold threshold number mean cluster discussion sensitivity method choice,4
586.json,investigate neural introduced kaiser sutskever section claim performance number step perform example subsequent section highlight importance curriculum training empirically show larger model generalize better section construct example reveal failure mode last section compare performance given different input format well written contains exhaustive experiment provide insight detail training neural push boundary algorithm learned hand seems lack coherent message also fails provide insight observation made curriculum training essential certain failure mode exist introduction contains several statement qualified explained aware statistical learning theory guarantee empirical risk minimization consistent number parameter larger number example generalization performance depends dimension function space instead furthermore suggested link adversarial example learning algorithm tenuous reference explanation provided contentious statement deep neural network able match performance parallel machine learning algorithm argue neural performs step example allows learn algorithm super linear complexity multiplication analysis seems overlook parallel nature neural architecture addition multiplication time complexity parallelism used carry lookahead adder wallace tree respectively section show larger model generalize better argue self evident however since training test error decrease likely smaller model underfitting case counter intuitive larger better generalization error interesting progressively decreasing number term increasing radix number system work well learning curriculum although nice stronger intuitive theoretical justification latter final section claim neural gpus cellular automaton justification statement useful since cellular automaton discrete model equivalence model obvious relationship global operation changing input format circuitous conclusion provides useful insight neural introduce original extension explain fundamental limitation several statement require stronger substantiation well written exhaustive experiment learning algorithm decimal representation available source code con coherent hypothesis premise advanced three bold statement without explanation reference unclarity experimental detail limited novelty originality factor typo minus chance carrying digit section remove larger model filter achieve section generalize section,3
586.json,investigates better training strategy neural model well study limitation pro well written many investigation available source code con misleading title extension neural training strategy comparison similar architecture grid lstm adaptive computation time experiment task nice tested task positive negative really understand negative good know missing make work studied detail remain unclear missing gradient noise used experiment length sequence figure misleading number computation step write actually variable limitation still remain unclear clear exactly fails despite showing example make fail studied detail failed example could problem,4
569.json,introduces attention based recurrent network learns compare image attending iteratively back forth pair image experiment show state omniglot though large part performance gain come extracted convolutional feature used input significantly improved original submission reflects change based review question however attempt made include qualitative still relatively weak could benefit example analysis also attention always attending full character although zooming attend relevant part character attending full character solid background seems trivial solution unclear large performance gain coming much polished still lacking detail respect detail convolutional feature extractor used give large performance gain,3
569.json,present attention based recurrent approach shot learning report quite strong experimental surpassing human performance hbpl omniglot dataset somewhat surprising seems make standard neural network machinery also note helped verify soumith chintala reproduce provide source code reading left little perplexed performance improvement coming seems share component previous work author could report result broader suite experiment like previous work matching network much convincing ablation study also help understanding well,4
617.json,describe implementation delayed synchronize method multi deep training comment described manual implementation delayed synchronization state protection helpful however dependency implemented dependency scheduler without threading manually overlap computation communication known technique implemented existing solution tensorflow described chen mxnet claimed contribution point somewhat limited convergence accuracy reported beginning iteration alexnet helpful include convergence curve till compared network summary implement variant delayed syncsgd approach find novelty system somewhat limited comment experiment improved demonstrate advantage proposed approach,4
617.json,present method speed gradient descent leveraging asynchronicity layer wise manner obtain speedup compared synchronous training baseline weak importantly dismiss parameter server based method becoming standard effectively compare current state also present wall time measurement flaw ready iclr acceptance,2
617.json,relatively difficult parse much exposition proposed algorithm could better presented using pseudo code describing compute flow diagram describing exactly update take place stand sure understand everything also liked exactly described various label correspond task wise comm mean layer wise couple major issue evaluation first comparison reported baseline async method using parameter server second using alexnet benchmark informative alexnet look different sota image recognition particular many fewer layer especially relevant discussion also us lot fully connected layer affect compute communication ratio way relevant interesting architecture today,2
494.json,interesting direction research consider problem predicting whether given statement useful proof conjecture posed binary classification task propose dataset deep learning based baseline expert theorem proving present review perspective feel goal present problem audience easy grasp well written section clear especially section term ocaml level debruijn index used without explaining reference term might trivial literature hard follow section describes data split train test thing unclear example train test statement conjecture always statement different conjecture also unclear deep learning model applied consider leftmost architecture figure character embedded vector processed global pooling layer layer take along feature across character input another concern deep learning method presented baseline great compare standard technique word followed sure outperformed neural network number give sense easy hard current problem setup look success failure case algorithm insight drawn analysis inform design future model overall think research direction using theorem proving interesting however also feel quite opaque many part data constructed unclear atleast someone little knowledge itps revise text make clearer great baseline model seem perform quite well however insight kind ability model lacking mention unable perform logical reasoning vague statement example mistake might help make message clearer since well versed literature possible judge valuable dataset reference seems like drawn benchmark conjecture proof used community possibly good dataset current rating weak reject address concern change accept,5
494.json,present dataset extraction method dataset first interesting machine learning supported higher order logic theorem proving experimental impressively good first baseline accuracy higher relevance classification better chance encourage future research direction well written term presentation argumentation leaf little room criticism related work seems well covered though note expert automated theorem proving,7
549.json,introduces efficient variant sparse coding us building block cnns image classification coding method incorporates input signal reconstruction objective well information class label proposed block evaluated recently proposed crelu activation block positive proposed method seems technically sound introduces efficiently train layer wise combining reconstruction discriminative objective negative performance gain term classification accuracy previous state clear using dataset cifar proposed method performs slightly better crelu baseline improvement quite small test strengthened demonstrate proposed method generally applicable various architecture datasets clear consistent performance gain strong baseline without practical significance work seems unclear,4
549.json,first like thank answer clarification find presentation multi stage version much clearer pro state sparse coding problem using cosine loss allows solve problem single pas energy based formulation allows directional coding incorporates bottom information feature extraction process con cost running evaluation could large multi class setting rendering approach le attractive computational cost comparable recurrent architecture competitive improves baseline convincing comparison text experimental evaluation limited single database single baseline motivation sparse coding scheme perform inference feed forward manner property hold multi stage setting thus optimization required clarified efficient performing directional coding scheme interesting clarified could necessarily case need evaluated many time performing classification maybe interesting combination without class specific bias evaluation prediction energy based setting said good include discussion direct comparison trade offs using proposed computational cost performance using bidirectional coding layer seems reasonable good level representation class agnostic however could studied detail instance showing empirically trade offs understand correctly setting reported finally mention benefit using architecture derived proposed coding method spherical normalization scheme lead smoother optimization dynamic baseline batch normalization seems relevant test minor comment find figure confusing plot setting lead function state text,5
725.json,strength interesting explore connection relu simplified sfnn small task mnist used demonstrate usefulness proposed training method experimentally proposed multi stage training method simple implement despite lacking theoretical rigor weakness reported real task large training clear exploration scalability learning method training data becomes larger hidden layer become stochastic share uncertainty representation deep bayes network deep generative model deep discriminative generative model pattern recognition book chapter pattern recognition computer vision november download connection discussed especially uncertainty representation benefit pattern recognition supervised learning bayes rule benefit domain knowledge explaining away like connection variational autoencoder model training also stochastic hidden layer,4
725.json,build connection simplified stochastic neural network sfnn sfnn proposes initialization simplified sfnn evaluated several small task positive connection different model interesting think connection sigmoid simplified sfnn mean field approximation known decade however connection relu simplified sfnn novel main concern whether proposed approach useful attacking real task large training task small training stochastic unit help generalize well,5
660.json,introduced extension adam optimizer automatically adjust learning rate comparing subsequent value cost function training empirically demonstrated benefit optimizer cifar convnets logistic regression problem following concern proposed method variant arbitrary shift scaling cost function fair comparison baseline method using additional exponential decay learning scheduling lower upper threshold suspect shrink exponential decay figure three additional hyper parameter beta overall think method fundamental flew offer limited novelty theoretical justification modification good discus potential failure mode proposed method furthermore hard follow section writing quality clarity method section improved,4
660.json,demonstrates semi automatic learning rate schedule adam optimizer called originality somehow limited method appears positive effect neural network training well written illustration appropriate pro probably sophisticated scheduling technique simple decay term reasonable cifar dataset although comparably small neural network con effect momentum term interest adam reference point conference publication arxiv comparison adam entirely conclusive,5
772.json,conduct detailed evaluation different architecture applied image retrieval focus testing various architectural choice propose compare learning framework technically contribution clear particularly promised clarification multiple scale handled representation however still entirely clear whether difference multi scale settting full cropped query focus comparing different baseline architecture based image retrieval several recent paper proposed learn representation specific task good result instance recent work gordo learning deep visual representation image retrieval clarify work orthogonal paper gordo ass instead performance network trained image classification fact also indicate image retrieval difficult image classification performed using feature originally trained classification partially accept argument however given recent paper clear training superior practice clear analysis developed work transfer useful case well,5
772.json,investigate pretrained cnns retrieval perform extensive evaluation influence various parameter detailed comment everything question posted earlier summary think learn much already knew last conv layer knew whitening knew original size image tolias resized image exactly reason evaluate holiday image basically used large possible image size effectively suggests well essentially concatenates method people already used performs parameter tweaking achieve state tweaking actually performed test test setting state quite misleading really come good choice parameter mainly usage deeper network furthermore think sufficient network claim best practice using cnns instance retrieval resnet inception know apply conclusion network conclusion even hold furthermore parameter tweaking done oxford really tell conclusion tuned example appropriate title best parameter value oxford paris benchmark think sufficiently novel interesting community,2
322.json,proposes nonparametric neural network automatically learns size training process idea randomly zero unit sparse regularizer automatically null weight irrelevant idea sound random search approach discrete space help sparse regularization eliminate useless unit important problem give interesting main comment listed additional computation complexity algorithm decomposition weight parallel component orthogonal component transformation radial angular coordinate require extra computation time need discus extra amount operation relative parametric neural network furthermore useful show running time experiment observed nonparametric network return small network convex dataset inferior parametric network insight,6
322.json,agree reviewer interesting part idea removing adding unit definitely interesting direction make grow shrink along line required problem data user prior knowledge offer interesting theoretical result prof regularization optimum error function achieved finite number parameter grow indefinitely fit perfectly data reminds traditional approach lasso elastic regularization produce sparse weight like intuition given theorem nice result somewhat expected last intuitive liked intuition given space example le discussion prior work nice important discussing studying main result could make room addressing theoretical please also point suggestion comment make interesting experiment show node neuron added removed automatically outperform number node complete learning size number node layer fixed start prove efficiency idea method interesting save node needed replace node needed optimize performance memory understand experiment along line given figure mixed figure must clear possible interpret careful inspection parametric net better others worse parametric one even case could usefulness method help discovering structure fully understand better sometimes could trained scratch nonparametric version learning better parametric version final known advance could give insight better discus meaning implication theorem feel theorem proper discussion beyond proof appendix insight theorem plain english conclusion seems almost natural obvious powerful insight mentioned previously feel theoretical result deserves space even experiment back example regularizer parameter lambda predicted given data property data help guessing right lambda feeling lambda factor determining final structure true much structure final depend initialization different net start different random weight different happens regularizers combined still theoretical result additional question adding zero unit change regularizer value example norm change zero value zero unit defined either weight zero think meant weight zero otherwise cannot remove unit keep output clarified better think changed rating hoping address comment,6
637.json,proposes process mine rule vector space representation learned using nonnegative rescal nicely written motivation unclear underlying motivation mine rule embedding space better performance link prediction show experiment compare performance original vector space better interpretability debugging representation learned vector space model element remark fact performance method figure compared baseline problematic scalability rule miner drawback addressed figure good convincing rule based system used prediction interpretation learned rule case,2
637.json,present nice idea directly finding rule brother father uncle knowledge base directly searching embedding space idea interpret successive application relationship multiplication relation dependent matrix negative rescal experimental section provides evaluation rule found algorithm nonetheless work seems first stage many question left open approach find rule seems general reason work unclear property embedding space initial algorithm required approach find meaningful rule apply principle algorithm negative rescal real evaluation term link prediction rule conjunction original algorithm improve link prediction performance gain expected rule find link found original algorithm first place scaling number parameter rule miner relationship path length method scale standard benchmark relationship,3
508.json,proposed method mainly graph classification proposal decompose graph object hierarchy small graph followed generating vector embeddings aggregation using deep network approach reasonable intuitive however experiment show superiority approach proposed method outperforms yanardag niepert social network graph quite inferior niepert informatics datasets report acccuracy yanardag similar ddatasets example significantly better achieved proposed method claim method tailored social network graph supported good argument model graph method suitable,4
508.json,contributes recent work investigating neural network used graph structured data tell proposed approach following construct hierarchical object within graph object consists multiple part object level potentially different way part part object different label maybe call membership type experiment object bottom level vertex next level radius vertex radius neighborhood around vertex membership type either root element depending whether vertex center neighborhood neighbor level object consisting neighborhood membership type radius neighborhood still vertex radius neighborhood every object representation vertex representation encoding degree construct object representation next level following scheme employed object representation part membership type concatenate sum obtained different membership type pas vector multi layer neural provided summary mainly description somewhat hard follow relevant detail scattered throughout text like verify understanding correct additional question clear text many layer hidden unit used dimensionality representation used layer final classification performed motivation chosen graph representation proposed approach interesting novel compression technique appears effective seem compelling however clarity structure writing quite poor took figure going initial description provided without illustrative example required jumping around figure example label actually used important detail around network architecture provided little motivation given many choice made choice decomposition object part structure investigated given generality shift aggregate extract formulation motivated choice graph degree initial attribute overall think contains useful contribution technical level presentation need significantly cleaned recommend acceptance,4
334.json,presented extension current visual attention learns deformable sampling lattice comparing fixed sampling lattice previous work proposed method show different sampling strategy emerge depending visual classification task empirically demonstrated learnt sampling lattice outperforms fixed strategy interestingly attention mechanism constrained translation proposed learns sampling lattice resembles retina found primate retina pro generally well organized written qualitative analysis experimental section comprehensive con could benefit substantially additional experiment different datasets clear table proposed learnt sampling lattice offer computation benefit comparing fixed sampling strategy zooming capability used draw overall really like think experimental section improved additional experiment quantitative analysis baseline current revision show experiment digit dataset black background hard generalize finding even verify claim linear relationship eccentricity sampling interval lead primate retina single dataset,4
461.json,present semi supervised learning encouraging feature invariance stochastic perturbation network input model described invariance term applied different instantiation input single training step second invariance applied feature input point across training step cumulative exponential averaging feature model evaluated using cifar svhn finding decent gain similar amount case additional application also explored showing tolerance corrupted label well also discus recent work sajjadi similar spirit think help corroborate finding largest critique nice application larger datasets well cifar svhn fairly small test case though adequate demonstration idea case unlabelled data especially good test order data sample labeled common case label missing similar note data augmentation restricted translation cifar horizontal flip standard note augmentation interesting particularly since designed explicitly take advantage random sampling detail might also mention handling horizontal flip different way variant rather restrict system particular augmentation think interesting push performance behaves larger array augmentation even fewer number label overall seems like simple approach getting decent though liked larger experiment better sense performance characteristic smaller comment mention dark knowledge couple time explaining bottom motivation analyzing think possible something concrete instance consistency term encourages feature invariance stochastic sampling strongly classification loss alone,6
461.json,work explores taking advantage stochasticity neural network output randomized augmentation regularization technique provide target unlabeled data semi supervised setting accomplished either applying stochastic augmentation regularization single image multiple time epoch encouraging output similar keeping weighted average past epoch output penalizing deviation current network output running mean temporal ensembling core argument approach produce ensemble prediction likely accurate current network thus good target unlabeled data approach seem work quite well semi supervised task show almost unbelievably robust label noise clearly written provides sufficient detail reproduce addition providing public code base core idea quite interesting seems result higher semi supervised accuracy prior work also found attention discussion effect different choice data augmentation useful little surprised standard supervised network achieve accuracy svhn given random training label give correctly labeled data chance unaltered suppose provide consistent training signal possible seem quite unintuitive tried look github experiment seem included resistance temporal ensembling label noise find somewhat believable given large weight placed consistency constraint task really include discussion main especially tremendous difference wmax incorrect label tolerance experiment temporal ensembling standard setting could comment towards scalability larger problem imagenet need store around gig temporal ensembling method spend long training discus sensitivity approach amount location dropout layer architecture preliminary rating think interesting quality clear presentation minor note paragraph page without neither without either,8
461.json,present semi supervised technique self ensembling us consensus prediction computed previous epoch target regress addition usual supervised learning loss connection dark knowledge idea ladder network work shown promising technique scenario labeled example present version idea computationally expensive high variance need pass example given step temporal ensembling method stabler cheaper computationally memory hungry requires extra hyper parameter thought work mostly positive drawback temporal ensembling work requires potentially memory trivial infrastructure book keeping imagenet sized experiment quite confused figure section experiment tolerance noisy label incredible making label random still train classifier either accurate accurate depending whether temporal ensembling used happen basically minor stuff please bold best category table think nice talk ramp main consider putting state fully supervised case table instead confused chose svhn example stated reason easy seems contrived used example also make easy compare previous work,7
524.json,tdlr present regularization method wherein noise representation space mainly applies technique sequence autoencoders without usage attention using context vector experimental show improvement author baseline task augmentation augmentation process simple enough take seqseq context vector noise interpolate extrapolate section reviewer curious whether process also work seqseq application reviewer liked comparison dropout context vector experiment since experimenting seqseq architecture little disappointing compare machine translation many published paper compare compare method several datasets le commonly used literature mnist cifar show improvement baseline several datasets improvement mnist cifar author baseline seems marginal best author also cite compare baseline published cifar much better lstm baseline cifar beat author baseline author method experiment much convincing seqseq almost excuse experiment task given first application seqseq born even least sentiment analysis task imdb rotten tomato heavily based sequence autoencoder reference something wrong reference latex setting seems like conference journal name omitted additionally update many cite conference journal name rather arxiv listen attend spell listen attend spell neural network large vocabulary conversational speech recognition icassp citing icassp also cite bahandau attention based large vocabulary speech recognition published parallel also icassp adam method stochastic optimization iclr auto encoding variational bayes iclr addressing rare word problem neural machine translation pixel recurrent neural network icml neural conversational icml workshop,3
748.json,system described work comparably directional lstm baseline naturally parallelizable idea include stacked encoding decoding translation connection position embeddings translation attempted previously described presumably combination various architectural choice attention position embeddings make present system competitive whereas earlier attempt describe system sensitivity choice experiment choose appropriate number layer experimental well reported detail figure definitely required help clarify architecture le way learning representation combination choice made existing technique order good reported task respect fairly confident represents good work machine learning quite confident particular conference,6
748.json,report clear easy understand result convolutional network used instead recurrent encoder neural machine translation apart known architectural element convolution pooling residual connection position embeddings feature unexpected architectural twist stack convolution computing alignment another computing representation empirical evidence necessary provided however question necessary remains open experimental evaluation extensive leaf doubt proposed approach work well convnet based faster evaluation clear main speed factor however hard argue fact speed advantage convnets likely increase parallel implementation considered main concern whether appropriate iclr contribution quite incremental rather application specific emnlp conference better venue think,5
318.json,main contribution seems introduction differential graph transformation allow learn graph graph classification task using gradient descent map naturally task learning cellular automaton represented sequence graph task graph node grows iteration node pointing neighbor special node representing value proposed architecture allows learn sequence graph although experiment task rule solved idea combined idea previous paper allow produce textual output rather graph output graph intermediate representation allows beat state babi task,6
318.json,proposes learning represent dialog graph act memory first demonstrated babi task graph learning part inference process though long term representation learning learn graph transformation parameter encoding sentence input graph seems first implementation differentiable memory graph much complex previous approach like memory network without significant gain performance babi task still preliminary work representation memory graph seems much powerful stack clarity major issue initial version constructive better read computer human author proposed hugely improved later version original technically accurate within understood thought provoking worth publishing preliminary tell highly complex graph based differentiable memory learning generalization capacity approach performance babi task comparable best memory network still worse traditional rule induction,8
436.json,present neural network problem designing natural language interface database query proposed approach us weak supervision signal learn parameter unlike traditional approach problem solved semantically parsing natural language query logical form executing logical form given data base proposed approach train neural network manner go directly natural language query final answer obtained processing data base achieved formulating collection operation performed data base continuous operation distribution learnt using standard soft attention mechanism validated smallish wikitablequestions dataset show single performs worse approach us traditional semantic parsing technique however ensemble model trained variety way comparable performance state feel proposes interesting solution hard problem learning natural language interface data base extension previously proposed model neelakantan experimental section rather weak though show work single smallish dataset love ablation study comparison fancier version memnns initial response testing memory network objection though detail rather convoluted section clearly written particular absence accompanying code super hard replicate wish better explaining detail exactly discrete operation modeled role selector scalar answer lookup answer full attention entire database think approach scale data base huge million row wish experimented larger datasets well,6
436.json,proposes weakly supervised neural network solving challenging natural language understanding task extension neural programmer work aim overcoming ambiguity imposed natural language predefining operation able learn interface language reasoning answer composition using backpropagation wikitablequestions dataset able achieve slightly better performance traditional semantic parser method overall interesting promising work involves real world challenge natural language understanding intuition design clear complication make difficult read mean also difficult reimplemented expect detail ablation help figure prominent part design,5
436.json,proposes weakly supervised neural network learn natural language interface table neural programmer applied wikitablequestions natural language dataset achieves reasonable accuracy ensemble boost performance combining component built different configuration achieves comparable performance traditional natural language semantic parser baseline dropout weight decay seem play significant role interesting error analysis major reason still accuracy compared many task headroom oracle number current approach,5
573.json,pro general idea behind seems pretty novel potentially quite cool specific technical implementation seems pretty reasonable well thought general type task approach span wide interesting spectrum cognition ability writing pretty clear basically felt like could replicate much description con evaluation success idea compared possible approach compared human performance similar task extremely cursory specific task quite simple really know whether approach better bunch simpler thing task taking con together feel like basically implementation done working somewhat wrote know feel deadline without complete used approach solve obviously hard problem previously completely unsolved even type cursory evaluation level chosen fine done thorough evaluation bunch standard model task human ideally compared great given complexity method fact task either well known benchmark challenging really hard tell much advance made seem like potentially fruitful research direction,5
420.json,explores variety memory augmented architecture value predict value additionally simpler near memory le architecture using attention access various decomposition interesting idea worth future exploration potentially different task type could excel even wikipedia corpus interesting feature wide variety different type model suggested strongest model dataset show comparable le convincing demonstration variation model also released wikipedia corpus already inspected consider positive interesting contribution still believe found could better handle longer term dependency better wikipedia dataset least within realm example first article train person named george abbot abbot mentioned next sentence token later next abbot token gap occurrence abbot dozen timesteps performing analysis based upon easily accessed information token reappears average sentence length useful approximation length attention window prefer well explained raise interesting question regarding span used existing language modeling approach serf potential springboard future direction,6
420.json,present investigation various neural language model designed query context information recent history using attention mechanism propose separate attended vector value prediction part suggest help performance also found simple concatenates recent activation vector performs similar level complicated attention based model experimental methodology seems sound general issue dimensionality vector involved attention mechanism chosen good hidden layer size adapted ensure similar number trainable parameter model control fact value prediction vector higher dimensionality simply work better regardless whether dimension dedicated particular task used together separation clearly save parameter could also benefit overlap information assuming vector lead similar prediction also required similar context example task also require dimension others explicit separation prevents discovering exploiting memory augmented rnns rnns attention mechanism architecture applied language modeling similarly acknowledged strategy separating value functionality proposed context natural language modeling sure novelty proposed gram recall seeing similar architecture understand novelty point architecture mainly serf proof lack ability complicated architecture better sense consider inventive baseline could used future work test ability model claim exploit long term dependency exact computation representation initially clear term hidden output ambiguous time besides quite clear generally well written important show learning long term dependency solved problem mean provide nice comparison prior fact gram often least competitive complicated approach clear indication method capture much context information previously thought success separation value prediction functionality attention based system also noteworthy although think something need investigated thoroughly control hyperparameter choice pro impressive also interesting good comparison earlier work gram interesting baseline con relation attention mechanism type number hidden unit weakens claim value prediction separation reason increase performance somewhat description entirely clear liked seen happens attention applied much larger context size,6
709.json,propose pretrain encoder decoder seqseq model large amount unlabeled data using objective obtain improvement using technique machine translation abstractive summarization effectiveness pretraining seqseq model known among researcher explored paper zoph believe first pretrain using encoder decoder technique simple gain large bleu addition perform extensive ablation study analyze performance coming hence think accepted,6
709.json,strength method proposed initialize encoder decoder seqseq using trained weight language model parallel data pretraining weight jointly fine tuned parallel labeled data additional language modeling loss shown pretraining accelerates training improves generalization seqseq model main value proposed method leverage separate source target corpus contrasting common method using large amount parallel training corpus weakness objective function shown middle highly empirical directly linked parallel data help improve final prediction compare discus objective function based expectation cross entropy directly linked improving prediction proposed arxiv chen unsupervised learning predictor unpaired input output sample training procedure proposed also closely connected pretraining method presented dahl comparison made highlighting proposed conceptually superior believe,4
532.json,easy follow idea pretty clear make sense experimental hard judge nice baseline faster training convergence question well tuned mentioning learning rate schedule also important test data set success filtering training data could task dependent,6
532.json,work proposes augment normal gradient descent algorithm data filter act curriculum teacher selecting example trained target network learn optimally filter learned simultaneously target network trained reinforcement learning algorithm receiving reward based state training respect pseudo validation stylistic comment please common style author year rather author year author referred used sentence variant adagrad duchi adagrad duchi proposed andrychowicz remain think paragraph containing need seeing mini batch training instance dynamically determine instance used training filtered clarified seeing mention explicitly forward pas first compute feature decide example perform backwards pas choice work understand wait episode update reinforce policy algorithm train actor critic step algorithm reinforce high variance true mean cannot trained step unless experiment suggest otherwise included mentionned similarly train reinforce reward actor critic vice versa claim several time limitation reinforce need wait episode considering data make episode anything single training step whole multi epoch training procedure qualm experimental setting figure obtained single setup experiment different initial weight proper knowing whether chance serious concern state work using optimization method adam rmsprop surprising experimented clear learning rate fast part adapt part clear experimented environment target network trained stationnary interesting measure much policy change function time figure could result policy adapting policy remaining fixed feature changing could indicate failure policy adapt fact really adressed environment stationary given current setup distribution feature change target network progress impact optimization pseudo validation data target policy chosen subset training data second paragraph section suggests something sort algorithm suggest data used train policy network unsure overall idea novel interesting well written part methodology flaw clearer explanation either justification experimental choice experiment needed make complete unless convince otherwise think worth waiting experiment submitting strong rather presenting potentially powerful idea weak,5
476.json,description describes experiment testing whether deep convolutional network replaced shallow network number parameter without loss accuracy experiment performed cifar dataset deep convolutional teacher network used train shallow student network using regression logit output show similar accuracy parameter budget obtained multiple layer convolution used strong point experiment carefully done thorough selection hyperparameters show interesting partially conclusion previous work area caruana well clearly written weak point cifar still somewhat dataset class interesting challenging problem imagenet large number class similar originality mainly experimental question asks interesting worth investigation experimental solid provide insight quality experiment well done clarity well written clear significance conclusion previous work published discussed overall experimental interesting well written solid experiment,6
476.json,aim investigate question shallow convolutional network affective deep convolutional one image classification given architecture number parameter conducted series experiment cifar dataset find significant performance approach favour deep cnns experiment well designed involve distillation training approach presented comprehensive manner also observe others student model shallower teacher trained comparable performance take suggest using deep conv net effective since class encodes form prori domain knowledge image exhibit certain degree translation invariance processed high level recognition task therefore perhaps quite surprising completely obvious either interesting point comment briefly among convolutional architecture one using hidden layer outperform hidden layer interpretation hypothesis case interesting discus point quite clear experiment limited parameter none experiment figure seem saturated although performance large think worthwhile push experiment final version state last paragraph expect shallow net relatively worse imagenet classification experiment could argue think case could argue much larger training dataset size could compensate shallow convolutional choice architecture since mlps universal function approximators could understand architecture choice expression certain prior function space large data regime prior could expected lesser importance issue could example examined imagenet varying amount training data also much higher resolution imagenet image might trivial impact comparison compared established cifar dataset experiment second data also help corroborate finding demonstrating extent finding variable across datasets,6
499.json,although trainable parameter might reduced significantly unfortunately training recognition speech cannot reduced unfortunately show could better le parameter however proposed structure even number parameter show significant gain reorganized shortened sometimes difficult follow sometimes inconsistent weight feedforward network depend embedding vector also previous comment linear bottleneck whereas recurrent network generated weight also depend input observation hidden representation could provide trainable parameter table probably presenting le could also improve readability marginal accept writing style,7
499.json,proposes interesting method training neural network hypernetwork used generate parameter main network demonstrated total number parameter could smaller achieving competitive image classification task particular hyperlstm shared weight achieve excellent compared conventional lstm variant couple talk inspiring pro work demonstrates possible generate neural network parameter using another network achieve competitive relative large scale experiment idea inspiring experiment solid con much stronger focused particular unclear advantage hypernetwork approach argued achieve competitive using smaller number trainable parameter however running time computational complexity standard main network static network convnet computational cost even larger dynamic network lstms improvement hyperlstms conventional lstm variant seem mainly come increasing number parameter minor question convnet lstm used experiment large softmax layer word level task either softmax layer could going challenging hypernetwork generate large number weight case going slowing training significantly,6
708.json,summary work proposes algorithm generate adversarial image modifying small fraction image pixel without requiring access classification network weight review summary topic adversarial image generation practical theoretical interest work proposes approach problem however suffers multiple issue verbose spending long time experiment limited interest disorganized detailed description main algorithm section piece added experimental section importantly resulting experiment limited interest reader main conclusion left unclear look like interesting line work materialize good document need significant writing good shape iclr pro interesting topic black setup relevant multiple experiment show flipping pixel adversarial image created con long detail well addressed experiment little interest main experiment lack measure additional baseline limited technical novelty quality method description experimental setup leave desired clarity text verbose somewhat formal mostly clear could improved concise originality aware another work exact type experiment however approach surprising significance work incremental issue experiment limit potential impact specific comment suggest start making shorter reducing text length force make argumentation description direct select important experiment section seems flawed modified single pixel value outside range test sample clearly outside training distribution thus surprising classifier misbehaves true classifier decision forest linear svms interesting modified pixel clamped range never specified compare reported proportion modification done normalization realistic clamping section implementing algorithm locsearchadv text unclear adjusted variable added confusion section happens adjusted happens simple greedy random search used time random pixel value section computed pixel including one modified thus locsearchadv value directly comparable fgsm since intermingles ptbpixels many case le average perturbation claim section discussion average number evaluation equivalent number request made system fool number important claim effectiveness black attack right text mention upper bound network evaluation number network evaluation change adjusting adjusting optimization claimed main point experiment provided please develop tune claim fgsm effective batch normalized network reported already published technique effective scenario comparing method interesting little note section concluded section possible obtain good modifying pixel selecting largest modified pixel fgsm enough please develop baseline specific conclusion interest minor comment abuse footnote inserted main text suggest repeat twice thrice meaning main variable used table figure last line first paragraph section uninformative tiny small,3
708.json,present method generating adversarial input image convolutional neural network given black access ability obtain output chosen input access network parameter however notion adversarial example somewhat weakened setting misclassification ensuring true label output instead misclassification desired target label similar black setting examined papernot black access used train substitute network attacked black access instead exploited local search input perturbed resulting change output score examined perturbation push score towards misclassification kept major concern regard novelty greedy local search procedure analogous gradient descent numeric approximation observe change output corresponding change input used instead backpropagation since access network parameter greedy local search algorithm devotes large amount discussion surprising fairly incremental term technical novelty,3
421.json,proposes novel interesting tackle difficulty performing inference atop hsmm idea using embedded approximate posterior reasonable clever idea said think aspect need improvement explanation provide accurate approximation modeling choice structured mean field us sequential formulate variational distribution needed think make stronger explain intuitive modeling choice better natural choice addition empirical verification real world datasets seem quite small le sequence experimental reported larger datasets also strengthen,6
421.json,putting score post full review tomorrow,6
437.json,introduces based implementation algorithm originally designed multi core cpu main innovation introduction system queue queue used batching data prediction training order achieve high occupancy system compared implementation well published reference score introduces natural architecture implementing gpus batching request prediction learning step multiple actor maximize occupancy seems like right thing assuming latency issue automatic performance tuning strategy also really nice appreciate response showing throughput higher reported original still missing demonstration learning speed data efficiency right ballpark figure comparing score different evaluation protocol number comparable convincing show learning speed comparable time score plot data score plot show similar improved speed example open source implementation seems match performance breakout,4
437.json,introduce variant agent multiple core computation computationally intensive part passed perform various analysis show gained speedup thanks replying question adjusting make clear interesting modification original algorithm section analysis utilization different configuration main weakness lack extensive experiment atari domain atari domain also multiple plot multiple run observing instability stability important issue also successful algorithm able achieve good various domain understand computational resource limitation especially academia fact work done outside nvidia,6
572.json,comment contrast adversarial attack classifier inspection input reveal original byte adversary supplied often telltale noise really true case imply training adversarial example easily make classifier robust adversarial example telltale noise pro question whether adversarial example exist generative model indeed definition adversarial example carry interesting finding certain type generative adversarial example really significant result finding generative model adversarial example also worth negative result adversarial example figure seem convincing though seem much overt noisy adversarial example mnist shown szegedy actually harder find adversarial example type generative model issue significantly length page beginning clearly motivate purpose generative model title tell whole concerned autoencoder type model kind annoying someone wanted consider adversarial attack autoregressive model might unreasonably burdened explain distinct called adversarial example generative model think introduction contains much background information could tightened,5
572.json,considers different method producing adversarial example generative model vaegan specifically three method considered classification based adversary us classifier hidden code loss directly us loss latent attack find adversarial perturbation input match latent representation target input think problem considers potentially useful interesting community best knowledge first considers adversarial example generative model pointed review comment also concurrent work adversarial image variational autoencoders essentially proposes latent attack idea distance divergence novelty originality find idea original proposed three attack well known standard method applied problem develop novel algorithm attacking specifically generative model however still find interesting standard method compare problem domain clarity presentation unsatisfying first version proposes classification based adversary report negative second revision core idea change almost entirely author submitted idea latent attack proposed work much better classification based adversary however keep around material first version page long different claim unrelated experiment attempt thorough hard time keeping length valid excuse short investigating interesting problem apply compare standard adversarial method domain novelty presentation limited,4
572.json,rebuttal contains interesting mainly produced initial submission novelty limited presentation suboptimal biggest problem title content correspond clearly attack deterministic encoder decoder model described generative model even though many generative model make architecture small experiment sampling interesting change overall focus inconsistency acceptable whole issue could resolved example simply replacing generative model encoder decoder network title tend towards recommending acceptance initial review describes three approach generating adversarial example deep encoder decoder generative network trained show comparative analysis phenomenon adversarial example discriminative model widely known relatively well studied aware previous work adversarial example generative network work novel concurrent work tabacof cited though significantly improved since initial submission still number remark presentation experimental evaluation borderline mode change rating discussion phase detailed comment page long significantly recommended page limit page reviewer read multiple paper multiple version work large portion shortened moved appendix make concise readable attempt thorough hard time keeping length excuse hard done intentionally avoided term generative obvious attack described indeed attack generative model clarify train encoder decoder generative model remove stochasticity sampling prior latent variable treat model deterministic encoders decoder surprise deterministic deep network easily tricked much interesting probabilistic aspect generative model make robust attack missing something like clarify view adjust claim necessary motivated possible attack data channel us generative network compressing information description attack scenario look convincing take huge amount space think add much first experiment natural image necessary judge proposed attack could succeed realistic scenario second aware existing practical application vaes image compression attacking jpeg much practical experiment limited mnist latest version svhn nice good generative model general natural image exist common evaluate generative model datasets face another natural domain testing proposed approach smaller remark usage oracle look appropriate oracle typically access part ground truth case understand beginning section three method work generative architecture relies learned latent representation technically applicable correct work confidentally,4
319.json,proposes transferring knowledge like idea transferring attention map instead activation however experiment show improvement compared knowledge distillation alone think experiment required imagenet section consider updating score extend last section,5
319.json,presented modified knowledge distillation framework minimizes difference statistic across feature teacher student network empirically demonstrated proposed method outperform fitnet style distillation baseline pro author evaluated proposed method various computer vision dataset general well written con method seems limited convolutional architecture attention terminology misleading proposed method really distill summed squared statistic summed norm activation hidden feature gradient based attention transfer seems place proposed gradient based method never compared directly used jointly attention based transfer seems like parallel idea added seem much value also clear induced norm computed matrix mathbb time whose induced norm largest singular value seems computationally expensive compute cost function possible really mean frobenius norm overall proposed distillation method work well practice organization issue unclear notation,5
319.json,proposes investigate attention transfer teacher student network attention transfer performed minimising distance teacher student attention map different layer addition minimising classification loss optionally knowledge distillation term define several activation based attention absolute feature value raise power value raised power also propose gradient based attention derivative loss input evaluate approach several datasets cifar scene imagenet showing attention transfer help improving student network test performance however student network performs worst teacher even attention remark question section claim network higher accuracy higher spatial correlation object attention figure compelling nice quantitative showing well choose hyperparameter value nice impact beta nice report teacher train validation loss figure experiment clear pro con different attention map lead better result teacher however student network le parameter interesting characterise corresponding speed keep architecture student teacher benefit attention transfer summary pro clearly written well motivated consistent improvement student attention compared student alone con student worst performance teacher model clear attention case somewhat incremental novelty relatively fitnet,5
525.json,summary proposes algorithm learn structure continuous spns single pas data basically growing variable correlated note expert spns really judge impressive presented lack familiarity datsets look like possibly impactful work proposing simple elegant algorithm learning structure single pas rather using random structure done work online setting heavily updated submission deadline submission review read like rush sloppily written least first version comparison literature severely lacking several automated structure learning technique proposed followed citation discussion related idea carry offline setting online setting also since work present joint structure parameter learning comparison online parameter learning paper cited appreciated specifically since prior approach seem principled bayesian moment matching jaini example know enough spns datasets properly judge strong seem underwhelming large datasets random remaining question update table random structure baseline parameter learned simple running average advanced method table presenting positive average likelihood value average value missing recommend reject mostly finished polished submission time review deadline time,3
362.json,paper add literature learning optimizers algorithm gained popularity recently choose framework guided policy search meta level train optimizers also train random objective ass transfer simple task pointed useful addition however argument using gradient meta level appears clear convincing urge experiment comparing approach present comparative important question scalability approach could well hinge fact indeed demonstrating scaling large domain transfer domain challenge domain summary idea good experiment weak,5
362.json,current version improved original arxiv version june exactly text oversell much also consider avoid word like mantra believe criticism given comment randomly generated task valid answer,6
677.json,proposes unsupervised graph embedding learning method based random walk skip thought show promising compared several competitor four chemical compound datasets strength idea learning graph embedding applying skip thought random walk sequence interesting well organized weakness current datasets small average number node graph around great explore larger graph datasets investigate method comparison recent work like line nodevec missing compare easily applying aggregation strategy node embeddings detailed question description split random walk sequence sequence missing also line lmin lmax section mistake provide standard deviation fold cross validation table curious stable algorithm,4
677.json,study graph embedding problem using encoder decoder method experimental study real network data set show feature extracted proposed good classification strong point idea using method natural language processing graph mining quite interesting organization clear weak point comparison state method graph kernel missing problem well motivated application different graph kernel method comparison graph kernel missing need experiment demonstrate power feature extraction method clustering search prediction presentation weak lot typo unclear statement author mentioned graph kernel thing experiment compare also compare classification accuracy using proposed method enough,4
677.json,take skip graph architecture kiros apply classifying labeled graph molecular graph creating many sentence walking graph randomly asking predict previous part next part middle part activation decoder part walk generated graph used feature binary classifier predict whether molecule anti cancer property well written except evaluation section missing detail embedding used actual classification classifier used unfortunately familiar dataset hard achieve demonstrate important factor weight paper acceptance,5
677.json,present method learn graph embeddings unsupervised using random walk well written execution appears quite accurate area learning whole graph representation seem well explored general proposed approach enjoys competitor nutshell idea linearize graph using random walk compute embedding central segment walk using skip thought criterion expert biology comment whether make sense gain reported table quite significant anonymous public comment compared work number others problem learning representation node considered arguably different goal natural baseline pool representation using mean pooling interesting comparison especially given considered approach heavily relies pooling figure think nice baseline ready increase numerical score,6
335.json,present information theoretic framework unsupervised learning framework relies infomax principle whose goal maximize mutual information input output propose step algorithm learning setting first leveraging asymptotic approximation mutual information global objective decoupled subgoals whose solution expressed closed form next serve initial guess global solution refined gradient descent algorithm story derivation seem sound clarity presentation material could improve example instead listing step step derivation equation nice first give high level presentation result maybe explain briefly derivation strategy detailed aspect derivation could obscure underlying message result could perhaps postponed later section even moved appendix question want clarify page last paragraph know maximizing result maximizing former hold equality latter related bound possible claim maximizing former indeed maximizes latter true paragraph section stated dropout used prevent overfitting fact regarded attempt reduce rank weight matrix provided case could elaborate page discus optimal solution specific case understand correctly actually guaranteed optimal solution either case best guarantee reaching local optimum nonconvexity constraint quadratic equality optimality cannot guaranteed please correct wording accordingly,7
335.json,proposes hierarchical infomax method comment follows first page without appendix long conference proceeding therefore easy reader follow make compact possible maintaining important message main contribution find good initialization point maximizing however unclear maximizing breve good maximizing proposition show breve upper bound difficult directly maximize function people often maximize tractable lower bound minor comment approximation approx used instead disappear divide section subsection,4
765.json,introduces additional reward predicting head existing architecture video frame prediction atari game playing scenario show successfully predict reward next frame pro well written easy follow clear understand con incrementally different baseline state purpose establish condition achieve make quite limited scope read like start really good long good short following future work proposed make great stand thin contribution,3
765.json,extends recently proposed video frame prediction method reward prediction order learn unknown system dynamic reward structure environment method tested several atari game able predict reward quite well within range step well written focussed quite clear contribution literature experiment method sound however really surprising given system state reward linked deterministically atari game word always decode reward network successfully encodes future system state latent representation contribution therefore minor much stronger could include experiment future work direction suggest conclusion augmenting training artificial sample adding monte carlo tree search suggestion might decrease number real world training sample increase performance interesting impactful,3
765.json,topic based learned important timely well written feel presented incremental augmenting frame prediction network another head predicts reward sensible thing however neither methodology novel surprising given original method already learns successfully increment score counter predicted frame many game much looking forward seeing applying learned joint frame reward based proposed,3
509.json,present approach make programming language forth interpreter differentiable learn implementation high level instruction provided example well written research well motivated overall find interesting pleasure read however experiment serve proof concept detailed empirical study strength comment knowledge proposed approach novel nicely bridge programming example sketch programmer proposed approach borrow idea probabilistic programming neural turing machine significantly different method also present optimisation interpreter speed training interesting present different type programming problem complex level code generated,6
509.json,present approach structured program induction based program sketch forth simple stack based language turn overall open problem program induction slot filling problem differentiable forth interpreter backprop slot random variable point sketch partial program learn complex program starting scratch prior information loss optimize program flow rmse program memory targeted masked adresses desired output show learn addition bubble sort permute sketch compare sketch idea making language fully differentiable write partial program sketch completed previously explored probabilistic programming community recently terpret think using forth simple stack based language sketch definition language interesting machine code neural turing machine stack neural approach higher level language church terpret problog section figure could made clearer explain color code explain parallel input list experimental section quite sparse even learning sort experimental setting train length test length study length generalization break seems possibly study relative runtime improvement training size length input sequence baseline even least exhaustive search neural approach plus compare similarly addition experiment section shortly described baseline either whereas staple neural approach program induction presented sketch trained single digit addition example successfully learns addition generalises longer sequence mean generalizes three digit overall interesting seems like experiment support claim usefulness enough,4
636.json,main observation made dropout increase variance neuron correcting increase variance parameter initialization test time statistic batch normalization improves performance shown reasonably convincingly experiment observation important applies many model used literature extremely novel observed literature simple dropout approximation test time achieve accuracy obtained full monte carlo dropout could experimental validation specifically guessing correction dropout variance test time specific batch normalization standard dropout network without batch normalization corrects mean test time dividing activation minus dropout probability work suggests beneficial also correct variance tested dropout variance correction compare using monte carlo dropout test time averaging large number random dropout mask,6
374.json,proposes gating mechanism combine word character representation proposed set state dataset gating mechanism also improves scalar gate without linguistic feature squad twitter classification task intuitively vector based gate working better scalar gate unsurprising similar lstm gate real contribution using feature tag help learn better gate visualization figure example table effectively confirm utility feature nice proposed gate nothing technically groundbreaking present focused contribution think useful community thus hope accepted,6
374.json,think problem well motivated approach insightful intuitive convincing approach although lacking variety application like fact term intermediate signal decision also compare sufficient range baseline show effectiveness proposed also convinced answer question think sufficient evidence provided show effectiveness inductive bias introduced fine grained gating,5
661.json,proposes aim learning label node graph semi supervised setting idea based graph structure regularize representation learned node level experimental provided different task underlying idea graph regularization already explored different paper learning latent representation node classifying heterogeneous social network jacob weston real graph structure used instead built experiment lack strong comparison graph model iterative classification learning labeled unlabeled data directed graph novelty experimental protocol strong enough accpet pro learning graph important topic con many existing approach already exploited type idea resulting close model lack comparison existing model,2
548.json,summary proposes regularizer claimed help escaping saddle point method inspired physic thinking optimization process moving positively charged particle error surface pushed away saddle point saddle point positively changed well show several different datasets overview review pro idea interesting diverse different datasets con justification strong enough well written experiment convincing enough criticism liked idea intuition coming however think written well variable introduced explained good enough example start talk without introducing defining properly place appears equation equation need work well work needed term improving flow introducing variable properly using equation appears without proper explanation justification necessary explain mean properly since think important equation analysis mean term optimization point view also appreciated parameter function hyper parameter alpha interesting report validation test task well since method introduced additional cost function effect validation test interesting well discus choose hyper parameter model figure much difficult understand draw conclusion lot figure without label caption really small difficult understand since label figure appear small somewhat unreadable small question also backpropagate tilde,4
548.json,proposes novel method accelerating optimization near saddle point basic idea repel current parameter vector running average recent parameter value method shown optimize faster variety method variety datasets architecture surface proposed method seems extremely close momentum valuable think clear diagram illustrating differs momentum might better near saddle point illustration better convergence saddle example mean optimization speed comparison always difficult many detail hyper parameter involved seeing work faster specific application useful conceptual diagram show critical case behave differently clearly qualitatively better momentum another getting relationship momentum find form yield exact momentum update could compare used overly general notation dropped enough theoretical removed irrelevant joint density specified experimentally valuable standardize allow comparison method instance recreating figure dauphin engaging method rather clearly demonstrate escape something momentum cannot think idea potentially valuable need rigorous comparison clear relation momentum work,3
495.json,summary contributes description comparison representational power deep shallow neural network relu threshold unit main contribution show approximating strongly convex differentiable function possible much le unit using network hidden layer pro present interesting combination tool arrives nice result exponential superiority depth con main result appears address strongly convex univariate function specific comment thanks comment still good idea clarify point possible main part also suggest advertise main result prominently still read revision maybe already addressed point problem statement close montufar pascanu bengio nip specifically arrives exponential gap deep shallow relu network albeit different angle suggest include overview lemma theorem tilde missing theorem lower bound always increase theorem,6
616.json,describes network architecture inverse problem computer vision example inverse problem considered image inpainting computing intrinsic image decomposition foreground background separation architecture composed generator produce target latent output foreground background region renderer composes latent output back image compared input measure reconstruction error adversarial prior ensures target output latent image respect certain image statistic strong point proposed architecture memory database interesting appears novel weak point experimental proof concept clearly demonstrate benefit proposed architecture unclear whether memory retrieval engine retrieves image based distance pixel value going generalize realistic scenario clarity clarity explanation also improved detailed evaluation originality novelty work lie adversarial prior place adversarial loss generated latent output single image retrieved large unlabelled database target output example called memory adversarial prior convolutional form matching local image statistic rather entire image particular form network architecture memory based fully convolutional adversarial loss appears novel potentially interesting motivation architecture weakest point proposed architecture memory retrieval engine section image retrieved memory measuring distance pixel intensity maybe simple problem considered work unclear generalize complicated datasets problem better discussed better justified ideally realistic shown quality experiment shown inpainting mnist digit intrinsic image decomposition intrinsic image database figure ground layer extraction synthesized dataset chair rendered onto background real photograph experimental validation strong proof concept experiment performed simplified mnist digit inpainting current state image inpainting real photograph pathak foreground background separation done synthetically generated test data even intrinsic image demposition problem relatively large scale dataset bell citation probably iclr diminishes significance work going useful real setting possibility address focus problem show challenging state data great benefit memory database bell bala snavely intrinsic image wild transaction graphic clarity clarity writing improved found terminology specially imagination memory confusing figure clear memory given input image obtained also took time understand help understand proposed architecture useful draw illustration happening feature space similar spirit figure,5
587.json,main idea replace pooling layer convolution stride retraining merge layer brand type layer misleading adding noise field using strided convolution rather pooling actually novel,3
587.json,proposes reduce size evaluation time deep model mobile device converting multiple layer single layer retraining converted showed computation time reduced accuracy loss specific reducing size speeding evaluation important many application several concern many technique reduce size example shown several group using teacher student approach people achieve sometimes even better accuracy teacher using much smaller however compare technique proposed limited applicability since designed specifically model discussed replacing several layer single layer relatively standard procedure example mean variance normalization layer batch normalization layer absorbed without retraining losing accuracy rank approximation technique first proposed speech recognition gong august restructuring deep neural network acoustic model singular value decomposition interspeech,3
587.json,look idea fusing multiple layer typically convolution pooling layer single convolution retraining layer show simpler faster model constructed minimal loss accuracy idea fine several issue introduces concept eeprebirth layer seems like going architecture discover convolution actually different kind convolution depending whether fuse serial parallel pooling layer understand desire give name technique case naming layer actually multiple thing architecturally confuses argument way perform kind operator fusion without retraining deep learning framework theano upcoming tensorflow implement nice baseline implement especially since additional energy cost fused operator come extra intermediate memory writes operator fusion remove batchnorm folded convolution layer without retraining scaling weight folded baseline figure reported table time writing provided detail make research reproducible particular depth fused layer relates depth original layer experiment retraining much time epoch retraining take consider using form distillation interesting experiment need improvement suitable publication open sourcing implementation open source always enhances usefulness requirement obviously,3
568.json,proposes sentence classification pro interesting architecture choice network con evaluation architecture choice ablation study critical understand important evaluation standard datasets existing dataset evaluated simple tfidf method state unconvincing,3
568.json,proposes neural network sentence representation inspired success residual network computer vision observation word morphology natural language processing although show could give best several datasets lack strong evidence intuition motivation support network architecture specific confused contribution character aware word embedding residual network claim using residual network section seems pretty thin since ignores fundamental difference image representation sentence representation even though show adding residual network could help still convinced explanation captured residual component perspective sentence modeling combine several component classification framework including character aware word embedding residual network attention weight type feature like contribution final performance table possible ablation test equation meaning citation format impropriate,3
354.json,work develops method quickly produce ensemble deep network outperform single network trained equivalent amount time basis approach cyclic learning rate quickly settle local minimum saving snapshot time quickly raising learning rate escape towards different minimum well attraction resulting snapshot collected throughout single training achieve reasonable performance compared baseline gain traditional ensemble much lower cost well written clear informative figure table provides convincing across broad range model datasets especially liked analysis section publicly available code ensure reproducibility also greatly appreciated like discussion accuracy variability snapshot comparison true ensemble preliminary rating interesting work convincing experiment clear writing minor note axis lambda figure lambda naturally,8
591.json,proposes criterion sample importance study impact sample training deep neural network criterion clearly defined term never defined defined despite unclear definition understood sample importance squared norm gradient sample time strangely scaled squared learning rate learning rate nothing importance sample context present experiment well known mnist cifar datasets correspondingly appropriate network architecture choice hyper parameter initialisation size hidden layer small mnist small cifar could explain poor performance figure error cifar study evolution sample importance training depending layer seems lead trivial conclusion overall sample importance different different epoch norm gradient expected vary output layer always largest average sample importance parameter contribution reach maximum early training stage drop since gradient flow backwards gradient expected stronger output layer expected become diffuse propagates lower layer stable learning progress expect output layer progressively smaller gradient norm gradient depends scaling variable question figure absurd sample importance negative likelihood sample course cifar discredit applicability mnist performance readable figure error rate presented despite important issue others manages raise interesting thing called easy sample hard sample seem correspond although study preliminary regard intuitively considered easy representative canonical sample hard edge case sample also experiment well presented,1
591.json,examines called sample importance sample training data effect overall learning process show empirical show different training case induces bigger gradient different stage learning different layer show interesting contrary common curriculum learning idea using easy training sample first however unclear define easy training case addition experiment demonstrating ordering either worse mixed random batch construction insightful possible improvement nice factor magnitude gradient contribution sample importance higher gradient function particular weight vector affected weight initialization thereby introducing noise also interesting improvement based sample importance could made batch selection algorithm beat baseline random batch selection overall good various experiment examining various sample influence various aspect training,6
591.json,summary introduce notion sample importance meant measure influence particular training example training deep neural network quantity closely related squared norm gradient summation performed parameter given layer across parameter summing quantity across time give overall importance used tease apart easy hard example quantity illustrate impact easy hard example training impact layer depth detailed review several objection first foremost convinced sample importance meaningful metric previously mentioned magnitude gradient change significantly learning sure conclusion draw sumt sumt example gradient tend higher norm early training convergence case weighting gradient equally seems problematic tried illustrating small thought experiment question period learning rate high training even converge case sample importance defined measure depends learning rate seems problematic norm input fisher norm mathbb frac partial partial given time step better suited speaks directly sensitivity classifier input insensitive change mean gradient norm summing fisher norm across time meaningful experimental analysis also seems problematic claim output layer primarily learnt early stage training however definitely case cifar debatable mnist sample importance remains high layer training despite small spike early output layer lower middle also seems highlight issue measure dominated input layer parameter thus readily impact gradient norm different architecture yielded different conclusion managed craft better curriculum given significant weight measure unfortunately negative pro extensive experiment con sample importance heuristic entirely well justified yield limited insight training neural net inform curriculum learning,2
712.json,propose recurrent variational neural network approach modelling volatility financial time series consists application chung vrnn volatility forecasting wherein variational autoencoder structure repeated time step series well written easy follow although reviewer suggests applying spelling checking since contains number harmless typo main technical contribution stack level recurrence latent process observables appears novel minor contribution larger contribution methodological area time series modelling great practical importance hitherto dominated rigid functional form demonstration applicability usefulness general purpose linear model volatility forecasting extremely impactful comment reservation although mentioned explicitly framework couched term carrying timestep ahead forecast volatility however many application volatility model instance derivative pricing require longer horizon forecast interesting discus could extended forecast longer horizon section mention garch conditionally deterministic true forecasting time step future longer horizon garch volatility forecast deterministic initially unhappy limitation experimental validation limited comparison baseline garch however provided comparison revision add quality although model compared cannot considered state well advised look package stochvol fgarch implementation variety model serve useful baseline provide convincing evidence modelled volatility indeed substantially better approach currently entertained finance literature section detail given network number hidden unit well embedding dimension section section detail given data generating process synthetic data experiment appendix puzzling around jump price series place volatility spike reacts instead huge drop volatility figure respectively around time step explained discussed think provides nice contribution volatility modelling spite flaw provides starting point broader impact neural time series processing financial community,5
529.json,suggests combining lstms trained large midi corpus handcrafted reward function help fine tune musically meaningful idea hand crafted reward great seems promising practical scenario musician like design rule rather melody even though choice made along seem rather simplistic music theoretical perspective sound like improvement upon note baseline also know cherry picked convinced approach scale much complicated reward function necessary compose real music maybe lstms wrong approach altogether much trouble learning produce pleasant melody relatively corpus data alternative differentiable model suitable dilated convolution based approach like short melody referenced composition meaningful music even polyphonic think great paper written help feedback people real musical training critical towards detail like make effort understand going table interesting instance however figure included real melody excerpt sound synthesis sample setup besides discussion shortcoming presented method added summary like idea imagine based fine tuning approach indeed useful musician even though novelty might limited serf documentation achieve solid practice,5
529.json,propose solution task synthesizing melody claim language type approach lstms generate melody certain shortcoming tend lack long range structure repeat note solve problem suggest could first trained pure style lstm trained reinforcement learning optimize objective includes differentiable music theory related constraint reinforcement learning methodology appropriate straightforward closely resembles previous work text modeling dialogue generation methodology offer technique contribution come novelty utility impact application clearly substantial effort crafting rule user study commendable hand music dealt somewhat naively user study reflects hard work seems premature semi plausible piano melody music lstm shakespeare pass poetry analogous conducting user study comparing lstm shakespeare gram shakespeare caution author motivation problem previously studied research contains abundant dead end necessarily burden motivate research forgotten especially true application primary thrust generally careful describing composing analogy shakespeare lstm language really composing english prose relationship constructing statistical sequence creating activity involves communication grounded real world semantics overstated appreciate effort respond criticism problem setup encourage anticipate argument better motivate work future main contribution application method used elsewhere motivation central importance also appreciate contention field benefit multiple datasets simply relying language modeling correct asserting midi capture information score merely gameboy music music european classical score central importance however overstate role score jazz music overall application enough impact methodology appropriate stand update thanks modification argument revised score point,4
483.json,formulate recurrent deep neural network predict human fixation location video mixture gaussians train using maximum likelihood actual fixation data apart evaluating good performs predicting fixation combine saliency prediction feature action recognition quality missing thorough evaluation fixation prediction performance center bias performance table differs significantly table state model reported table performance worse center bias performance reported table really better center bias additionally missing detail central bias human performance modelled human performance cross validated claim close human performance difference difference actually larger difference central bias reported table apart dangerous compare performance difference saturation issue clarity explanation table confusing also clear conv model differ saliency used least also evaluate conv multiplying input saliency much difference come different way saliency much different feature issue cite learns indirectly rather explicit information human look however trained fixation data using maximum likelihood apart issue think make interesting contribution spatio temporal fixation prediction evaluation issue given sorted happily improve rating,6
483.json,work proposes spatiotemporal saliency network able mimic human fixation pattern thus helping prune irrelevant information video improve action recognition work interesting shown state predicting human attention action video also shown promise helping action clip classification benefit discussion role context attention instance context important people give attention context incorporated automatically weak point action recognition section comparison seems unfair attention weighted feature map fact reduce classification performance improve performance doubling feature associated complexity concatenating weighted map original feature combine context attention without concatenation rational concatenating feature extracted original clip feature extracted saliency weighted clip seems contradict initial hypothesis eliminating weighting pixel important improve performance also mention current state table comparison comment abstract typo mixed irrelevant time consistency video expands temporal domain frame second point clear probably need write contribution trained without engineer spatiotemporal feature need collect training data human though section number fixation point controlled fixed frame done practice freeze layer network value pretrained tran etal happens allow gradient flow back layer better allow feature best tuned final task precise feature concatenated need clarified section minor typo added trained central bias,5
483.json,proposes method estimating visual attention video input clip first processed convnet particular extract visual feature visual feature passed lstm hidden state time step lstm used generate parameter gaussian mixture finally visual attention generated gaussian mixture overall idea reasonable well written lstm used lot vision problem output discrete sequence much work using lstm problem output continuous like experimental demonstrated effectiveness proposed approach particular outperforms state saliency prediction task hollywood datasets also show improvement baseline action recognition task gripe missing important baseline comparison particular seem show recurrent part help overall performance although table show rmdn outperforms state might fact us strong feature method table traditional handcrafted feature since saliency prediction essentially dense image labeling problem similar semantic segmentation dense image labeling lot method proposed past year fully convolution neural network deconvnet straightforward baseline simply take apply frame proposed method still outperforms baseline know recurrent part really help,5
600.json,proposed group sparse auto encoder feature extraction author stack group sparse auto encoders cnns extract better question sentence representation task pro group sparse auto encoder seems extensive experiment task con idea somewhat incremental writing need improved lack ablation study show effectiveness proposed approach moreover convinced author answer regarding baseline separate training stage comparison fine purpose validate analyze proposed preferred rather group lasso joint training could improve proposed group sparse regularization outperforms norm however current experiment,4
600.json,propose classify question leveraging corresponding answer proposed method us group sparse autoencoders question group proposed method offer improved accuracy baseline baseline used little stale interesting compare recent based method also interesting contribution component example much contributed improvement,5
315.json,interesting definitely provides value community discussing large batch gradient descent work well,7
315.json,empirical study justify smaller batch size converges flatter minimum flatter minimum better generalization ability pro con although little novelty think work great value shedding light interesting question around generalization deep network significance think impact theory practice respectively suggesting assumption legitimate real scenario building theory used heuristically develop algorithm generalization smart manipulation mini batch size comment earlier concern correctness claim made resolved claimed proposed sharpness criterion scale invariance took care removing claim revised version,9
552.json,discus recurrent network update rule form embedding input space orthogonal unitary matrix shared orthogonal unitary matrix interesting mean idea using matrix represent input object multiplication update state often used embedding knowledge base embedding logic literature using matrix symbolic relationship ilya sutskever geoffrey hinton holographic embeddings knowledge graph maximillian nickel think experiment analysis work much understanding particular experiment especially weak consisting simplified version copy task already much know several people played setting language modeling reviewer note inability forget actual annoyance think incumbent show really useful nontrivial task accept question reason shared instead absorbing find nice way using fact linear linear,3
552.json,nice proposal could lead efficient training recurrent net really love experimental evidence asked question already answer question concern resulting still universal approximator providing large enough hidden dimension number layer generally compare expressiveness equivalent without orthogonal matrix number parameter instance experiment disappointing number distinct input output sequence fact small noted authr training becomes unstable understand success meant case point experiment section need expanded tell still unfortunately,4
694.json,present well thought constructed system performing lipreading primary novelty nature system lipreading sentence level prediction also differentiating prior work described neural network architecture contains convolutional recurrent layer sequence loss beam search decoding done obtain best performance evaluated grid dataset saliency confusion matrix analysis provided well overall work seems high quality clearly written detailed explanation final analysis appear good well gripe novelty lie choice application domain opposed method lack word level comparison also make difficult determine importance using sentence level information choice architecture decoding finally grid dataset appears limited grammar gram dictionary clearly system well engineered final impress though unclear much broader insight yield,5
694.json,proven training deep network give large gain traditional hybrid system hand crafted feature nice small vocabulary grammar task defined grid corpus engineering clearly good interesting performance large vocabulary task comparison human reading performance conversational speech interesting traditional system apply weighted audio visual posterior fusion reduce pure reading weight visual many curve showing performance channel audio condition grammar task traditional hybrid approach also sentence level sequence trained fmpe objective reference cannot first sentence level objective lipreading analogous saying sequence training hybrid lvcsr system,3
381.json,propose strategy pruning weight eventual goal reducing gflop computation pruning strategy well motivated using taylor expansion neural network function respect feature activation obtained strategy remove feature map small activation small gradient ideally gradient output respect activation function optimal result stochastic gradient evaluation practically never zero small variance gradient across mini batch indicates irrespective input data specific network parameter unlikely change intuitively parameter closer convergence parameter weight close convergence also result small activation intuitively good candidate pruning essentially conveys likely reason removing weight result small activation good pruning strategy shown kind difference weight removed activation taylor expansion weight high activation gradient removed taylor expansion activation alone weight activation high gradient removed activation criterion taylor expansion interesting analyze contribute difference weight removed taylor expansion activation criterion intuitively seems weight satisfy important converged contribute significantly network activation possible modified criterion lambda feature activation lambda need found cross validation lead even better cost parameter tuning another interesting comparison optimal damage framework first order gradient assumed zero pruning performed using second order information also discussed appendix critically diagonal hessian computed comparison optimal damage claim memory computation inefficient back envelope calculation suggest result increase memory computation pruning loss efficiency testing therefore standpoint deployment think missing comparison justified eventual goal reduce gflops recent paper proposed using lower precision computation comparison gflops lower precision pruning great approach complementary expected combining lead superior performance either unclear operating precision regime much pruning performed analysis tradeoff great necessary finetuning report alexnet different datasets flower bird respectively case great network datasets report small drop performance pruning suppose network originally trained iteration finetuning iteration performed pruning mean pruned network trained iteration correct comparison accuracy original network also trained iteration figure performance parameter report accuracy iteration iteration overall think technically empirically sound proposes strategy pruning based taylor expansion feature normalization reduce parameter tuning effort iterative finetuning however like comparison mentioned comment comparison made change rating accept,6
440.json,introduces approach based control stochastic dynamical system policy search based learning stochastic dynamic underlying system bayesian deep neural network allows input stochastic policy optimization method based simulated rollouts learned dynamic training carried using alpha divergence minimization specific form introduced previous work validation comparison approach undertaken simulated domain well real world scenario tightly written easy follow approach fitting bayesian neural network alpha divergence interesting appears novel context resulting application based control appears significant practical impact particularly light explainability system bring specific decision made policy think brings valuable contribution literature said question suggestion section explained random input used neural network concatenated input used special treatment moreover much case made need stochastic input scalar input seems provided throughout enough computationally difficult providing stochastic input higher dimensionality important normality assumption variance gamma established mentioned hidden layer neural network made rectifier utilization fact made assumption somehow important optimization alpha divergence beyond know rectifier mitigate vanishing gradient problem equation denominator mathbf mathbf section helpful overview discussion computational complexity training bnns understand whether practicably used citation statement time embedding theorem helpful well indication embedding dimension chosen figure subplots letter referenced text section clear turbine data publicly available addition detail provided dimensionality variable perhaps comparison gaussian process include variant support stochastic input girard provide modelling capability made least strand work mentioned section reference girard rasmussen quionero candela murray smith gaussian process prior uncertain input application multiple step ahead time series forecasting advance neural information processing system,6
440.json,considers problem based policy search consider bayesian neural network learn environment advocate alpha divergence minimization rather usual variational bayes ability alpha divergence capture modality however come price devoted finding tractable approximation therefore approach hernandez lobato proxy alpha divergence environment system dynamic clearly defined well policy parametrization section constitute useful reference point researcher simulated roll out using learned provide sample expected return since environment available stochastic gradient descent performed usual without policy gradient estimator automatic differentiation tool experiment demonstrate alpha divergence capable capturing multi structure competing method variational bayes otherwise struggle proposed approach also compare favorably real world batch setting well written technically rich combine many recent tool coherent algorithm however repeated approximation original quantity seems somehow defeat benefit original problem formulation scalability computational effectiveness approach also questionable uncertain many problem warrant complexity solution bayesian method proposed approach probably shine sample regime case might preferable method class,6
339.json,present simple method affix cache neural language model provides effect copying mechanism recently used word unlike much related work neural network copying mechanism mechanism need trained long term backpropagation make efficient scalable much larger cache size demonstrate good improvement language modeling adding cache baseline main contribution observation simply using hidden state key word query vector naturally give lookup mechanism work fine without tuning backprop simple observation might already exist knowledge among people nice implication scalability experiment convincing basic idea repurposing locally learned representation large scale attention backprop normally prohibitively expensive interesting could probably used improve type memory network main criticism work simplicity incrementality compared previously existing literature simple modification existing model good empirical success simplicity practicality probably suitable specific conference however think approach distill recent work simple efficient applicable form rewarded tool useful large enough portion iclr community recommend publication,6
339.json,proposes simple extension neural network language adding cache component store,4
513.json,summary evaluates ability unsupervised learning model learn generalizable physical intuition governing stability tower block model predicts final state tower given initial state predicts sequence state tower time given initial state generalizability evaluated training tower made certain number block testing tower made different number block strength explores interesting evaluate representation term generalizability domain data opposed standard method train test data drawn distribution experiment show prediction deep unsupervised learning model domain data seem help even though model trained explicitly help weakness based seems model trained block generalize block however plausible model attention bottom block block tower order determine stability work correctly significant fraction time therefore model might actually overfitting block tower really generalizing physic block possibility think careful control needed make claim feature actually generalize example test block block test make block unstable still work well could argue actually generalizing experimental analysis seems somewhat preliminary improved particular help visualization final state look like model trained block test vice versa help understand generalization really working discriminative objective give indication might obfuscate aspect physical realism really want test figure mentioned whether model tested number block trained seems task predicting final state really binary task whether remove block replace gray background place block land case fall probably quite hard predict even human small perturbation impact final state seems order generalizable physic could help high frame rate sequence prediction task currently video subsampled time step quality detailed analysis careful choice testing condition increase quality strengthen conclusion drawn work clarity well written easy follow originality particular setting explored novel significance provides valuable addition growing work transferability generalizability evaluation method unsupervised learning however detailed experiment analysis needed make significant enough iclr minor comment suggestion acronym used without mentioning expansion anywhere text seems strong dependence data augmentation given synthetic dataset clear data generated first place table might better draw grid row corresponding model column corresponding test set mentioning train redundant since already captured name might make easier read overall excellent direction work preliminary look great however control detailed analysis needed make strong conclusion experiment,4
513.json,summary train model predict whether block tower fall show additional block fall predicting sequence frame unsupervised learning help original supervised task generalize better work construct synthetic dataset block tower containing block place le precarious position includes label tower fall video frame sequence tower evolution according physic engine three kind model trained first simply take image tower starting state predicts whether fall type take start state final state tower fallen predict whether fallen differ final state provided convdeconv predicts final frame using start frame convlstmdeconv predicts series intermediate frame coming final frame unsupervised trained tower particular heigh tested tower unseen height height train tower test tower height model perform roughly percentage point however test height greater train height extremely helpful explicitly final state block tower deciding whether fallen model pro clear large gain accuracy adding unsupervised final frame predictor generalization problem also particularly clear train test different number block make nice example unsupervised learning provides clear benefit writing clear con major concern lack detailed analysis establishes base result explore idea extent think iclr general direction potential analysis follow limitation particular block tower rendered lstm could limited sampling strategy look like sampling coarse provided example tower figure fall fallen time step quickly tower fall happens lstm trained higher frame rate frame frame video prediction accuracy lstm quantity meaningful much performance improve lstm provided ground truth first frame generalization different block height limited limited capacity architecture design happen type model made wider deeper fall predictor capacity fixed limited precise task specification happen network trained tower multiple height apparently experiment work appreciate experiment direction provided limited training procedure model trained manner double frame fall predictor trained ground truth final frame instead generated final frame minor concern asking much implement zhang physnet newly proposed dataset help baseline directly comparable proposed think major concern point role unsupervised learning rather creating best fall prediction network auxiliary experiment provided motivated follows solution could train model predict many block fallen instead binary stability label clear intuition might make task easier dataset code generate released overall evaluation writing presentation experiment clear high enough quality iclr however experiment provide limited analysis past main result comment idea clear extension idea behind unsupervised learning video prediction recent intuitive physic lerer zhang moderate novelty however provide valuable addition literation especially analysis provided,4
513.json,summary proposes learn predictive predict next video frame given input image us prediction improve supervised classifier effectiveness approach illustrated tower stability dataset review summary work seems rather preliminary term experimentation using forward modeling pretraining already proposed applied video text classification task discussion related work insufficient task choice motion might best advocate unsupervised training detailed review work seems rather preliminary comparison alternative semi supervised strategy approach consider next frame latent variable privileged information considered also sure supervised stability prediction actually needed next frame predicted basically task reduced predict whether motion video following current frame instance comparing first frame last prediction density gray part video might work well also training predict presence motion unsupervised data probably well suggest stir away task label inferred trivially unsupervised data meaning unlabeled video considered labeled frame case related work section miss discussion previous work learning unsupervised feature video predictive model dimensionality reduction helping classification still image video fathi mabahi srivastava recently wang gupta obtained excellent imagenet feature trained unlabeled video vondrick shown generative model video help initialize model video classification task also field text classification training classifier language form predictive modeling also suggest report test dataset lerrer understand need video train predictive stability prediction require still image overall feel experimental section preliminary better focus task solving unsupervised task necessarily imply supervised task trivially solved conversely simple rule turn unlabeled data label data reference fathi alireza greg mori action recognition learning level motion feature computer vision pattern recognition cvpr ieee conference ieee mobahi hossein ronan collobert jason weston deep learning temporal coherence video proceeding annual international conference machine learning srivastava nitish elman mansimov ruslan salakhutdinov unsupervised learning video representation using lstms corr semi supervised sequence learning nip unsupervised learning visual representation using video wang gupta iccv generating video scene dynamic vondrick pirsiavash torralba nip,2
456.json,work introduces regularization learning domain invariant representation neural network regularization aim matching higher order central moment hidden activation source target domain compared proposed method state domain adaptation algorithm amazon review office datasets showed comparable performance idea proposed simple straightforward empirical suggest quite effective biggest limitation proposed method assumption hidden activation independently distributed example assumption clearly violated hidden activation convolutional layer neighboring activation dependent guess start output dense layer image dataset insight beneficial start adaptation lower level insight relax assumption scenario advantage make assumption figure seems clearly support boost performance shown table class regularization brings source target domain closer seem mouse class pointed performance improvement coming single class,5
456.json,proposed metric central moment discrepancy matching distribution application domain adaptation compared well known variant benefit penalizing mean therefore focus shape distribution around center term discriminative power ability tell distribution apart equivalent practice understand better try match moment penalize data zero centered used order central moment used rather diagonal entry considered objective think mostly motivated computation efficiency natural comparison therefore made also explicitly include moment order another thing compare include moment diagonal term objective computationally expensive done order since experiment compare form kernelized claim explicit moment matching helpful well supported make solid claim compared explicit moment claim kernel parameter hard tune parameter applies kernel explicit kernel also study parameter example sriperumbudur kernel choice classifiability rkhs embeddings probability distribution gretton kernel sample test also using multiple kernel remove need tune tuning beta directly like done usually tuned least simple heuristic like dividing dimensionality mean pairwise distance first applied first trying beta done overall think could better could application many domain also problem easily kernelizable argue way though experiment demonstrating better could done convincinly,6
790.json,submission proposes modify typical architecture slightly include encrypt alice decrypt module well module trying decrypt signal without repeated transmission signal adversarial game intended converge system alice communicate securely least designated part signal secure sophisticated cannot break code example given data proof concept implemented alice network take random plain text value produce entry floating point ciphertexts plaintext value uniformly distributed idea considered cute necessarily signal meant secure module learn encrypt decrypt signal adversary simultaneously learned try break encryption data remain unencrypted portion correlated encrypted signal encrypted order able predict encrypted part nice thought experiment significant barrier submission practical impact gans convergence figure also objective considered quite unstable optimize guarantee privacy converged strong adversary stronger dedicated attack time sort reliable guarantee safety data transmission proposed approach least outline guarantee public encryption system readily available computationally feasible successfully applied almost anywhere example given convince solving real world problem point perhaps good example come near future work shown justified example shown approach interesting thought experiment,4
790.json,deal interesting application adversarial training encryption considers standard scenario alice exchange message conditioned shared unable encrypt message experiment performed simple symmetric encryption task application privacy concept idea previous literature quite nicely carefully presented major concern apologize raising earlier experiment section particular quite scenario reasoning seems follows given information want give public value movie watched without releasing information gender scenario need able reconstruct good possible without gaining information described section however public reconstructed reconstruct latter public particular allowed reconstruct tested also try estimate thus rendering scenario different scenario considered section minor concern raised review actually stronger alice order able compensate missing noted experiment going natural encryption case expect length much shorter length message however could potentially make scenario much easier although doubt change long enough like creative application adversarial training completely different domain believe could starting point interesting direction cryptographic system privacy application although unclear whether weak guarantee neural network based approach ever overcome time application privacy setting leaf quite confused symmetric encryption example particularly strong either appreciate could address major concern raised quite happy raise score case confusion resolved,5
544.json,definitely love research great tool however lack detail particular feel impossible someone reimplement research mostly lack detail however replicability crucial part science publication proposing software tensorflow theano edward paper come along open source code case therefore picture quite incomplete convinced iclr right venue robotics conference iros icra might appreciate much nevertheless encouragement interact community,4
544.json,differentiable physic engine indeed wonderful thing selling point proposed software speed however comparison physic engine besides describing engine speed rather creative unit second reader idea fast slow todorov engine simulator choice computes dynamic step derivative state control using finite difference le full humanoid code available mujoco book programming html saderivative think actually faster engine described sure engine limited collide sphere sphere sphere plane trivial build example model several popular engine bullet mujoco simply compare performance comparison done consider incomplete,4
401.json,read well idea sadly many detail needed replicating layer size cnns learning rate missing training introspection network could described detail also think closer current state used imagenet experiment made convincing novelty idea recommend increase rating updated draft address mentioned issue,7
401.json,edit updated score additional comment quite like main idea based observation find many predictable pattern independent evolution weight neural network training encouraging simple neural network used speed training directly predicting weight however technical quality current leaf much desired encourage rigorous analysis approach concrete suggestion finding section motivate approach clearly presented presently stated anecdote central issue training introspection network completely glossed well training work term training validation test loss well need work order useful speeding training important question anyone interested approach additional important issue baseline simple linear quadratic also work instead neural network simple heuristic rule increase decrease weight think important compare baseline understand complexity weight evolution learned neural network think default tensorflow example hyperparameters used mentioned openreview scientific basis using instead first hyperparameters produce good reasonable time selected baseline added benefit introspection network speed training reaching similar result shown state discussion openreview also tried rnns introspection network work small state size work mean context underfit find hard imagine large state size required task even rule evaluation memory issue weight mini batch mode general think baseline important question jump point trained trajectory using speed training several jump point input weight cross previous jump point get input data weight evolution altered seems problematic seem affect experiment feel highlight importance baseline perhaps something extremely simple affected issue since main idea interesting happy update score concern addressed,8
397.json,prior equivalent inverse posterior elegant way improve unfortunately poor generative quality incremental important step incremental judging lack cifar picture vlae creative regime sampling prior answer many question hanging hope accepted relative term shine landscape paper rich engineering hack lacking theoretical insight disagreement theoretical supposition posterior converges prior faster like gradient generative error divergence prior posterior sigma simple differentiable function magnitude exceeds magnitude resp gradient reconstruction error especially hairy decoder like pixelcnn used always considered obvious certainly worthy page mumbo jumbo explain dumbing decoder variation dropout complexifying sampler slapping generative error deepmind constant betavae natural band aid seem fail creative regime real life set like cifar complex one conceptual solution needed discussed claim near section extra coding cost variational error exist negligible speculation empirical experience least incorrect variational error quantifiable gibbs exponential family prior posterior described section salimans know previous work case cifar example variational error negligible even simple sampling family like gaussian laplacian moreover hindsight using closed form generative error divergence prior posterior pioneering paper likely mistake inherited unnecessary bayseanism inspired beautiful mistake nonetheless combo generative variational error together approximated naive monte carlo used reconstruction error easily follows equation arithmetic average observation lighter side guy please recycle ridiculous term like optimizationally challenged section english language recently acquired mentally challenged emotionally challenged political correctness sadly found machine,8
397.json,introduces notion variational lossy autoencoder powerful autoregressive conditional distribution input given latent code crippled force meaningful three main contribution give interesting information theoretical insight type model tend take advantage latent representation conditional distribution given powerful enough show insight used efficiently train vaes powerful autoregressive conditional distribution make latent code present powerful parametrize prior form autoregressive flow transformation equivalent using inverse autoregressive flow transformation approximate posterior think information theoretical explanation vaes latent code conditional distribution given powerful enough constitutes excellent addition understanding related approach however intuition empirically evaluated weak crippling method used feel hand crafted task dependent qualitative evaluation lossyness learned representation carried three datasets mnist omniglot caltech silhouette feature black white image little texture figure show reconstruction discard level information observed slight variation stroke input reconstruction analysis compelling complex image datasets tried applying vlae datasets think caltech silhouette benchmark treated caution comparison made competitive approach like pixelrnn conv draw mean vlae significantly outperforms state four setting examined question relevant latent representation autoregressive help improve density modeling performance touch question briefly setting vlae compared recent autoregressive approach show win pixelrnn small margin proposal transform latent code autoregressive flow equivalent parametrizing approximate posterior inverse autoregressive flow transformation also interesting however important distinction made approach former prior latent code potentially complex whereas latter prior limited simple factorized distribution clear powerful prior necessarily good thing representation learning point view oftentimes interested learning representation data distribution untangled composed roughly independent factor variation degree achieved using something simple spherical gaussian prior discussion finding good balance ability prior data usefulness high level representation certainly warrant thought interested hearing opinion overall introduces interesting idea despite flaw outlined weakness empirical evaluation prevent recommending acceptance update rating revised following reply,6
723.json,explores open bigram target representation word application handwriting image recognition pro novel interesting clearly written explained con comparison previous state author generated ablation study needed fill table clear performance coming seems single character modelling word ending actually beneficial open bigram novel work bigram ngrams model really compared explored,4
723.json,us lstm predict call open bigram bigram character letter inbetween handwriting data open bigram subsequently used predict written word decoding step experiment indicate system slightly better baseline us viterbi decoding major concern find cortical inspired claim troublesome anything psychology cognitive science inspired sense open bigram appear help word recognition touzet implied cortical characteristic implicitly referred pointing analogy deep neural net object recognition case visual cortex unfounded direct evidence neuroscience open bigram constitute wholly separate layer cortex handwriting recognition task dehaene work proposal need describe finding cognitive neuroscience research reading substantiate claim worried fact seem think deep neural network based series five pair neuron layer unless misunderstand something specifically referring krizhevsky alexnet probably cited hope mean imply deep neural net need five layer also true quite close number layer efficient deep network task clearly explained short paragraph appendix roughly describes setup include objective function answer network output considered consecutive time step rather time step seems probably argues focused decoder rather whole problem find problematic case effectively measuring easy reconstruct word open bigram little handwriting recognition could evaluated text corpus fact example page show handwriting necessary illustrate open bigram hypothesis lead wonder particular task chosen interested decoding mechanism comparison really fair viterbi decoder access unigrams tell better baseline access information much better viterbi access word boundary information point rather confusingly called extremity pushed open bigram edge term performance comparison unigram bigram boundary marker dataset also appears biased favor proposed approach longer word convinced really show open bigram help much like idea simply convinced claim minor point quite typo sample independant evaluate handwritten hand written word approach include letter bigram word considered database easy many time bigram occurs improve decoding process normalize full count instead binary occurrence count table different precision table except edit distance added confusing,3
723.json,submission investigates usability cortical inspired distant bigram representation handwritten word recognition instead generating neural network based posterior feature character optionally local context set posterior character bigram different length used represent word investigate viability approach compare standard approach overall submission well written although information missing comparison proposed approach standard approach desirable complexity different model used number parameter used language model used since different model utilize different level context language model expected different effect different approach therefore suggest include language model evaluation comparative experiment data choosing longer word hand well known shorter word prone result misrecognitions question remains choice advantageous task corresponding quantitative provided able better evaluate effect using constrained corpus without clarification readily agree error rate competitive better standard approach stated motivation introducing open bigram unordered corresponding evidence cognitive research however decision theoretically wonder order given underlying sequential classification problem clearly monotonous nature interesting experiment order varied differentiate effect order effect aspect approach page whole language method please explain meant page define notation rnnd number target rnns modeling order unigrams effectively rnns modeling order larger much different therefore precision recall number table seem readily comparable order order least column order visually separated highlight minor comment spell check recommended state state predict character sequence predict character sequence approach include approach includes handwritten handwritten bottom consituent constituent classical approach classical approach transformed vector transformed vector build built reference first name written wrongly thodore bluche theodore bluche,6
666.json,proposes shot learning framework used existing trained network grouping filter produce similar activation grouped filter learned together address overfitting training sample available idea interesting encouraging current version seem ready publication performance method compared state shot learning method matching network vinyals clear method compare missing explanation experimental setting shot learning detailed measure accuracy difference look like good idea comparing baseline method proposed accuracy fine many grammatical error inappropriate formatting citation imagenet alex judy reference appears three time reference section,3
666.json,proposes regularization technique shot learning based orthogonal grouping unit neural network unit within group forced maximally similar time unit different group encouraged orthogonal like motivation approach empirical analysis provided look particularly convincing main concern following method sensitive value alpha beta poor choice hyperparameters lead quite drastic drop performance comparing minor gain get alpha beta properly seems strange best performance obtained group size ratio figure follows usually orthogonal group filter bank impression empirical evidence align well motivation proposed approach contains significant amount typo incorrectly formatted reference also several place manuscript found hard understand unusual phrasing like thank answering addressing review question grateful could provide clarification following question sure modifying theta alone result learning understand correctly theta used define group proposed method used purely unsupervised regime question referring fixed clustering based filter trained network perform clustering every step shot learning process sure understand visualize grouping filter actual algorithm group activation overall quite interesting need stronger empirical justification approach well better presentation material,3
774.json,proposes variety technique visualizing learned generative model focussing specifically model somewhat challenging ass since propose algorithm application hand technique highly relevant generative modeling community think deserves wide audience technique proposed simple well explained immediate working generative model however sure appropriate iclr conference track provide greater theoretical insight sampling generative model comparison quantitative evaluation technique proposed overall much fence since think technique useful read interested generating modeling willing increase core author could present case iclr appropriate venue work,4
774.json,proposed different thing name sampling generative model focusing analyzing learned latent space synthesizing desirable output image certain property gans single clear message idea rather proposed technique seem produce visually good looking interesting idea also number problem spherical interpolation idea interesting second thought make much sense proposed slerp interpolation equation page implicitly assumes point sphere case parameter theta angle corresponding great connecting point sphere however latent space matter trained uniform distribution gaussian distribution distribution sphere many point different distance origin author justification come well known fact high dimensional space even uniform distribution point thin shell unit cube true high dimensional space outer shell take volume space inner part take small fraction space term volume mean density data outer shell greater inner part though uniform distribution data density equal everywhere point outer shell likely point inner part gaussian data density hand higher center much lower side good data sampling likely point give plausible looking sample sense spherical interpolation better normally used linear interpolation question answer seems author recognize distinction shown seem indicate spherical interpolation better visually rather hard make concrete conclusion three pair example really case must something else wrong understanding learned aside diagram nearest neighbor latent space traversal seems good way explore latent space learned attribute vector section transforming image one desired attribute also interesting provides way make latent space interpretable overall feel technique proposed nice visualization tool contribution however mostly design visualization much technical side spherical interpolation provides mathematical equation correctness technique arguable visualization tool also quantitative evaluation maybe science,4
774.json,propose various technique sample visualization generative model high dimensional latent space like vaes gans example highlight well known often sufficiently appreciated fact probability mass high dimensional gaussian distribution concentrate near thin hyper shell certain radius therefore propose spherical interpolation great arc instead commonly used linear interpolation similar spirit propose visualisation analogy technique reinforce structure latent space find hard give clear recommendation hand enjoyed reading might want proposal spherical interpolation diagram future work mine hand obvious typical machine learning propose training method provide theoretical empirical insight scientific quality depth seen many iclr submission describing useful trick thing considered think deserves wider audience convinced iclr right venue,5
324.json,prune entire group filter reduce computational cost time result sparse connectivity result important speed compress neural network able standard fully connected linear algebra routine improvement resnet like imagenet also achieved better design network network also compared know time consuming good useful,6
324.json,proposes simple method pruning filter type architecture decrease time execution pro impressively retains accuracy popular model imagenet cifar con justification norm good selection criterion easy critical missing baseline randomly pruning filter pruning filter activation pattern norm training direct comparison multitude pruning speedup method flop reported clear empirical speedup method give people interested method care wall clock speedup trivial report lack wall clock speedup suspect,5
631.json,please provide evaluation quality clarity originality significance work including list pro con summary work proposes rnns inside convolutional network architecture complementary mechanism propagate spatial information across image promising classification semantic labeling reported review summary text clear idea well describe experiment seem well constructed overclaim overall earth shattering good piece incremental science pro clear description well built experiment simple effective idea overclaiming detailed comparison related work architecture con idea somewhat incremental seen derivative bell good improve state quality idea sound experiment well built analysed clarity easy read mostly clear relevant detail left comment originality minor different combination idea well known significance seems like good step forward quest learn good practice build neural network task semantic labelling classification specific comment section introduction nonlinearities convolutional layer convolutional layer linear operator section exactly cannot pooling operator impede section computational block block seems like typo please rephrase figure present please figure reference maybe short description appendix completeness section last sentence sure meant convolution relu pooling resnet provide linearity layer please clarify section appendix learning rate increased decreased manually important detail made explicit learning rate schedule experiment table human loop variance human scheduler section last sentence certainly strong baseline pascal competition report miou best known certainly strong please tune statement section module module ignore mention increased memory usage computation cost small detail please discussion topic section adding multi scale spatial adding spatial nothing inherently multi section furthermoe furthermore appendix redundant figure,6
631.json,proposes method integrating recurrent layer within larger potentially trained convolutional network objective combine feature extraction ability cnns ability rnns gather global context information validate idea task image classification cifar semantic segmentation pascal positive side clear well written apart occasional typo proposed idea simple could adopted work deployed beneficial perturbation existing system practically important want increase performance system without retraining scratch evaluation also systematic providing clear ablation study negative side novelty work relatively limited validation lacking regarding novelty idea combining recurrent layer something practically similar proposed bell technical difference cascading versus applying parallel recurrent layer understanding minor change idea initializing recurrent network reasonable level improving wrong choice original work bell rather really proposing something novel contribution rnns within layer repeatedly mentioned including intro conclusion understanding part bell modulo minor change regarding evaluation experiment cifar interesting proof concept furthermore noted early question wide residual network sergey zagoruyko nikos komodakis bmvc report better cifar error using recurrent layer rather using instead wide type resnet variant answer wide residual network depth network spread receptive field across entire image densenet huang similarly us depth thus need recurrence within layer capture contextual information contrast show shallow receptive field limited capture contextual information within whole image used agree need recurrence still better point question practically whether using recurrent layer really necessary understand answer want keep network shallow necessarily want keep network shallow probably evaluation imagenet bring insight merit layer regarding semantic segmentation question boost obtaining something special recurrent layer simply adding extra parameter trained network admit missed detail experimental evaluation answer pascal segmentation trained network add recurrence parameter show boost performance adding number parameter extra layer able long range dependence could find experiment adding number parameter extra layer understand connection recurrence interesting spreading recurrent residual layer clearly going criterion rejection acceptance since easily make fail mostly asking sanity check furthermore misleading table fcns lrnn since give impression lrnn give boost practice fcns prefix fcns lrnn long indicated table original quite worse another thing clear boost come table mention inserting pool pool able learn contextual information much larger range receptive field pure local convolution potentially true also case property recurrence rather factor right additional point seems like figc never made figure unstructured throw box reader surprised anyone able information table appendix mysterious learning rate schedule polynomial performance apply standard training schedule step appendix map map,5
631.json,proposes cascade paired left right rnns module cnns order quickly global context information feature without need stacking many convolutional layer experimental presented image classification semantic segmentation task pro clear easy read enough detail given likely reproduced without source code using rnns inside cnns topic deserves experimental exploration exists literature con elaborated contribution relative bell minor disappointed actual proposed module versus sold intro classification experiment convincing introduction state bell substantial difference fold first treat module general block inserted layer modern architecture residual module second show section formulated inserted trained initializing zero recurrence matrix entire network fine tuned felt positive contribution reading intro much le reading experimental section based first contribution general block inserted layer strongly expected block integrated throughout starting near input however architecture classification segmentation place module towards network exactly bell many technical detail differ close compare design bell advantage proposed design variation performs similarly happens integrated earlier network suggested introduction second difference solid still rise ubstantive difference view note bell also integrate rnns imagenet pretrained however think method integration proposed zero initialization elegant require stage training first freezing lower layer later unfreezing generally skeptical utility classification experiment cifar presented isolation imagenet issue cifar interesting task unto method work well cifar necessarily generalize task imagenet useful thus produce feature generalize well task showing good imagenet much likely demonstrate learns generalizable feature however even necessarily true ideally like well imagenet fact transfer benefit least detection additional issue cifar experiment expect direct comparison model without hard understand presented actually add much hard time taking away valuable information cifar experiment minor suggestion figure hard read pixelated rounded corner yellow box distracting,4
631.json,propose vertical horizontal dimensional denoted module capture long range dependency summarize convolutional feature map module alternative deeper wider network rnns dilated atrous convolutional layer simple flatten global pooling layer applied last convolutional layer classification module faster rnns since row column processed parallel easy implemented inserted existing convolutional network demonstrate improvement classification semantic segmentation however evaluation required show case rnns superior alternative summarizing convolutional feature map suggest fixed certain number layer summarize last feature flatten layer global average pooling dilated convolutional layer segmentation report time number parameter variant addition prediction performance segmentation number dilated convolutional layer chosen number parameter similar single module compare classification performance cifar image higher resolution image benefit module capture long range dependency might pronounced therefore suggest evaluating classification performance additional dataset higher resolution image imagenet bird dataset additionally following minor comment vanilla rnns might worth investigating lstms grus instead classification summarize hidden state final vertical recurrent layer global pooling different common global average pooling concatenating final forward backward recurrent state table hard understand since mingles datasets pascal coco method post processing suggest using additional column suggest listing number parameter runtime possible section clearly describe order batch normalization applied residual block figure suggest newer relu conv order described used mentioned text finally text need revised reach publication level quality specifically following comment equation update vanilla stated clearly suggest first describe bidirectional rnns reference grus lstms describe applied image figure also referenced text section suggest describe bell clearly using eight instead four rnns section start verbose description transfer learning compressed single reference skipped entirely equation seems missing index particular section contain clutter slang avoided page seen turn case next page high value noted earlier le context contribute page fact deeper simple matter much left investigate,6
332.json,proposes learn across different view object insight triplet loss encourages different view object closer image different object approach evaluated object instance category retrieval compared baseline cnns untrained alexnet alexnet fine tuned category classification using feature cosine distance furthermore comparison human perception tenenbaum object shown positive leveraging triplet loss problem novelty although somewhat limited given concurrent work reasonably written negative missing relevant reference related work space compare existing approach detail image purification related work joint embeddings shape image image purification yangyan charles fish daniel cohen leonidas guibas siggraph asia learn feature hand designed light field descriptor shape view invariant object retrieval possible good compare directly approach cross view retrieval experiment table appears code data available online,4
332.json,think learning deep feature representation supervised group dissimilar view object interesting technically especially novel bother good exploring form supervision dataset also bothered dataset synthetic good experiment real data well think go linking human vision prefer intro much cognitive science neuroscience second fourth paragraph intro particular feel like stating contribution somehow revealing truth human vision really narrative much simpler often want deep feature representation viewpoint invariant supervise deep network accordingly human also capability viewpoint invariant widely studied citation skeptical claimed connection bigger think based tree tree distance comparison instead based entire matrix instance instance similarity assessment lossy conversion tree first think remarkably justified statement remarkably found opnets similarity judgement match data human similarity judgement significantly better alexnet expert human vision browsing online learned seems object persistence frequently relates concept occlusion occlusion never mentioned feel like human vision term might misleading overclaiming,5
762.json,aim characterize perceptual ability neural network different input condition done manipulating input image various way downsamplig foveating training auto encoder reconstruct original full resolution image qualitative shown compared different input condition unfortunately seems lack focus presenting preliminary inspection concrete conclusion example result surprising given contains additional information suggests small number foveations containing rich detail might neural network need hypothesis left dangling detailed region needed sort task secondly clear reconstruction behavior caused fundamental perception input artifact autoencoder pixelwise loss prime example texture autoencoder fails recover pixelwise loss network must predict high frequency texture nearly pixel pixel training time impossible generate pixelwise average training sample flat region network inability reconstruct texture problem generating specifically averaging training loss necessarily issue perceiving texture network trained different perhaps adversarial network infer texture even able generate pixelwise sense similarly ability perform color reconstruction given color glimpse think much disambiguating color object scene ambiguity network know choose white flower yellow flower output average many sepia tone however section measure reconstruction error different amount color given drill hypothesis behavior occurs interesting measurement amount color needed foveation reconstruct color image discussion global feature start mechanism glimpse propagate entire reconstruction overall hard know take away larger concrete conclusion garnered detail mechanism bring thoroughly explored focus,3
762.json,like idea exploring nevertheless issue analysis better understanding quality think least state comparison included setting time pixel patch average applying denoising autoencoder perform significantly better indicates presented taking information input image could used supposed test much information restored fovea alone opposed fovea together resolution periphery however additional difference condition according part image zero removed alltogether hidden layer could easily imagine making difference page compare performance error attribute information periphery autoencoder extract fovea might case least part reduced error fact fovea hopefully perfectly reconstructed answer actual question much additional information periphery extracted fovea consider calculating error periphery part image exactly input decreased error additional fovea information issue image figure row factor factor factor look blurry seems interpolation going although slighly different bilinear interpolation make hard ass much information image think much insightfull print nearest interpolation figure caption vague maybe something like footnote often figure appear early lead lot distance text figure,5
762.json,motivated ability human visual system recognize content environment critical feature tried investigate whether neural network also kind ability specifically proposed auto encoder network reconstruct fidelity visual input moreover similar mnih also proposed recurrent fashion mimic sequential behavior human visual system think well motivated however several concern baseline weak nearest neighbor bilinear bicubic cubic interpolation without learning procedure course performed worse based model author compare stoa method,4
627.json,proposes multimodal neural machine translation based upon previous work using variational method attempt ground semantics image considering improve translation visual information seems like sensible thing data available pointed previous reviewer actually correct selection done make gain reported marginal addition author also said question response clear really learning capture useful image semantics unfortunately hard conclude contributes direction originally motivated,2
627.json,proposes approach task multimodal machine translation namely case image available corresponds source target sentence idea seems latent variable condition image practice equation figure image used training inference said approach appears flawed image really used translation experimental weak selection done properly using validation considered bring meteor bleu advantage baseline view overall variance improvement considered significant qualitative analysis subsection appears inconclusive unconvincing overall major issue approach execution,3
559.json,proposes improved version matching network better scalability property respect support shot classifier instead considering support point individually learn embedding function aggregate item class within support combined episodic shot training randomly sampled partition training class training testing scenario match closely although idea quite straightforward great many prior work zero shot shot learning proposed technique novel knowledge achieves state several benchmark datasets addition think improve clearer description training algorithm perhaps pseudocode current form vague,5
735.json,proposes nonlinear regularizer solving posed inverse problem latent variable causal factor corresponding observed data assumed near dimensional subspace rkhs induced predetermined kernel proposed regularizer seen extension linear rank assumption latent factor nuclear norm penalty cholesky factor kernel matrix used relaxation dimensionality subspace empirical reported task involving linear inverse problem missing feature imputation estimating rigid structure sequence orthographic projection proposed method shown outperform linear rank regularizer clarity scope improvement particularly introduction back forth dimensionality reduction technique inverse problem confusing time clearly defining posed inverse problem first motivating need regularizer brings dimensionality reduction technique picture clear flow opinion motivation behind relaxation rank nuclear norm clear setting relaxation yield convex problem also increase computation algo need full every time discus pro con alternate approach fix rank selected using cross validation selected leaving first term simpler objective interesting question kernel function solved scalable manner proposed alternating optimization approach current form computationally intensive seems hard scale even moderate sized data every iteration need compute kernel matrix perform full kernel matrix algo empirical evaluation also extensive dataset used feature imputation standard structure estimation motion dataset compare linear rank regularization comment study convergence alternating procedure algo,3
735.json,present approach linear kernel dimensionality reduction trace norm regularizer feature space proposed iterative minimization approach order obtain local optimum relaxed problem contains error experimental evaluation convincing technique compared datasets claim state however dataset real benchmark comparison approach experimental evaluation demonstrate robustness complex noise outlier motivation introduction address sample problem problem kernel based method lvms thus address contains error last paragraph section say proposes closed form solution robust kpca simply wrong proposed approach consists iteratively solving iterativey closed form update levenberg marquard optimizationd closed form paragraph later text claim proposed approach trivially generalized incorporate cost function true general inner loop closed form update need solve much complex optimization problem third paragraph section claim present novel energy minimization framework solve problem general form however solve solve different problem subject least relaxation clear solving local optimum double relaxed problem related original problem want solve say geiger defined linearity latent space defined dimensionality wrong discovers dimensionality latent space mean regularizer encourages singular value sparse thus fixed dimensionality latent space bounded smaller equal dimensionality original space clear author lvms gplvm latent space learned priority clean training data different noise model within framework furthermore proposed approach assumes gaussian noise also trivial case based lvms clear mean training saying technique training phase kpca trained closed form update still training,3
467.json,provides interesting idea extends taking account bidirectional network totally well written easy follow contribution theoretical part proposed method bigan inherits similar property experimental show bigan competitive method drawback convex optimization problem bigan still suitable accepted opinion,6
488.json,propose apply virtual adversarial training semi supervised classification quite hard ass novelty algorithmic side stage huge available literature semi supervised learning especially related literature work applied neural network unfortunately mention relate approach stick adversarial world term novelty adversarial side propose perturbation level word embeddings rather input mind application concerning experimental section focus text classification method comparison existing related literature important ass viability proposed approach example wang report imbd simple linear without transductive setup overall read well propose semi supervised learning algorithm shown work practice theoretical experimental comparison past work missing,5
488.json,applies idea adversarial training virtual adversarial training lstm based text context general well written easy follow extending idea adversarial training text task simple trivial overall worth publish minor comment also interesting much adversarial training help performance simpler easier analyze,6
522.json,proposes convergence analysis layer relus first analysis maybe novel assumption used analysis focus relu nonlinearity pretty popular practice quite hard read many english mistake typo nevertheless analysis seems generally correct novelty insight however always well motivated presented argument work us realistic assumption gaussian input example opposed work quite debatable actually overall look like correct analysis work form really suboptimal term writing presentation novelty relevance always clear unfortunately main intuition clearly presented detail could moved appendix example could help improve visibility impact interesting,3
522.json,work analyzes continuous time dynamic gradient descent training layer relu network input output thus layer relu unit work interesting sense involve unrealistic assumption used previous work similar goal importantly work assume independence input activation rely noise injection simplify analysis nonetheless removing simplifying assumption come expense limiting analysis layer nonlinear unit discarding bias term relu keeping input gaussian thus constant input trick cannot used simulate bias term imposing strong assumption representation input output bias le relu network existence orthonormal base represent relationship said tell present original analysis setting interesting valuable example exploiting symmetry problem assumption listed able reduce high dimensional dynamic gradient descent bivariate dynamic instead dealing original size parameter reduction allows author rigorously analyze behavior dynamic convergence saddle point symmetric case optimum symmetric case clarification needed first paragraph page near paragraph initialization arbitrarily close origin beginning paragraph state initialized randomly standard deviation order sqrt inconsistent minor comment draft section paragraph assume gaussian thus network bias free mean zero mean gaussian standard deviation spelled standard derivation multiple time page last paragraph first line corollary corollary,7
430.json,proposes learn decomposition sequence word speech recognition address important issue forsee useful application machine translation approach novel well motivated much like comparison byte pair encoding natural important baseline dynamic fixed decomposition performance obtained various vocab size minor point learned decomposition correspond phonetically meaningful unit example appendix hard tell learning phoneme frequent character gram thought application outside speech recognition shown effective domain really strong contribution probably outside scope,7
430.json,interesting proposes jointly learning automatic segmentation word word acoustic model although training handle word segmentation hidden variable depends also acoustic representation decoding maximum approximation used present nice improvement character based however compare word segmentation assume dependency acoustic obviously text based segmentation result simpler independent task order extract segmentation several publicly open tool available cited tool also exploit unigram probability word perform segmentation look improvement come longer acoustical unit longer acoustical constraint could lead le confused search pointing towards full word model another le token probable le multiplication probability thought experiment extreme case possible segmentation possible mixture word fragment character full word proposed word fragment closed vocabulary task good show word could outperform even full word segmentation estimate,6
430.json,submission proposes learn word decomposition word word sequence mapping jointly attention based sequence sequence particular feature approach decomposition static instead also condition acoustic input mapping probabilistic word multiple word sequence argue dynamic decomposition approach naturally reflect acoustic pattern interestingly motivation behind approach analogous learning pronunciation mixture based speech recognition probabilistic mapping word pronunciation also condition acoustic input mcgraw badr glass learning lexicon form speech using pronunciation mixture ieee transaction audio speech language processing ghoshal renals acoustic data driven pronunciation lexicon large vocabulary speech recognition proc asru singh stern automatic generation subword unit speech recognition system ieee transaction speech audio processing interesting work context linking previous work framework overall well written theoretically convincing experimental study could solid reasonable word level baseline proposed approach lie character level word level system vocabulary size dataset maximum large softmax layer closed vocabulary task guess word level system also competitive number reported furthermore explain computational bottleneck proposed approach downsampled data factor using still took around day converge expensive especially given take sample computing gradient table little misleading language seqseq language bahdanau much closer best number reported table small improvement using language finally day converge sound,6
575.json,clear contributing deep andrew already give gradient derivation correlation objective respect network output back propagated update network weight give gradient correlation objective network output confusing differentiable version enables back propagate directly computation,2
575.json,second look still confused trying achieve objective differentiable sense singular value trace norm differentiable appears title section trying solve problem however simply reformulate objective change objective need explicit relationship retrieval objective layer could imagine different way combining combination level optimization could find discussion section equation helpful even though objective differentiable sense caused major problem training principle need batch training empirically using large minibatches work fine need justify original gradient computation problematic trying achieve response question seems still sure proposed method advantage computational efficiency term organization better describe retrieval objective earlier experiment still encourage conduct comparison contrastive loss mentioned previous comment,3
349.json,proposed integration memory network reinforcement learning experimental data simple interesting relatively novel question extend case multiple variable single sentence answer vocabulary handle hope provide analysis curriculum learning part since important training training iteration data sample selected random simple depth multiple depth,5
349.json,introduces nice dataset data generator creates babi like task except questioning answering agent required clarify value variable order succeed think baseline test task appropriate worried task easy babi task still think locally useful generation code well written task extensible useful time,6
349.json,investigates task augment basic babi problem particular people object scenario replaced unknown variable variable must known solve question thus agent must learn query value variable interestingly measure performance agent correctly answering question efficiency asking value correct unknown variable variable unnecessary answer question inferring unknown variable go beyond required vanilla version babi task le solved well written contribution clear limited vocabulary structure babi problem general think task variant viewed basic reasoning task natural language understanding convinced claim really test interaction capability agent task phrased kind interaction think aptly described simply inferring important unknown variable important related reasoning sure whether connection ability interaction superficial said certainly true conversational agent need basic reasoning ability converse meaningfully human sympathise general goal babi task test reasoning ability synthetic environment complicated enough drive construction interesting model convinced extension task interesting worthy future investigation thus recommend acceptance,6
426.json,focus bilingual word representation learning following setting bilingual representation learnt offline manner already monolingual representation source target language learning common mapping representation direct word word alignment available source target language practically useful setting consider done good unifying existing solution problem providing theoretical justification even though propose method offline bilingual representation learning significant following contribution theory offline bilingual representation learning inverted softmax using cognate word language share similar script showing method also work sentence level extent addressed review question response comment header table say word frequency misleading word frequency could mean rare word occur guess meant rare word occur removed precision table space constraint different trend like appendix table difference difference inverted softmax difference please elaborate another suggestion running additional experiment expert dictionary cognate dictionary comparing method setting give valuable insight usefulness cognate dictionary,7
426.json,extends preceding work create mapping word embedding space language word embeddings independently trained monolingual data various form bilingual information used learn mapping mapping used measure precision translation propose change inverted softmax looking table better dina case improvement fact obtained introduction inverted softmax normalization overall wonder aspect really mention faruqui dyer already used dimensionality reduction xing argued already mikolov linear matrix orthogonal could make clear aspect work different faruqui dyer fact applied method measure translation precision using cognate instead bilingual directory nice trick please explain obtained list cognate obviously work language alphabet instance greek russian excluded also seems linguistics term cognate refers word common etymological origin necessarily written form night nuit noche nacht maybe different term word probably proper name news text,5
563.json,extend using bregman divergence density ratio matching argument generalization regular actual objective optimized generator training different theoretically motivated objective gradient issue theoretically motivated objective gans discriminator density ratio estimator generator try minimize divergence writing main problem unclear useful connection density estimation interesting derived conclusion seem questionable example previous density estimation literature pearson divergence stable claim hold gans show experiment unfortunately experiment section confusing unilluminating figure looking graph density ratio particularly illuminating claim pearson divergence modified divergence learning stop looking graph density ratio completely hand wavey evidence given back claim also normal objective tried light analysis furthermore seems despite criticizing normal gans using heuristic objective generator multiple heuristic objective trick used make work think much improved rewritten clear fashion stand difficult understand motivation intuition behind work,4
563.json,proposes train discriminator estimating density ratio minimizes bregman divergence also discus gans relate original work providing unifying view lens density ratio estimation note unifying view applies variant optimize density ratio general gans discriminator step framework except special choice kernel confused dual relationship condition function case difference fact former derived using divergence latter derived using bregman divergence original claim gans optimize divergence directly opposed however practice optimize approximation divergence quality approximation quantified anywhere seem principled experiment left confused illuminating choice overall liked connection density ratio estimation literature appendix seems like scattered collection right writing text significantly improve,5
534.json,noticed submitted review review question sorry thought added present great tell accurate honest overview emerging theory gans likelihood ratio estimation divergence minimisation perspective well written good read recommend people like involved gans main problem submission hard reviewer precisely novelty beyond perhaps articulating view better paper done past sentence left unsatisfied since gained insight needed choose summarises feeling nice unifying review type lack novel insight summary assessment mixed think great enjoyed reading left disappointed lack novel insight singular idea often expect conference presentation highly confident conference submission hence score open convinced either detailed comment think probably discus connection kliep kullback leibler importance estimation shugiyama colleague quite part equation flow point established view gans estimating likelihood ratio using likelihood ratio improve generator paragraph read like also tried derive another particular formulation failed practical typo spelling csiszar divergence equation known least square importance estimation kanamori variant least square likelihood estimation us kernel trick find function rkhs best represents likelihood ratio distribution least square sense think interesting think function related witness function commonly used property function compared witness function perhaps showing thing simple distribution stumbled upon work sugiyama collaborator direct density ratio estimation found work insightful generally work cited felt could highlight great work group made highly significant contribution density ratio estimation albeit different goal mind likelihood ratio estimation method approximate likelihood ratio directly least square importance estimation thought approximating quantity logistic regression denoising autoencoders unbiased estimate ratio provide biased estimate logarithm vice versa feel like estimating ratio directly useful generality estimating convex function ratio used define divergence seems like good approach could comment think hypothesis testing angle oversold sure additional insight gained mixing hypothesis testing terminology using quantity appear hypothesis testing test statistic work really talk hypothesis testing tool hypothesis testing literature sense contrast sutherland review iclr borrow concept sample testing optimise hyperparameters divergence used,5
534.json,provides exposition multiple way learning implicit generative model generative adversarial network example clear exposition insightful presented material clearly important hard ass novelty work individual piece novel exposition space clear outline connection novel believe work significant provides bridge language method used multiple part statistic machine learning potential accelerate progress recommend publishing iclr even though typical published conference offer empirical validation make particular claim relative merit different method,7
308.json,make valuable contribution provide clear understanding generative adversarial network training procedure insight training dynamic well variant reveal reason gradient either vanishing original unstable variant importantly also provide avoid difficulty introducing perturbation believe inspire principled research direction interested perturbation trick avoid gradient instability vanishment fact quite related dropout trick perturbation viewed bernoulli distribution great connection discussed besides theoretical analysis empirical study justify trick could please experiment like perturbated comparison,7
308.json,summary address important question difficulty training generative adversarial network discus consequence using asymmetric divergence function source instability training gans proposes alternative using smoothening approach pro theory good question nice answer make interesting concept form analysis differential topology proposes avenue avoid instability gans con long technical part consequence still need developed perfectly fine future work minor comment section maybe shorten section move proof appendix section provides nice intuitive simple solution page second bullet also mean smaller data distribution turn make divergence zero page generating plausibly looking picture generating plausibly looking picture lemma also hold generality theorem seems basic analysis word reference could spare proof theorem good remind reader lemma seems basic analysis word reference could spare proof specify domain random variable relly rely theorem closed manifold boundary already question corollary assumption theorem could find theorem theorem therefore theorem number theorem confusing,6
308.json,strong submission regarding important recently introduced method neural network generative adversarial network analyze theoretically convergence gans discus stability gans important best knowledge first theoretical paper gans contrary submission field actually provides deep theoretical insight architecture stability issue regarding gans extremely important since first proposed version gans architecture unstable work well practice theorem novel introduces mathematical technique interesting technical question regarding proof theorem pretty minor,9
685.json,proposes representing unseen word neural language proposed achieves poor slight improvement baseline work need comprehensive analysis comparison related work trying address problem intrinsic evaluation investigation work better missing make bolder claim investigation done morphologically rich language especially addition going languagex mrlx mrlx mrly done,1
390.json,well written interesting construction thoroughly enjoyed reading found formalism hard follow without specific example clear first specific component figure constitutes controller control optimizer optimized specific case algorithm box helpful especially case experiment description existing model fall conceptual framework might help well practical bayesian optimization machine learning algorithm larochelle adam propose optimizing respect expected improvement second balance computation cost performance loss might interesting fall framework experimental presented clearly well illustrated usefulness metacontroller curious using metaexperts,7
390.json,pro quality clarity originality significance present novel metacontroller optimization system learns best action shot learning task framework potential wider application metacontroller free reinforcement learning agent selects many optimization iteration function expert consult fixed action value state transition function experimental presented simulation experiment spacecraft must fire thruster reach target location presence heavy body metacontroller system similar performance loss shot learning task iterative standard optimization procedure however taking account computational complexity running classical iterative optimization procedure second resource loss term metacontroller shown efficient moreover metacontroller agent successfully selects optimal expert consult rather relying informed choice domain expert designer experimental performance contribution merit publication also exhibit interaction network learning dynamic simulated physical system dataset developed task also potential baseline future work shot physical control system dataset constitutes ancillary contribution could positively impact future research area con clear approach could applied broadly type optimization moreover reinforce gradient estimation method known suffer high variance yielding poor estimate curious method used ameliorate problem performance trick necessary train well content type could form useful additional appendix critique communication formal explication content clear could improved missing clear visual demarcation exactly metacontroller agent considered plate bounding around corresponding component likely speed uptake formal description generally clear lack axis tick mark subplots make challenging necessary compare among expert also overlap among point confidence interval upper left subplot interferes quantitative meaning symbol perhaps thinner bar different color help moreover figure lack legend different line impossible compare lastly second sentence appendix typo terminates without completion,7
543.json,validity presented work seems technically valid code matrix library sushi library github including live demo browser,5
543.json,interesting done useful seem like audience really mainstream iclr audience afraid conventional toolkit algorithm meta design improvement make easier expert design train neural network system think relatively little interest iclr really advance state significant objection presentation methodology,3
406.json,address systematic discrepancy simulated real world policy control domain proposed method contains idea training ensemble model adversarial fashion learn policy robust error adaptation source domain ensemble using data real world target domain significance address important significant problem approach taken addressing also interesting clarity well written require domain knowledge understand main concern well addressed rebuttal corresponding revision,6
406.json,look problem transferring policy learned simulator target real world system proposed approach considers using ensemble simulated source domain along adversarial training learn robust policy able generalize several target domain overall tackle interesting problem provides reasonable solution notion adversarial training used seem recent literature gans useful detail component discussed question response round also encourage including alternative policy gradient subroutine even perform well reinforce well without baseline value function useful researcher,6
514.json,pro clear formalism invariance signal known structure good numerical con structure must specified structure dataset simple large sometimes complex theory introduced numerical experiment consequently reader could lost since example might missing besides personal point view think topic content could suitable conference author improves content thus rejected think consider workshop option wish publish later conference conference might consider workshop paper iclr publication issue deal point,4
781.json,goal learn collection expert individually meaningful disjoint responsibility unlike standard mixture different mixture dimension seem promising exposition need significant improvement comment jump motivation application even algorithm architecture used addressed beginning subsequent exposition clear assertion made justification expert small variance subset variable variance variable large since learning expert weight rephrased term dictionary learning please discus relevant related literature horse data quite small respect feature dimension conclusion necessarily generalize,5
502.json,address issue evaluate automatic dialogue response important issue current practice automatically evaluate bleu based gram overlap correlated well desired quality human annotation proposed approach based lstm encoding dialogue context reference response response appropriate scoring essence training dialogue evaluate another however proposed solution depends reasonably good dialogue begin guaranteed rendering metric possibly meaningless,4
502.json,overall address important problem evaluate appropriately automatic dialogue response given fact current practice automatically evaluate bleu meteor often insufficient sometimes misleading proposed approach using lstm based encoding dialogue context reference response response scored linearly transformed space overall approach simple also quite intuitiv allows training rightly argue simplicity feature interpretation well speed experimental section report quite range experiment seem fine convince reader applicability approach mentioned also others insight experiment great mentioned depth failure case analysis also suggest beyond current dataset really show generalizability proposed approach opinion somewhat weaker front overall like idea forward approach seems sensible though thus accepted,6
447.json,introduces simulator synthetic question answering task interaction teacher asking question desired motivation intelligent agent improve performance asking question getting corresponding feedback user study problem offline supervised online reinforcement learning setting show model improve asking question idea novel relatively unexplored research community serf good first step direction study three different type task agent benefit user feedback well written provides clear detailed description task model experimental setting comment question motivation behind using vanilla memnn cont memnn using resulting conclusion adding contribution question clarification setting distribution misspelled word question entity answer entity relation entity none misspelled word come relation entity might much easier problem seems first point page performance testmodelaq worse testaq better testqa true task number table happens conversational history smaller none figure task accuracy good student drop stop asking question already know relevant fact asking question providing additional information good student figure task poor student able achieve almost question correct even without asking question expect number quite explanation behind figure task last sentence negative response instead positive currently shown preliminary evaluation good first step research direction learning dialogue agent unstructured user interaction,6
447.json,goal analyze behaviour dialogue agent must answer factoid question must query oracle additional information interpreted form interaction dialogue agent teacher problem investigation indeed important create synthetic environment test agent main strength test many different combination environment either knowledge missing agent query misspelling teacher question different way agent extra information concerned many task easy question paraphrase also concerned environment presented limited quite term richness linguistic structure real human interact chatbots think better positioned testing basic reasoning capability agent ability question answering rather dialogue however think ground approach start simple environment indeed worthy analysis make interesting contribution direction course much convincing human experiment additional note think simulation synthetic environment first mistake learner make dialogue learner problem understanding surface form text dialogue partner phrasing question particularly limited since word misspelling considered model used work character level course tiny fraction way agent misunderstand context particularly interested discussion plan scale realistic setting edit updated score reflect addition mechanical turk experiment,6
693.json,present meta learning algorithm learns learn generative model small example similar structure matching network vinyals trained meta learning framework input correspond datasets shown omniglot term likelihood term generated sample proposed idea seems reasonable struggling understand various aspect exposition hard follow partly existing method described using terminology fairly different original importantly tell aspect meant novel since sentence devoted matching network even though work build closely upon brought reviewer question revised make clearer also confused meta learning setup natural formulation meta learning generative model input consist small datasets task predict distribution sampled imply uniform weighting data point different proposed method based seems like additionally sort query clear represents term experimental validation comparison prior work seems necessary since several method already proposed similar spirit,3
693.json,proposes interesting idea rapidly adapting generative model data regime idea similar technique used shot learning specifically idea matching network propose generative matching network effectively variational auto encoder conditioned input dataset given query point match query point point conditioning using attention embedding space similar matching network omniglot dataset show method successfully able rapidly adapt input distribution given example think method interesting however major issue lack clarity outline detail overall found somewhat difficult follow detail feel scattered throughout sense reading able implement method replicate suggestion consolidate major implementation detail single section explicit functional form different embedding function variant disappointed weak supervision form label used method perform completely unsupervised setting could interesting baseline lack definition different function basic insight functional form nice otherwise unclear going section state recurrent controller used matching reading section several pass pseudo input used place regular input correct otherwise sentence section need clarification noticed upon reading section version pseudo input used pseudo input used conditional version difference functional form formula embeddings change setting since result fully contrastive apply binarization mean result fully contrastive clarity figure table refer number shot never defined assume made consistent figure value case mean stated earlier assume shot corresponds also look like continue improve increased number step like maybe step well presumably come point diminishing return table fair baseline mention ctest affect evaluation fact associated ctest implies model evaluated different metric clarify important comparison apple apple mnist much common omniglot evaluating generative model possible perform similar experiment dataset compared many model negative likelihood value monotonically decreasing number shot ever case increasing number shot hurt thing happens minor grammatical issue missing determiner several sentence point referred instead figure changed figure experiment section,4
369.json,show different approach ternary quantization weight strength show performance improvement existing solution idea learning quantization instead using defined human made algorithm nice much spirit modern machine learning weakness incremental addressed narrow audience clearly assumes reader familiar previous work ternary quantization topic update really standalone description main algorithm concise least probably clear read previous work narrow subject unsuitable broader deep learning audience convincing motivation work presented engineering gimmick cool valuable really used production really needed anything practical application require refinement find motivation related mobile therefore cool sufficient small step niche research long provide sufficient practical motivation pursuing particular topic next step long list small refinement think belongs iclr broad diversified audience also code released understanding,2
369.json,present compressing weight particular us neural network quantization method compress network weight ternary value group recently published multiple topic offer possibly lowest return seen fraction percentage imagenet alexnet little interest given group already showed kind older style network compressed large amount also liked group release code compression also report data amount effort required compress flop time number pass required original dataset data important decide compression worth effort,6
369.json,work present novel ternary weight quantization approach quantizes weight either layer specific learned value unlike past work quantized value separate learned stochastically alongside network parameter approach achieves impressive quantization retaining surpassing corresponding full precision network cifar imagenet strength overall well written algorithm presented clearly approach appears work well experiment resulting good compression without loss sometimes gain performance enjoyed analysis sparsity change course training though uncertain useful conclusion drawn point energy analysis table assumes dense activation unpredictability sparse activation provide average activation sparsity network help verify assumption even assumption hold relatively close value average activation network make comparison convincing section suggest fixed threshold parameter layer allows varying sparsity owed relative magnitude different layer weight respect maximum section paragraph developed suggesting additional sparsity achieved allowing layer different value value multiple threshold style network appear table figure added claim quantized weight play role learning rate multiplier back propagation benefit using trained quantization factor benefit figure table caption descriptive preliminary rating think interesting convincing somewhat lacking novelty minor note table list flop rather energy full precision section peeding figure reference error last line,6
369.json,present ternary quantization method convolutional neural network weight represented ternary value multiplied scaling coefficient ternary weight scaling coefficient updated using back propagation useful compression experiment alexnet show proposed method superior ternary weight network dorefa work following strength weakness strength good shown cifar dataset massive energy saving ternary weight comparing weight well written easy understand weakness seems work incremental improvement existing work main difference binary weight network proposed xnor using ternary weight instead binary weight ternary weight used many previous work proposed method learn scaling factor training comparing ternary weight network main difference independent quantization factor used positive negative weight utilizes scaling factor weight experiment process first conv layer last fully connected layer processing layer given fair comparison experiment comparison reported error alexnet imagenet comparable method proposed however us binary weight layer binarized including first conv layer last fully connected layer baseline method full precision alexnet reported accuracy seems error commonly using batch normalization boost accuracy reported accuracy much lower alexnet without batch normalization hand error rate trained alexnet reported official matconvnet proposed method evaluated original alexnet whose accuracy publicly available almost deep learning framework like caffe moreover experiment added imagenet like googlenet resnet previous method xnor multiply operation replaced addition using binary ternary weight however proposed method utilizes independent scaling factor positive negative weight thus seems multiply operation replaced addition reference mohammad vicente joseph xnor imagenet classification using binary convolutional neural network eccv,2
369.json,study depth idea quantizing convolutional layer bit different positive negative layer scale go provide exhaustive analysis performance essentially loss real benchmark remarkably mnist free relevance likely provides lower bound quantization approach sacrifice performance hence plausibly become approach choice resource constrained inference might suggest hardware design take advantage proposed structure furthermore provides power measurement really main metric anyone working seriously space care measurement full precision baseline loved sota result imagenet result strong lstm baseline fully convinced also liked discussion wall time result using training procedure,7
555.json,aim attacking problem preselecting deep learning structure domain reported series experiment various small task feed forward dnns claim ranking algorithm learned based guide selection structure domain although goal interesting found conclusion neither convincing useful practice several reason explored really simple network feed forward dnns significantly limited search space also limited value experiment fact best architecture highly task domain dependent type lstm often much important size network experiment conduced important hyper parameter learning rate schedule fixed however well known learning rate often important hyper parameter training without adjusting important hyper parameter conclusion best architecture convincing experiment seem indicate training data difference important however unlikely true definitely want larger model total number parameter training magnitude larger datasize important feature likely experiment large datasets addition think title accurately reflect modified also cited sainath work lead breakthrough speech recognition however breakthrough happened much earlier first three component published deng dahl december role training fine tuning context dependent hmms real world speech recognition proc nip workshop deep learning unsupervised feature learning detailed published dahl deng acero context dependent trained deep neural network large vocabulary speech recognition ieee transaction audio speech language processing conclusion presented preliminary result although interesting ready publishing,2
410.json,address problem detecting example misclassified distribution important topic study provides good baseline although miss strong novel method task study contributes community,5
410.json,propose statistic softmax output estimate probability error probability test sample domain contrast performance proposed method directly using softmax output probability statistic measure confidence great elaborate idea ignoring logit blank symbol interesting performance proposed method confusable setting case domain example similar domain example case speech recognition might correspond using different language speech system trained particular language acoustic characteristic speech signal different language might similar compared noisy clean speech signal language section description auxiliary decoder setup might benefit detail recent work performance monitoring accuracy prediction area speech recognition work listed ogawa tetsuji delta measure accuracy prediction application multi stream based unsupervised adaptation proceeding icassp hermansky hynek towards machine know know proceeding icassp variani ehsan multi stream recognition noisy speech performance monitoring interspeech,5
607.json,propose hierarchical attention video captioning introduce composed three part temporal modeler take input video sequence output sequential representation video hierarchical attention memory mechanism implement soft attention mechanism sequential video representation finally decoder generates caption related second series question seems though chosen refer lstm equivalent output bahdanau attention mechanism hierarchical memory mechanism actually sympathetic terminology sense recent popularity memory based model seems neglect memory implicit lstm state vector said seems seriously misrepresent significance contribution appreciate ablation study presented table enough researcher bother kind analysis show value contribution actually clear particular case quite weak regarding quantitative evaluation presented table carving fairly specific feature describe fair comparators literature given variability model alternate training datasets find compelling trying achieve best includes fine tuning frame value work application discovery incorporation element significantly improve performance seems warranted overall point sufficient contribution warrant publication iclr,3
607.json,address video captioning architecture module attends attended output module generating description give kind level attention evaluated charade msvd datasets quality clarify found poorly written relatively hard understand tell module section straight forward attention frame encoder bahdanau decoder section standard lstm likelihood module section novel module well described look attention lstm attention lstm output attention weight additionally conditioned decoder state small problem description notational discrepancy using textbf equation using text also spent long time trying understand order network remember attended temporal structure video propose memorize previous attention encoded version input video language using enables network memorize previous attention frame also learn multi layer attention input video corresponding language bold word propose assumed kind novel technical contribution could find detail specified later section fact lstm clear piece information section discus decoder sloppy part example table number significant digit semantics horizontal line table explained text experimental ablation study show mixed adding looking meteor shown highest correlation human coco compared evaluation criterion adding improves clear significant improvement especially given test video doubt result table meteor score higher discrepancy addressed text surprising explicitly claim state originality significance introduces additional layer attention standard sequence sequence setup argued alleviate burden lstm memory moderately novel believe experimental make sufficiently clear also worth made standard somehow simpler instead complex inclined judge favorably minor response author comment sure cause think part architecture using appreciated please elaborate confusion figure address accordingly created diagram hopefully make clear,3
312.json,explores important part field automating architecture search technique currently computationally intensive trade likely become better near future technology continues improve cover standard vision text task tackle many benchmark datasets showing gain made exploring beyond standard search space always want technique applied datasets already sufficient show technique competitive human architectural intuition even surpass also suggests approach tailor architecture specific datasets without resulting hand engineering stage well written interesting topic strong recommend accepted,8
312.json,present search optimal neural architecture based actor critic framework method treat variable length sequence us find target architecture act actor node selection action context evaluation error outcome architecture corresponds reward auto regressive layer lstm used controller critic method evaluated different problem compared number human created architecture exciting hand selecting architecture difficult hard know optimal hand designed network presented method novel excellent describing detail improvement needed done tested data represents well capability method interesting difference generated architecture human generated one written clearly accessible coverage contrast related literature done well interesting data time needed training correlation time resource needed train quality also interesting human bootstrapped model perform involve overall excellent interesting,8
742.json,present theoretical empirical approach problem understanding expressivity deep network random network deep network random gaussian weight hard tanh relu activation studied according several criterion number neutron transition activation pattern dichotomy trajectory length seem solid justification newly introduced measure expressivity really measure expressivity instance trajectory length seems discutable measure expressivity justification given good measure expressivity proportionality measure expressivity specific case random network obscure long work interesting idea seem properly replaced context finding seem trivial detailed comment much work examining achievable function relies unrealistic architectural assumption layer exponentially wide think deep belief network compact universal approximators leroux proof given deep narrow feed forward neural network sigmoidal unit represent boolean expression neural network layer unit number input neutron comparing architecture fashion limit generality conclusion knowledge much previous work focused mathematical proof general conclusion representative power deep network example leroux much harder generalise approach propose based random network used practice study family network arising practice behaviour network random initialisation network arise practice intermediate step used perform computation mean representative power intermediate network priori irrelevant need justify random network provide natural baseline compare trained network random network natural study expressivity deep network clear representative power random network kind random network seems important question linked representative power whole class network class network training class network one priori care need justify study random network help understanding either random neural network suggest point enough away independent sign direct proportionality length number time cross decision boundary seems proportionality measure depends network random seems invalidate generalisation network network random assume path length proportional expressivity remaining depth seems trivial concern completely equivalent expressivity depth make remark figure number achievable dichotomy depends number layer layer swept seem trivial figure network width mnist seems much small accordingly performance poor difficult generalise relevant situation,2
742.json,summary study expressive power deep neural network various related measure expressivity discus measure relate trajectory length shown depend exponentially depth network expectation least experimentally intuitive level theoretically certain assumption also emphasis importance weight earlier layer network larger influence represented class function demonstrates experimental setting pro advance topic related expressive power feedforward neural network piecewise linear activation function particular elaborating relation various point view con advance elaborates interesting topic appraisal contribute significantly aspect discussion comment long especially appendix seems written rush overall main point presented clearly conclusion could clearer assumption experimental theoretical nature connection previous work could also clearer page find statement furthermore architecture often compared hardcoded weight value specific function represented efficiently architecture shown inefficiently approximated another partially true neglect important part discussion conducted cited paper particular montufar pascanu bengio discus hard coded function class function given number linear region show deep network generically produce function least given number linear region shallow network never generically meaning fixing number parameter function represented network parameter value form open positive measure neighbourhood belongs class function least certain number linear region particular statement directly interpreted term network random weight measure expressivity discussed present number dichotomy statistical learning theory notion used define dimension context high value associated high statistical complexity meaning picking good hypothesis requires data page find statement discover prove underlying reason three measure directly proportional fourth quantity trajectory length expected trajectory length increasing exponentially depth interpreted increase decrease scale composition form scale input scaling certainly underlying cause increase number dichotomy activation pattern transition seems least assumption considered type trajectory also play important role probably related another observation page variance bias comparatively large longer exponential growth specific comment theorem good specific random neural network fixed connectivity structure random weight also kind dimensional trajectory finite length closed differentiable almost everywhere notation used theorem read literally large enough argument could also read smaller function bounded hold instance whenever expressing asymptotic lower bound notation omega,5
742.json,summary study quantity related expressivity neural network analysis done random network define trajectory length dimensional trajectory length trajectory point dimensional space embedded layer network provide growth factor function hidden unit number layer growth factor exponential number layer relates trajectory length quantity transition activation pattern dichotomy consequence study suggest training earlier layer network lead higher accuracy training later layer experiment presented mnist cifar clarity little hard follow since motivation clear introduction definition across clear novelty studying trajectory length function transforming data multilayer network interesting idea relation transition number term growth factor quantity quantity relationship hence hard understand implication significance geometry input dimension show weakly activation pattern analysis trajectory study tell network organizes input observed experiment network becomes contractive selective train network interesting study phenomenas using trajectory length measure disentangling nuisance factor invariance supervised setting network need contractive every need selective class label theoretical study selectivity contraction using trajectory length appealing detailed comment theorem raised reviewer definition dimensional input trajectory missing theorem tell design architecture neural network promised introduction clear connection transition theorem rather weak theorem proof theorem clear meant notation confusing expectation taken respect weight understand want overload notation maybe help keeping track recursion applied different definition seems random variable fixed fixing looking random proof recursion analysis time study mathbb time case assume assume analyzed alone know scale theorem main text proof missing theorem main text theorem appendix figure trajectory length reduction training network becoming contractive enable mapping training point label instance contraction deep network,4
715.json,proposes simple randomized algorithm selecting weight convnet prune order reduce theoretical flop evaluating deep neural network provides nice taxonomy pruning granularity coarse layer wise fine intra kernel pruning strategy empirically driven us validation select best randomly pruned model make claim intro shot near optimal cannot supported shot sense network generated tested evidence theory found solution near optimal pro nice taxonomy pruning level comparison recent weight pruning method con experimental evaluation touch upon recent model resnets large scale datasets imagenet somewhat hard follow feature pruning obviously accelerate computation without specialized sparse implementation convolution case finer grained sparsity since considers fine grained sparsity provide evidence introducing sparsity yield performance improvement another experimental downside evaluate impact filter pruning transfer learning example much direct interest task mnist cifar even imagenet instead main interest academia industry value learned representation transferring task might expect pruning harm transfer learning possible main task performance transfer learning strongly hurt missed opportunity explore direction summary proposed method simple good experimental evaluation somewhat incomplete cover recent model larger scale datasets,4
345.json,propose method compress neural network retraining putting mixture gaussians prior weight learned mean variance used compress neural network first setting weight mean infered mixture component resulting possible loss precision storing network format save fixture index exploit weight enforced training quality course serious drawback method seem work render method unusable production right maybe improved guess alexnet take long process otherwise might valuable addition figure noticing thing left large number point improved accuracy case lenet caffe intuition case additionally regarding spearmint optimization found clue hyperparameter setting worked well might helpful people trying apply method really like figure latest version clarity especially section written well give nice theoretic introduction section short seem contain relevant information might helpful least detail used model maybe number layer number parameter claim even though variance seem reasonable small large figure hard ass especially vertical histogram essentially show zero component might helpful either histogram separate histogram componenent large point figure opposed smaller one seem good compression accuracy loss ratio point listed originality work compressing neural network using reduced number bit store parameter exploiting sparsity structure like idea directly learn quantization mean gaussian mixture prior retraining seems principled approach significance method achievs state performance shown example mnist however network deep network used state model obviously drawback practical usability method therefor significance method could made work state network like resnet consider contribution high significance minor issue page seems space front first author name page scenario fixed missing backslash page wrong blank number component page experience experiment page figure figure,6
596.json,proposes method link prediction knowledge base method contains main innovation iterative inference process allows refine prediction shared memory component thanks element introduced achieved remarkable benchmark fairly written interesting experimental strikingly good still rate weak accept following reason main problem little explanation element aforementioned leading better instance performance without shared memory size grown performance impacted varies tmax chosen value experiment assume give indication often termination gate work also interesting give proportion example inference terminated hitting tmax proportion example prediction changed along several inference iteration value lambda section seems indicate temperature softmax attention finally attending mostly single cell softmax activation change type relationship entity type quite overused benchmark interesting test larger condition,5
579.json,updated review admirable responding incorporating reviewer feedback particular effort additional experiment even incorporating much stronger baseline convnet lstm baseline requested multiple reviewer still lingering concern previously stated architecture hidden unit tuned independently pure time series forecasting baseline without trend preprocessing tried going bump score clear rejection borderline concerned time series prediction problem prediction target include slope duration upcoming local trend setting great interest several real world problem setting financial market decision sell often driven local change trend primary challenge problem distinguishing true change trend downturn share price noise tackle interesting hybrid architecture trenet four part preprocessing extract trend lstm accepts trend input ostensibly capture long term dependency convnet accepts local window data input time step higher feature fusion dense layer combine lstm convnet output three univariate time series data set trenet outperforms competing baseline including based constituent part lstm trend input strength interesting problem setting plausibly argued differ sequential modeling problem deep learning video classification nice example fairly thoughtful task driven machine learning accepting author assumption true moment proposed architecture seems intuitive well designed weakness although interesting problem setting decision driven trend change make strong argument formulated machine learning task trend target provided high data oracle extracted data using deterministic algorithm thus could easily formulate plain time series forecasting problem forecast next step apply trend extractor convert prediction trend forecast accurate extracted trend proposed architecture interesting justified particular choice feed extracted trend data separate lstm convnet layer combined shallow equally straightforward intuitive choice feed output convnet lstm perhaps augmented trend input without solid rationale unconventional choice come across arbitrary following point convnet lstm convnet trend lstm architecture natural baseline experiment presupposes rather argues value extracted trend duration input unreasonable think enough training data sufficiently powerful convnet lstm architecture able learn detect trend data predictive following point obvious baseline omitted lstm convnet trend basically propose complex architecture without demonstrating value part trend extraction lstm convnet baseline unnecessarily weak thing uncertain general validity practice using lstm convnet architecture baseline trenet sound like apple apple comparison world hyperparameter tuning could fact disadvantage either seems like thorough approach optimize architecture independently regarding related work baseline think fair limit scope depth analysis experiment reasonable representative baseline least conference submitted deep learning conference said ignored large body work financial time series modeling using probabilistic model related technique another frame separate trend noise problem treat observation noisy semi recent example hernandez lobato lloyd hernandez lobato gaussian process conditional copula application financial time series nip appreciate research direction general moment believe work described manuscript suitable inclusion iclr policy interactive review keep open mind willingness change score large revision unlikely encourage instead time energy reviewer feedback order prepare future conference deadline icml,5
579.json,revision review commendable including additional reference baseline experiment present hybrid architecture time series prediction focusing slope duration linear trend architecture consists combining convnet local time series lstm time series trend descriptor convnet lstm feature combined predicting either slope duration next trend time series method evaluated small datasets summary relative well written presenting interesting approach several methodology flaw handled experiment pro idea extracting upward downward trend time series although ideally learned rely technique given submission iclr methodology section mean predicting either duration slope trend prediction valid prediction done jointly loss combined training entire trend slope duration need predicted jointly predicting time series without specifying horizon prediction meaningless duration trend short time series could alternatively duration trend long slope might close zero prediction specific horizon needed general time series prediction application trading load forecasting pointless decision made trading strategy radically different short term noisy oscillation long term stable upward downward trend actual evaluation term trading profit loss added baseline including nave baseline mentioned earlier review question important baseline missing feeding local time series convnet connecting convnet directly lstm without trend extraction convnet lstm architecture need analysis convnet filter trend prediction representation trend prediction segmentation convnet could extra supervised loss detailed analysis trend extraction technique missing section baseline local trend local time series vector concatenated approach used lstm baseline multivariate input convnet operates local important nave baseline missing next local trend slope duration previous local trend slope duration missing reference related work section partial omits important work hybrid convnet lstm architecture particular vinyals oriol toshev alexander bengio samy dumitru show tell neural image caption generator corr donahue jeff hendricks lisa anne guadarrama sergio rohrbach marcus venugopalan subhashini saenko kate darrell trevor long term recurrent convolutional network visual recognition description corr karpathy andrej toderici george shetty sanketh leung thomas sukthankar rahul large scale video classification convolutional neural network cvpr organization need improvement section explain selection maximal tolerable variance trend segment appendix moved core part section unnecessarily long give well known detail equation convnets lstms variation standard algorithm description concatenated feature fusion layer expressed simple concatenation detail could moved appendix additional question section many datapoints dataset listing number local trend uninformative typo duration slop,4
646.json,novel similar existing confusing need human interactive training process could practical human join training iteration sound impractical human interactive question predefined interactive since based current state output,4
646.json,work describes stage encoding story babi like setup used encode sentence word word conditioned sentence level sentence level keep track sentence level encoding used modifying babi task necessary question correctly solve problem convinced paper architecture significantly better view similar problem architecture solve several paper second thing example dialog based language learning weston learning goal oriented dialog bordes weston think carefully compelling way current work correct answer question seems given independent agent asks output unknown input extra response advantage essentially architecture used solve babi modified indeed accuracy appendix show sort module appended standard model trained output question sequence word furthermore suspect could generate question setting enumerating question occur training taking softmax instead generating word word,3
353.json,nice think quite clever attempting best latent variable model auto regressive model implementation specific architecture choice discussed review also seem reasonable experimental side liked something measurement sample maybe show useful task classification though think huge leap forward certainly nice recoemmend acceptance,6
703.json,present tartan derivative previously published accelerator architecture dadiannao difference tartan compute unit serial unroll operation several cycle enables unit better exploit reduction precision input activation improvement performance energy efficiency comment second earlier review requesting present detail methodology used estimating energy number tartan claimed tartan give improvement energy efficiency however suspect small improvement clearly within margin error energy estimation tartan derivative dadiannao heavily relies overall architecture dadiannao novel aspect contribution introduction serial compute unit unfortunately turn incur severe area overhead nearly dadiannao compute unit nonetheless idea serial computation certainly quite interesting opinion better appreciated perhaps even relevant circuit design architecture focused venue,4
703.json,summary describes dadiannao dadn accelerator improved employing serial arithmetic replace parallel multiplier dadn multiplier accept weight parallel activation serially serial parallel multiplier increase number unit keeping total number adder constant enables tailor time energy consumed number bit used represent activation show configuration used process fully connected convolutional layer dnns strength using variable precision layer network useful previously reported judd good evaluation including synthesis place route unit also evaluation identical judd weakness idea combining serial arithmetic dadn architecture small already published almost everything micro judd increment analysis architecture fully connected layer everything else previous publication energy gain small additional flip flop energy shifting activation almost offset energy saved reducing precision arithmetic compare conventional approach variable precision using parallel arithmetic unit data gating lsbs relevant portion arithmetic unit toggle provide speedup likely provide better energy gain serial parallel approach overall tartan stripe architecture interesting incremental contribution adding support fully connected layer three previous publication topic particular judd small idea worth good four,4
703.json,seems like reasonable study though area expertise found fault work presentation follow detail know comparable literature seem real gain technique though term efficiency hardware changing accuracy task task chosen alexnet seem reasonable simulation rather actual hardware topic seems little specialized iclr since describe advance learning representation albeit includes hardware think appeal among attendee rather limited please learn parenthetical reference correctly reference make reading harder,5
703.json,feel qualified review studied digital logic back university think work deserves reviewer sophisticated background area certainly seems useful advice also submit another venue,3
703.json,proposed hardware accelerator utilized fact tolerant precision inference outperforms state parallel accelerator without loss accuracy energy efficient requires network retraining achieved super linear scale performance area first concern seem well suited iclr circuit diagram make interesting hardware circuit design community second concern take away machine learning community seeing response take away using precision make inference cheaper novel enough last year iclr least paper discussing using precision make efficient idea also explored previous paper,3
754.json,present improved neural language model designed selected long term dependency predict accurately next identifier dynamic programming language python improvement obtained replacing fixed widow attention pointer network memory consists context representation previous identifies introduced entire history conventional neural lstm based language combined sparse pointer network controller linearly combine prediction component using dynamic weight decided input hidden state context representation time stamp avoids need large window size attention predict next identifier usually requires long term dependency programming language partly validated python codebase another contribution experiment still miss critical information like including sparse pointer network performance chance different size computationally efficient training inference time compared lstm attention various window size ablation experiment much contribute respectively might interest iclr community accepted,5
754.json,take standard auto regressive source code augments fixed attention policy track certain token type like identifier additionally release python open source dataset expected augmentation fixed attention policy improves perplexity seems important deeper show contribution different token type achieve perplexity alluded text thorough comparison welcome idea attention policy take advantage expert knowledge nice contribution perhaps limited novelty example maddison tarlow cite scoping rule track previously used identifier scope,4
304.json,argues able handle recursion important neural programming architecture handling recursion allows strong generalization domain test case learning smaller amount training data riff reed freitas neural programmer interpreter iclr learns program trace train model trace recursive call show verify correctness evaluating learned program small base case reduction rule impressively show architecture able perfectly infer bubblesort tower hanoi problem like idea super simple even mention change execution trace training pipeline get actually sure right take away mean effectively solved neural programming problem execution trace available problem easy begin example larger input domain reviewer also mention mnist digit imagine problem must infer sort mnist digit highest lowest setting execution trace effectively decouple problem recognizing digit inferring program logic problem harder learning recognize mnist digit learning bubble sort symbol problem access execution trace cannot infer using proposed method,7
304.json,interesting fairly easy read present small nifty approach make neural programming interpreter significantly powerful allowing recursion generalizes better fewer execution trace interesting example small trivial extension make machine learning method significantly practical also appreciate notation used original deepmind expert topic easy read original tandem point critique generalization prof vague numerical example iterate possible execution path next recursive call however approach generalize continuous input space example original seems prove generalization still intractable continuous case planning releasing source code,7
538.json,propose learn symbolic expression representation reasonably extensive evaluation similar approach motivate approach well expressed preliminary question think could improve motivation subexpforce loss page mention compare layer residual connection think direct comparison subexpforce loss helpful included keep residual connection normalization main concern evaluation score appears precision query basis believe standard metric precision recall informative particular chosen metric expected perform better equivalence class larger since taken account denominator likelihood random expression matching query increase,6
538.json,goal learn vector representation boolean polynomial expression equivalent expression similar representation proposed based recursive neural network introduced socher given syntactic parse tree formula either boolean polynomial representation node obtained applying representation child process applied recursively obtain representation full expression contrary socher proposes layer especially important capture operation surprising also introduces reconstruction error called subexpforce expression child recovered expression parent understood correctly trained using classification loss label given expression corresponds equivalence class method evaluated randomly generated data compared baseline standard recursive neural network agree learning good representation symbolic expression capture compositionality important task entirely convinced experimental setting proposed indeed stated task deciding boolean expression equivalent hard understand better implicitly computing truth table expression sometimes hard follow technically sound particular proposed well adapted problem outperforms baseline pro relatively simple sound using classification loss equivalence class compared using similarity con convinced setting believe really better truth table boolean expr computing value polynomial expression randomly chosen point part hard follow justification subexpforce discussion softmax work comparison classification loss similarity loss missing,4
538.json,work proposes compute embeddings symbolic expression boolean expression polynomial semantically equivalent expression near embedded space proposed us recursive neural network architecture made match parse tree given symbolic expression train parameter create dataset expression semantic equivalence relationship known minimize loss function equivalent expression closer equivalent expression margin loss function also subexpression forcing mechanism understand correctly encourages embeddings respect kind compositionality shown symbolic expression datasets created proposed method demonstrated outperform baseline pretty convincingly especially like visualization action negating expression shown correspond roughly negating embedding vector space like woman queen king type embeddings wordvec glove style paper weakest part probably setting seems somewhat contrived really think real setting easy training known semantic equivalence still worth neural network prediction also punted dealing variable name assuming distinct variable refer different entity domain understandable variable name whole layer complexity already difficult problem also seems high limiting example proposed method useable equation search engine unless able accurately canonicalize variable name miscellaneous point regarding problem believe problem determining expression equivalent actually undecidable word problem thue system related able figure determine ground truth equivalence training set expression simplified canonical form grouped seems possible general question possible equivalent expression training data mapped different canonical form easier possible construct compare truth table combine operation us describe residual like connection looking equation reason actually residual connection weight matrix multiplied lower level feature true residual connection passed feature unchanged identity connection also better fighting gradient explosion reason used rather identity connection table first entry seems equivalent vertical spacing figure caption body text small look like caption continues body text,5
493.json,analysis misclassification error discriminator highlight fact uniform probability prior class make sense early optimization distribution deviate prior significantly parameter move away initial value consequently optimized upper bound loss get looser optimization procedure based recomputing bound proposed well written main observation made well known fact presented clear refreshing make useful wide audience venue like draw author attention close connection framework curriculum learning found relevant reference cited discussion could enrich quality large body work directly optimizing task loss reference therein also discussed related particularly section optimizing curve training highly multiclass classifier gupta direct loss minimization structured prediction mcallester generalization bound consistency latent structural probit ramp loss mcallester keshet final comment believe material presented interest wide audience iclr problem studied interesting proposed approach sound recommend accept increase score,7
493.json,proposes alternative conditional likelihood training discriminative classifier argument conditional likelihood upper bound bayes error becomes lousy training proposes better bound computed optimized iterative algorithm extension idea developed regularized loss weak form policy learning test performed different datasets interesting aspect contribution revisit well accepted methodology training classifier idea look fine seem validate however still preliminary work like idea pushed globally lack coherence depth part policy learning well connected rest link motivated example optimization uncertainty experimental part need rewriting find legend identifying different curve figure make difficult appreciate,3
610.json,performs series experiment systematically evaluate robustness several defense method including improved version provides interesting observation overall distillation best performance none method really resist additional attack adam since experimental main concern clarity comment detail pro provides good comparison performance selected method section additional attack interesting investigation although final result defense method negative still inspiring overall provides interesting inspiring experimental selected method con several method literature missing example defense method attack method paper although long list experimental provided many detail skipped example detail experiment generate table without explanation analysis experimental contribution seems limited proposed improved version algorithm experimental seems promising minor comment page equation also convex convexity equation motivation equation,4
610.json,compare several defense mechanism adversarial attack retraining kind autoencoders distillation conclusion retraining methodology proposed work best approach document series experiment making model robust adversarial example method proposed original proposed distillation proposed goodfellow explaining harnessing adversarial example stacked autoencoders proposed szegedy intriguing property neural network original part improved version autoencoders proposed establishes experimental evidence framework provides best defense mechanism adversarial attack make introduction improved autoencoder mechanism le appealing although establishes interesting measurement point therefore potential cited reference relative lack originality decrease significance,4
755.json,think write improved also might somewhat misleading behavior weight revealing work general think work also underestimate effect nonlinearities learning dynamic,3
305.json,thing like specific jpeg jpeg implementation used configured major weakness many paper include specific encoders configuration used comparison without knowing hard know comparison done suitably strong jpeg implementation properly configured example comparison jpeg unfortunately interesting since codec widespread usage likely never better comparison webp performance even better nice software implementation available play,6
305.json,convincing image compression deep neural network read well written rate distortion theory objective fit smoothly framework compared reasonable baseline jpeg opposed previous paper considering jpeg expect good impact please include lena barbara baboon sorry gibbon along state reference classical method mentioned question think important clearly state compare best previous method submitted version still know category method positioned,8
305.json,extends approach rate distortion optimization deep encoders decoder simple entropy encoding scheme adaptive entropy coding addition discus approach relationship variational autoencoders given approach rate distortion optimization already published novelty submission arguably high correct missed trick way even represents step backward since earlier work optimized perceptual metric used however visible improvement jpeg know learned encoding shown achieve level performance well written equation appears wrong believe partition function depend theta mean approach equivalent euclidean metric reason optimizing rather perceptual metric previous work given author background surprising even evaluation performed term psnr contribution adaptive entropy coding versus effect deeper encoders decoder seems like important piece information interesting performance without adaptation previous detail adaptive coder effect provided happy give higher score,7
581.json,machine learning algorithmic contribution us combination lstm bivariate mixture density network graf detailed explanation appendix even miss essential point gaussian parameter obtained transformation output lstm also numerical evaluation suggesting algorithm form improvement state think appropriate conference like iclr part describing handwriting task data transformation well written interesting read could valuable work conference focused handwriting recognition expert field,2
581.json,present method sequence generation known method applied feature extracted another existing method heavily oriented towards chosen technology lack literature sequence generation principle rich literature motion prediction various application could relevant recent model exist sequence prediction primed input various application skeleton data model learn complex motion processing evaluation concern quantitative evaluation comparision method still wonder whether intermediate representation developed plamondon useful context fully trained sequence generation whether could pick necessary transformation evaluated detail several typo word omission found carefully rereading beginning section still unclear application prediction dynamic parameter section give better motivation work concerning following paragraph method superior handwriting analysis biometric purpose le precise method berio leymarie le sensitive sampling quality aimed generating virtual target sequence remain perceptually similar original trace method explained self contained mentioned conditioned enough detail given generally speaking effort could made make self contained,2
581.json,take based graf retrofit representation derived work plamondon part goal deep learning avoid hand crafted feature network learn feature representation somewhat grain relies qualitative example demonstration system seem provide strong motivation progress provide true text conditional handwriting synthesis shown graf original work consistent bibliography variant plamondon name bibliography,2
647.json,unfortunately even reading response review question feel current form lack sufficient novelty accepted iclr fundamentally suggests traditional iterative algorithm specific class problem posed image inverse problem replaced discriminatively trained recurrent network also note rolled network iterative inference used replace type inference also solve image inverse problem ref therefore argue fundamental idea proposed seek formalize approach inverse problem although nothing specific analysis tie inverse problem show express gradient descent prior likelihood objective also find claim benefit prior approach compelling comment parameter sharing work way possible untying parameter lead better performance fewer number iteration given training synthetically generated learning larger number parameter seem issue also argue sharing parameter obvious approach prior method choose parameter better accuracy hold able handle different noise level scale size single always trained handle multiple form degradation likely better trained specific degradation level importantly evidence current experiment show property architecture moreover claim go motivation training single prior different observation model train entire inference architecture possible proposed method offer practical benefit beyond prior work benefit come idea simply unrolling iteration novel strongly recommend consider significant write detailed discussion prior work mentioned comment highlight experiment specific aspect recurrent architecture enables better recovery inverse problem also suggest claim mantle olving inverse problem consider broader inverse task painting deconvolution different noise model possibly working multiple observation like,3
702.json,present model extractive document summarization classifier architecture selector architecture model basically either classification ranking sequential order pick candidate sentence summarization experiment show either better close sota technical comment equation position relevant component call positional importance wondering important component possible show performance without component especially discussion impact document structure trained shuffled order tested original order similar question equation content richness component really necessary since score function already salience part could measure important respect whole document dynamic summary representation equation updating equation training test procedure test time actually know decision made decoder consistent training test think section interesting part also convincing difference architecture little disappointing decoding algorithm used simple minimal case could beam search could better,5
702.json,present architecture extractive document summarization first classifier take account order sentence appear original document whereas second selector chooses sentence arbitrary order architecture concatenated hidden state sentence forward backward pas used feature compute score capture content richness salience positional importance redundancy model trained supervised manner used pseudo ground truth generation create training data abstractive summary experiment show classifier performs better achieves near state performance evaluation metric proposed general extension cheng lapata unfortunately performance slightly better sometimes even worse mentioned difference transform abstractive summary become gold label supervised method however experiment described potential reason model consistently outperform extractive cheng lapata unsupervised greedy approximation generate noisier ground truth label cheng lapata reason construct training data similar cheng lapata turn better method order proposed model convincing need outperform baseline similar proposed method consistently since main contribution improved neural architecture extractive document summarization,3
578.json,empirically study invariance equivariance equivalence property representation learned convolutional network various kind data augmentation additional loss term presented make representation invariant equivariant idea measuring invariance equivariance equivalence representation lenc vedaldi first systematically study effect data augmentation property unclear surprising interesting useful really surprising data augmentation increase invariance training augmentation lead similar representation training different augmentation regarding presented method increase invariance equivariance could representation generalize better invariant equivariant clear want increase equivariance indeed lead improvement performance present evidence training increased invariance equivariance lead substantial improvement performance combined fact loss substantially increase computational burden think technique useful minor comment time equivaraince argmax properly formatted think data augmentation already considered essential krizhevsky really correct attribute claim related idea whether cnns collapse invariance linearize equivariance view manifold object idea equivariance mean manifold orbit linearized incorrect linear representation create nonlinear manifold simple example given rotation matrix clearly linear generating nonlinear manifold circle equivariance called equivariance value representation equivariant high equivariant also us paradigm us word paradigm strange manner definition inverted otherwise seems like transformation applied twice instead undone,3
578.json,work present empirical study influence different type data augmentation performance cnns also proposes incorporate additional loss function encourage approximate invariance equivariance show benefit read well objective clear study invariance cnns important topic advance area greatly appreciated split different part empirical study equivariances existing cnns proposal equivariance objective however taken separately part could better executed empirical study breath relatively limited hard draw reaching conclusion network studied least architecture made better generalization layer studied present issue layer invariant least convolutional layer possibly considered reliance text dataset help however imagenet definitely encouraging nice performance degrades degree transformation interpret better analysis limited conclusion drawn evaluating network jittered data could propose interesting way ass invariance equivariance potentially draw insightful conclusion proposed loss function quick treatment given section half page differ much known invariance equivariance objective studied literature previously decoste scholkopf training invariant support vector machine machine learning sure dividing different contribution best approach feel incomplete full treatment make overall better,4
651.json,approach like adaptive dropout also binary mask function input neuron similar proposed approach clear even draft proposed approach differs adaptive dropout term functionality experimental validation also extensive since comparison sota included,4
714.json,implement method jonschkowski brock learn dimensional state representation represented last layer neural network experiment apply method learning dimensional state representation simulated robot head position synthetic image learning state representation active useful area research learning representation interactive domain robotics however seems novelty method jonschkowki brock primary contribution experimental evaluation performed task evaluates correlation learned state representation ideal state representation task robot head position acknowledged experiment preliminary showing simple task dimensional learned representation dimensional discrete action space make experiment compelling need comparison prior method lange watter nip finn icra also learn state representation image image also useful comparison especially simple task without comparison impossible evaluate effectiveness method lastly mentioned review question related work include discussion state representation learning method watter nip finn icra hoof iros summary lack novelty significance implement existing method demonstrates simple task without comparison impossible interpret challenging task experimental comparison significantly improve additionally introduce novel contribution state representation learning solving challenge domain generally written clearly,2
714.json,proposes representation learning approach jonschkowski brock deep network function approximator general task approach interesting contribution work limited experimental evaluation absolutely unsatisfactory cannot accepted publication approach tested simple synthetic task small training test set little variation data admitted preliminary proposed method compared existing approach simple hand crafted baseline impossible judge proposed method useful performs well compared existing approach make unfit publication proper experiment method work interesting realistic scenario could become good,2
344.json,present learning algorithm micromanagement battle scenario real time strategy game focus complex problem full problem assumption restriction made greedy distance based action encoding clear make sense problem main contribution zero order optimization algorithm used structured exploration nice application zero order optimization meet deep learning quite well motivated using similar argument show clear win vanilla learning reinforce hard believe although interesting challenging domain certainly worthy domain focused research nice domain mainly seems algorithm could generally applicable game also evaluation complex domain make difficult predict kind domain benefit zero order approach maybe could text clarify motivate seemingly arbitrary choice justified worked practice example using sign theta later also neglected argmax operation chooses action suppose dividing could keep thing nicely within close might make sense truncating normalizing seems much information must lost taking sign also line extensively experiment structure network found maxpooling tanh nonlinearity particularly important claiming importance adagrad rmsprop without elaboration providing detail feel somewhat unsatisfactory leaf reader wondering could true setup presentation improved idea presented without context making unnecessarily confusing example defining tilde page vector explained reader left wondering come explained later course sentence role help contextualize purpose maybe refer later section described fully also page neglected single sampled entire episode actually mention text clear pseudo code perturbated perturbed response period rebuttal entered therefore review remains unchanged,6
344.json,interesting timely multiple contribution proposes setup dealing combinatorial perception action space generalizes arbitrary number unit opponent unit establishes deep baseline collection starcraft subdomains proposes algorithm hybrid black optimization reinforce facilitates consistent exploration mentioned earlier comment gradient average cumulative reward reasonable choice compared average reward weight late reward expense early one update matching measured objective state observe large difference preliminary experiment case choose correct objective characterized incorrectly despite name collect trace following deterministic policy instead follows stochastic behavior policy learns policy deterministic policy please revise gradient free optimization also characterized incorrectly scale parameter recent work shown overcome torcs koutnik also suggests preliminary experiment direct exploration parameter space followed best practice neuroevolution recent variant neat example applied similar domain past specific wondering transfer obtaining best rate transfer despite reaching worst training domain typo explain,6
344.json,work introduces starcraft micro management task controlling individual unit battle task difficult recent deeprl method high dimensional variable action space action space task unit number unit vary large action space simple exploration strategy epsilon greedy perform poorly introduce novel algorithm tackle problem algorithm combine idea policy gradient deep network trained backpropagation state embedding gradient free optimization algorithm well explained compared existing baseline gradient free optimization providing much better structured exploration performs better well written novel algorithm applied relevant problem success deeprl approach learning large state space visual environment significant interest applying structured state action space task introduced interesting environment sort problem helpful able share source code specification task allow group compare work found section detail input feature encoding somewhat difficult understand addition clarifying might wish consider whether could provide source code algorithm least encoder allow careful comparison work although discussed baseline comparison valued based approach attempt better exploration modeling uncertainty bootstrapped useful understand approach also promise better exploration compare also interesting discus whether action embedding model energy based approach,7
485.json,summary look ability neural network represent dimensional manifold efficiently embed lower dimensional euclidian space define class manifold monotonic chain affine space intersect hyperplanes separating monotonic interval space give construction embed chain neural network hidden layer also give bound number parameter required examine happens manifold noisy experiment involve looking embedding synthetic data monotonic chain using distance preservation loss experiment support theoretical bound number parameter needed embed monotonic chain another experiment varies elevation azimuth face known monotonic chain regression loss comment direction investigation looking happens manifold neural network compelling strongly encourage continue exploring direction however current version could work experiment regression loss shallow network part reason interest question large high dimensional datasets require deeper network seems important address case also seems important confirm embedding work well classification loss used instead regression theory section could clearly written familiar literature area proof method used relatively elementary difficult understand exactly proved formally stating could expected embedding accurately efficiently preserve monotonic chain,4
606.json,method training neural network mimic abstract data structure presented idea training network satisfy abstract interface interesting promising empirical support currently weak significantly strengthened method could shown useful realistic application shown work better standard approach algorithmic learning task claim mental representation well supported remove reference mind brain well philosophical point write really emphasizes aspect support claim,3
743.json,explore whether halting time distribution various algorithm various setting exhibit universality rescaling zero mean unit variance distribution depend stopping parameter dimensionality ensemble idea described universality interesting however several shortcoming order practical relevance actual stopping time might relevant scaled discussion exponential tailed halting time distribution good start sure often might actually helpful still finding might interesting theoretical point view especially iclr think interesting look comparison stochastic gradient descent momentum adam different deep learning architecture parameter universality hold different initialization influence halting time distribution expect sensible initialization part right tail distribution additionally found quite hard read clarity issue abstract even input changed drastically abstract sure input refers introduction stopping condition essentially time find minimum seem make sense condition time guess wanted stopping condition minimum reached notion dimension epsilon ensemble introduced without clarification later part idea example hard understand parameter example already helpful dot random sample training sample formulation make sense either random sample took long time find meaning parameter seems crucial universality case helpful point explicitly refers,4
743.json,summary several algorithm previous research shown halting time follows parameter distribution called universal property investigated work extend investigation algorithm spin glass gradient descent deep learning algorithm considered satisfy universality property centered scaled halting time fluctuation empirical distribution halting time depend algorithm depend target accuracy epsilon intrinsic measure dimension probability distribution random ensemble clear left empirical halting time distribution depends epsilon right approximation depends algorithm argue empirically universal property observed algorithm spin glass deep learning perform well observed perform well moment based indicator introduced ass whether universality observed review present several problem page sufficiently large dependence epsilon troubling page universality measure stability algorithm example halting time power method infinite expectation hence type universality present could conclude power method naive therefore presence universality desirable feature numerical method algorithm naive better way answer problem could conclude halting time infinite expectation solving problem extremely quickly time looping forever case infinite variance algorithm naive moreover universal property restrictive finite halting time expectation even many specific case finite halting time expectation desirable property showing presence universality desirable require demonstration restrictive aspect also desirable also paragraph concern algorithm conclusion generalise numerical method even universality property arguably desirable event conclusion paragraph assumed correct paragraph support given conclusion comparing figure universality mean centered scaled halting time fluctuation depend epsilon approximated distribution depends epsilon experiment varies figure validity approximation varying epsilon never tested ensemble distribution parameter halting fluctuation depend algorithm halting fluctuation allowed depend well defined especially common word optimisation setting told functional form landscape function part answer question reviewer part functional form computation landscape known functional form black conclusion claim attempt exhibit case answer question robust quantitative question condition ensemble lead universality quantitative moment based indicator however example universality observed concern algorithm conjugate gradient type failure demonstrate robustness method question constitutes good hyper parameter given algorithm proposed choose test whether universality observed hyper parameter good hyper parameter correspondance hyper parameter observing universality concern algorithm type failure algorithm fail universal regime perform well universal regime show answer question robust question beyond inspection tuning system question vague general probably robust quantitative answer question infer algorithm good match system hand fails demonstrate convincingly universality either good robust approach studied algorithm suggested generalisation system algorithm extremely fetched question connection universal regime structure landscape question extremely vague cannot answered robust quantitative fact corresponds corresponds clear help conclusion written validates claim universality present nearly sensible computation properly test whether universality present parameter vary tested properly test whether universality lost computation longer sensible failure case tested finally experiment apply nearly computation specific algorithm,1
411.json,interesting pleasant superoptimization extends problem approached stochastic search stoke learned stochastic search stoke proposal output neural network take program embedding input reinforce learn mcmc scheme objective minimizing final program cost writing clear highlight efficacy method comment question correct understanding entire stochastic computation graph feature proposal part learned rest still effectively stoke mcmc scheme imply uniform effectively stoke baseline probably made explicit consider learning feature instead using feature could difficult given relatively small amount data feature extractor might generalize different context markov chain monte carlo variational inference bridging salimans suggests considering mcmc scheme stochastic computation graph optimizing using variational criterion problem different us instead mcmc might worth citing similar approach meta optimized mcmc algorithm,7
411.json,thing really liked whole idea data dependent proposal distribution mcmc familiar although apparently previously published went back unreadable jampani informed sampling good perhaps good reason accepting iclr quite impressive rough rule thumb optimization help speed code standard mcmc presented randomly generated program roughly match fact proposed algorithm speedup quite surprising worth publishing argument accepting match goal iclr iclr hear generic machine learning paper nip icml instead learn automatically represent data model maybe talk represent generated program tangentially life umbrella iclr compete relevant paper conference poster sending programming language conference eventual impact nonetheless give accept learned something valuable good,6
692.json,bother responding fixing review comment hence repeat please make incredibly unscientific statement like working procedure like human being read text answer related question really human being lstm like read text cite actual neuroscience claim answer please delete statement future draft generally experiment simple classification method competing simple model like change title abstract introduction accordingly attempt hyperbole like learning understand title lastly attention level approach seems similar dynamic memory network kumar also experiment sentiment interesting understand difference compare reviewer included missing related work fitting context current literature given effort made review question feedback doubt become ready time publication,2
368.json,review proposes quantitative evaluation decoder based generative model annealed importance sampling estimate likelihood quantitative evaluation indeed much needed since model like generative adversarial network gans generative moment matching network gmmns qualitative evaluation sample still frequently used ass generative capability even though exist quantitative evaluation like kernel density estimation show accurate used perform fine grained comparison generative model gmms variational autoencoders report empirical comparing different decoder architecture trained continuous mnist dataset using gmmn objective also trained importance weighted autoencoder iwae binarized mnist show case iwae bound underestimate true likelihood least significant dataset according evaluation pro evaluation framework public definitely nice contribution community give insight behaves likelihood perspective disconfirm commonly proposed hypothesis memorizing training data also observed gans miss important mode data distribution con question clear sometimes experiment done using different number example coming different source trainset validset testset simulation generated instance table reported using example testing clear figure slower encoder number intermediate distribution independent chain seems literature salakhutdinov murray desjardins etal used chain could increasing number chain help tighten confidence interval reported table like give intuition bdmc nats order magnitude compared others minor comment table referenced text lack description different column represent figure reported value represents average likelihood total training validation example mnist described section figure guessing binarized mnist also fewer point compared iwae encoder bdmc gap mentioned section one reported table typo caption figure gmmn actually showing gmmn according graph title subcaption,6
387.json,investigates simple extension gatys based texture descriptor image generation similar gatys method us texture descriptor empirical intra channel correlation matrix feature response layer deep network differently gatys longer range correlation measured introducing shift correlated feature response translates simple modification original architecture idea simple interesting effect generated texture extended transformation translation longer range correlation could accounted considering response deeper feature original method gatys show modelling explicitly using shallower feature effective reasonable important limitation work share peer lack principled quantitative evaluation protocol judging effectiveness approach remains almost entirely qualitative affair considered significant drawback objective difficulty solving open issue nevertheless somewhat limiting principled evaluation method could devised implemented suggest future work possible evaluation method could based classification task potentially interesting approach merit investigation,6
503.json,introduces gated multimodal unit gmus multiplicative weight select degree hidden unit consider different modality determining activation also introduces dataset multimodal imdb consisting movie summary poster labeled genre gmus related mixture expert different example classified different part rather routing gating entire example individual hidden unit gated separately related attention model different part input weighted differently emphasis gating modality input dataset nice contribution many experiment varying text representation single modality modality lacking careful discussion experimentation analysis comparison multiplicative gate model core intellectual contribution example could imagine mixture expert attention model gated model might perform well least provide interesting scientific comparative analysis encourage continue work submit revised ready consider good workshop ready major conference,3
446.json,parallel work bigan idea using auto encoder provide extra information discriminator approach seems promising reported result,6
446.json,extends framework allow latent variable observed data expanded drawing latent variable conditional distribution joint distribution modeled using joint generator trained trying fool discriminator constitutes worthwhile extension gans giving gans ability inference open many application could previously addressed vaes promising cifar sample best seen counting method class label matching semi supervised salimans without feature matching also indicates proposed method improve stability training gans,7
780.json,present linear pipeline reduce approach parallel neural network multiple provides theoretical analysis experiment overall presented interesting writing improved comment compare proposed approach several alternative approach demonstrate strong performance proposed approach unclear improvement proposed approach implementation easy follow writing improved many place aside typo missing reference specifically provide intuition proposed approach introduction section proposition analysis section suggest communication cost linear pipeline approximately faster respectively claimed many place instead suggests cannot faster method time specifically show tbroadcasebe tbroadcaselp provide upper bound tbroadcaselp arbitrary worse comparing tbroadcasebe inequality therefore instead showing tbroadcasebe tbroadcaselp state tbroadcasebe tbroadcaselp approach infinity interesting emphasize difference designing parallel algorithm motivate,4
780.json,primary point made given certain architectural characteristic multi system namely directional communication integration independent engine recent device providing support simultaneous independent communication given characteristic communication pattern required synchronous trainer deep neural network namely message large dense fixed length make sense design communication collective broadcast reduce allreduce specifically case synchronous training multi system describes implementation three collective broadcast reduce allreduce using linear pipelining scheme logical ring topology compare collective alternative collective based minimal spanning tree topology collective based bidirectional exchange first theoretical comparison made using standard cost used high performance computing community assumption based multi system architecture latency message communication characteristic synchronous training large message integrated find collective le costly collective factor le costly collective factor number gpus used second empirical comparison performed time required perform different collective device system measured function message size time required perform different collective message length measured function number device system measurement show based collective consistently fastest third training experiment alexnet googlenet performed device system using three different synchronous algorithm different implementation collective total different algorithm measurement communication computation cost show collective reduce communication cost without affecting computation cost expected measurement convergence training loss function time architecture show collective lead faster training theory say cost collective invariant number device multi system empirical work show practice hold going device tested configuration device system message must traverse practical consideration aware affect scaling collective mentioned sentence worringen proposed pipeline collective shared memory environment data communication different process sharing memory within socket really figure word communication different process trying convey sentence comprehensible please note latency term smallest among algorithm table therefore suit high frequent short message claim collective suitable high frequency short message follow statement collective smallest latency term also need consider cost scale message size bandwidth term collective better bandwidth term collective also superior large message take appropriate block size ensure look wrong since however parameter consistent several iteration precision issue float multiplication gradient update sure inconsistency weight estimate across device multiplication expect gradient accumulated different order floating point addition commutative recommend replacing term gradient partial gradient optimization literature term gradient specific meaning differs term,5
515.json,present application tensor factorization linear model allows consider higher order interaction variable classification regression problem maintains computational feasibility linear dimension factorization employed based format first proposed oseledests also propose adoption riemannian optimization scheme explicit consider geometry tensor manifold thus speed convergence general well written present interesting application tensor format linear model together application riemannian optimization opinion quite interesting since wide range possible application different algorithm machine learning side concern experimental part consider level rest instance term number experiment real datasets role dropout real datasets comparison algorithm real datasets moreover take account explicitly problem choice rank used experiment general experimental section seems collection preliminary experiment different aspect tested organic think close weak acceptance weak rejection rate full acceptance mainly satisfactory experiment setting case extra experiment confirming goodness approach believe could much better score minor comment formula obvious comment learning parameter done also way depending approach using fact rank bounded formula explained lubich formula projection total cost since element summation rank cost ttrank rank wrong section explain random initialization freeze convergence seems interesting motivated guess section adopt dropout comment particular advantage give context exponential machine real datasets choose experiment validation section among variable section typo experiment section simplicity binarized think problem english language sentence section report dropout help quite general statement tested synthetic dataset section provide dataset instance term training inference time test algorithm,4
515.json,describes tensor factorization method called tensor train modeling interaction feature supervised classification task tensor train approximates tensor dimension using rank product matrix rank used parameter controlling complexity approximation experiment performed different datasets binary classification problem core consists demonstrating formalism developed could adapted modeling interaction feature another contribution gradient algorithm exploit geometrical structure factorization idea probably machine learning algorithm reasonable complexity inference could probably adapted large size problem although case experiment experimental section well structured incomplete could improved miss description datasets characteristic performance different datasets provided dataset used illustrating aspect could also provide classification performance comparison baseline experiment experiment datasets show optimization performance training could provide curve test set show algorithm generalizes comparison approach section performed artificial data designed interacting feature representative diverse situation hold role dropout comparison movielens dataset incomplete besides test performed small size problem overall original contribution could worth publication experiment incomplete conclusive detailed comparison competing method like factorization machine could also improve,5
450.json,like first apologize delay summary framework sample statistical test using binary classification proposed allows multi dimensional sample testing interpretability test lack theoritical analysis provided various empirical test reported interesting approach however main concern clarity presentation obscured much content interesting presentation could somewhat self contained could consider making paper seriously cram experiment setting experiment really explained supposed read jitkrittum radford okay reduces public example mistaken never explained despite fact performance reported second point given also number submission conference exploding like challenge following question work significant representation learning community,6
450.json,submission considers setting sample testing perspective evaluating classifier classifier sample distribution distribution classification accuracy follows simple form null hypothesis straightforward threshold derived classifier finding powerful test amount training better classifier focus effort deep neural network statistic difficult characterize approach sound general timely deep learning huge impact classification prediction setting impact statistical hypothesis testing kernel method discussion relationship kernel always realistic could example kernel also seen classifier based approach fair discussion could provided also form kernel used comparison contradictory discussion well linear kernel used le powerful quadradic kernel justified perspective computation time kernel argued unwieldy distribution null linear time kernel also zaremba nip gaussian distribution null arthur gretton comment discussion period insightful helpful insight additional experiment comparing kernel classifier threshold blob dataset could included helpful understanding open review format give excellent opportunity assign proper credit experiment insight citing comment,7
450.json,summary reconsiders idea using binary classifier sample testing idea split sample disjoint training test set train classifier training accuracy test test statistic accuracy chance level concludes sample different distribution reject theoretical result asymptotic approximate test power provided implication test consistent assuming classifier better coin tossing experiment problem evaluation gans causal discovery verify effectiveness test addition classifier neural examining first linear filter layer allows feature activated result interpretable visual indicator sample differ review summary well written easy follow idea using binary classifier sample testing made clear main contribution analysis asymptotic test power modern deep net classifier context empirical study various task empirical satisfactorily convincing although much discussion made method work well practice overall contribution potential start direction research criticism generative model well visualization fails vote acceptance major comment question main concern theorem asymptotic test power assumption understand fixed discussed distribution test statistic classification follows binomial stated however term independent identical bernoulli random variable term depends data point either paragraph random variable follows binomial correct essentially depends follow poisson binomial distribution paragraph reason alternative distribution binomial risk probably correct guess mention moivre laplace asymptotic normality anyway reason need statement binomial required proof asymptotic normality variant central limit theorem instead moivre laplace theorem independent identical variable still allow conclude asymptotic normality poisson binomial condition example,6
329.json,well written well presented method using denoise autoencoder learn implicit probability distribution help reduce training difficulty neat view joint training auto encoder providing extra auxiliary gradient information improve generator providing auxiliary information methodology improve extra comment please discussion ebgan next version,6
329.json,using denoising autoencoders improve performance gans particular feature determined discriminator image generated generator denoising reconstructed well think interesting idea extra information namely feature representation learned discriminator seems much spirit iclr main concern though wholly convinced nature improvement method achieves higher inception score method case hard time interpreting score thus hard time getting excited particular convinced benefit outweigh required additional sophistication conceptually implementation wise speaking code released thing curious know hard thing actually work also view gans mean particularly excited generating realistic image especially excited future potential based system nice improvement inception score translate improvement useful task criticism could probably apply many paper perhaps fair think idea exploiting extra information like discriminator feature interesting inside outside context,5
779.json,present several strategy select small subset target vocabulary work source sentence significant speedup convincing think offer practical value general seqseq approach language task however little novelty work mostly extend work vocabulary selection strategy thorough experiment better venue,3
779.json,evaluates several strategy reduce output vocabulary size order speed decoding training could quite useful practitioner although main contribution seem somewhat orthogonal representation learning neural network sure iclr ideal venue work reported decoding time take account vocabulary reduction step aside machine translation might application setting language modeling large vocabulary also scalability challenge proposed method helpful difficulty induced using word level least opinion starting character even lower level abstraction seems obvious solution huge vocabulary problem,4
542.json,study problem abstract hierarchical multiagent policy sketch high level description abstract action work related much previous work hierarchical add element using neural implementation prior work hierarchical learning skill representation sketch sequence high level symbolic label drawn fixed vocabulary initially devoid meaning eventually sketch mapped real policy enable policy transfer temporal abstraction learning occurs variant standard actor critic architecture experiment provided standard game like domain maze minecraft written suffers problem idea policy sketch nice sufficiently fleshed real impact useful spelled context abstract smdp model bring table get specialized invocation idea context specific approach proposed second experiment thorough enough term comparing related work example ghavamzadeh explored maxq like abstraction context mulitagent great detailed comparison maxq based multiagent approach value function explicitly decomposed,4
542.json,proposes architecture aim learning policy sketch sequence high level operation execute solving particular task relies hierarchical structure policy chosen depending current operation execute sketch learning algorithm based extension actor critic particular case also involves curriculum learning technique task solve hard experimental provided different learning problem compared baseline method well written easy follow really convinced impact since problem solved seen option learning problem richer supervision sequence option given thus corresponds easier problem limited impact moreover really understand concrete application setting corresponds example learning natural langage instruction clearly relevant since proposed article major contribution share many common idea existing hierarchical reinforcement learning method lack strong motivation concrete application marginal interest community pro original problem well design experiment simple adaptation actor critic method problem learning policy con simple task seen simplification complex problem like option discovery hierarchical learning instruction strong underlying application could help inforce interest approach,2
407.json,study problem transferring solution existing task tackle novel task framework reinforcement learning identifies important issue avoiding negative transfer selective transfer proposed approach based convex combination existing solution learned solution novel task negative weight solution implies solution negative effect ignored weight allocated relevant solution state derives called learning algorithm policy transfer value transfer reinforce actor critic algorithm experiment synthetic chain world puddle world simulation atari game pong present novel approach transfer reinforcement learning experiment cleverly designed demonstrate ability proposed method important aspect transfer learning algorithm automatically figure existing solution known task sufficient solve novel task save time energy learning scratch issue studied experiment learning scratch solution base network interesting well algorithm performs without base network addition figure proposed algorithm seems accelerate learning speed overall network seems better solo base network convincing show example existing solution complementary base network ignoring base network proposed network considered ensemble reinforcement learning take advantage learned agent different expertise solve novel task,6
407.json,tackle important problem multi task reinforcement learning avoid negative transfer allow finer selective transfer method based soft attention mechanism general demonstrated applicable policy gradient value iteration method introduction base network allows learning policy prior policy directly applicable state dependent policy selection allows finer control thought assigning state space different policy expert task relatively simplistic sufficient demonstrate benefit limitation method simple claim mostly empirical interesting extension option based framework stochastic hard attention mechanism policy pruning progressive network figure read curve seems perform worse rest term final performance perhaps alternative information figure attention mask activation statistic learning observe learns turn adversarial policy rely newly learned base policy mostly also generally good check weird adaptation happening,6
391.json,proposes method pruning weight neural network training obtain sparse solution approach applied based system trained evaluated speech recognition dataset indicate large saving test time computation obtained without affecting task performance much case method actually improve evaluation performance experiment done using state system methodology experiment seems sound like effect pruning investigated network large size computational gain clearly substantial unfortunate experiment done using private dataset even private training data nice evaluation known test like conversational speech also nice comparison pruning approach given similarity proposed method work verify relative merit proposed pruning scheme single stage training look elegant first sight save much time experiment needed find good hyperparameter setting threshold adaptation scheme finally dense baseline convincing involved compression trick like training soft target provided bigger network overall easy read table figure caption could detailed still clear enough discussion potential future speed sparse recurrent neural network memory saving interesting specific proposed pruning algorithm motivate detail method well clear threshold ramp certain period time example based preliminary finding mention sparse neural network subject research long time includes recurrent neural network sparse recurrent weight matrix standard echo state network proposed method also similar work threshold used prune weight training followed retraining phase remaining weight think certainly elegant replace three stage procedure single training phase proposed scheme still contains multiple regime resemble process first training without pruning followed pruning different rate finally training without pruning main novelty work application scheme rnns typically tricky train feedforward net improving scalability important driving force progress neural network research think present much novelty idea scientific insight show weight pruning successfully applied large practical system without sacrificing much performance fact possible simple heuristic result worth sharing pro proposed method successful reducing number parameter rnns substantially without sacrificing much performance experiment done using state system practical application con proposed method similar earlier work barely novel comparison pruning method data private prevents others replicating jaeger echo state approach analyzing training recurrent neural network erratum note bonn germany german national research center information technology technical report song pool jeff tran john dally william learning weight connection efficient neural network advance neural information processing system,5
391.json,summary present technique convert dense sparse network rnns algorithm increasingly weight zero training phase provides le storage requirement higher inference rate pro proposes pruning method need training affect training phase method achieves sparsity hence le number parameter con question judiciously choosing hyper parameter different model different application cumbersome equation sparsity final formula know sparsity number parameter accuracy final given hyper parameter going training question answered table trade number unit sparsity achieve better number parameter accuracy table better speed good sparse mean accuracy must similar still decent compression rate speed like sparse medium compared dense much advantage pruning getting high speed sacrificing much accuracy issue fixed updated data sparsity table table different text average sparsity table model used table different table issue fixed,6
391.json,updated review thanks including comparison previously published sparsity method comparison plausible though clearer state best comparison table sparse result table updated review reflect evaluation revised although also leaving original review place preserve history three main contribution proposes approach training sparse rnns weight falling given threshold masked zero schedule used threshold pruning applied certain number iteration performed threshold increase course training provides experimental baidu internal task deep speech network architecture showing applying sparsification large lead final trained better performance fewer zero parameter dense baseline provides timing experiment cusparse library showing potential faster evaluation sufficiently sparse model current cusparse implementation optimal pro mostly clear easy understand tackle important practical problem deep learning successfully deploy model lowest possible computational memory cost con second baseline compare distillation approach,5
309.json,proposes adding unsupervised auxiliary task deep agent like propose bunch auxiliary control task auxiliary reward task evaluate agent labyrinth atari proposed unreal agent performs significantly better also learns faster definitely good contribution conference however surprising result since adding additional auxiliary task relevant goal always help better faster feature shaping proof concept idea well written easy follow reader deep expertise comment computational resource needed train unreal agent overall architecture quite complicated willing release source code rebuttal change review,6
470.json,present approach modifies variational auto encoder framework stochastic latent dimensionality achieved using inherently infinite prior stick breaking process coupled inference tailored specifically kumaraswamy distribution approximate variational posterior resulting named also semi supervised extension similar vein original interest vaes day many line work seek achieve automatic black inference model example mention parallel work blei also others towards direction however merit investigating bespoke solution model indeed useful side effect providing efficient inference drawing attention kumaraswamy distribution popular although general well structured found confusing part think major source confusion come fact specification inference discussed somehow mixed manner review question clarified part main concern regarding methodology motivation firstly conditioning directly stick breaking weight seems little initially thought mixture probabilistic involved case fair discus issue became clearer review question explain investigating apparently challenging problem using base distribution question whether relaxation still useful experiment seems method least competitive answer hopefully extension come future mention second concern motivation method seems fails clearly explain convincing beneficial reformulate understand parametric property induced prior might result better capacity control however feel advantage potentially others still unclear sufficiently explained demonstrated perhaps comparison dropout approach thorough discussion related dropout make clearer overall found interesting good iclr,7
470.json,summary first work investigate stick breaking prior corresponding inference method vaes background material explained clearly well explanation prior posterior dncp form really well written experiment find stick breaking prior generally improve upon spherically gaussian prior completely unsupervised setting measured likelihood fact report negative result suggests good scientific taste semi supervised setting better comment plenty previous work gaussian draw generative resnet ladder vaes comma text flow please refer appendix closed form divergence sampled posterior sampled clear talking posterior instead prior last paragraph section great density estimation technically also mass estimation sample side figure interesting work well pixel,7
535.json,apply image captioning architecture video captioning extended attention multiple layer convnet instead single layer experiment youtubetext show work better using layer time think solid work level well executed course project workshop make sense adequately described experiment show attending multiple layer work better attending layer isolation unfortunately think enough excited technical perspective clear value brings community aspect including hard attention component seem take space want contribute detailed focused exploration multi level feature could become valuable case expect much thorough exploration choice tradeoff different scheme without many spurious aspect video feature hard attention,3
535.json,summary proposes video captioning based space time convnet encoder lstm decoder investigate benefit using attention mechanism operating spatio temporal layer feature abstraction level contribution well motivated implemented attention mechanism handle different shape feature map along space time feature dimension convincing quantitative qualitative experiment three challenging datasets youtubetext showing clearly benefit proposed attention mechanism interesting comparison soft hard attention showing slight performance advantage simpler soft attention mechanism case suggestion improvement hypercolumns comparison mentioned review question interesting compare hypercolumns,6
427.json,work propose interesting approach visualizing prediction made deep neural network manuscript well written provides good insight problem also appreciate application medical image simply illustrating point imagenet interesting enough question comment correctly point approximating conditional probability feature marginal distribution realistic advocate translation invariance position pixel image affect probability suggest pixel appearance depends small neighborhood around however well known global context make impact semantics pixel object context show given neighborhood pixel take different semantic meaning based global context image context deep neural network work parsenet also illustrate importance global context spatial label distribution necessarily invalidate approach significant limitation great provided modification empirically verified change figure show distribution prediction softmax expected even fairly uniform distribution transform toward delta function softmax normalization additional insight finally state take minute analyze single image goolenet computationally expensive complexity seems make algorithm impractical analyzing datasets statistical relevance seems prohibitive,5
562.json,proposes address mode collapsing problem gans training large generator discriminator pairing different one different time throughout training idea generator discriminator pair locked together since swapped idea nice addressing important issue training however think lacking experimental particular need work motivate metric intuitively obvious metric good evaluating generator network since relies prediction discriminator network fixate artifact perhaps could explore metric correlate inception score human evaluation currently quantitative evaluation us criterion really clear relevant quantity measuring related comment need compare method evaluate inception score compare previous method similarly generation quality compared previous method obvious sample quality better method repeating question review section instead swapping simply train gans split data gans differing initial condition without swapping improvement similarly train larger capacity model dropout since dropout essentially average many model interesting effect figure appears validation cost remain parallelization increase training cost go shrinking really imply better generalization summary interesting address important issue training compelling missing,3
348.json,present theoretical treatment transformation group applied convnets present empirical showing efficient usage network parameter basic idea steerability make huge sense seems like important idea develop also idea image processing go back simoncelli freeman adelson well perona greenspan others early approach formal treatment group theory idea seems pretty simple feature representation transformed image equivalent transformed feature representation original image given limiting analysis discrete group example rotation formality brought group theoretic analysis seem overkill sure buy seems real challenge lie implementing continuous transformation theory could guide direction immensely helpful also description experiment fairly opaque hard time replicating exactly term implementing capsule transformation group,5
348.json,propose parameterization cnns guarantee equivariance large family geometric transformation mathematical analysis rigorous material interesting novel overall read well real effort explain math accessibly though small improvement could made theory general enough include continuous transformation although experiment restricted discrete one could seen negative point justified experiment show transformation powerful enough yield good cifar another form intertwiner studied recently lenc vedaldi studied equivariance empirically cnns offer orthogonal view addition recent reference scale rotation deep network suggested geometric equivariance studied extensively mentioning least work appropriate probably come closest proposed method work reisert studied steerable filter invariance equivariance using group theory difference course focus time kernel machine rather cnns many tool theorem relatable notation could simplified make formula easier grasp first read working lattice unnecessarily abstract since input always image make much later math easier parse generalization straightforward think lose anything back latex later anyway could natural away layer index appears throughout notation current next layer instead instead case leave decide whether include suggestion notation urge consider way unburden notation minor issue statement better supported accompanying reference explicit formula exist page introduction intertwiners page finally tiny mistake balduzzi ghifary reference extra information included author name lenc vedaldi understanding image representation measuring equivariance equivalence reisert group integration technique pattern analysis kernel view,6
348.json,essentially present inductive bias architecture convolutional neural network mathematical motivation derivation proposed architecture detailed rigorous proposed architecture promise produce equivariant representation steerable feature using fewer parameter traditional cnns particularly useful small data regime interesting novel connection presented steerable filter called steerable fiber architecture strongly inspired author previous work well capsule hinton proposed architecture compared cifar state inspired architecture resnets shown superior particularly small data regime lack empirical comparison large scale dataset imagenet coco make largely theoretical contribution also liked empirical evaluation equivariance property intuitively clear exactly architecture performs better cifar clear capturing equivariances help classify different instance object category action recognition video example better illustrative dataset,7
718.json,reframes feed forward neural network multi agent system seems start wrong premise multi layer neural network created expressed full matrix multiplication ignores decade long history development artificial neural network inspired biological neuron thus started unit arbitrarily sparse connectivity envisioned computing parallel matrix formulation primarily notational convenience note also working sparse matrix operation convolution zero neither stored multiplied besides change terminology essentially renaming neuron agent find brings nothing interesting table pulling useful insight different communitiy multi agent system welcome compelling largely unheard element neural research clear supporting empirical evidence significantly improve accuracy efficiency achieved present,0
431.json,introduces analytical performance estimate training evaluation time given network different software hardware communication strategy clear included many freedom variable calculating time network number worker bandwidth platform parallelization strategy consistent reported literature furthermore code open source live demo looking good mentioned comment allow user upload customized network split coming release interface tool become useful interesting newer network architecture skip connection resnet densenet,6
431.json,paleo propose simple execution deep neural network turn even simple allows quite accurately predict computation time image recognition network single machine distributed setting ability predict network running time useful show even simple reasonably strength test performed network similar type alexnet inception setting much broader experiment including variety model rnns fully connected adversarial variety setting different batch size layer size node placement device probably reveal weakness proposed simplified reviewer considers borderline first step basic without sufficiently large experimental underpinning experiment added updating score,5
466.json,provides theoretical guarantee identity parameterization showing arbitrarily deep linear residual network spurious local optimum residual network relu activation universal finite sample expressivity well written studied fundamental problem deep neural network positive overall feel result quite significant essentially showing stability auto encoder given fact hard provide concrete theoretical guarantee deep neural network question extent result general nonlinear actuation function case minor line time,7
466.json,summary investigate identity parametrization linear linear case detailed comment linear residual network show linear residual network critical point global optimum problem convex interesting simple parametrization lead result linear residual network propose construction map point label resnet using initial random projection followed residual block cluster data based label last layer map cluster label seems dimension matching please clarify construction seems fine special resnet construction similar construction identity discus point linear case clear spectral point view identity helping optimization please provide intuition existence network residual class overfits give intuition residual network outperform architecture existence result network tell representation power simple linear assumption point close overfit data fast convergence rate instance tsybakov noise condition construction tell number layer clustering activation independently label pretrain network could centroid weight next layer also related nystrom approximation instance,4
489.json,present methodology analyzing sentence embedding technique checking much embeddings preserve information sentence length word content word order examine several popular embedding method including autoencoding lstms averaged word vector skip thought vector experiment thorough provide interesting insight representational power common sentence embedding strategy fact word ordering surprisingly entropy conditioned word content exploring sort information encoded representation learning method important researched area example tide word embeddings research mostly stemmed thread careful experimental showing embeddings essentially equivalent culminating improving distributional similarity lesson learned word embeddings levy goldberg dagan representation learning becomes even important sort research even important make valuable contribution setting exploring methodology evaluating sentence embeddings evaluation quite simple necessarily correlate real world desideratum sentence embeddings note comment performance task normative measure embedding quality example note ability averaged vector encode sentence length trivially expected given central limit theorem accurately concentration inequality like hoeffding inequality word order experiment interesting relevant citation sort conditional ordering procedure generating text recurrent neural network sutskever marten hinton refer conversion word sentence debagging although first step better understanding sentence embeddings important recommend publication,7
523.json,synopsis introduce efficient approximation softmax function speed empirical calculation softmax gpus leverage unbalanced distribution word specific empirical timing matrix multiplies gpus devise algorithm selects optimal placement vocabulary cluster show empirical show speedup alternative method losing much accuracy compared full softmax thought since goal work speed training curious compare flat level sqrt speedup best deeper binary tree speedup best overall clear easy understand well written notation issue pointed reviewer add interesting extra tool language modeling toolbox idea based several previous work optimize vocabulary clustering improve speed accuracy tradeoff often experienced practice hierarchical method interesting result seems particular clustering objective improves speed designed apparently losing much accuracy designed although speculate reason latter part suspect largely related fact flat region timing graph mean head group actually include sizeable portion frequent word vocabulary constant cost reduces approximation error region support papprox next previous compared preal turn mitigates perplexity compared full softmax however since method intimately related speed optimal method proposed zweig albeit without explicit tailoring towards feel direct comparison warranted understand underway performance accuracy improvement still hold update rating,5
523.json,provide interesting computational complexity driven approach efficient softmax computation language modeling based gpus adaptive softmax approach proposed based hierarchical dynamic programming applied optimize structure hierarchical approach chosen computational complexity based gpus however remains unclear robust specific configuration obtained dynamic programming performance perplexity corresponding comparative perplexity based clustering desirable especially paragraph baseline table respectively interesting result zweig afaik first successful application lstm based language large vocabulary published sundermeyer missing sumary prior work bottom mainly well written accessible though notation case improved detailed comment prior work lstm language modeling sundermeyer lstm neural network language modeling interspeech notation clearly defined constant notation reused matrix batch size notation kind misleading minor comment item list bottom first item take take second paragraph contained contain third paragaph associated associate first paragraph time time right right second term equation second term right hand side equation second last line smaller smaller itemize first item million million last sentence,6
671.json,proposes neural architecture called dragnn transition based framework dragnn us tbrus neural unit compute hidden activation current state transition based system prof dragnns cover wide range transition based method literature addition easily implement multitask learning system dragnns experimental show using dragnns built near state system task parsing summarization contains major part dragnn demonstration usage regarding first part proposed dragnn neat tool building transition based system however difficult whether dragnn novel transition based framework already well defined huge trend using neural network implement transition based system opinion difference stack lstm dyer dragnn slight course dragnn powerful architecture contribution considered mainly term software engineering second part used dragnn implement transition based system different multi task implementation neat confirming dragnn powerful architecture especially multitask learning however bear mind solution employed already literature thus making difficult judge novelty part theme conference,5
671.json,overall nice developing unifying framework newer neural model worthwhile endeavor however unclear dragnn framework current form significant standalone contribution main idea straightforward transition system unroll computation graph implement model reuse code module mixed matched nice opinion good software engineering machine learning research moreover appears little incentive dragnn free thing benefit using framework example write neuralnet automatic differentiation library tensorflow dynet gradient free framework efficiency trick credit assignment compiler provides tedious implement also variety algorithm training principled without exposure bias feel question limitation framework satisfactorily addressed different give example model nicely express dragnn framework wanted implement,4
558.json,introduces extending count based exploration approach domain count readily available hash function experiment conducted several domain including control atari nice confirmed bellemare given right density estimator count based exploration effective also great observe given right feature crack game like montezuma revenge extend however several complaint first using hashing seem able achieve significant improvement past approach without feature engineering achieved fraction performance achieved bellemare montezuma revenge proposed approach control domain also outperform vime experimentally hard justify approach second hashing although could effective domain tested best estimating density going forward environment complicated learning method required understanding environment instead blind hashing claim advantage proposed method bellemare design density estimator argue density estimator become readily available pixelcnn vaes real gans easily applied hashing training density estimator difficult problem,3
558.json,proposes exploration scheme reinforcement learning using locality sensitive hashing state build table visit count used encourage exploration style mbie strehl littman several point appealing approach first quite simple compared current alternative vime density estimation pseudo count second present across several domain including classic benchmark continuous control domain atari game addition comparison several algorithm variant many quite recent indicate approach clearly improves baseline exploration algorithm clear dependent individual domain game think fine appeal technique simplicity third present sensitivity granularity abstraction main complaint seems engineering involved work much confidence robustness conclusion left uncertain story change given slight perturbation hyper parameter value enabling disabling certain choice example critical using pixelcnn tying weight noisifying output autoencoder happens remove custom addition bass granularity show choice resolution sensitive even across game story consistent decide state based count instead state action based count deviating theory reason used first place closer mbie advise tabular count several explanation state based versus state action based count perform similarly atari offer seems like technique could easily used well many variant compare based omitting seems strange justify choice trpo saying ensures safe policy improvement though clear still true adding exploration bonus case study montezuma revenge interesting involves using domain knowledge really well rest simple elegant idea help exploration tested many domain though certain many piece critical story hold versus slightly helpful could hurt long term impact response thank thorough response apology late reply appreciate follow version robustness simhash state counting state action counting address important problem exploration suggesting simple compared density estimation counting method hashing nice alternative approach offered bellemare discussion among reviewer possible assemble argument accept specifically concerned beating state montezuma reviewer merit current simplicity hashing wide comparison domain baseline trpo show give simple hashing still seems bunch fiddly bit work still confident easily reproducible nonetheless interesting contrasting approach exploration deserves attention important decision argument rebuttal concerning straw mention anything strictly referred le sensitive parameter tuning also bellemare main result montezuma used hence omission technique applied still seems strange atari experiment figure mnih point instance asynchronous step sarsa varied thread count course sensitive parameter asynchronous online algorithm parameter varied thread count hardly indicative sensitivity parameter since single threaded us experience replay leading slower policy change another source stability us target network change infrequently perhaps made mistake reference graph figure figure,5
763.json,manuscript proposes approach modeling correlated timeseries combination loss function depend neural network loss function correspond data term autoregressive latent state term term capture relation pair timeseries relation given prior information modeling relational timeseries well researched problem however little attention given neural network community perhaps reason importance uncertainty representation correctly identify need consider approach considers distribution state space formulation quite straightforward combining loss function add ziat certain aspect well motivated unfortunately implemented unconvincing start uncertainty treated principled since inference rather naive expect employing framework better uncertainty handling furthermore gaussian variance collapse variance opposite want modelling correlated time series approach take correlation account state moreover treatment uncertainty allows linear decoding function significantly reduces power state method timeseries modeling moved beyond constraint especially gaussian process community comparing method least discussing useful reference kingma welling auto encoding variational bayes arxiv damianou variational gaussian process dynamical system nip mattos recurrent gaussian process iclr frigola bayesian time series learning gaussian process university cambridge thesis frigola variational gaussian process state space model nip innovation prior structure correlation need given potentially useful also original structural component however also constitutes limitation sense since unrealistic many scenario prior information moreover particular regularizer make similar timeseries closeness state space seems problematic timeseries group might similar others also similarity might different nature across group variation cannot well captured distilled simple indicator variable furthermore variable practice taken binary looking experiment make even harder rich correlation experiment show proposed method work entirely convincing importantly shed enough light different property different part example effect sensitivity different regularizers state review answer amended revision openreview please know missed performance point view particularly exciting especially given fact clear loss better making difficult method practice also interesting report optimized value parameter lambda idea different loss behave timeseries analysis well researched area given clear prefer approach methodology wise novel component offer proven advantage respect past method uncertainty state correlation time series aspect could advantage adequately researched,3
763.json,absence response rating maintained introduces nonlinear dynamical multiple related multivariate time series model linear observation conditioned latent variable linear nonlinear dynamical consecutive latent variable similarity constraint time series provided prior data learnable prediction constraint given three component gaussian predicts mean variance covariance matrix inference forward evaluated four datasets compared several baseline plain auto regressive model feed forward network dynamic factor graph dfgs rnns forward backward inference latent variable introduces lateral constraint different time series predicts mean covariance seems interesting present limitation first refer variational auto encoders deep gaussian model also predict mean variance inference secondly datasets extremely small example contains time series time point although experiment seem suggest proposed tends outperform rnns datasets small high variance indicates experiment longer time series required could also easily extended information architecture well time complexity comparison model especially dfgs minor remark footnote page seems refer structural regularization term dynamical term,3
626.json,summary look structure preimage particular activity hidden layer network prof particular activity preimage piecewise linear subspace pro formalizing geometry preimages particular activity vector increase understanding network con analysis seems quite preliminary novel theoretical clear practical conclusion main theoretical conclusion seems preimage stitch lower dimensional subspace direct inductive approach worked working backwards penultimate layer definitely interesting direction great depth width affect division space happens training seem ready,3
626.json,read revised version detail summary study preimages output feedforward neural network relus pro present neat idea change coordinate individual layer con quite unpolished enough contribution finished comment first version contains many typo appears still quite unpolished contains nice idea opinion contribute sufficiently many conference happy recommend workshop track irreversibly mixed several notion present closely related concept discussed montufar pascanu bengio nip feel cited connection discussed particular also contains discussion local linear map relu network curious practical consideration computing image definition rather straight forward really implementation computation could troublesome detailed comment page easily shown many general page point parenthesis superscript missing mapping unique missing linearly independent vector collected weight vector bias period missing page illustrate preimage case point line respectively please indicate figure sketch actual illustration network latter case please state specific value weight depicted also define explain arrow precisely arrow gray part page mean preimage point point map page first display equation index left right hand side quantifier right hand side clear generated mapping subscript mapped hyperplane zero remaining remaining using grassmann cayley algebra using elementary linear algebra give rise linear manifold dimension lower intersection hold hyperplanes general position complete input space form basis remaining kernel remaining kernel kernel referring nullspace matrix orthonormal basis vector nullspace specifically figure nullspaces linear map pas origin pairwise intersection indicated arrow shaded area description clear typo peieces diminsions netork,3
626.json,really appreciate direction taken think something quite interesting come hope continue path able come something quite interesting soon however feel right quite ready clear work preimage construction obviously least helpful feel like right direction point identify underlying mechanism behind model know relu model need split apart unite different region space think agree construct mechanism come fact relu model universal approximators though speak happens practice think work need work,3
630.json,proposed incremental idea extend current state summarization work based seqseq model attention copy pointer mechanism introduces pas reading representation pas used wight contribution word sequential representation pas described called read process applies lstm decoder side also softmax choose generating decoder vocabulary copying source position twist representing previous decoded word differently allows author explore smaller decoder vocabulary hence faster inference time without losing summarization performance claim state comparison gigaword seems incomplete missing recent rush overall work solid also thing missing scientifically example much computational cost pas reading system decoder small vocabulary trick work without pas reading encoder side summarization performance runtime speed way improve embedding sentence pas reading compare recent work multiple self attention lstmn example cheng long short term memory network machine reading parikh decomposable attention natural language inference,5
519.json,think build upon previous work attempt something similar batch norm specific rnns experiment convincing think clear work better layer norm significantly convinced significant speed either appreciate faster feel like order magnitude faster theoretical analysis also provide insight think good incremental work maybe significant enough iclr,5
688.json,present iterative power policy variation power policy gradient algorithm reward weighted family familiar enough type lower bound scheme comment look like result le conservative step size policy parameter space expectation based algorithm regularized cousin trpo take smallish step might sensible accelerate description experiment section insufficient reproducibility cart moved right supposed positive force applied cart negative force applied representation state distribution initial state linear policy insufficient swing balance cart pole balancing noise magnitude policy chosen long episode footnote bottom page threw using newton method discussion gradient hessian thought argmaxtheta operator stand style step read kober,2
688.json,considers problem reinforcement learning number policy update required problem well motivated author provides interesting modification power algorithm along variational bound value function lemma interesting also provide numerical cartpole problem problem online advertising real data overall strong well written main reservation whether completely appropriate iclr since concavity assumption relies appear restrict simpler model representation fact learned comment general lack baseline numerical experiment section acknowledge somewhat unusual setting even simple well justified baseline welcome since cartpole relatively simple problem advertising dataset presumably private perhaps generate synthetic advertising dataset interesting confused control variate constant scalar meant constant baseline appear treated hyperparameters learned estimated interesting section constrained optimization feel disconnected rest appears applicable problem online advertising mentioned corresponding experimental section also might worth adding citation literature constrained mdps develops similar lagrangian idea,6
688.json,present interesting modification power algorithm well motivated main limitation lack comparison method richer problem experiment given confidence show claimed benefit generality scalability prior method giving confidence necessarily require running method large scale domain exhaustic hyper parameter search example could beyond current domain cartpole optimizes parameter targeting task lack comparison alternative method since method built power closely connected likely limit performance become apparent method tried domain benchmark method even standard one like importance sampling based policy learning known suffer high dimensional continuous action space limit power like method based connection entropy regularized,4
372.json,memory module based presented well written convincing omniglot good sanity test performance surprisingly good artificial task show claim hold highlight need better benchmark domain translation task eventually make strong point practical usefulness proposed specialist memory network trust double check relevant reference included another reviewer mentioned associative lstm besides think nice useful hope publish code,7
372.json,proposes memory module used addition existing neural network model pro clearly written original idea useful memory module show nice improvement tested task con comparison memory module associative lstms,6
667.json,read response maintain rating introduces approach integrating direct acyclic graph structure data word code embeddings order leverage domain knowledge thus help train scarce data applied code medical visit code part ontology represented code correspond leaf node different code share common ancestor leaf node instead embedding merely leaf node also embed leaf node embeddings code ancestor combined using convex convex seen attention mechanism representation attention weight depend embeddings weight meaning separate learning code embeddings interaction code embedding code pretrained using glove fine tuned properly evaluated medical datasets several variation isolate contribution gram gram randomdag pretraining embeddings gram gram shown help achieve best performance evaluation methodology seems thorough also well written case attention instead plain product embeddings made comment softmax equation given loss multivariate cross entropy predicted visit several code could equal single class cross entropy embedding dimension,5
667.json,summary present method enriching medical concept parent node ontology method employ attention mechanism parent node medical concept create richer representation concept rationale infrequent medical concept attention mechanism rely general concept higher ontology hierarchy frequent one focus specific concept attention mechanism trained together recurrent neural network accuracy tested task first task aim prediction diagnosis category time step second task aim predicting whether heart failure likely happen step show proposed work well condition data insufficiency overall judgment proposed simple interesting idea presented worth expand also point could done better learning representation concept ontology naive example could used kind knowledge base factorization approach learn concept graph convolutional approach general factorization method knowledge base apply case ontology learning also found strange representation leaf fine tuned inner node specific reason regarding presentation clear qualitative evaluation insightful detailed comment figure please image format resolution,5
536.json,summary present study number hidden unit training example needed learn function particular class class defined boolean function upper bound variability output pro promotes interesting theoretical computer science community investigate efficiency representation function limited variability term shallow feedforward network linear threshold unit con analysis limited shallow network analysis based piecing together interesting however without contributing significant innovation presentation main conclusion somewhat obscure therein appearing term constant express clear relation increased robustness decreasing number required hidden unit comment abstract read universal approximation theorem neural network say reasonable function well approximated layer neural network sigmoid gate provide good bound number hidden layer node weight page point reader review article could good idea include also recent reference given motivation presented abstract good idea also comment work discussing class boolean function representable linear threshold network instance hyperplane arrangement separating arbitrary vertex class cube wenzel paseman discus various class function represented shallow linear threshold network provides upper lower bound number hidden unit needed representing various type boolean function particular also provides lower bound number hidden unit needed define universal approximator certainly good idea discus learning complexity term measure dimension thank explanation regarding constant noise sensitivity kept constant larger value epsilon associated smaller value delta epsilon nonetheless description theorem term poly epsilon delta still could increase also lemma reducing sensitivity constant noise increase bound fact description independent seems related definition noise sensitivity expectation input certainly deserves discussion good start could discus example function upper bound noise sensitivity aside linear threshold function discussed lemma also reverse statement lemma interesting describing noise sensitivity junta specifically even simple example page variable polynomial noise sensitivity parameter inverse minor comment page proposition lemma,4
536.json,work find connection bourgain junta problem existing circuit complexity approximation boolean function using layer neural think finding connection different field applying insight gained valid contribution reason recommend acceptance current major concern work constrained domain boolean hypercube done practice continuous domain indeed could argue understanding former first step connection suitable case adaptable general scenario probably limited interest,5
561.json,present simple method constructing visual hierarchy imagenet class based trained discriminate class investigates metric measuring inter class similarity softmax probability output class confusion matrix distance feature along three method constructing hierarchy given distance matrix approximation central point minimal spanning tree multidimensional scaling borg groenen claimed contribution construct biology evolutionary tree give insight representation produced deep network regarding motivation work grounded biology practice method based visual similarity constructed tree thus expected reflect evolutionary hierarchy fact quantitative experiment demonstrate regarding technical depth exploration sufficient iclr sure conclude beyond fact cnns able group category together based visual similarity deeper network able better shallow network summary unfortunately ready publication time,2
424.json,show relation stochastically perturbing parameter training time considering mollified objective function optimization aside found hard understand weak gradient exactly represents intuitive subsequent section clearly establishes given class mollifiers equivalence minimizing mollified loss training gaussian parameter noise introduce generalized mollifiers achieve sophisticated annealing effect applicable state neural network architecture deep relu net lstm recurrent network resulting annealing effect counterintuitive section binomial bernoulli parameter grows deterministic identity layer deterministic relu layer meaning network go initially phase adding noise might effectively reverse effect annealing annealing scheme used practice seem engineered algorithm determines unit activated given layer consists successive step conceptual nature contribution various annealing scheme proposed application mollifying framework original could useful reserve portion analyze simpler model basic generalized mollifiers example liked simple case perturbation scheme derived mollifier framework demonstrably suitable optimization standard heuristically defined perturbation scheme,5
577.json,proposes linear classifier probe informativeness hidden activation different neural network layer training linear classifier affect training neural network well motivated investigating much useful information good representation layer observation agrees existing insight many random layer harmful training helpful lower layer converge faster higher layer deep network hard train skip link remedy problem however following problem sufficiently justified linear classifier good probe crystal clear good intermediate feature need show high linear classification accuracy theoretical analysis intuition helpful provide much insight design better network based observation designing better network also best justify usefulness analysis overall tackling interesting problem technique linear classifier probe novel importantly need better justified moreover important show design better neural network using observation,3
609.json,previous literature us data derived adjacency matrix obtain neighbor foundation graph convolution propose extending neighbor additionally including node reachable step graph introduces extra tunable parameter need justification previous solution experiment provided merk using worked better specify used enough node obtained neighbor second experiment mnist used experiment previous work coat proposed well compelling experiment compare show using give improvement strong enough justify extra hyper parameter,2
609.json,update thank comment reading decided increase rating proposes variant convolution operation suitable broad class graph structure node graph neighbour devised mean random walk neighbour ordered expected number visit result graph transformed feature matrix resembling matlab caffe imcol output convolution becomes matrix multiplication although proposed convolution variant seems reasonable convinced empirical evaluation mnist experiment look especially suspicious think dataset appropriate demonstration purpose case order make method applicable data remove important structural information relative location pixel thus artificially increasing difficulty task time comparing approach regular cnns conclude former performs poorly even reach acceptable accuracy particular dataset guess justify presence mnist similar datasets experimental section modify method incorporate additional graph structure relative location node case relation node cannot fully described similarity matrix believe current form ready publication later resubmitted workshop another conference concern addressed,5
609.json,work proposes convolutional architecture graph like input data structure example dependent generally data input dimension related similarity matrix instead input example associated transition matrix random walk algorithm used generate similarity matrix developing convolutional recurrent architecture graph like data important problem like develop neural network handle input molecule structure social network however think work contributes anything significant work already done area main proposal data associated transition matrix proposes transition matrix converted similarity matrix seems obvious data associated similarity matrix nearest neighbor node computed supply context information node also seems obvious perhaps misunderstood contribution presentation also lack clarity cannot recommend publication specific comment page interesting attribute convolution compared convolution graph preserve locality still applicable different graph different structure false proposed architecture applied input different structure duvenaud lusci architecture molecule specifically,2
520.json,proposes extension pixelcnn method conditioned text spatially structured constraint segmentation keypoints similar reed except extension built pixelcnn instead reading author comment realized argument conditional pixelcnn much better conditional think discussion pro con agree reviewer analysis training generation time helpful understand take instead pixelcnn method sampling main reason experiment conducted resolution also think since quantitative comparison make sense show visualization example overall generated look reasonably good enough diversity color mistake issue author provided explanation comment technical novelty incremental since extension straightforward similar previous work lean toward accepting relevant iclr community provides good opportunity future investigation comparison different deep image synthesis method,4
520.json,work focus conditional image synthesis autoregressive framework based pixelcnn train model condition text well segmentation mask keypoints experiment show keypoint conditional synthesis bird human pose dataset segmentation conditional synthesis coco object extension keypoint segment conditioning primary contribution existing pixelcnn work qualitative comparison made approach synthesis pro demonstrates additional capability image generation autoregressive framework suggesting keep pace latest capability gans qualitative comparison figure suggests pixelcnn based method make different kind mistake pixelcnn robust introducing artifact effort forth establish quantitative evaluation term likelihood table con comparison work difficult limited qualitative qualitative still somewhat difficult interpret believe supplementary material appendix many additional example could partially alleviate problem extension pixelcnn conditioning additional data fairly straightforward solid engineering contribution surprising concept,6
520.json,first allows ass whether auto regressive model able match reed though resolution resolution limitation addressed second last paragraph figure show picked randomly picked favorable pixelcnn hardly seems conclusive segmentation mask keypoints pretty strong input constraint hard tell much coherent object scene detail emerging resolution example cow figure look like color blob basically color blob follows exact segmentation mask look like amount variation impressive though ass much replaying training data figure try wonder much bird instance mostly copied particular training example unsatisfied answer review question answer question benefit concrete number training time epoch testing time make high resolution comparison slower test time prohibitively slow slightly inconvenient really many comparison anyway take hour generate result seem prohibitive clear bias think pixelcnn right deep image generation texture synthesis method used causal neighborhood success clear optimization globally kwatra texture optimization example based synthesis first alternative seems simply incorrect make hard decision pixel value part image synthesizing another part image another texture synthesis strategy help fight back strict causality coarse fine synthesis deep image synthesis method exploring seems much correct deeper network output pixel conditioned pixel conditioning implicitly emerge intermediate part network said could totally wrong advantage stated could outweigh disadvantage feel honest disadvantage overall think somewhat tantalizing especially ability generate diverse output resolution extremely especially compared richness input network get hand holding rich input least learn obey deep image synthesis literature moving quickly field need move proof concept paper first show particular result possible thorough comparison opportunity depth comparison deep regard really apple apple comparison pixelcnn conclusion statement impossible large scale comparison either qualitative quantified user study quality,5
465.json,present interesting detailed study targeted targeted adversarial example cnns fence leaning towards acceptance detailed empirical exploration difficult time consuming construct serve important stepping stone future work length strength since allows depth look effectiveness transferability different kind adversarial example however concern length strength mind contribution made much clear evidenced comment earlier confused point ensemble ensemble method contribution clarifai evaluation focusing strongly suggest radical revision clearly focus story first demonstrate targeted attack easy targeted attack hard evidenced experiment comparing refer appendix later section extensive exploration current section thus propose ensemble method able handle targeted attack much better evidenced experiment focusing comparison ensemble ensemble method controlled setting clarifai also detail exploration instead using resnet resnet three five model better resnet architecture alexnet network network make ensemble compelling,5
465.json,present experimental study robustness state cnns different type attack context image classication specifically attack aim fool classification system specially corrupted image making misclassify image wrong class targeted attack target class chosen advance attacker targeted attack instance attacker could corrupt image ostrich classified megalith even though attacker agenda clear example still interesting study weakness current system view improving general actual risk autonomous vehicle mostly experimental short compare different strategy already published previous paper popular network googlenet resnet aforementionned type attack experiment well conducted clearly exposed convincing point attack also conducted clarifai black classification system analysis insightful explanation also provided help understanding cnns prone attack section main finding targeted attack easy perform even black system targeted attack difficult realize existing scheme propose approach vastly improves existing attack even though still perfect success rate clarifai versus previous scheme arguably still weakness treating resnet based network different obviously clearly correlated table instance naturally expected architecture similar depth varies hence sound fair state interesting finding first misclassified label targeted model except googlenet three resnet based network subjective measure employed evaluate effectiveness attack black system good reason clarifai return image label different imagenet certain reported number fair even though qualitative look convincing novelty proposed approach optimizing ensemble network instead single network limited however really point effective seems overall quite long expected extensive evaluation study still suggest prune near duplicate content section high overlap section benefit additional discussion recent related work fawzi nip section indeed work fawzi mostly theoretical well aligned experimental finding observation particular section conclude think somewhat useful community could help improve existing architecture well better ass flaw weakness,6
465.json,reviewed manuscript december summary investigate transferability adversarial example deep network confirm transferability exists even large model demonstrate difficult manipulate network adversarially perturb image specifically desired label additionally demonstrate real world attack vision service explore geometric property adversarial example major comment contains list many clear single message provides mentioned comment effectively page page appendix heavily discussed throughout main body although strict page limit conference feel push spirit conference publication rule acceptance based length hold negative clarity presentation important quality ultimately accepted suggest make effort length even beyond page posted elsewhere marked section highlight area trimmed section geometric understanding similar adversarial perturbation deep neural network warde farley goodfellow figure clear show beyond additional finding emphasize expand observation goodfellow szegedy demonstrating large scale model susceptible adversarial perturbation also kurakin additionally demonstrate attempting perform adversarial manipulation convert image particular desired label difficult demonstrate target real world vision compelling clear demonstrate beyond papernot understand think interesting result previously described literature note unique difficulty performing adversarial manipulation convert image particular desired label rest appear expand already appeared literature need better explain make unique beyond previous work area trim table necessary cite write number text condense section cite heavily figure panel overlaid highlight comparison,4
388.json,summary introduces question answering called dynamic coattention network extract dependent representation document question us iterative dynamic pointing decoder predict answer span proposed achieves state performance outperforming published model strength proposed introduces concept model using attention direction dynamic decoder iterates multiple answer span convergence maximum number iteration also present ablation study proposed show importance design choice interesting idea attention performing well different domain visual question answering machine reading comprehension performance breakdown document question length figure strengthens importance attention task proposed achieves state result squad dataset architecture clearly described weakness future thought provides performance maximum number iteration like performance change number iteration performance number clear trend type question able correct iteration many deep learning approach overall architecture seems quite complex design choice seem driven performance number future work might analyze qualitative advantage different choice proposed type question correctly answered attention mechanism instead attention single direction using maxout highway network instead simple preliminary evaluation novel state question answering approach clearly described detail thought clear accept,7
388.json,summary proposes novel deep neural network architecture task question answering squad dataset consists main component coattention encoder dynamic pointer decoder encoder produce attention question well document parallel thus learns dependent representation question document decoder predicts starting token answer iteratively motivation multiple iteration help escape local maximum thus reduce error made proposed achieved state result squad dataset time writing report analysis performance across question type document question answer length also performs ablation study performing single round iteration decoder strength well motivated main motivation attending document question iteratively producing answer proposed architecture novel design choice made seem reasonable experiment show proposed outperforms existing time writing squad dataset significant margin analysis ablation study performed someone request provide insight various modelling design choice made weakness question suggestion order gain insight much additional iteration decoder help like following every iteration report mean question converged iteration along number question converged iteration example question figure interesting example unable decide multiple local maximum despite several iteration could please report often happens order estimate much modelling attention encoder help good could report performance attention modeled encoder neither question document like variation performance proposed question require different type reasoning table squad provide insight strength weakness proposed type reasoning required wang jiang attention predicted question word document table performing ablation study make proposed similar wang jiang attention document word question similar wang jiang attention think similar wang jiang attention since attention question word document please clarify section swapped explaining document question encoding matrix please review summary present novel interesting task question answering squad dataset show outperforms existing model however gain insight functioning like analysis ablation study weakness section,7
388.json,proposed dynamic coattention network question answering task long contextual document able encode dependent representation question document dynamic decoder iteratively pointing potential answer span locate final answer overall well written although complicated coattention encoder iterative dynamic pointering decoder highway maxout network intuition behind detail clearly presented also performance squad dataset good recommend accepted,7
737.json,strength interesting proposal smaller architecture designed embedded application balanced exploration macroarchitecture microarchitecture fire module le memory usage alexnet keeping similar accuracy strong experimental weakness nice test multiple task lack insight rigorous analysis factor responsible success squeezenet example resnet googlenet connected current architecture another analysis correlation structure neural predictive application speech recognition neural network also showed pas architecture mixing linear nonlinear prediction term improves long term dependency based rigorous perturbation analysis current work placed rigorously theoretical analysis,4
737.json,summary present smaller architecture called squeezenet embedded deployment explores macroarchitecture microarchitecture develop squeezenet composed fire module pro achieves le memory usage alexnet keeping similar accuracy con question complex pas le accuracy simple pas simple pas like resnet bottleneck complex pas like inception module googlenet valiants squeezenet adaptation concept seen googlenet resnet squeezenet like achieves similar accuracy compared googlenet resnet,6
367.json,present novel look binary auto encoders formulating objective function reconstruction error training given observed intermediate representation author show formulation lead convex problem solved alternating minimisation method part trivial main contribution proof concept experiment performed showing improvement hidden layer auto encoders respect vanilla approach experimental section fairly weak literature auto encoders huge many variant shown perform better straightforward approach without complicated denoising auto encoders present analysis lead learning algorithm problem likely worth discussing,6
367.json,author attack problem shallow binary autoencoders using minmax game approach algorithm though simple appears effective well written sound analysis although work extend deep network immediately connection popular minmax approach gans could fruitful future,6
625.json,present architecture corresponding algorithm learning across multiple task described natural language proposed system hierarchical closely related option framework however rather learning discrete option learns mapping natural instruction embedding implicitly dynamically defines option novel interesting perspective option slightly explored linear setting comment find policy distillation particularly relevant setting could takeaway many reader might necessarily interested application general describe single simple recipe learning architecture rather relies many recent advance skillfully combined generalized advantage estimation analogy making regularizers regularization memory addressing matrix factorization policy distillation liked analysis understand certainly easy task example parameter subtask controller frozen sound like kind timescale stochastic gradient descent also unsure deal smdp structure gradient update move temporal abstraction setting inclined believe approach potential scale large domain currently demonstrate empirically like typical reviewer tempted perform larger experiment however also glad shown system also performs well domain characterization figure insightful make good point analogy regularizer need hierarchy overall think proposed architecture inspire researcher worth presented iclr also contains novel element subtask embeddings could useful outside deep community traditional community parameterized option sutton explore concept parameterized option originally came later perhaps first optimal policy switching algorithm reinforcement learning comanici precup unified inter intra option learning using policy gradient method levy shimkin konidaris also line work parametrized skill learning parameterized skill silva konidaris barto reinforcement learning parameterized action masson ranchod konidaris also feel important distinction made expression parametrized option work parametrized come flavor spirit policy gradient method option whose policy termination function represented function approximators function approximation value function option parameter might call parameterized setting comanicy precup levy shimkin bacon precup mankowitz mann mannor example second case option policy skill take parameter input accordingly konidaris mean parameterized whose meaning differs function approximation case work embedding subtasks argument input option therefore behave parameter sense konidaris related work ctrl could find reference branavan work branavan thesis using control technique order interpret natural instruction achieve goal example reinforcement learning mapping instruction action agent learns window troubleshooting article interact element environment softmax policy linear feature learned policy gradient method mention instruction execution focus work generalization treated explicitely afaik branavan work still share important algorithmic architectural similarity discussed explicitly perhaps even compared experiment baseline zero shot uvfa might also want consider learning shared representation value function multi task reinforcement learning borsa graepel shawe taylor section zero shot task generalization minor issue first read abstract without knowing confused second sentence talk longer sequence previously seen instruction know clearly meant instruction second last sentence specify instruction described natural language could perhaps order sentence make clear second sentence interested problem zero generalization familiar term shot zero shot second sentence similar zero shot follows first sentence might well hold shot setting could perhaps citation zero shot define explicitly beginning compare shot setting could also useful explain zero shot relates notion learning prior section cooperate sound much like multi agent setting work explore might want choose different terminology explain precisely connection multi agent setting second sentence section long difficult parse could probably split three sentence,6
625.json,present hierarchical algorithm solves sequence navigate task maze domain training evaluation list goal represented text given agent goal learn learned skill order solve list goal demonstrate method generalizes well sequence varying length well combination goal agent know pick diamond visit apple also visit diamond overall high technical quality present interesting trivial combination state advancement deep learning deep reinforcement learning particular present agent hierarchical sense learn skill plan using skill learned using differential temporally extended memory network attention mechanism also make novel analogy making parameter prediction however find difficult understand presented problem interesting solved since domain evaluated simple maze using deep network well motivated similar problem solved using simpler model particular reach literature planning skill ignored completely since skill trained prior evaluation hierarchical agent problem solved much similar supervised learning reinforcement learning since using trained skill reward particularly delayed generalization demonstrated seems limited breaking sentence describing subtask word item location action difficult read constantly switching describing algorithm giving technical detail particular find overloaded detail interfere general understanding suggest moving many implementation detail appendix self contained please assume reader familiar method introduce relevant notation believe benefit addressing problem described make better contribution community future conference,2
760.json,discus method learn interpretable hierarchical template representation given data illustrate approach binary image present novel technique extracting interpretable hierarchical template representation based small standard operation shown combination standard operation translates task equivalent boolean matrix factorization insight used formulate message passing technique shown produce accurate type problem summary present novel formulation extracting hierarchical template representation discussed form unfortunately experimental smaller scale data extension proposed algorithm natural image seems trivial quality think technique could described carefully better convey intuition clarity derivation intuition could explained detail originality suggested idea reasonable limited binary data point time significance since experimental setup somewhat limited according opinion significance hard judge detail main concern related experimental evaluation discussed approach valuable application seems limited binary image point time comment existing technique extract representation image want mention work based grammar,4
760.json,present approach learn object representation composing template leaned binary image particular hierarchical learned combining pool operation learning performed using approximated inference product follow heuristic threshold activation binary learning hierarchical representation interpretable interesting topic brings good intuition light modern convolutional neural net however concern fails cite discus relevant literature claim first able learn interpretable part like discussion proposed approach compared variety paper compositional hierarchy sanja graph used alan yuille object template song chun group ucla claim first discover part removed experimental evaluation limited datasets paper mentioned applied real image using contour binarize image also like good proposed approach classification well known benchmark comparison generative model gans also useful also like discussion relation difference advantage proposed approach product network grammar comment claim learning inference feed forward since message passing used recurrent network algorithm tech discussion moved appendix main introduction claim compression prove understanding disagree statement removed also like discussion relating proposed approach deep rendering obvious constraint satisfied message passing also constraint well known difficult optimize product handle learning inference algorithm seems heuristic clipping heuristic message could analyze choice make multiple step single backward pas reconsider score light answer,4
760.json,present generative binary image image composed placing binary feature location image feature together produce image hierarchical variant feature class possible template active variable defined control template present layer joint probability distribution feature appearance instance location variable defined overall goal work interesting satisfying semantically meaningful feature could extracted allowing compositionality generative image however clear necessarily result proposed process learned feature building block necessarily semantically meaningful motivating example text rather discovering letter feature could correspond many unit part letter feature lacking direct semantic meaning current instantiation limited model binary image pattern experiment done synthetic data mnist digit method recovers structure effective classification synthetic data directly compositional mnist data test error quite large worse except synthetic data corruption added work enhance ability method handle natural image naturally occuring data variation enhance,3
449.json,present strategy building deep neural network rule expansion merging network pro idea novel approach described clearly con experimental evaluation convincing improvement svhn number parameter mentioned model fair comparison effect drop path seems vanish data augmentation,5
449.json,proposes architecture explicitly residual construct architecture composed network fractal structure using expand join operation using fractal architecture argue demonstrate large nominal network depth many short path training ultra deep network residual incidental main bottleneck number parameter needed fractalnet significantly higher baseline make hard scale ultra deep network replied wide resnets also require many parameter case resnet resnet variant resnet resnet stochastic depth scale depth parameter depth parameter much le number parameter depth table huang clear whether fractalnet perform better depth reasonable computation report le parameter layer scaling trick validated depth including depth table hand number parameter layer scaling trick clearly still large compared baseline unsatisfactory comparison baseline make claim unconvincing also claim drop path provide improvement compared layer dropping procedure huang however show empirical gain specific regularization disappears well known data augmentation technique applied therefore empirical effectiveness drop path convincing densenets huang also included comparison since outperforms state net cifar imagenet importantly outperforms proposed fractalnet significantly requires significantly le computation table variant baseline however table resnet therefore imagenet comparison show fractalnet imagenet perform comparably well resnet satisfactory result given improvement baseline resnet addition improvement svhn dataset discussed empirical analysis also give list improvement inception szegedy intuitive claim effectiveness change supported empirical analysis although attempt explore many interesting intuitive direction using proposed architecture empirical support given claim large number parameter make restrictive practice hence contribution seem significant pro provides interesting architecture compared resnet variant investigates difference residual network stimulate promising analysis con number parameter large compared baseline even much higher depth smaller number parameter claim intuitive supported well empirical evidence path regularization yield improvement data augmentation used empirical show whether method promising ultra deep network,4
449.json,proposes design principle computation block convolutional network based repeated application expand join operation resulting fractal like structure primarily experimental evaluation since objective show residual formulation necessary obtain good performance least task however opinion evaluation convincing primary issue lack proper baseline improvement clearly demonstrated making isolated change understand baseline hard construct since novel architecture principle effort core insight useful even better performing architecture discovered number parameter amount computation used indicate fair comparison architecture detailed comment table comparison resnets resnets wide resnets compared fractalnet lieu proper baseline first outperforms fractalnet cifar second outperforms compare without augmentation perform additional experiment without augmentation architecture layer fractal compared model unless parameter reduction trick utilized model well proper comparison inception network also performed network guess reason behind seemingly design inception module reduce computational footprint central motivation fractal net since directly related inception module shorter longer path without shortcut easily simplify inception design build strong baseline converting concatenation operation mean operation among equally sized convolution output aside note inception network already shown residual network necessary obtain best performance noted residual highway architecture type anytime property shown lesioning experiment srivastava viet architecture specific drop path regularization interesting used along regularizers dropout batch norm weight decay benefit clear overall clear experiment clearly demonstrate utility proposed architecture szegedy christian sergey ioffe vincent vanhoucke inception inception resnet impact residual connection learning arxiv preprint arxiv,5
633.json,proposes interesting idea connecting energy based descriptor generator network help sample generator used initialization descriptor inference revised sample descriptor turn used update generator target image proposed idea interesting however think main flaw advantage architecture convincingly demonstrated experiment example reader expect quantative analysis initializing sample generator help also quantative experiment reconstruction also compared quite model considering quite close bengio reader also expect comparison minor wondering analysis convergence sound considering fact sample sgld biased sample fixed step size explain also dependent,3
633.json,introduces coopnets algorithm train deep energy descriptor help auxiliary directed bayes generator descriptor trained standard maximum likelihood langevin mcmc sampling generator trained generate likely sample single feed forward ancestral sampling step thus used shortcut expensive mcmc sampling hence reference cooperative training idea interesting novel unfortunately sufficiently validated experimental first foremost three experiment feature train test split ignore standard training evaluation protocol texture generation datasets also much small experiment seem confirm ability overfit third painting task baseline almost existent vaes rbms make difficult evaluate benefit proposed approach future revision also encourage answer following question experimentally impact missing rejection step langevin mcmc train without impact generator burn process markov chain show sample auto correlation approximation training generator tilde instead tilde tilde comparative experiment also greatly benefit rewrite focusing clarity instead hyperbole pioneering work reference closely related peer reviewed work prose tale net example fail specify exact form energy function seems like glaring omission pro interesting novel idea con improper experimental protocol missing baseline missing diagnostic experiment heess williams hinton learning generative texture model extended field expert,2
633.json,proposed joint training scheme probabilistic model signal image deep neural network based termed generator descriptor network scheme termed cooperative training network train together assist generator network provides sample work initial sample descriptor network descriptor network update sample help guide training generator network interesting approach coupling training model however quite weak empirical study particular training datasets tiny set image reason using larger set think small datasets leading training really masking true value proposed cooperative training approach experiment presented hard ass specific value brought proposed cooperative training approach baseline missing comparison provided face completion experiment even comparison descriptor generator network trained separately deep auto encoders missing thus hard conclude much gain obtained cooperative training individually training descriptor generator network another comment related work section think relation variational auto encoders kingma welling included despite limitation mentioned think idea presented intuitively appealing worth discussing iclr considerably strengthened adding relevant baseline addressing training data size issue,5
776.json,proposes method iteratively improving output existing machine translation identifying potential mistake proposing substitution case using attention based motivated method assumed human translator operate interesting imaginative however general term somewhat sceptical kind approach whereby machine learning method used identify correct prediction another method first case method better outset place method second case since method information compared previously likely identify past mistake correct identify past correct term turn error unless specific reason iterative approach shown converge better solution several epoch convince point indeed unsurprisingly note probability correctly labelling word mistake remains admittedly beat random chance baseline compared something meaningful simply contrasting existing system powerful convolutional labelling discrepancy mistake oracle experiment rather meaningless serve confirm improving translation easy existing mistake identified much harder although like whole really convince main objective iterative improvement beneficial satifactorily demonstrated necessary include stronger baseline particular show iterative refinement scheme really improve system closely matched attention based used isolation used system combination pbmt system demonstrate pbmt system simply acting regulariser attention based minor comment find notation excessively fiddly time matrix surely length slice dependent discussion section seems still creates mismatch training test condition could anything done,4
776.json,disclosure expert machine translation algorithm summary human translator come final translation right away instead us iterative process starting rough draft corrected little little idea behind implement similar framework automated system generally well written opinion however drawing illustrating architecture help understanding different algorithm relate another like report preliminary experiment give intuition difficult task highlight link task finding error guess translation task iterative refinement could post edited text solid ground truth main concern experimental section iterative approach try improve upon type machine translation immediately prompt question choose approach improve part improvement come choice initial draft maybe draft minor typo lookup table replace word might mistanken seems used different thing confusing take input representation output,6
664.json,proposes interesting application framework steganography domain addition normal discriminator steganalyser discriminator receives negative example generator positive example generator image contain hidden payload result generator learn generate realistic image fooling discriminator also learn secure container fooling steganalyser discriminator method tested training independent steganalyser real image generated image given iclr community many people familiar literature steganography think provided context exactly method used practice related work setganalysis secure message embedding probably thorough set experiment dataset proposed sgan framework figure make sense think general application steganography domain clear fooling steganalyser discriminator necessarily mean fool independent discriminator also find surprising different seed value make huge difference accuracy short idea interesting potentially useful think presentation improved becomes suitable iclr machine learning community,4
664.json,found original thought provoking also difficult understand exciting practical case image generating gans potentially meaningful benchmark aside subjective realism found interesting introduces potentially differentiable black function stego training fact backprop stego function train test split sgan trained image cleaner split training sgan steganalysis purpose could account sensitivity random seed shown table steganographic generative adversarial network potentially used universal tool generating steganography container tuned deceive specific steganalysis algorithm experiment showed sgan fool hugo tuned deceive hugo could tuned general particular steganalyzer although seems fooled proposed method general image generation discriminator almost never fooled contemporary gans never converge actually fooling discriminator even produce sample sometimes fool human created additional steganalyzer think extremely difficult fool reliably requires realistic image generation reading several time still unclear precisely trained sgan think could greatly improved detailing step step workflow hypothetical user trained sgan description aimed reader know nothing little steganography iclr attendee,5
371.json,proposes neural physic engine provides factorization physical scene composable object based representation predicts future state given object function composition pairwise interaction near object nice physical interpretation force additive investigated context world ball obstacle overall approach interesting interesting flavor combining neural network basic property physic overall seems like lead interesting significant follow work field concern mainly evaluation place appears weak significance originality approach interesting method tried build model deal physical prediction idea summing pair wise term best knowledge novel much line underlying principle mechanic relatively simple seems important contribution clarity generally well written however large portion early introduction rather abstract difficult parse get paragraph suggest editing early part introduction include specific approach even example make text tangible experiment generally issue experiment opinion added indirect comparison fragkiadaki appears quantitatively flattering respect proposed approach quantitative experiment role size mask performance really added mention observe mask helpful clear helpful sensitive overall performance parameter experiment really added feel despite mentioned shortcoming make stronger interesting published,6
721.json,train generative image patch dictionary element undergo gated linear transformation combined transformation motivated term group operator though practice fixed linear transformation motivated strongly term learning hierarchy transformation though layer used experiment except case appendix like motivation algorithm realization seems similar group block sparse coding implementation disappointed restriction linear transformation experiment case demonstrating algorithm learn group gabor center surround like feature somewhat underpowered five year seemed extremely small today standard specific comment based common practice literature strong bias think input network weight latent variable often depending target audience suggest permuting choice symbol reader quickly interpret number equation easier reference weird transformation fixed still written function updated text confuses actually thought using fixed linear transformation motivating term group actually taking matrix exponential algorithm equation second half section suggest working matrix exponential though sure direction confused probably good clarify text either another possible solution local minimum difficulty used sohl dickstein introduce blurring operator matched transformation operator gradient descent escape local minimum detouring coarser blurred scale believe degree freedom mean number parameter number latent coefficient must inferred make clear appropriate compare reconstruction error matching number parameter number latent variable wonder convolutional version algorithm practical make suited generative whole image post rebuttal update thank taking time write rebuttal read significantly effect rating,4
721.json,proposes approach unsupervised learning based modification sparse coding allows explicit modeling transformation shift rotation opposed simple pooling typically done convnets shown training natural image demonstrating algorithm learns feature transformation data comparison traditional sparse coding show represents image fewer degree freedom seems like good interesting approach work seems like still early formative stage rather complete work compelling punch line example motivation like represent pose along identity object work seems well goal quite leaf dot still connected also number thing clear central idea seems transformational sparse coding tree make tractable inference group parameter exactly done clear example sentence main idea gradually marginalize increasing range transformation suggestive clear need much better defined mean marginalization context connection group operator tree leaf weight clear learning rule spell gradient group operator used learn leaf tree clear left imagination especially confusing although group operator introduced earlier stated tractable inference many local minimum motivates tree approach instead clear learning group operator stated averaging many data point smoothens surface error function understand average many data point seems transformation data train generated generate patch known transformation show recover please explain shown figure look interesting given lack clarity difficult interpret understand mean significance encourage rewrite clearly also work developing idea seem promising,3
721.json,sparse coding introduced learns feature jointly transformation found inference image transformation variable hard suggest tying variable across data point turning global parameter using multiple transformation feature furthermore suggested tree transformation path tree generates feature multiplying root feature transformation associated edge layer tree achieves similar reconstruction error traditional sparse coding using fewer parameter nice addition literature sparse coding literature learning transformation model identify deal difficult inference problem occur transformation model said skeptical usefulness general approach take given learning sparse feature transformation jointly important goal never really argued demonstrated experiment seem like method enables application extends understanding learning pathway brain improve ability natural image claim extract pose information although explicitly capture transformation relates different feature tree test time inference performed sparse coefficient associated feature transformation combination like sparse coding clear gain knowing coefficient associated transformation especially since many model general split good check actually change significantly initialization value loss surface still look pretty even tied transformation actually move much proposed work better according measure compared fixed chosen reasonable range parameter value either randomly spaced evenly conceptually interesting aspect idea tree transformation advantage deeper tree never demonstrated convincingly look like gotten approach work data vertical horizontal bar finally clear method could extended multiple layer transformation operator defined first layer input space cannot done learned feature space also clear pose information processed hierarchical manner learning deep version work summary recommend publication clear problem solved method moderately novel novel aspect convincingly shown beneficial,3
490.json,proposes augmenting based language model pointer network order deal better rare word pointer network point word recent context hence prediction time step mixture usual softmax output pointer distribution recent word also introduces language modelling dataset overcomes shortcoming previous datasets reason score gave find proposed direct application previous work gulcehre follows similar approach machine translation summarization main difference find gulcehre encoder decoder architecture attention weight encoder point location word input used pointer network produce distribution full vocabulary summing softmax probability word recent context context query vector pointing network also different also direct consequence different application describes difference proposed approach gulcehre approach find claim either wrong significant example quoting section rather relying hidden state decide pointer recent work gulcehre allow pointer component decide softmax vocabulary sentinel tell also us recent hidden state form query vector matched pointer network previous word please clarify mean addition quoting section describes gulcehre rather constructing mixture work switching network decide component correct gulcehre also mixture sigmoid output switching network used form mixture softmax prediction location input text finally following quote also section pointer network used source information switching network clear mean source information fact switching probability part pointer softmax wondering significant difference regard proposed dataset also datasets typically used language modelling including hutter prize wikipedia enwik dataset hutter text dataset mahoney please comment difference dataset well happy discus point raised open changing vote misunderstanding part,6
490.json,work extension previous work pointer model mix output standard softmax output idea appealing general context biasing specific approach appears quite simple idea novel extent previous already tried combine pointer based standard model mixture clearly written seem promising dataset created wikitext also seems high interest comment regarding notation symbol pptr used different way pptr pptr confusing different domain domain word domain list context word helpful different symbol object,7
306.json,describes approach meta learning interpreting update rule gated recurrent trainable parameter idea original important research related transfer learning clear structure clarity could improved point pro interesting feasible approach meta learning competitive proper comparison state good recommendation practical system con analogy closer grus lstms description data separation meta set hard follow could visualized experimental evaluation partly satisfying especially effect parameter interest much value remark small typo mean coordinate plan releasing code used evaluation experiment certainly major plus,5
613.json,proposes causality score weight sparsity regularizer selected variable trade causal discriminative framework primarily evaluated proprietary health dataset dataset give good motivation problem setting fall short iclr lack additional controlled experiment relatively straightforward methodology given approach chalupka arxiv preprint interesting technical perspective paucity theoretical motivation core approach effectively weight sparsity regularizer causal variable determined separate objective likely selected generally good idea proper validation experiment ground truth absent theorem identifiability causal discriminative variable data sample combined adequate synthetic experiment probably sufficient example push towards accept technical perspective lacking insight reproducibility,4
613.json,extend method causal discovery chalupka include assumption sparsity regularization apply extension interesting private dataset sutter health interesting direction found presentation somewhat confused methodological novelty smaller bulk iclr work central perhaps data inadequate address question causality first found presentation somewhat unclear point seems entirely focused healthcare data point us motivating example point neglected also algorithm seems unreferenced entirely sure needed figure needed community methodological advance work appears section causal regularizer introduced amidst example without clear terminology standard methodological assumption build section bottom first paragraph data seem relegated appendix thus overall read rather haphazardly finally seems assumption throughout fairly intimate familiarity cholupka preprint think avoided stand alone second technical contribution novelty focus presentation concerned lack methodological advance essentially regularization objective added previous method idea point technical novelty community without third fundamentally experiment address central question causality show regularization behaving expected rather influencing weight expected think really meaningful quantitative evidence causality learned briefly discussed ground truth causality response appreciate technical challenge impossibility dataset case think work premature since really validate overall clearly sincere effort found wanting term critical area,3
582.json,proposes combining different modality product content review text image purchase info order learn unified product representation recommender system idea combining multiple source information indeed effective approach handling data sparsity recommender system reservation approach proposed modality necessarily relevant recommendation task item similarity example cover image book movie product type experiment tell much content clearly motivate show different modality contribute final task connection proposed joint product embedding residual network awkward original residual layer composed adding original input vector output several affine transformation followed linearity layer allow training deep neural network layer result easier gradient flow contrast pairwise residual unit add product item vector product vector applying simple linearity motivation architecture obvious well motivated minor point choice term embedding product item usual embeddings usually refer vector specific entity refers final output render output layer figure pointless finally believe improved focusing motivating architectural choice concise description currently long page strongly encourage shorten,2
582.json,proposes method combine arbitrary content recommender system image text various feature previously used improve recommender system though novel contribution general purpose framework combine arbitrary feature type positively idea combining many heterogeneous feature type ambitious fairly novel previous work certainly sought include various feature type improve though combining different feature type successfully difficult negatively aspect particular piece glued together build system different part trained separately combined together using another learning stage nothing wrong thing indeed straightforward likely work approach push contribution toward system building direction opposed learning direction focus conference make hard easily generalize arbitrary feature type audio video feature describing item incorporate feature system require implementation work opposed system throw feature expect work review comment address issue response entirely convincing better baseline across table rather dropping case already made elsewhere like effort combine several different feature type real recommender system datasets entirely sure strong baseline seem like ablation style experiment rather comparison state,4
351.json,present application deep learning genomic data comparison possible approach dealing high data dimensionality approach look interesting experiment limited draw firm conclusion strength different approach presentation benefit precise math quality basic idea interesting applied deep learning methodology appears reasonable experimental evaluation rather weak cover single data limited number cross validation fold given significant variation performance method seems difference better performing method probably statistically significant comprehensive empirical validation could clearly strengthen clarity writing generally good term biology mathematical rigour make easier understand precisely done different architecture explained intuitive level might benefit clear mathematical definition ultimately left unsure endend given parameter cannot work dimensional input could figure kind embedding used might clearer scaled maximum class avoid confounding different number subject different class text please standard italic math font symbol originality application approach appear quite novel significance clearly strong interest deep learning genomics area seek address major bottleneck early tell whether specific technique proposed ultimate solution least provides interesting idea others work comment think releasing code promised must,5
351.json,address important problem deep learning proposed approach based lower dimensional feature embeddings reasonable make applying deep learning method data large possible well written show improvement reasonable baseline,6
701.json,proposed perform finetuning augmentation fashion freezing original network adding aside idea interesting complement existing training finetuning approach although think baseline approach compared ensemble principle idea similar ensembling approach multiple network ensembled together final prediction approach figure compared ensemble baseline taking multiple source domain predictor possibly modular setting proposed method compare performance comparison late fusion combine pretrained network network finetuned pretrained late fusion basically think valuable argument section figure finetuning small amount data hurt performance general build ground freezing pretrained network augmenting changing agree argument although currently figure seem little empirical study justifies worth noting figure seems suggest module filter either converging learning unuseful feature like first filter overall think interesting idea love better developed thus giving weak accept recommendation confidence experiment section convincing,5
701.json,present technique adapting neural network task training data widely used current technique fine tuning idea instead learn network learns feature complementary fixed network additionally consider setting network feature stitched various level hieararchy rather parallel tower work similar spirit detail progressive net rusu already discussed motivation experiment certainly different submission merit idea learning residual stitched similar spirit resnet work nice compare contrast approach never seen batch used time training work better regular figure nice label axis figure also benefit simply emulating figure much readable figure untrained immediately obvious good idea simply fine tuning layer retrain softmax think section weak usage network definitely like change using smaller claim throughout purpose added connection layer learn complementary feature show figure latter convinving evidence proof guarantee actually happening suggest consider adding explicit constraint loss encourages soft orthogonality constraing assuming project intermediate feature common feature dimensionality usage small regularization maybe achieves thing evidence visualization happens question reading ensemble trained net task consider especially relevant car classification example suspect strong baseline fine tuning task fine tuning resnet task possibly training linear combination output averaging naively disappointing figure except one really hard situate actually know compare previously published general interesting potentially useful piece work problem efficiently reusing previously trained classifier retraining small certainly interesting community think take good step right direction fall short dimension comparison serious baseline understanding,5
644.json,address important problem namely improve diversity response applaudable show several task showing applicability across different problem view weakness point improvement essentially task seem rather minor really overall claim approach seems quite unclear something widely adopted said gist proposed solution seems interesting somewhat premature,3
644.json,summary present modified beam search algorithm promotes diverse beam candidate well known problem rnns also neural language model beam search tends generate beam candidate similar cause separate related problem search error beam search able discover globally optimal solution easily fall beam early simple common diverse output resulting output text tends generic common aim address second problem modifying search objective function distinct term score diversity among beam candidate word goal presented algorithm reduce search error original objective function contrast stack decoding future cost estimation common practice phrase based address search error problem merit think diverse beam search algorithm proposed merit useful cannot rely traditional beam search original objective function either trained strong enough search error objective align goal application weakness however entirely clear proposed method compare traditional approach like stack decoding future cost estimation task like machine translation compare algorithm mainly diverse model simple beam search fact modification objective function applied even neural context example equation page following google neural machine translation system bridging human machine translation,5
594.json,author proposes rank matrix feedfoward rnns particular approach feedforward highway network author also present contribution passthrough framework describe feedforward recurrent network however framework seems hardly novel relatively formalism introduced lstm highway network empirical evaluation performed different datasets mnist memory addition task sequential permuted mnist character level penntreebank however problem evaluation highway network experiment author compare baseline ass impact rank parameterization also interesting compare result highway network capacity bottleneck across layer first layer size second layer size third layer size gate function also select hyperparameter value unfortunate character level penntreebank experimental setting previous work prevents direct comparison also overall perplexity seems relatively high dataset therefore clear rank decomposition perform task applied stronger baseline author claim state memory task however approach us parameter urnn memory make comparison little unfair toward urnn informative rank performs using overall parameter generally good impact matrix rank given state size informative well baseline urnn curve figure memory addition task clear rank rank diagonal experiment overall evaluation current form really convincing except sequential mnist dataset,3
594.json,proposes rank version pas network better control capacity useful case shown experiment said found convincing overall overall good state sequential mnist memory task hyper parameter tune said help show table figure competing approach like urnns,4
594.json,study rank approximation matrix multiply rnns reduces number parameter large factor diagonal addition called rank plus diagonal shown work well fully parametrized network number task solid weakness claim conceptual unification first line conclusion presented framework unifies description various type recurrent feed forward neural network passthrough neural network claiming framework contribution untrue general framework well known community rnns presented aside small point true contribution making rank rnns work generally good fully parametrized network hardly better though make unclear rank network used contribution thus strong term even achieving fewer parameter easy study well executed explained,5
717.json,analyze trained neural network quantifying selectivity individual neuron network variety specific feature including color category pro clearly written good figure think executed specific stated goal reasonably well technically various index seem well chosen purpose con must admit biased whole enterprise think well motivated provides useful insight whatever view done produced summarized anecdotally catalog piecemeal fact neural network without larger reason think particular fact important feel like suffers problem plague typical line research neurophysiology catalog selectivity distribution various neuron various property produced full stop important useful information feel either original neural version project current based virtual electrophysiology useful care distribution color selectivity knowing distribution constitute understanding mind least directly could done make useful investigation neuroscience point view could compared property measure model property measured neuron real brain could show model better match property actual neural data others really interesting result isolated catalog selectivity neuron real neuron alone seem pretty pointless correspondence catalog made term neuron real neuron similar especially importantly different beginning nontrivial understanding also complement growing body literature attempt link cnns visual brain area finding good neural data challenging whatever result comparison interesting artificial intelligence point view could shown metric prescriptive constraint suppose shown specific color class selectivity index compute imposed loss function criterion untrained neural network cause network develop useful filter achieve significantly chance performance original task network trained really great result give priori reason care specific property metric chose also help contribute effort find unsupervised semi supervised learning procedure since metric compute estimated comparatively small number stimulus high level semantic label perspective imagine actually tested hypothesis found false metric used loss function constraint improve performance noticeably chance performance make whole investigation reasonable think measured property essentially epiphenomenal contribute power neural network solving perceptual task could said neurophysiology experiment thing actually tried thing like year found exactly disappointing result specifically found number high level generic statistical property dnns seem like might potentially interesting apparently correlate complexity appear illustrate difference intermediate high layer dnns every single imposed optimization constraint basically lead nowhere challenging task like imagenet cause dnns interesting first place basically mind evidence point highly summarized generic statistical distribution selectivity like illustrated place interesting constraint filter weight course tried specific property highlight paper maybe something important know asks pretty hard know else work otherwise seems like step backwards community ought spending time,2
717.json,attempt understand visualize deep net representing ascends level high level network shown previously lower level local image feature based whereas higher level correspond abstract property object identity semantic space find higher level node semantically selective whereas level node diffuse seems like good attempt tease apart deep representation perhaps important finding color figure prominently level network performance gray scale image significantly diminished measure proposed sensible still based image shown network really want know function node computing space possible image activate unit course difficult problem nice getting closer understanding answer color analysis think brings closer semantic analysis nice sure insight gain,6
469.json,detail implementation sparse full convolution work potential speed various sparsity level cnns first contribution engineering make source code available greatly appreciated second contribution perhaps interesting pruning method focused saving memory modest speed gain imbuing knowledge running speed pruning algorithm seems like proper tackle problem build evaluate thoroughly seems idea could used pruning existing model also building architecture selecting layer parameter achieve optimal throughput rate could make nice direction future work point missing discussion transferable performance gpus make technique easier adopt broadly area improvement point figure hard distinguish small circle small square overall figure could made bigger specifying whether base learning rate section start rate annealing schedule typo punning spare,6
469.json,provide well engineered solution exploiting sparsity convolutional layer deep network recasting sparse matrix vector multiplication lead nice speedup analysis possible also useful practitioner main concern research aspect seems rather minimal mostly performance engineering comparison upto area chair decide well fit iclr,5
310.json,thank interesting read pro tackle crucial problem understanding communication agent application reinforcement learning explored approach brings back basic question problem solving approach machine similar human task simple enough make post learning analysis intuitive interesting informed agent made multiple symbol transmit message agnostic agent relied symbol con task effectively boil image classification image sent different category symbol used effectively image class second agent learns assign either image mean approach boil transfer learning problem could probably trained much faster reinforcement learning algorithm,6
310.json,train natural language system putting multiple agent within interactive referential communication game nice mention although seemingly much previous work using multi agent game teach communication certainly seems like direction worth pursuing moreover approach switching game supervised learning experiment described section suggested section seems particularly fruitful note clarity believe network connection omitted however given rather highly customized architecture slightly hard follow description section shorthand diagram add confusion diagram probably need fine tuned least especially misunderstanding caption must still added help reader interpret figure overall framework section great seems quite effective useful various way reasonable expect interesting future variation work well caveat quite confident understood confidence score feel sufficiently familiar closely related literature accurately ass place work within context,6
310.json,referential game proposed agent agent observe image first agent called sender receive binary target variable must send symbol message second agent called receiver agent recover target agent reward receiver agent predict target proposes parametrize agent neural network pretrained representation image feature vector train using reinforce setting shown agent converge optimal policy learned communication symbolic code transmitted sender receiver meaningful concept addition present experiment variant game grounded different image class setting agent appear learn even meaningful concept finally multi game setup proposed sender agent alternating playing game playing supervised learning task classifying image surprisingly anchored supervised learning task symbolic communication even meaningful concept learning shared representation communication multi agent setup interesting research direction explore much harder task compared standard supervised learning single agent reinforcement learning task justifies starting relatively simple task best knowledge approach first learning communication agent grounding communication human language novel remark alternative paradigm standard sequence sequence model tend focus statistical property language rather functional aspect believe contribution proposed task framework analysis visualization communicated token represent useful stepping stone future work reason think accepted comment target incorporated sender network please clarify table table percentage value differently first percentage seem written interval second interval please correct perhaps related table column chance purity seems extremely small value assume mistake assest ass usufal usual,6
740.json,present architecture parallelize optimization nested function based method auxiliary coordinate carreira perpinan wang method decomposes optimization training individual layer updating auxiliary coordinate focus binary autoencoders proposes partition data onto several machine allowing parameter move machine relatively good speedup factor reported especially larger datasets theoretical performance presented match experiment main concern even though method presented general framework nested function experiment focus restricted family model binary autoencoders linear kernel encoders linear decoder component speedup factor encouraging hard sense importance binary autoencoder considered well studied researcher widely used encourage apply framework generic architecture problem question framework apply form generic multi layer neural network experimental useful implication applying framework component encoder decoder linear component desired plot performance function time different setup demonstrate speedup convergence seems focus speedup factor iteration example increasing mini batch size improve speed iteration hurt convergence speed consider scenario dataset storing data auxiliary variable multiple machine simultaneously possible cite arxiv manuscript title multiple time please make self contained include supplementary material appendix believe without applying framework generic architecture beyond binary autoencoders appeal wide audience iclr hence weak reject,4
605.json,propose variational autoencoder specific form tree generating generative tree seems reasonable fully motivated previous reference suggest tree specification clear motivation extension beyond given beyond sentence provided given tree natural specify tree encoder posterior distribution respect structure prior posterior distribution couple tree distant variable fact good reason form general network could compared approach provides sensible differentiable function encoding network test indicative similar tested approach clear best evaluation metric ought significance work well significant future currently somewhat preliminary lack motivation chooses tree structured encoder without particular motivation lacking wider comparison also lack current motivation comparison tractable model need variational autoencoder originality original moment clear originality necessary clarity good experiment sensible extensive conclusive,2
384.json,look problem locating answer question text task answer always part input text proposes combine existing work match lstm relate question text representation pointer predict location answer text strength suggested approach make sense task achieves good performance although mention recent concurrent work achieve better evaluated squad dataset achieves significant improvement prior work weakness unclear well applicable problem scenario answer subset input text experimental evaluation clear table used ensemble although achieves best performance interested approach generalizes datasets minor discussion point task approach seem similarity locating query image visual question answering might want consider pointing related work direction wondering much task seen guided extractive summarization question guide summarization process page last paragraph missing searching summary present interesting combination approach task answer extraction novelty moderate experimental encouraging remains unclear well approach generalizes scenario seems rather artificial task,5
445.json,discussed multiple concurrent contribution different package submission part difficult disentangle despite fact impressive system learning natural feedback online fashion best knowledge quality result achieved particular close full supervision reached case le constraint setting several point raised turn addressed formalisation task learning dialogue precise declare success answer partially satisfying particular work might make sense precisely goal good full supervision along line previous question dialogue seen form noisy supervision please report classic supervision baseline particular used give sense fraction best case performance achieved dialogue learning provided additional information along line think help understand much overall goal achieved open challenge understanding much difficult setting feedback could hand labeled positive negative analysis handcrafted baseline could tested either extract reward template matching maybe even us length feedback proxy baseline look short feedback highly correlated high reward correct answer replied clearer could quantified suggested baseline order confirm simple handcrafted baseline well data concern marginal relation prior work weston fully clear understand submission understood independent submission prior work weston replacing case weston make submission appear incremental understanding punch line submission online part lead turn exploration analysis much aspect matter find experiment clarified raised issue application reinforcement learning particular convincing incremental nature impression emphasised multiple concurrent contribution research thread comparison prior work particular weston made explicit text also experiment partially reply reviewer question nevertheless particular contribution assessed significant worth sharing seems likely impact learn le constraint setting,6
445.json,build work weston using memory network model limited form dialogue teacher feedback state comment closely related question answering problem exception teacher provides response answer always come positive reward thus must learn teacher feedback significantly improve performance overall written clearly several interesting model tested certainly limited form dialogue considered closer question answering since question require agent look back context investigating direction could prove fruitful task scaled difficult main concern novelty word primary difference work weston earlier work natural reinforcement learning online setting cheated fixed policy given advance important address realistic online setting ass whether method particularly still work else change exploration balancing table needed earlier work simulated data real language data work us mechanical turk real experiment important ass method particularly work real language point much appreciated adding additional human testing data sufficient conference thus main point also work collect data online agent policy used collect data rather fixed policy beforehand step right direction sure significant enough iclr little novelty required solve additional requirement task beyond using epsilon greedy exploration thus borderline accept reject edit updated score slightly light author response make good point real world implementation strongly considered part contribution,5
445.json,summary describes experiment evaluating technique training dialogue agent reinforcement learning standard memory network architecture trained babi version wikimovies dataset weston work extends numerous experiment performed comparing behavior different training algorithm various experimental condition strength experimentation comprehensive agree provide additional useful insight performance henceforth weakness essentially appendix earlier machine learning content secondarily seems confuse distinction training adaptive sampling procedure training interactive environment generally particular comparison presented experiment static exploration policy presented training evaluated side side meaningful change work involve simple already well studied change form exploration policy primary concern remains novelty extra data introduced welcome enough probably belongs short technical report work stand iclr submission appropriate vehicle presenting reinforcement learning update concern section addressed attempt make hard distinction reinforcement learning condition considered condition considered think distinction nearly sharp made already noted weston objective special case vanilla policy gradient zero baseline policy sample sense version considered different exploration policy reinforce objective nontrivial baseline similarly change change sampling policy fixed dataset online learning distinction especially meaningful fixed dataset consists endless synthetic data noted variant exploration policy provide stronger training signal available scratch setting particular piacc training sample feature much denser reward however correctly understand figure completely random initial policy achieves average reward babi movie good better exploration policy think clearer delta expressed directly term different exploration policy rather trying cast previous work straightforwardly accommodated framework quite confused fact direct comparison made training condition earlier work think symptom problem discussed adopts position work previous work becomes possible declare training scenario incomparable really think mistake extent policy sample generator used previous worse chance always possible compare fairly evaluating everything online setting presenting side side experiment provide much informative picture comparative behavior various training objective policy policy vanilla policy gradient method like one typically policy sample without little extra hand holding importance sampling trust region method seem work experiment interesting result nice discussion might case note claim batch size related policy learning little lot policy algorithm require agent collect large batch transition current policy performing policy update think experiment fine tuning human worker exciting part work preferred discussed explored much detail rather relegated penultimate paragraph,4
629.json,author work compare dnns human visual perception quantitatively qualitatively first result involves performing psychophysical experiment human comparing actually think psychophysical data collected different work used specific psychophysical experiment determined separately approx image noise level additive noise make noticeable difference human discriminating noiseless image noisy define metric neural network allows measure posit might similar property network correlate pattern noise level neural network human deep neural network much better predictor human pattern noise level simpler measure image perturbation contrast second result involves comparing dnns human term pattern error series highly controlled experiment using stimulus illustrate classic property human visual processing including segmentation crowding shape understanding used information theoretic single neuron metric discriminability ass similar pattern error dnns layer dnns able reproduce human pattern difficulty across stimulus least extent third result involves comparing dnns human term pattern contrast sensitivity across series sine grating image different frequency classic result vision research pattern make natural target comparison model define correlate propertie term cross neuron average distance response blank image response sinuisoid contrast frequency qualitatively compare metric dnns model known literature human finding like human apparent bandpass response contrast grating mostly constant response high contrast pro general concept comparing deep net psychophysical detailed quantitative really nice nicely defined linking function metric express specific behavioral result generated neural network metric information theoretic measure result framework setting linking function seems like great direction actual psychophysical data seems handled careful thoughtful folk clearly know psychophysical con mind biggest problem something really know already existing shown dnns pretty good model human visual system whole bunch way add way great showing metric comparison human sufficiently sensitive could pull apart various model making clearly better others identifying wide dnns human still unfilled sort since dnns good reproducing human judgement result perfect explained variance inter human consistency potentially important really like explored widening identifying image caused focusing test closing training neural network pattern correct seeing made better cnns measured metric task word definitely traded deeper exploration result think overall approach could fruitful really carried enough found thing confusing layout especially found quantitative clearly displayed figure relegated appendix quantification human similarity data shown figure whole meat second result really presented clear quantification human similarity data show figure human contrast sensitivity curve compare model quantitively precise rather note qualitative agreement seems done,5
629.json,compare performance term sensitivity perturbation multilayer neural network human vision many task tested multilayer neural network exhibit similar sensitivity human vision task used conclude multilayer neural network capture many property human visual system course well known adversarial example small perceptually invisible perturbation cause catastrophic error categorization backdrop difficult know make system exhibit similar phenomenology case could mean number thing nice depth analysis happening case others example noise perturbation described first section see already conv correlated human sensitivity examine first layer filter combined produce contextual effect might actually learn something neural mechanism although like sympathetic direction author taking feel scratch surface term analyzing perceptual correlate multilayer neural net,5
783.json,easy read main idea presented clearly main point concern summarized follows synchronous algoriths suffer struggeling node algorithm wait experience never happend amazon cloud however happens cluster university cluster shared user make node busy maybe node dedicated user concer sure kind cluster used produce figure also many experiment experience time gradient time node equality fast maybe le iteration observe took maybe twice long node also increasing shape curve somehow implying weird implementation communication somehow serialize communication maybe much faster mpireduce used even wait slowest asynchronous algorithm cutting waiting time however convergence speed slower moreover algorithm divergence special care given stale gradient also nice guarantee convex function convex cause pain propose take gradient first worker worker available concern focused worker parameter server became slow parameter server bottleneck address situation still number node large deep used imagine communciation take time largest concern experiment different batch size implies different learning rate chosen right tune learning rate parameter figure provide formula clearly bias figure right meaning tune gamma beta could somehow representative also nicer experiment many time report average best worst case behaviour coinsidence right,5
783.json,proposed synchronous parallel employing several backup machine parameter server wait return machine perform update reduce synchronization overhead sound like reasonable straightforward idea main concern approach suitable specific scenario learner except small number learner pace return efficiency learner follow distribution think proposed algorithm work suggest revision provide experiment show performance different efficiency distribution learner assuming learner follow distribution efficiency show expected idle time minor using proposed algorithm,5
783.json,claim supported number backup worker synchronized actually work better async first analyze problem staled update async sgds proposed sync backup worker experiment show effectiveness proposed method application inception pixelcnn idea simple practice quite useful industry setting adding backup workders problem cost nevertheless think proposed solution quite straightforward come assume worker contains full dataset budge worker setting seems quite natural better performance additional backup worker avoid staggering worker problem assumtion sure proposed solution solving difficult enough problem novel enough idea experiment fair comparison think async also mechanism update much staledness proposed method ignores remaining update update example measure average time spent obtain update sync setting time threashold async async perform poorly,4
453.json,present second order method training neural network ensuring time weight activation binary binarization method aim achieve compression subsequent deployment memory system method abbreviated binarization using proximal newton algorithm method incorporates supervised loss function directly binarization procedure important desirable property mention existing weight binarization method ignore effect binarization loss method clearly described related analytically previously proposed weight binarization method experiment extensive multiple datasets architecture demonstrate generally higher performance proposed approach minor issue feed forward network experiment test error reported information really give evidence higher optimization performance also comment anonreviewer question stating baseline achieve near perfect training accuracy making optimization problem harder including explicit regularizer training objective using data extension scheme monitoring training objective instead test error could direct demonstrating superior optimization performance superiority however becoming clearly apparent subsequent lstm experiment,6
404.json,introduces quasi recurrent neural network qrnn dramatically limit computational burden temporal transition sequence data briefly slightly inaccurately start lstm structure remove diagonal element transition matrix also generalizes connection lower layer upper layer general convolution time standard lstm though convolution receptive field time step discussed related number recent modification rnns particular bytenet strongly typed rnns light existing model novelty qrnn somewhat diminished however opinion still sufficient novelty justify publication present reasonably solid empirical support claim indeed seem particular modification lstm warrant attention others feel contribution somewhat incremental recommend acceptance,5
404.json,introduces novel architecture named qrnn qnns similar gated however gate state update function depend recent input value depend previous hidden state gate state update function computed temporal convolution applied input consequently qrnn allows parallel computation since le operation hidden hidden transition depending previous hidden state compared lstm however possibly loose expressiveness relatively model instance clear deal long term dependency without stack several qrnn layer various extension qrnn leveraging zoneout densely connected seqseq attention also proposed evaluate approach various task datasets sentiment classification world level language modelling character level machine translation overall enjoyable read proposed approach interesting pro address important problem nice empirical evaluation showing benefit approach demonstrate speed relatively lstm con somewhat incremental novelty compared balduzizi specific question densely layer necessary obtain good result imdb task simple layer qrnn compare layer lstm pooling perform comparatively qrnn deal long term time depency simple task copy adding task,6
404.json,describe convolutional layer intermediate pooling layer efficiently long range dependency sequential data compared recurrent architecture whereas convolutional layer related pixelcnn architecture oord main novelty combine gated pooling layer integrate information previous time step additionally describe extension based zone regularization densely connected layer efficient attention mechanism encoder decoder model report striking speed rnns factor achieving similar even higher performance major comment qrnns closely related pixelcnns leverage masked dilated convolutional layer speed computation however cite bytenet build upon pixelcnn manuscript include evaluation cite pixelcnn already introducing qrnn method section include evaluation least qrnn compared bytenet language translation well fully convolutional without intermediate pooling layer perform effect introduced pooling layer performance difference pooling investigate dilated convolutional layer minor comment without dense connection perform effect dense connection illustrate dense connection might draw figure refer section time shown figure helpful understood breakdown shown left side measured language modeling referred whereas dependency batch sequence size shown right side sentiment classification referred suggest consistently show either sentiment classification language modeling least figure caption describe task explicitly labeling left right figure improve readability section describes high speed long sequence small batch size suggest motivating case computation parallelized along sequence length le obvious smaller batch size speed computation proposed encoder decoder attention different traditional attention attention vector computed used input decoder sequentially decoder output state parallel described motivated text sentiment classification size hold development created text describes data split equally training test without describing hold convolutional filter size speed best hyper parameter batch size sequence length figure easier interpret actually showing text axis sake space might smaller text passage plot along axis show activation fewer neuron along axis showing example appendix make claim neuron interpretable even convincing language modeling size training test validation convolutional filter size denoted correct high learning rate used epoch beginning show learning curve model without zone translation size training test validation translation performance depend,4
404.json,point take lstm make gate function last input instead standard network faster work better moving compute serial stream parallel stream also making serial stream parallel unfortunately simple effective interesting concept somewhat obscured confusing language encourage improve explanation another improvement might explicitly calculation give example exactly speed improvement coming otherwise experiment seem adequate enjoyed could high value contribution become standard neural network component replicated turn work reliably multiple setting,6
392.json,proposes autoencoder approach lossy image compression minimizing weighted reconstruction error code length architecture consists convolutional encoder pixel convolutional decoder experiment compare psnr ssim ssim performance jpeg jpeg recent based compression approach mean opinion score test also conducted pro clear well written decoder architecture take advantage recent advance convolutional approach image super resolution proposed approach quantization rate estimation sensible well justified con experimental baseline appear entirely complete task using autoencoders perform compression important large practical impact though directly optimizing rate distortion tradeoff entirely novel enterprise enough difference quantization approach pixel convolutional decoder sufficiently distinguish earlier work image compression expert approach seem compelling main shortcoming implementation toderici appears incomplete comparison balle overall feel fact architecture achieves competitive performance jpeg simultaneously setting stage future work varies encoder decoder size data domain mean community find work significant interest specific comment time answered sufficiently review question,6
668.json,experimentally investigates slightly modified version label smoothing technique neural network training report various task smoothing idea investigated previously wide range machine learning task comment report state speech recognition task timit even model directly comparable error back propagation label smoothing softmax straightforward efficient efficient solution entropy smoothing softmax although classification accuracy could remain estimate true posterior distribution kind smoothing might issue complex machine learning problem decision made higher level based posterior estimation language model speech recognition motivation necessary proposed smoothing,5
668.json,specifically suggests regularizing estimator probability distribution prefer high entropy distribution avoids overfitting generally like idea regularizing behavior often make sense regularizing parameter behavior interpretable whereas parameter uninterpretable work together mysterious way produce behavior might able choose sensible prior behavior word prefer parameter individually close jointly lead distribution plausible risk priori believe idea natural sound share doubt anonreviewer possible well explored neural network sure experimental look good maybe everyone kind regularizer kind pollution scientific literature introduce idea community unconnected almost anything else machine learning many many paper include scaled entropy term optimization objective reinforcement learning please long list connection review question comment experimental always accompanied significance test error analysis trained actually better distribution test data test small tell improvement robust across many different training set error error introduce summary recommendation revise resubmit iclr lot submission prefer reward tried something properly contextualized carefully evaluated otherwise race bottom everyone want first something reader confronted confusing slapdash paper unclear relationship,4
668.json,propose simple idea penalize confident prediction using entropy predictive distribution regularizer consider variation idea penalize divergence uniform distribution variation penalize distance base rate term variation unigram find name never seen multi class label described unigrams bigram idea simple used context reinforcement learning popularized regularizer improving generalization supervised learning justification idea still lack analysis author response comparing regularization hole simple number line example polynomial regression make clear regularization could prevent badly overfitting accommodate every data point contrast seems trivial every data point satisfy arbitrarily high entropy course regularized optimization maximize likelihood simply maximize accuracy perhaps something interesting happening interplay likelihood objective regularization objective indicate precisely could imagine following scenario network output probability near high loss label entropy regularization could stabilizing gradient preventing sharp loss outlier example regularization might mainly faster convergence could analyze effect empirically distribution gradient norm strength empirical rigor take idea pace host popular classic benchmark spanning cnns rnns appears datasets especially language modeling confidence penalty outperforms label smoothing present rate borderline contribution open revising review pending modification typo related work penalizing entropy mean penalizing entropy,4
393.json,nice writing clear start traditional attention mechanism case interpreting attention variable distribution conditioned input query proposed method naturally treat latent variable graphical model potential computed using neural network view show traditional dependency variable structure modeled explicitly attention enables classical graphical model semi markov attention mechanism capture dependency naturally inherit linguistic structure experiment prove usefulness various level seqseq tree structure think solid experiment carefully done also includes careful engineering normalizing marginals think solid contribution approach benefit research problem,7
393.json,solid proposes endow attention mechanism structure attention posterior probability becoming structured latent variable experiment shown segmental atention semi markov model syntactic attention projective dependency parsing synthetic task tree transduction real world task neural machine translation natural language inference small gain using structured attention simple attention latter task clear accept clear approach novel interesting experiment seem give good proof concept however structured attention neural seems seem fully exploited segmental attention could approaching neural phrase based syntactic attention offer incorporating latent syntax seem promising direction particular interesting semi supervision attention mechanism posterior marginals computed external parser help learning attention component network least help initializing seems first interesting backprop forward backward inside outside stoyanov stated general probabilistic model forward step structured attention corresponds computation first order moment posterior marginals backprop step corresponds second order moment gradient marginals potential hessian partition function extends applicability proposed approach arbitrary graphical model quantity computed efficiently generalized matrix tree formula allows backprop projective syntax negative side suspect need second order statistic bring numerical instability problem caused signed space field seen practice minor comment typo last paragraph standard attention attention third paragraph potential information source ordering mean,7
393.json,propose extend standard attention mechanism extending consider distribution latent structure alignment syntactic parse tree latent variable modeled graphical potential derived neural network well written clear understand proposed method evaluated various problem case structured attention model outperform baseline model either without attention using simple attention real world task improvement obtained proposed approach relatively small compared simple attention model technique nonetheless interesting main comment japanese english machine translation example relative difference performance sigmoid attention structured attention appears relatively small case curious analyzed attention alignment determine whether structured model resulted better alignment word ground truth alignment available dataset human annotated test example interesting measure quality alignment addition bleu metric final experiment natural language inference thought surprising using pretrained syntactic attention layer appear improve performance instead appear degrade performance curious hypothesis case minor comment typographical error equation section past work demonstrated technique necessary approach past work demonstrated technique necessary approach,7
686.json,present method reduce memory footprint neural network increase computation cost generalization hashednets chen icml parameter neural network mapped smaller memory array using hash function possible collision instead training original parameter given hash function element compressed memory array trained using back propagation trick proposed including compression space shared among layer neural network multiple hash function used reduce effect collision small network used combine element retrieved multiple hash table single parameter describes gist approach hashednets positive side proposed idea novel seem useful theoretical justification presented describe using multiple hash function good idea experiment suggest proposed approach outperforms hashednets negative side computation cost seems worse hashednets discussed immediate practical implication clear given alternative pruning strategy perform better faster inference said believe benefit deep learning community shed light way share parameter across layer neural network potentially leading interesting follow recommend accept asking address comment comment please discus computation cost hashednets fully connected convolutional layer experiment configuration please multiple time report average standard error completeness please table table listed twice different number sentence grammatically correct please improve writing,5
405.json,present action conditional recurrent network predict frame video game hundred step future claim three main contribution modification architecture used using action time directly predict hidden state exploring idea jumpy prediction prediction multiple frame future without using intermediate frame exploring different training scheme trade observation prediction frame training lstm modification architecture motivation seems good past work action influence state lstm could fixed making lstm state dependent however minor technical novelty also pointed reviewer question similar effect could achieved adding input lstm time could done without modifying lstm architecture stated claim combining performs worse current method combine liked empirical difference combining also stronger motivation required support current formulation benefit change architecture well analyzed experiment provides difference traditional lstm current method however performance difference composed component difference training scheme architecture contribution architecture performance clear experiment claim review phase show difference performance architecture seaquest however plot appears gain step small fraction overall gain difficult judge significance architecture modification result game exploring idea jumpy prediction stated omitting intermediate frame predicting future frame could significantly sppedup simulation present interesting observation omitting intermediate frame lead significant error increase least game however clear whether modification current lead effect could achieved previous model like observation interesting better provide detailed analysis game also novelty dropping intermediate frame speedup marginal exploring different training scheme perhaps interesting observation presented present difference performance different training scheme training scheme varied based fraction training phase us observation frame fraction us prediction frame show change training significantly affect prediction biggest contributor performance improvement compared observation interesting effect previously explored detail work like schedule sampling bengio extent clarity presentation exact experimental setup clearly stated instance us architecture however stated response reviewer question difficult interpret qualitative difference current method could highlighted explicitly minor qualitative analysis section requires reader navigate various video link order understand section lead discontinuity reading particularly difficult reading printed copy overall present interesting experimental observation however technical novelty contribution proposed architecture training scheme clear,4
540.json,proposes methodology morphing trained network different architecture without retrain scratch manuscript read well description easy follow however convincing selected baseline considerably state include comparison state example wide residual network table also report number parameter architecture help fair comparison,6
452.json,show extending deep algorithm decide action take well many time repeat lead improved performance number domain evaluation thorough show simple idea work well discrete continuous action space comment question table could easier interpret figure histogram figure could easier interpret table subset atari game selected atari evaluation show convincing improvement game requiring extended exploration freeway seaquest nice full evaluation game become quite standard make possible compare overall performance using mean median score also nice direct comparison straw vezhnevets aim solve problem figar figar currently discard frame action decision might tradeoff repeating action time throwing away information thought separating effect could train process intermediate frame thought overall nice simple addition deep algorithm many people probably start using increasing score based rebuttal revised,7
452.json,proposes simple effective extension reinforcement learning algorithm adding temporal repetition component part action space enabling policy select long repeat chosen action extension applies reinforcement learning algorithm including discrete continuous domain primarily changing action parametrization well written experiment extensively evaluate approach different algorithm different domain atari mujoco torcs comment question improving introduction state algorithm repeatedly execute chosen action fixed number time step statement strong actually disproved experiment repeating action helpful many task task sentence rephrased precise related work discussion relation semi mdps useful help reader better understand approach compare differs response review question experiment provide error bar experimental running multiple random seed useful experiment parameter sharing trpo experiment consistent domain especially since seems improvement trpo experiment smaller domain right hard tell smaller improvement nature task lack parameter sharing something else trpo evaluation different reported duan icml benchmark video show policy learned figar uninformative without also seeing policy learned without figar also include video policy learned without figar comparison point many lap ddpg complete without figar difference reward achieved seems quite substantial table visualized histogram seems like effectively efficiently communicate minor comment plot figure label first changed idea deciding necessary seems like better idea deciding necessary space durugkar missing space could letter indicate constant instead different notation,7
628.json,discus modular product network tractable extension classical product network proposed approach evaluated semantic segmentation task early promising provided summary think present compelling technique hierarchical reasoning mrfs experimental convincing moreover writing confusing time detail quality think technique could described carefully better convey intuition clarity derivation intuition could explained detail originality suggested idea great significance since experimental setup somewhat limited according opinion significance hard judge point time detailed comment think clarity benefit significantly fix inaccuracy alpha expansion belief propagation scene understanding algorithm rather approach optimizing energy function computing state sspn time linear network size seems counterintuitive mean allowed visit node network term deep probabilistic probably defined state infersspn computes approximate state sspn equivalently optimal parse image wondering approximate state optimal albeit formulated scene understanding task experiment demonstrate obtained proposed technique ass applicability proposed approach detailed analysis required specifically technique evaluated subset image make comparison approach impossible according opinion either conclusive experimental evaluation using metric given comparison publicly available possible simplify understanding intuitive high level description desirable maybe even provide intuitive visualization approach,4
782.json,introduces novel hierarchical memory architecture neural network based binary tree leaf corresponding memory cell allows memory access experiment additionally demonstrate ability solve challenging task sorting pure input output example dealing longer sequence idea novel well presented memory structure seems reasonable advantage practice however main weakness experiment experimental comparison external memory based approach discussed related work experimental analysis computational efficiency given overhead cost beyond computational complexity despite main advantage furthermore experimental setup relatively weak artificial task moderate increase sequence length improving greatly strengthen core idea interesting,4
782.json,proposes hierarchical softmax speed attention based memory addressing memory augmented network memnn build hierarchical softmax input sequence time step search relevant input predict next output search discrete corresponding embedding update state lstm produce output finally embedding used input update write function lstm working take hidden state lstm input discrete component search thus trained reinforce experimental section test approach several algorithmic task search sort main advantage replacing full softmax hierarchical softmax inference complexity go great gain complexity allows tackle problem order magnitude bigger addressed full softmax however test sequence token quite small requires relatively complex search mechanism trained reinforce seems work problem relatively small simple sequence great performance change size problem overall idea replacing softmax attention mechanism hierachical softmax appealing work quite convincing approach natural hard train simple scale experiment section weak,2
782.json,introduce memory allows memory access time pro well written everything clear aware similar clear memory access time issue longer sequence clear solves problem con motivation access time able long sequence clear definition computation time design clear really generalize well long sequence also tested real world task think experiment added show whether really work long sequence real world task otherwise clear useful,4
690.json,evaluates recent development competitive ilsvrc architecture perspective resource utilization clear work evaluation finding well presented topic important however surprising people working cnns regular basis even convinced practical value hard tell actually learn finding approaching problem computational constraint production setting opinion mainly discus realistic circumstance main concern evaluation tell much realistic scenario mostly involve fine tuning network ilsvrc starting point case instance really shine fine tuning cumbersome train scratch work well compression possibly good choice standard step taken account question high practical relevance compressed network much higher parameter density comparison well model compressed important least comparing well known publicly available compressed network analysis actual topology network bottleneck useful well minor concern choose batch normalization alexnet,3
690.json,solid work collecting reported data however finding seem surprising finding mainly show architecture batch size manage utilize fully percentage regarding finding agree linear relationship figure could conclude said hyperbolic relationship however finding relevant hold especially latest generation model cluster upper left corner figure seem show much linear behaviour therefore think enough evidence conclude asymptotic hyperbolic behaviour linear behaviour stronger model approach upper left corner finding seems simple conclusion finding long slower model better faster model draw power finding hold finding similar finding architecture manage fully utilize inference time proportional number operation maybe interesting finding tested model seem percentage computational resource available might expect complex model manage utilize much computational resource inter dependency however actual utilization evaluated choose older expect model manage make available computational power additionally think finding relation compressing technique tested actual production network interest,3
413.json,presented method improving efficiency deep network acting sequence correlated input performing computation required capture change adjacent input clearly written approach clever neat practical algorithm driven essentially spiking network benefit approach still theoretical practical seems unlikely worthwhile current hardware strongly suspect deep network trained appropriate sparse slowness penalty reduction computation much larger,7
413.json,present method improve efficiency cnns encode sequential input slow fashion small change representation adjacent step sequence demonstrates theoretical performance improvement video data temporal mnist natural movie powerful deep improvement naturally limited slowness representation transformed sigma delta network cnns specifically designed slow representation benefit also likely specialised hardware fully harness improved efficiency achieved proposed method thus full potential method cannot thoroughly evaluated however since processing sequential data seems broad general area application conceivable work useful design application future cnns introduces interesting idea address important topic show promising initial demonstration actual usefulness relevance presented method relies future work,5
413.json,interesting quantized network work temporal difference input basic idea network process difference computational much efficient specifically natural video data since large part image fairly constant network process informative section image video stream course human visual system work hence interest even beyond core machine learning community aside strong community interested event based vision group tobi delbrck might interesting connect community might even provide reference comment page guess biggest novel contribution rounding network replaced sigma delta network order discretization summation make difference actual processing load think followed step question already answer review period question remaining page noted refer temporal difference refer change signal time change input presented sequentially output network depends value order input temporal spacing make sense understand take difference frame regardless call temporal change frame statement rather confuses maybe dropped unless miss something case explanation necessary figure made bigger improvement could think better discussion relevance finding show sigma delta network save operation compared threshold difference essential specific task solution relevance neuroscience,7
741.json,game considered board combination chosen single move result victory either black white player possible move player location trained visual rendering game board possible output technique used visualize salient region input responsible prediction make find prediction correspond winning board location claim interesting finding figured game rule cross modal supervision applicable higher level semantics think claimed knowledge game rule tested experiment stage game last move considered training bare minimum requirement implicit explicit representation game rule ability previously unseen state generalization even generalize avoid making claim knowledge game rule author definition cross modal seems training image game move image classification image label different domain already know cnns perform mapping cnns used image action mnih ddpg lillicrap classical work alvin unclear point trying make interesting implicit attention mechanism subjective matter claim difference concept happen claim supervising happen automatically learn extensively studied predictive control literature happen next used infer control however experimental setup presented happen seem exact thing analysis learnt recommend visualizing respect incorrect class visualize respect player lose instead winning split data train prediction visualization much informative kind generalizable feature pay attention summary understanding make decision make interesting area research emergence implicit attention mechanism considered interesting finding many claim made supported experiment comment,2
741.json,board rendered various way board legal board next legal play game category board different location next play color next play supervision basically saying place black square middle right black place white square upper left white trained predict category accuracy focus using zhou class activation mapping show focus making decision understand input class interest class black win play bottom right square deciphered figure correctly figure really clear class determine area focus deciding whether class exhibited focus end empty bottom right square certainly exhibit class bottom right square occupied also need condition decision part board need know whether direction maybe conditioning weaker kind interesting sure deeper statement discovering game rule hint also sure connection work weakly supervised learning multi modal learning pretty well written overall grammatical mistake simply surprising discovery work also concern contrived scenario using expressive simple game domain using particular visualization method expert reinforcement learning happening related work game playing maybe appreciating appropriately,2
604.json,address automated argumentation mining using pointer network although task discussion interesting contribution novelty marginal single task application among many potential task,3
604.json,address problem argument mining consists finding argument type predicting relationship argument proposed pointer network structure recover argument relation also propose modification pointer network perform joint training type link prediction task overall reasonable sure iclr best venue work first concern novelty pointer network proposed proposed multi task learning method interesting verified task make feel maybe submission conference rather iclr stated pointer network le restrictive compared existing tree predicting method however datasets seem contain single tree forest stack based method used forest prediction adding virtual root node example done dependency parsing task therefore think experiment right cannot reflect advantage pointer network model unfortunately second concern target task given want analyze structure sentence argumentation mining best dataset example could verify applying task require tree structure dependency parsing application found assumption boundary given strong constraint could potentially limit usefulness proposed overall term also feel baseline method compared probably strong argument mining task necessary strong enough general tree forest prediction task tree forest prediction method term application think assumption boundary restrictive maybe iclr best venture submission,4
487.json,experimental look reasonable validated task reference could improved example rather rumelhart cited back propagation deep learning book,6
487.json,proposes sparsely connected network efficient hardware architecture save memory compared conventional implementation fully connected neural network remove connection fully connected layer show performance computational efficiency increase network three different datasets also good addition combine method binary ternary connect study show improvement hard understand misleading statement propose sparsely connected network reducing number connection fully connected network using linear feedback shift register lfsrs think lfsrs reduced connection keeping information register however lfsr used random binary generator random generator could used lfsr chosen convenience vlsi implementation explanation clearer propose sparsely connected network randomly removing connection fully connected network random connection mask generated lfsr also used vlsi implementation disable connection algorithm basically training network back propogation layer binary mask disables connection explanation added text using random connection idea cnns used layer yann lecun others,5
468.json,proposes network quantization method compressing parameter neural network therefore compressing amount storage needed parameter assume network already pruned compressing pruned parameter problem network compression well motivated problem interest iclr community main drawback novelty heavily built marginally extends overcome drawback noted proposed method proposed well structured easy follow although heavily build still much longer believe still redundancy experiment section start page whereas experiment start page therefore believe much introductory text redundant efficiently experimental show good compression performance compared losing little accuracy mention comparison hang resnet table comment clear whether procedure depicted figure contribution literature section approximate hessian matrix diagonal matrix please explain approximation affect final compression also much lose making approximation minor typo revised version page parag line fined tuned fine tuned page para last line assigned assigned page line page section line explore explored,6
468.json,proposes novel neural network compression technique goal compress maximally network specification parameter quantisation minimum impact expected loss assumes pruning network parameter already performed considers quantisation individual scalar parameter network contrast previous work gong proposed approach take account effect weight quantisation loss function used train network also take account effect variable length binary encoding cluster center used quantisation unfortunately submitted page rather recommended length seems unjustified since first three section first five page generic redundant largely compressed skipped including figure although strict requirement submission guideline suggest compress page improve readability take account impact network loss propose second order approximation cost function loss case weight originally constitute local minimum loss lead formulation impact weight quantization loss term weighted mean clustering objective weight derived hessian loss function original weight hessian computed efficiently using back propagation algorithm similar used compute gradient shown cited work literature also propose alternatively second order moment term used adam optimisation algorithm since loosely interpreted approximate hessian section argue approach natural quantise weight across layer together hessian weighting take account variable impact across layer quantisation error network performance last statement section however clear deep neural network quantising network parameter layer together efficient since optimizing layer layer clustering jointly across layer requires exponential time complexity respect number layer perhaps could elaborate point section develop method take account code length weight quantisation clustering process first method described based previous work uniform quantisation weight space optimised hessian weighted clustering procedure section case nonuniform codeword length encode cluster index develop modification hessian weighted mean algorithm code length cluster also taken account weighted factor lambda different value lambda give rise different compression accuracy trade offs propose cluster weight variety lambda value pick accurate solution obtained given certain compression budget section report number experimental obtained proposed method compare obtained layer wise compression technique uncompressed model experiment used three datasets mnist cifar imagenet data specific architecture taken literature suggest consistent significant advantage proposed method work comparison work gong made illustrate advantage hessian weighted mean clustering criterion advantage variable bitrate cluster encoding conclusion quite interesting work although technical novelty seems limited quantisation expert interestingly proposed technique seem specific deep conv net rather generically applicable quantisation parameter associated cost function locally quadratic approximation formulated useful discus point,6
716.json,summary work present enet convnet architecture semantic labeling obtains comparable performance previously existing segnet faster using le memory review summary albeit seem interesting lack detailed experimental limited interest iclr audience pro faster smaller design rationale described detail con quality reference baseline instance cityscape state thus limited interest support design rationale provided important provide experimental evidence support claim quality work interesting feel incomplete faster smaller build longer obtain improved focus nimbleness cost quality using weak baseline limit interest iclr audience clarity overall text somewhat clear description section could clear originality work compendium practitioner wisdom applied specific task thus limited originality significance find work establishes best practice quite interesting however must shine aspect fast cost quality limit impact work minor comment overall text proper english sentence construction often unsound specific example improve chance acceptance invite also explore bigger model show collected wisdom used reach high speed high quality proper trade curve shown aiming quality versus speed curve limit much section mobile battery powered require rate energy budget watt rule idea rule seem strong word guideline utmost importance importance important already important present trainable network therefore compare large majority inference sentence make sense logical link therefore scen parsing scene parsing arguable encoder decoder called separate unlike relevant make explicit remove real time vague mean existing architecture architecture section layer include bias term good without bias term table initial layer downsampling since half size input section linear operation mean settle recurring pattern section dimensionality change computationally expensive relative section dimensionality change technique speed time provide without experimental validation changing apple orange make orange better apple section dimensionality change found problem problem imply something conceptually wrong issue miss match using resnet semantic labelling section factorizing filter unsure call filter asymmetric filter could symmetric simply call rectangular filter section factorizing filter change increase variety expected opposite section regularization define much better section adequate practical application application section quickly vague depends reader expectation please quantitative section haver section work work section unclear class weighting class balancing section cityscape cityscape section weighted average instance weighted relative average object size section fastest cityscape fastest public cityscape,3
716.json,aim designing real time semantic segmentation network proposed approach encoder decoder architecture many existing technique improvement performance speed concern design choice pretty lack ablation study validate choice moreover component community indexed pooling dilated convolution prelu steerable convolution spatial dropout called early sampling ecoder size also straightforward trade speed performance reducing size depth layer performance inference comparison conducted rather weak baseline segnet also make le convincing public benchmark proposed achieve comparable state reviewer raised stronger similar efficiency compared segnet speed improvement good reasonable given component used however also sacrifice performance benchmark make trick le promising fact found impressive size good practical helpful dump mobile device however analysis trade size performance design result much reduction size find memory consumption report inference stage perhaps even crucial embedded system perhaps practical value practical segmentation network design embedding system believe brings insightful idea worthy discussed iclr either perspective compression semantic segmentation,3
716.json,describes fast image semantic segmentation network many different technique combined create system much faster baseline segnet approach accuracy comparable somewhat worse three datasets evaluated choice technique used achieve speed optimization enumerated described along intuition behind however section lack measurement experimental showing effect choice component stand final evaluation number appear describe speed accuracy tradeoff little insight piece addition feel could thorough comparison different existing system segnet shown comparison table even though many current system outlined related work additional datasets pascal coco interesting well perhaps larger version enet system look fast decent accuracy majority benchmark described however practical implementation feel need thoroughly demonstrate effect component well possibly sizing tuning order provide robust picture,4
595.json,claim improved inference density estimation sparse data text document using deep generative gaussian model variational auto encoders method deriving word embeddings generative parameter allows degree interpretability similar bayesian generative topic model discus contribution quickly review generative story first dimensional latent representation sampled multivariate gaussian parameter theta predicts unnormalised potential vocabulary word potential exponentiated normalised make parameter multinomial word observation repeatedly sampled make document intractable inference replaced formulation inference network parameter independently predicts document mean variance normal distribution amenable reparameterised gradient computation first rather trivial contribution feature inject first order statistic global information local observation claim particularly helpful case sparse data text second contribution interesting optimising generative parameter theta variational parameter turn treatment reminiscent original procedure variational parameter global variational parameter predicted mean covariance sigma observation treated local variational parameter original local parameter directly optimised instead indirectly optimised optimisation global parameter utilised prediction shared parameter local parameter optimised holding generative parameter fixed line algorithm optimised local parameter used gradient step generative parameter line algorithm finally global variational parameter also updated line whereas indeed proposed optimise local parameter think deriving procedure familiar make contribution le trick easier relate thing entirely clear think nice shown functional form gradient used step algorithm gradient step global variational parameter line algorithm us first prediction local parameter thus ignoring optimisation step unclear perhaps missing fundamental reason case either please clarify argue optimisation turn helpful modelling sparse data evidence generative theta suffers poor initialisation please discus expect initialisation problem worse case sparse data final contribution neat procedure derive word embeddings generative parameter embeddings used interpret learnt interestingly word embeddings context sensitive latent variable model entire document figure caption say solid line indicate validation perplexity optimisation local parameter dashed line indicate iteration optimisation local parameter legend figure suggest different reading interpret figure based caption seems indeed deeper network exposed data benefit optimisation local parameter pretty sure figure model reached plateau longer training allow catch curve explain caption axis comparable running time thus question analysis singular value seems like interesting investigate using capacity however barely interpret figure think could walked reader word embedding missing evaluation predictive task also illustrative table barely reproducible text read create document comprising subset word context wikipedia page rather vague wonder whether construct need carefully designed order table feeling inference technique embedding technique useful perhaps presented separately could explored greater depth,5
595.json,first like apologize delay reviewing summary variational inference adapted deep generative model showing improvement negative sparse dataset offer well method interpret data parameter writing generally clear method seem correct introspection approach appears original found interesting experiment polysemic word embedding however like obtained embedding perform respect common embeddings solving supervised task minor many closing parenthesis,6
595.json,introduces three trick training deep latent variable model sparse discrete data weighting iteratively optimizing variational parameter initializing inference network technique improving interpretability deep first idea sensible rather trivial contribution second idea also sensible conceptually novel finding work well dataset used third idea interesting seems give qualitatively reasonable quantitative semantic similarity seem convincing familiar relevant literature therefore cannot make confident judgement issue,4
700.json,proposed approach consists greedy layer wise initialization strategy deep followed global gradient descent dropout fine tuning initialization strategy us first randomly initialized sigmoid layer dimensionality expansion followed sigmoid layer whose weight initialized marginal fisher analysis learns linear dimensionality reduction based neighborhood graph constructed using class label information supervised dimensionality reduction output layer standard softmax layer approach thus added growing list heuristic layer wise initialization scheme particular choice initialization strategy reasonable sufficiently well motivated relative alternative thus feel rather arbitrary lack clarity description approach poorly explained undefined notation properly defined precise alluded denoising also unclear really training additional denoting objective input corruption question arguably mild inconsistency applying linear dimensionality reduction algorithm trained without sigmoid passing learned representation sigmoid even raised addition fact sigmoid hidden layer longer commonly used also consider using relus importantly suspect methodological problem experimental comparison mention using default value learning rate momentum arbitrarily fixed epoch early stopping regularization model hyper parameter always properly hyper optimized using validation cross validation including early stopping separately comparison ideally also including layer size important since considering smallish datasets various initialization strategy mainly different indirect regularization scheme thus need carefully tuned cast serious doubt amount hyper parameter tuning close none went training alternative model used comparison marginal fisher analysis dimensionality reduction initialization strategy well offer advantage currently stand make sufficiently convincing case provide useful insight nature expected advantage also suggest image input cifar qualitative tool showing filter back projected input space learned different initialization scheme consideration could help visually gain insight set method apart,2
700.json,proposes initialize weight deep neural network layer wise marginal fisher analysis making potentially similarity metric pro experiment albeit small datasets tested proposed method con lacking baseline discriminatively trained convolutional network standard dataset cifar also unclear costly computation compute association matrix equation idea proposed combined existing idea greedy layerwise stacking dropout denoising auto encoders however many paper similar idea perhaps year spcanet therefore main novelty marginal fisher analysis layer baseline demonstrate approach work better missing particular like conv fully connected trained scratch good initialization problem improve demonstrate without doubt initializing layer better random weight matrix,3
645.json,proposed method simple elegant build upon huge success gradient based optimization deep linear function approximators combine established linear many view method major contribution derivation gradient respect linear encoding network project different view common space derivation seems correct general approach seems interesting could imagine might applicable many similarly structured problem well written could enhanced explicit description complete algorithm also highlight joint embeddings updated prior experience style many view technique therefore hard judge practical empirical progress presented experiment seem reasonable convincing although generally performed small medium sized datasets detailed comment colour sign axis figure seem flipped compared figure nice additionally continuous rainbow version figure better identify neighbouring datapoints importantly like average reconstruction error individual network output learned representation develop training mismatch different view validation test useful metric cross validation general seems method sensitive regularization hyperparameter selection many parameter compared gcca different regularization parameter chosen different view wonder clear metric optimize,6
583.json,examines computational creativity machine learning perspective creativity defined ability generate type object unseen training argue likelihood training evaluation construction suited class generation propose evaluation framework relies held class object measure ability generate interesting object type familiar literature computational creativity research judge well work context existing work machine learning perspective find idea presented interesting thought provoking understand hypothesis ability generate interesting type know correlate ability generate interesting type know latter good proxy former extent true depends bias introduced selection like measuring generalization performance careful reuse held class selection evaluation nevertheless appreciate effort made formalize notion computational creativity within machine learning framework view important first step direction think deserves place iclr especially given well written approachable machine learning researcher,6
583.json,first frustratingly written grammar fine first four page completely theoretical difficult follow without concrete example section benefit greatly common example woven different aspect theoretical discussion ordering exposition also frustrating found constantly refer ahead figure back detail important seemingly presented order perhaps reordering detail could recommendation give naturally ordered oral presentation work order similarly finally description experiment cursory found wondering whether detail omitted important including experimental detail supplementary section could help fear good well gather together past work novelty generation propose unified framework evaluate past future model done repurposing existing generative evaluation metric task evaluating novelty experiment basic even basic experiment beyond previous work area reviewer knowledge overall recommend accepted strongly recommend rewriting component make digestible novelty paper read thoroughly interested likely fight uphill battle majority reader outside field novelty generation reason theory made even intuitive clear experiment even accessible,5
583.json,proposed measure generation distribution novelty method implied trained mnist digit could generate sample like letter judged anther trained mnist letter trained mnist could seen ability generate novel sample empirical experiment reported novelty hard define proposed metric also problematic naive combination mnist letter dataset represent natural distribution handwritten digit letter mean trained combination could properly distinguished digit letter proposed class count class thus pointless novel sample clearly digit guess quantize sample binary quantize sample resulting image look even like digit,3
307.json,attempt chatbots every form human computer interaction major trend claim could solve many form dialog beyond simple chit chat represents serious reality check mostly relevant dialog natural language venue educate software engineer limitation current chatbots also published machine learning venue educate researcher need realistic validation applied dialog consider work high significance important conjecture underlying likely open research writing antoine bordes clearly stated nip workshop presentation covered work considering metric chosen performance endend approach still insufficient goal oriented dialog comparing algorithm relative performance synthetic data good predictor performance natural data quite departure previous observation made strong effort match synthetic natural condition original algorithmic contribution consists rather simple addition memory network match type first time deployed tested goal oriented dialog experimental protocol excellent overall clarity excellent accessible readership beyond dialog researcher particular impressed short appendix memory network summarized well followed table explained influence number hop represents state exploration rigorous metric dialog modeling also reminds brittle somewhat arbitrary remain note recommendation future research revision first response accuracy basically next utterance classification among fixed list response looking table clearly show absurd practice matter correct call reasonably short dialog though give accuracy response needed reach call also exact dialog accuracy response must correct better table show sensitive experimental protocol initially puzzled accuracy subtask much lower accuracy full dialog pointed task definition requires displaying option requires displaying concierge data happen correct meant best among best cannot fault using standard dialog metric coming one actually pessimistic think represent dialog could result meaningful metric goal oriented dialog suppose sell virtual assistant service paid upon successful completion dialog metric maximize revenue restaurant problem loss probably weighted number error call number turn reach call number rejected option user however loss cannot measured canned dialog either require real human user realistic simulator another issue closely related representation learning fails address explain properly happens vocabulary used user match exactly vocabulary knowledge base particular match type algorithm code indian type cuisine word occur exactly imagine situation us obfuscated terminology like learn association rather human hand describe,7
307.json,synopsis introduces dataset evaluating goal oriented dialog system data generated restaurant setting goal find availability eventually book table based parameter provided user part dialog data generated running simulation using underlying knowledge base generate sample different parameter cuisine price range applying rule based transformation render natural language description objective rank candidate response next turn dialog evaluation reported term response accuracy dialog accuracy show memory network able improve basic word baseline thought want thank interesting contribution said skeptical utility trained system narrow domain setting open domain setting strong argument made hand coding state response scale hence trained method make sense however narrow domain setting usually know understand domain quite well goal obtain high user satisfaction make sense case domain knowledge engineer best system possible given domain already restricted also disappointed goal rank instead generate response although understand make evaluation much easier also unsure candidate response actually obtained practice seems model rank response train test last sentence since argument training approach ease scaling domain without manually engineer system information obtained domain practice generating response allow much better generalization domain opposed simply ranking list hand collected generic response mind weakest part work finally data generated using simulation expanding cuisine price tuples using generation rule necessarily constrains variability training response course traded ability generate unlimited data using simulator unable list rule used good publish well overall despite skepticism think interesting contribution worthy publication conference updated score following clarification,6
307.json,present public dataset task goal oriented dialogue application dataset task constructed artificially using rule based program different aspect dialogue system performance evaluated ranging issuing call displaying option well full fledged dialogue welcome contribution dialogue literature help facilitate future research developing understanding dialogue system still pitfall taking approach first clear suitable deep learning model task compared traditional method rule based system shallow model since deep learning model known require many training example therefore performance difference different neural network simply boil regularization technique task also completely deterministic mean evaluating performance task measure ability model handle noisy ambiguous interaction inferring distribution user goal executing dialogue repair strategy important aspect dialogue application overall still believe interesting direction explore discussed comment baseline word order information think strong weakness make neural network appear unreasonably strong simpler baseline could likely competitive better proposed neural network maintain fair evaluation correctly ass power representation learning task think important experiment additional neural network benchmark take account word order information convincly demonstrate utility deep learning model task example could experiment logistic regression take input word embeddings similar supervised embeddings gram feature match type feature baseline included increase rating final minor comment conclusion state existing work well defined measure performance really true trainable model task oriented dialogue well defined performance measure example network based trainable task oriented dialogue system hand goal oriented dialogue generally harder evaluate given human subject also evaluated fact twitter also strategy policy learning task oriented conversational system updated score following added,7
409.json,introduces musicnet dataset application technique music limited scarcity exactly kind data provided meticulously annotated carefully verified organized containing enough hour music genre well constrained order allow sufficient homogeneity data help ensure usefulness great community description validation dataset interesting indicates careful process followed provide enough basic experiment show dataset enough good level feature expected sinusoidal variation indeed learned context might argue term learning representation work presented contributes dataset experiment technique used however given challenge acquiring good datasets given essential role datasets play community moving research forward providing baseline reference point feel contribution carry substantial weight term expected future reward research group making great datasets available regular basis place different context case otherwords experiment technique necessarily accepted paper review criterion guessing dataset better,7
409.json,describes creation corpus freely licensed classical music recording along corresponding midi score aligned audio also describes experiment polyphonic transcription using various deep learning approach show promising little disorganised somewhat contradictory part example find first sentence section musicnet better pushed paragraph section allowed begin survey tool available researcher music also description table probably appear somewhere method section last example abstract intro say purpose note prediction later paragraph intro claim focus learning level feature music find slightly disorienting although others uehara example discussed collection platform corpus work interesting size approach generating feature interested expand offering corpus term volume diversity,5
370.json,training highly convex deep neural network important practical problem provides great exploration interesting idea effective training empirical evaluation comment discussion convincingly demonstrates method achieves consistent improvement accuracy across multiple architecture task datasets algorithm simple alternating training full dense network sparse version actually positive since mean adapted practice research community revised incorporate additional experiment comment discussion particularly accuracy comparison number epoch,7
370.json,summary proposes training strategy achieve higher accuracy issue train large going capture noise prune model make small miss important connection thus proposed method involves various training step first train dense network prune making sparse train sparse network finally connection back train dense method generic method used lstm reason model better accuracy escape saddle point sparsity make robust noise symmetry break allowing richer representation main point want show capacity achieve higher accuracy shown possible compress without losing accuracy lossless compression mean significant redundancy model trained using current training method important observation large model better accuracy better training scheme used con question issue accuracy slightly increased model question price paid improvement resource performance concern arises training large computationally expensive hour even day using high performance gpus second question keep adding dense sparse dense training iteration higher higher accuracy improvement limitation dsdsd approach,7
370.json,present training strategy deep network first network trained standard fashion second small magnitude weight clamped rest weight continue trained finally weight jointly trained experiment variety image text speech datasets demonstrate approach obtain high quality proposed idea novel interesting sense close dropout though noted deterministic weight clamping method different main advantage proposed method simplicity three hyper parameter needed number weight clamp number epoch training used first dense phase sparse phase given plugged training range network shown experiment concern regarding current empirical evaluation noted question phase seems baseline method trained many epoch proposed method standard trick dropping learning rate upon convergence continuing learn employed response seems indicate approach effective think thorough empirical analysis performance epoch learning rate strengthen exploration regarding sparsity hyper parameter also interesting,4
720.json,method click prediction presented input categorical variable output click rate categorical input data embedded feature vector using discriminative scheme try predict whether sample fake embedding vector passed series mult gate important interaction identified pooling process repeated multiple time multiple layer final feature passed fully connected layer output click prediction rate claim gate pooling allow modeling interaction lead state straightforward apply idea paper like wordvec obtain feature embeddings consequently idea discriminating fake true sample feature learning theoretically convolution gate pair input dimension make interaction explicit imposed structure using gate merit proposed method tested network using gate outperforms network without gate baseline critically missing embedding vector followed series convolution pooling layer another related issue sure number parameter proposed baseline model similar instance total number parameter ccpm proposed overall idea ground rejection outperforms established baseline however comparison weak encourage perform comparison,3
720.json,proposes learn continuous feature input data consists multiple categorical data idea embed category learnable dimensional continuous space explicitly compute pair wise interaction among different category given input sample achieved either taking component wise product component wise addition perform pooling select subset informative interaction repeat process number time final feature vector given input feature vector used input classifier regressor accomplish final task embeddings category learnt usual experiment section show synthetic dataset procedure indeed able select relevant interaction data real world dataset ipinyou seems outperform couple simple baseline major concern nothing idea embedding categorical data mixed category already handled past literature essentially learns separate lookup table class category input represented concatenation embeddings lookup table linear function deep network plugged feature input rather marginal contribution explicit modeling interaction among category equation nothing else feel interaction automatically learned plugging deep convolutional network embeddings input sure useful contribution experimental section rather weak test method single real world data couple rather weak baseline much preferred evaluate numerous model proposed literature handle similar problem including wsabie argued response wsabie suited problem strongly disagree claim original wsabie showed experiment using image input training methodology easily extended type data set including categorical data instance conjecture proposed embed categorical input concatenate embeddings plug deep conv train using margin loss perform well better hand coded interaction proposed course could wrong convincing tested baseline,4
720.json,author proposed approach feature combination embeddings done first computing pairwise combination element complicated nonlinearity pick output vector triple higher order combination consecutive pairwise combination performed yield final representation seems approach directly related categorical data applied embeddings even motivation brings particular approach connection many paper similar idea ccpm convolutional click prediction compared also proposes similar network structure conv conv author mention conceptual similarity difference versus ccpm compact bilinear pooling,3
777.json,introduces supervised deep learning layer wise reconstruction loss addition supervised loss class conditional semantic additive noise better representation learning total correlation measure additional insight auto encoder used derive layer wise reconstruction loss combined supervised loss combining supervised loss class conditional additive noise proposed showed consistent improvement baseline experiment mnist cifar datasets changing number training example class done extensively derivation equation total correlation hacky moreover assuming graphical carefully derived estimate current proposal encoding decoding encoded representation really well justified sigma equation trainable parameter hyperparameter trainable trained correspond class proposed feature augmentation sound like simply adding gaussian noise softmax neuron said proposed method different gaussian dropout wang manning icml applied different layer addition missing reference disturblabel regularizing loss layer cvpr applied synthetic noise process loss layer experiment done multiple time different random subset provide mean standard error overall believe proposed method well justified limited novelty,3
777.json,present regularization technique neural network seek maximize correlation input variable latent variable output achieved defining measure total correlation variable decomposing term entropy conditional entropy explain actually maximize total correlation lower bound ignores simple entropy term considers conditional entropy clearly explained rationale discarding entropy term entropy measure applying probability distribution implies variable random link conditional entropy formulation reconstruction error made explicit order link view expected example noise unit network later claimed original ladder network suitable supervised learning small sample empirical seek demonstrate theoretical explanation case welcome mnist shown particular convolutional neural network architecture however ladder network dataset produced standard fully connected architecture neural network architecture desirable comparability original ladder neural network,2
624.json,main merit draw attention crucial initialization deep network counter popular impression modern architecture improved gradient descent technique make optimization local minimum saddle point longer problem provides interesting counter example showcase initialization mixed particular data lead optimization stuck poor solution feel like contrived artificial construct importantly consider popular heuristic likely help avoid getting stuck saturating activation function leaky relu batch norm skip connection resnet thought contributing keep gradient flowing put warning sign potential initialization problem standard relu net without proposing solution workarounds carrying systematic analysis picture affected commonly used current heuristic technique architecture initialization training broader scope analysis especially lead insight practical relevance could much increase value reader,4
624.json,study error surface deep rectifier network giving specific example error surface local minimum several experimental show learning trapped apparent local minimum variety factor ranging nature dataset nature initialization develops good intuition useful example way training awry even though example constructed contrived necessarily remove theoretical importance useful simple example thing wrong however broader theoretical framing appears going strawman underlying easiness optimizing deep network simply rest emerging structure high dimensional space rather tightly connected intrinsic characteristic data model believe perspective already contained several work cited belonging perspective choromanska instance analyze gaussian input clearly make claim based characteristic data model broadly loss function determined jointly dataset parameter account error surface separated dataset property clear emerging structure high dimensional space could make independent dataset initial parameter emerging structure error surface necessarily related dataset parameter worry aiming strawman replica method characterize average behavior infinite system surprising specific finite sized system might yield poor optimization landscape seems surprised training broken initialization initialization known critical even linear network saddle point innocuous initialization dramatically slowing learning saxe seems like proof proposition error suppose cdfb cdfw learning fails meaning probability failure increase number hidden unit increase seems like rather ignoring bias fails case limit infinity depends scale longer necessarily true globally good behaviour learning regardless size also appears insufficiently distinguish local minimum saddle point section state show training stuck local minimum based training fixed budget epoch possible tell whether result reflects genuine local minimum saddle point based simulation also case rectifier suffer genuine blind spot sigmoid soft rectifier nonlinearities problem hidden node instance thought local minimum fact none hamey analysis error surface network hidden node desire simply show training converge particular finite problem much simpler counterexample constructed suffice hidden unit weight zero instance response prereview question write complete characterization error surface indeed universally valid able break learning initialization mentioned previously basic even deep linear network show initialization near saddle point break learning seems attacking straw along line nothing possibly wrong neural network training prior theoretical result claim figure explanation seems counterintuitive simply scaling input weight matrix initialized zero bias change region relu activates manipulation achieve goal concentrating data point linear region likely explanation much weaker scaling compensated learning algorithm algorithm converge longer response note training conducted order magnitude longer required unscaled input converge scaling data five order magnitude indeed training converge without issue scaling four order magnitude response note adam compensate scaling factor depends detail adam implementation epsilon factor used protect division zero example contains many interesting variety small technical concern remain,4
331.json,present approach skill transfer task another control setting trained forcing embeddings learned different task close penalty experiment conducted mujoco experiment state joint link experiment pixel exhibit transfer arm different number link torque driven tendon driven limitation suppose time alignment trivial task episodic domain time alignment form domain adaptation transfer dealt could dealt subsampling dynamic time warping learning matching function neural network general remark approach compared relevant baseline however purely experimental another baseline worse random projection embedding function domain check performance transfer version specialisation embeddings also information problem learning invariant feature space also linked metric learning xing generally parallel drawn multi task learning case knowledge transfer make sense anneal alpha experiment feel rushed particular performance baseline always transfer uninformative least much bigger sample budget tested also figure contain direct mapping another concern experiment author control fact embeddings trained iteration case transfer overall study transfer welcomed experiment interesting enough publication could thorough,6
331.json,explores transfer reinforcement learning agent morphologically distinct idea source target agent learned shared skill construct abstract feature space enable transfer unshared skill source agent target agent related much work transfer us shared latent space variant including manifold alignment kernel report experiment using simple physic simulator robot arm consisting three four link comparison simple based approach shown although preferable comparison something current date manifold alignment kernel three layer neural used construct latent feature space problem transfer extremely important receives le attention work us interesting hypothesis trying construct transfer based shared skill source target agent promising approach however comparison related approach date domain fairly simplistic little theoretical development idea using theory,5
761.json,take first step towards learning statically analyze source code develops simple programming language includes loop branching determine whether variable program defined used try variety shelf sequence classification model develops make differentiable keep track variable defined result show lstm achieve accuracy differentiable achieve accuracy sequence level supervision accuracy strong token level supervision additional result used whereby lstm language trained correct code probability threshold determine tuned hand token highlighted source possible error question could clarify reasoning pattern needed solve problem need statically determine whether condition ever evaluate true order solve task simple checking whether variable appears appears later textual representation program strength learning static analyzer interesting concept think good potential line work ability determine whether variable defined used certainly prerequisite complicated static analysis experimental setup seems reasonable differentiable seems like useful albeit simple modelling tool weakness setup clear make much progress towards challenge arise trying learn static analyzer model mostly simple novelty modelling front differentiable provides small task clear useful general construct overall think interesting start eager line work progress opinion early accept work iclr excited seeing happens push system learn analyze property code push towards scenario learned static analyzer useful perhaps leveraging strength machine learning available standard programming language analysis,3
761.json,trying understand whether static analysis learned hinted question think interesting complexity static analysis removed language extraordinarily simple logic using solve problem posed lstm unsurprisingly learn extraordinarily simple logic given differentiable object extreme simplicity give confidence realistic static analysis problem solved lstms deep learning remarkable success solving messy real world language problem certainly possible lstms could solve static analysis technically timid right,2
389.json,proposed novel samplernn directly waveform signal achieved better performance term objective test subjective test mentioned discussion current status lack plenty detail describing hopefully addressed final version attempted compare wavenet manage better baseline lstm make comparison wavenets le convincing hence instead wasting time space comparing wavenet detailing proposed better,8
389.json,introduces samplernn hierarchical recurrent neural network audio trained evaluated using likelihood human judgement unconditional sample three different datasets covering speech music evaluation show proposed compare favourably baseline shown subsequence length used truncated bptt affect performance significantly interestingly subsequence length sample sufficient good even though feature data modelled span much longer timescales interesting somewhat unintuitive result think warrant discussion attempted reimplement wavenet alternative audio fully convolutional unable reproduce exact architecture original attempted build instance receptive field could trained reasonable time using computational resource commendable architecture wavenet described detail found challenging find detail proposed samplernn architecture value used different tier many unit layer think comparison term computational cost training time number parameter also informative surprisingly table show vanilla lstm substantially outperforming term likelihood quite suspicious lstms tend effective receptive field hundred timesteps best expect much larger receptive field wavenet reflected likelihood score extent similarly figure show vanilla outperforming wavenet reimplementation human evaluation blizzard dataset raise question implementation latter discussion result whether expected welcome table figure also show tier samplernn outperforming tier term likelihood human rating respectively counterintuitive expect longer range temporal correlation even relevant music speech discussed think useful comment could happening overall interesting attempt tackle modelling long sequence long range temporal correlation quite convincing even always said comparison baseline interesting performs conditional generation seeing easily objectively compared model like wavenet domain remark upsampling output model done separate linear projection choice upsampling method motivated linear interpolation nearest neighbour upsampling advantage learning operation linear projection learning largely thing give take noise third paragraph section indicates linear used contrast wavenet encoding used supposedly improves audio fidelity sample well section mention discretisation input softmax discretised input without reference prior work made observation reference given probably moved avoid giving impression novel observation,7
521.json,pro introduction nice filter bank implementation good numerical refinement representation back propagation demonstration speed learning con algorithm section necessary even affect presentation however source code great link scattering transform clear sometimes mentionned comment writing could improved personal point view also believe negative point mention easily removed,5
521.json,advocate chirplets basis modeling audio signal introduce fast chiplet transform efficient computation also introduced idea initializing training layer mimic chirplet transform audio signal similar idea proposed mallet scattering transforms fairly easy follow place contains undefined term idea using chirplet transform interesting main concern empirical evidence provided rather narrow domain bird call classification furthermore accuracy gain shown domain relatively small feature chirplet transforms recommend provide evidence generalizes audio including speech task,3
464.json,much review comment well written interesting idea lot people currently want combine vogue nobody gotten work really groundbreaking influential actually superior performance highly relevant competitive task people struggle fact requires efficient method large datasets super slow hence believe direction shown much promise clear ever slowness many direction need explored maybe eventually reach point become relevant interesting learn obviously inherent grammatical structure language though sadly tree capture much intuition regardless interesting exploration worthy discussed conference,6
608.json,propose using periodic activation function instead tanh gradient descent training neural network change go common sense need strong evidence show good idea practice experiment show slight improvement mnist configuration show strong improvement almost higher accuracy iteration algorithmic task clear activation function good broad class algorithmic task present hence evidence shown insufficient convincing good idea practical task,3
599.json,present modified gated caled deal time series display missing value input work front first deal missing input directly using learned convex combination previous available value forward imputation mean value mean imputation second includes dampening recurrent layer unlike second reset gate parametrized according time elapsed since last available value attribute positive clear definition task handling missing value classification time series many interesting baseline test presented deal missing value novel type learn dampening parameter extensive test done datasets probably greatest asset negative could double checking typo section really belongs main article deal important related work swap imprecise diagram need space mention method statistic litterature main point review informs decision promising expectation able convince simple without interval well suited task handling missing input main simple presented main baseline includes extraneous parameter interval according table probably hurt help third parameter dubious value brings question fairness comparison done main especially since table simple without interval present significantly outperforms second concern biggest claim peppered first relationship presence rate data dataset diagnostics might wrong indicates doctor charge patient requested relevant analysis done according patient condition mean expert system based data always seem step behind second claim last sentence introduction set huge expectation another simply concatenating masking time interval vector fails exploit temporal structure missing value unsubstantiated actually later another conclusion since model displayed best improvement subsample dataset whole mean improvement going continue grow data added fails consider model actually started much better one lastly claim capture informative missingness incorporating masking time interval directly inside architecture make change fact also concatenate mask input like simple without interval lead question actual improvement made given find work average accept without reframing finding better focus real contribution believe novel parametrize choice imputation method,5
599.json,proposed deal supervised multivariate time series task involving missing value high level idea still using recurrent neural network specifically sequence supervised learning classification modification made input hidden layer rnns tackle missing value problem pro insight utilizing missing value critical observation decaying effect healthcare application also interesting experiment seems solid baseline algorithm analysis also done properly con novelty work enough adding decaying smooth factor input hidden layer seems main modification architecture datasets used small decaying effect might able generalize domain,4
599.json,propose method time series classification missing value make potential information missing value based simple linear imputation missing value learnable parameter furthermore time interval missing value computed used scale computation downstream demonstrate method outperforms reasonable baseline small sized real world datasets clearly written propose reasonable approach dealing missing value intended application domain data abundant requires smallish model somewhat sceptical benefit carry datasets general le handcrafted multi layer rnns option,5
560.json,investigates impact orthogonal weight matrix learning dynamic rnns proposes variety interesting optimization formulation enforce orthogonality recurrent weight matrix varying degree experimental demonstrate several conclusion enforcing exact orthogonality help learning enforcing soft orthogonality initializing orthogonal weight substantially improve learning optimization method proposed currently require matrix inversion therefore slow wall clock time orthogonal initialization soft orthogonality constraint relatively inexpensive find practical experiment generally done high standard yield variety useful insight writing clear experimental based using fixed learning rate different regularization strength learning speed might highly dependent different strength admit different maximal stable learning rate instructive optimize learning rate margin separately maybe shorter sequence length soft orthogonality impact stability learning process instance show sigmoid improves stability perhaps slightly reducing learning rate sigmoid gaussian prior make learning well behaved weighting le show singular value converging around rather initializing orthogonal matrix multiplied confer noticeable advantage standard orthogonal matrix especially copy task curiously larger margin even model without sigmoidal constraint spectrum margin performed well long initialized orthogonal suggesting evolution away orthogonality serious problem task consistent analysis given saxe deep linear net singular value initialized dy away training must zero implement desired input output broadly open question whether orthogonality useful initialization proposed saxe role mainly preconditioner make optimization proceed quickly fundamentally change optimization problem whether useful regularizer proposed arjovsky henaff additional constraint optimization problem minimize loss subject weight orthogonal experiment seem show mere initialization orthogonal weight enough reap optimization speed advantage much regularization begin hurt performance substantially changing optimization problem undesirable point also apparent term training loss mnist margin almost indistinguishably margin however term accuracy margin best show large nonexistent margin orthogonal initialization enable fast optimization training loss among model attain similar training loss nearly orthogonal weight perform better start separate optimization speed advantage conferred orthogonality regularization advantage confers useful explicitly discus initialization regularization dimension text overall contributes variety technique intuition likely useful training rnns,6
560.json,well motivated part line recent work investigating orthogonal weight matrix within recurrent neural network using orthogonal weight address issue vanishing exploding gradient unclear whether anything lost either representational power trainability enforcing orthogonality empirical investigation examines property affected deviation orthogonality useful contribution clearly written primary formulation investigating soft orthogonality constraint representing weight matrix factorized form give explicit control singular value clean natural albeit necessarily ideal practical computational standpoint requires maintaining multiple orthogonal weight matrix requiring expensive update step unaware approach investigated previously experimental side however somewhat lacking evaluates task copy task using architecture without transition linearity sequential permuted sequential mnist reasonable choice initial evaluation problem shed much light practical aspect proposed approach evaluation realistic setting valuable language modeling task furthermore investigating pure make sense evaluating effect orthogonality feel somewhat academic lstms also provide mechanism capture longer term dependency task proposed approach compared directly lstm significantly outperformed interesting effect proposed soft orthogonality constraint additional architecture deep feed forward architecture whether benefit embedded within lstm although seems doubtful overall address clear question well motivated approach interesting finding datasets think could provide valuable contribution however significance work restricted limited experimental setting datasets network architecture,4
649.json,investigates issue whether syntactic dependency unsupervised word representation learning model like cbow skip gram focus issue bound word dependency type nsubj unbound word alone representation context training time empirical extremely mixed specific novel method consistently outperforms existing method systematic major concern soundness however think broad interest iclr community focused fairly narrow detail representation learning entirely specific primarily negative short conference reasonable target,3
472.json,propose method explicitly regularize sparse coding encode neighbouring datapoints similar set atom dictionary clustering training example input space resulting algorithm relatively complex computationally relatively expensive provide detailed derivation argument proximal gradient descent method prove convergence follow derivation general well written explain motivation behind algorithm design detail abstract mention extensive experimental find experiment convincing experiment usps handwritten digit dataset mnist coil coil datasets relatively small algorithm dictionary size seems surprising state implemented srsc cuda extreme efficiency page importantly find hard interpret compare report accuracy normalized mutual information image retrieval clustering task proposed srsc used feature extractor improvement relative standard sparse coding seem small often term look promising term accuracy understand description page correctly test used select hyperparameters best similarity measure clustering step comparison baseline state image clustering method besides providing feature small scale image clustering system maybe way directly evaluate property quality sparse coding approach reconstruction error sparsity maybe even denoising performance summary think current form lack evaluation experimental iclr publication intuitively agree proposed regularization interesting direction experiment directly show regularization desired effect improvement clustering task srsc used feature extractor modest,5
472.json,like thank detailed response question proposes support regularized version sparse coding take account underlying manifold structure data purpose augment classic sparse coding loss term encourages near point similar active convergence guarantee optimization procedure presented experimental evaluation clustering semi supervised learning show benefit proposed approach well written nice read relevant contribution work including optimizing regularization function approximation surrogate derive styple iterative method present convergence analysis thanks clarification regarding assumption used section nice include manuscript also propose fast encoding scheme proposed method included experiment semi supervised consists interesting method fast approximation interesting addition think using fast encoders particularly novel main part work converting iterative optimization algorithm feed forward net accelerating inference process done past several time quite similar problem natural done surprising maybe interesting evaluate important architecture matching optimization algorithm compared generic network though analysis also performed past,6
479.json,first like apologize delay reviewing summary work introduces novel memory based artificial neural network reading comprehension experiment show improvement state originality approach seems implementation iterative procedure loop testing current answer correct order better sense reason improvement interesting complexity time analysis algorithm might mistaken reporting anything actual number loop necessary reported experiment dataset description section moved section datasets described,6
479.json,thie proposed iterative memory updating cloze style question answering task approach interesting result good comment actually single proposed model consists reading writing adaptive computation answer module reading composing writing gate querying answer module based method section experiment seems adaptive computation simpler performs better without time memory update single iteration composing module similar neural turing machine setting composing module tested different size hidden state find relation number could find trick helping find number need ablation study using different according understanding adaptive computation stop distribution testing data,5
750.json,summary extends analyzes gradient regularizer hariharan girshick regularizer proposed penalizes gradient magnitude shown shot learning performance work show previous regularizer equivalent direct penalty magnitude feature value weighted differently example analysis go provide example feature penalty favor better representation first example address problem constructing network feature penalty encourages representation linearly separable second example analyzes layer linear network showing improved stability order optimizer feature penalty added last analysis show regularizer interpreted gaussian prior feature weight since prior interpreted soft whitening effect feature regularizer like soft version batch normalization experiment show small improvement synthetic test omniglot dataset feature regularization better baseline worse moment matching network experiment imagenet similar hariharan girshick also show effective shot learning strength core proposal simple modification hariharan girshick idea feature regularization analyzed multiple angle theoretically empirically connection batch normalization could broader impact weakness section gradient regularizer hariharan girshick introduced introducing concept concern expressed motivation clear small gradient every sample produce good generalization experimentally seems central issue detail related analysis offer clear answer problem purpose generality section clear analysis provides specific case standard architecture feature regularization intuitively help learn better representation however intended take away clear take away since feature penalty help case help case hesitant argument specific architecture used section result seems rely choice linearity often encountered recent neural literature point might also highlight difference weight penalty feature penalty seem encourage different value case however comparison weight penalty section tell depends either assuming cross entropy loss general class loss hold provided made clear presented omniglot imagenet experiment performed batch normalization point feature regularization similar effect batch norm since resnet baseline includes batch norm clear improvement baseline proposed regularizer clear additional positive effect however provided without batch norm comparison method performed imagenet experiment like hariharan girshick particular split class used provided appendix performance measured using novel example class using nearest neighbor minor brief comparison matching network provided section performance matching network also reported table approach section intuitively close convergence half data case recommend update parameter left half recommend right could intuition clarified many direction high dimensional space many way divide group penalty hariharan girshick implemented using code either acceptable clarification appreciated first equal sign proportional equal work dense nature think presentation could improved particular detailed derivation could provided appendix detail could removed main version order increase focus derviation section overall evaluation provides interesting analysis value clear clear reason gradient feature regularizer improve shot learning performance despite experiment support conclusion analysis interesting analysis help lead clearer explanation work somewhat novel extension analysis hariharan girshick point completely clear mentioned,5
750.json,proposes analysis regularization weight froebius norm feature norm showing equivalent another proposed regularization gradient magnitude loss argue helpful shot learning numerically stable soft version batch normalization finally demonstrate experimentally regularization improves performance shot task first nice analysis simple model proposes interesting insight optimization issue unfortunately demonstrate argue convincing manner analysis extends deep linear computation structure feel like could write full derived convex differentiable linear activation function relu analysis experimentation measure numerical stability second show interesting correspondance batch normalization fail experimentally show relevance finally understand appeal proposed method numerical stability point view convinced effect shot learning high dimensional space deep network used commend contributing mathematical understanding field think demonstrate large scale effectiveness propose time feel like clear strong message make various interesting claim number thing seem le disparate loosely related shot learning note expectation taken respect empirical distribution generated training generally training viewed montecarlo sample underlying unknown data distribution mathcal learns meaningful representation get improvement baseline analysis meaningfulness representation table table please mindful formatting citation parenthesized numerous extraneous missing spacing word sentence,5
750.json,proposes last layer feature penalty regularization last layer neural although equation suggest weighting example dropping weight alphai work equally well proposed approach relates batch norm weight decay experiment given shot settting seem story feature penalty soft batch norm version shot learning feature penalty specifically adapted shot learning classical supervised task regarding result omniglot believe still worse matching network refer table overall idea simple feel like preliminary supposed soft get better performance feature penalty together give even better something still missing explanation edits revised version thank adding information feel still long hopefully reduce page promised however still convinced ready accepted mainly following reason omniglot still significantly current state experiment really confirm infirm relationship added explanation work shot setting showing control dimension hence good control overfitting small number training example discussion basic really shed light obvious pushing score improved version still think acceptance level,4
615.json,proposes second order method train deep neural network claimed method address important optimization problem setting poor conditioning hessian proliferation saddle point method viewed concatenation algorithm nocedal wright limited memory representation byrd first missing formal theoretical argument work general providing intuition helpful instead provided work dauphin marten experimental section convincing considering performance term wall clock time reported advantage competitor method strong even term epoch understand optimizing implementation still question considering experiment convincing anybody bother implement train deep model work ready published,3
584.json,work investigates joint learning setup task stacked based complexity experimental evaluation done tagging chunking dependency parsing semantic relatedness textual entailment improves model trained solely target task although hypothesis work important experimental evaluation lack thoroughness first simple multi task learning baseline implemented hierarchy task test hypothesis task ordered term complexity second since test chunking included training data dependency parsing related chunking jmtall informative third since guarantee well formed dependency tree thus table fair minor issue chunking word level task although annotation word level chunking structured prediction task like learn structured annotation sequence,2
584.json,introduce train joint model many task traditionally treat task pipeline later task depending output previous task propose neural approach includes task single higher level task take prediction lower level task hidden representation lower level task also proposed successive regularization intuitively mean training high level task want change lower level much lower level task keep reasonable accuracy prediction modeling side think proposed similar comparing zhang wei spinn bowman even simpler number experiment good sure convinced number table since pattern clear sometimes performance higher level task even go training task sometimes also significant stable dependency score although think serious problem comparing output guaranteed well formed tree strictly speaking fair admit successive regularization make sense intuitively interesting direction however without careful study training schema current successive regularization convince right thing model current strong enough show training method need explored including thing iteratively train different task relationship number training iteration task training size loss task,4
357.json,introduces approach future frame prediction video decoupling motion content encoded separately additionally using multi scale residual connection qualitative quantitative shown weizmann datasets idea decoupling motion content interesting seems work well task however novelty relatively incremental given previous cited work multi stream network clear particular decoupling work well broader interest beyond specific task future frame prediction weizmann convincing significantly outperform baseline le impressive le constrained dataset qualitative example convincing discussed review question overall well executed work interesting though extremely novel idea given limited novelty decoupling motion content impact beyond specific application strengthened could shown broader interest video task,6
357.json,present method predicting video sequence line mathieu contribution separation predictor different network picking motion content respectively interesting novelty compared referenced work also pointed anonreviewer similarity stream network also whole body work building seminal separating motion content also proposed application pose estimation detail clearly understood basic framework like gans known presentation general good enough broad public example loss well known matthieu however make self contained properly explained mentioned additional loss main target loss adversarial part properly enough introduced agree adversarial training well enough known community still properly introduced also involves explanation ldisc loss second network discriminator explaining role equation explained motion vector also overloaded feature dimension residual nature layer made apparent equation several typo absence article preposition reread carefully,5
707.json,present type language treat entity reference latent variable structured three specialized model three application dialog generation reference database entry recipe generation reference ingredient text generation coreference mention despite opaqueness detail discus later great making main idea coming think quite interesting definitely worth pursuing seems rushed deadline major weakness first major weakness claimed latent variable hardly latent actual empirical evaluation clarified review mention assumed given variant seem like claim call variable latent fact treated observed variable model latent variable difficult train right related problem perplexity evaluation measure comparing reference aware language model vanilla language model essentially comparing language model defined different event space fair comparison mention assumed given reference aware language model fact mention generator designed similar pointer network probability score mention naturally higher compared regular language need consider much bigger vocabulary effect analogous comparing language model aggressive small vocabulary language model much larger vocabulary mitigate problem need perform following additional evaluation either assuming mention boundary marginalizing possibility treating latent variable truly latent showing type evaluation beyond perplexity example bleu meteor human evaluation corresponding generation task major weakness writing term technical accuracy completeness found many detail opaque confusing even wonder main challenge hinders quality writing something three specialized model detail worked extremely important main story nonetheless negligible order understand going perhaps restructure important detail clearly worked main body especially term latent variable handling make mention detection conference resolution truly latent entity update help current version elaborated mentioned briefly third application coreference resolution without empirical comparison motivate update operation,5
707.json,explores language modeling application explicit modeling reference expression dialog receipt generation coreference important task done number experiment limited reason clearly written pretty hard follow detail particular many obvious math error missing marginalization page pointer switch section major novelty seems dimensional attention table pointer table customization existing work particular task table part input seqseq attention pointer network empirical conclusive limited either relatively small data size lack well established baseline application recipe generation task overall suitable workshop rather main conference,4
642.json,study impact using customized number representation accuracy speed energy consumption neural network inference several standard computer vision architecture including googlenet considered experiment concluded floating point representation preferred fixed point representation floating point number bit sufficient considered architecture resulting small loss accuracy provides nice overview floating fixed point representation focus important aspect deep learning well studied several aspect could improved overall leaned toward weak accept assuming address issue clear focusing neural network inference please include word inference title abstract clarify point mention finding necessarily apply neural network training training dynamic could different discus possibility adopting quantization trick training result fewer bit inference clear whether computing running time power consumption includes module multiply accumulate unit also accurate number given different possible design potential difference simulation production please elaborate detail simulation whole discussion efficient customized precision search seem unimportant important hardware consideration concerned even spending simulation time important exhaustive search process could easily parallelized rather spend time simulation cost finding exact best configuration rather approximation said weak configuration could easily filtered evaluating example nvidia pascal support discussed relevant nvidia paper document cited comment part discussing efficient customized precision search clear future work impact number representation batch normalization recurrent neural network could studied,5
592.json,refreshing elegant handling sampling problem good reconstruction requires node latent layer sampled creative regime one choose offer sensible solution problem real life data set like cifar tried reader hard pressed choose many natural solution parallel classifier choose best epitome spirit spatial transformer reference list hope find conference address important problem elegant paper like secondary note regarding terminology avoid using term section many term related ultimately get control generative error descriptive term minimizing indispensable generative quality variational error example also term equation reference upper bound commonly used formula equivalent expression formula reference latter expression frequently used handy importance sampling reference,5
592.json,present version variational autoencoder us discrete latent variable mask activation latent code making subset epitome latent variable active given sample justification choice letting different latent variable active different sample forced latent code usual problem latent variable pruning important highlighted literature context variational inference proposed solution seem solve beyond instance mixture vaes indeed mixture vaes great baseline experiment us categorical variable mixture component along multiple vaes main difference mixture epitomic sharing parameter different mixture component epitomic case experimental section present misleading likelihood proposed model evaluated parzen window estimator significantly accurate lower bound likelihood available vaes reported reviewer experience continuous mnist likelihood upwards nats easy obtain modestly sized exposition change dealing binary mnist continuous mnist experiment confusing version dataset present different challenge modeling likelihood based model continuous mnist harder high capacity likelihood optimizing model dataset lie proper subspace dimensional space pixel always almost always equal hence probability density arbitrarily large subspace model maximize likelihood often exploit option maximizing likelihood concentrating probability around subspace expense actually modeling data sample well tuned trained binary mnist trained continuous mnist noise appropriately added tend look much better one presented experimental claim us capacity overfit training data justified evidence presented reconstruction likelihood training data significantly higher reconstruction likelihood test data misleading technical term like overfitting mean something else dropout dropout specified dropout applied latent variable hidden layer encoder decoder option exhibit different behavior mnist evae sample reconstruction look like diverse version sample reconstruction blurry encode precise position stroke consistent interpretation evae kind mixture smaller vaes rather higher dimensional misleading claim outperforms high dimensional based evidence reviewer opinion ready publication stronger baseline evaluated evidence lower bound another reliable method essential comparing proposed evae vaes,4
711.json,proposes rasor method efficiently representing scoring possible span extractive task test squad released look likely going state said idea enumerating possible span proposed could potentially improve many architecture well written analysis ablation final section mostly interesting especially figure confirms intuitively believe based potential positively impact researcher working squad recommend accepted,6
711.json,present architecture answer extraction task evaluates squad dataset proposed build fixed length representation span answer document based recurrent neural network outperforms baseline exact match squad unfortunate blind test obtained copyright issue quite system submission squad leader board available comparison given result test reported grid search hyperparameters directly also concern even though cross validation experiment,5
711.json,proposed rasor address problem finding best answer span according given question focus mainly relationship question answer span idea proposed reasonable ground breaking analysis interesting potentially useful hope extra mile analyze different choice boundary prediction model make convincing case necessity modeling score span globally main idea behind rasor globally normalize rank score possible answer span rasor able achieve first modeling hidden vector word lstms representation text span formed concatenating corresponding hidden vector start word corresponding chunk approach reasonable earth shattering also table show improvement prediction point large appreciate fact conduct several analysis experiment quite interesting example seems question independent representation also import performance addition current analysis also want clear idea make current better match lstm hyper parameter tuning question independent representation another good thing proposed relatively simple chance proposed technique combined newly proposed one,5
341.json,proposed novel adversarial framework train demonstration third person perspective perform task first person view adversarial training used extract novice expert third person first person independent feature agent perform policy different view point idea quite elegant novel enjoy reading experiment needed justify approach probably important issue baseline train image viewpoint better proposed approach close performance change gradually change viewpoint third person first person another important question maybe network blindly remembers policy case extracted feature could artifact input image implicitly count time tick thus domain agonistic still perform reasonable policy since experiment conduct synthetic environment might happen easy check algorithm multiple viewpoint blurred differently rendered image random initial condition ablation analysis also needed example fully convinced gradient flipping trick used experiment ablation analysis style training versus gradient flipping trick experiment error bar convincing,4
341.json,present interesting problem setup imitation learning agent try imitate trajectory demonstrated expert said trajectory demonstrated different state observation space accessible agent although dynamic underlying shared proposes solution strategy combine recent work domain confusion loss recent method based generative adversarial network believe general problem relevant agree natural formulation imitation learning might widely applicable however issue current state make fall short great exploration novel idea list concern following arbitrary order feel time hurriedly written also mainly manifest experiment comment make fairly strong claim introduction opinion backed approach example advancement class algorithm significantly improve state robotics enable anyone easily teach robot skill given current method understanding issue come standard training instability requires accurate simulator work well since trpo require large number simulated trajectory step seems like overstatement sentence ungrammatical switch tense middle sentence making harder read necessary page find simple approach able solve problem general idea third person imitation learning nice clear least understanding also novel however instead exploring generally adapt current algorithm setting pick specific approach find promising using gans extend significant amount time spent explaining current algorithm fail third person setting fail situation based approach different existing algorithm clear reason behavioral cloning could extended domain confusion loss exactly approach presented nice rather discus algorithm adapted also test one cannot straightforward approach apply algorithm example train autoencoders domain share higher layer domain confusion loss highest layer result feature directly usable general argument existing algorithm fail proposed setting seems reasonable still unfortunate attempt made validate empirically comparison made regarding happens performs supervised learning behavioral cloning using expert observation transfer changed domain well work practice also fast different algorithm solve target task general assuming first person perspective although like idea presenting experiment directed towards answering specific question feel like posed question somewhat distract main theme question suddenly make additional velocity information main point importance experiment regarding question contain evaluation regarding hyperparameters ignoring parameter parameter trpo number rollouts iteration number presented expert episode design choice understand evaluated thoroughly conference feel like experiment least discussion helped presented experimental evaluation somewhat hide cost trpo training obtained reward function many roll out necessary step experiment lack detail expert trajectory obtained domain pendulum experiment seem identical except coloring pole correct surprised small change seems detrimental effect figure show average performance trial figure also average performance variance given gans easy train often training fail able hyperparameters across experiment update updated score please response rebuttal,5
654.json,present method training probabilistic model maximizing stochastic variational lower bound type objective training involves sampling learning transition based inference walk back sample data focus transition used learn transition operator rather purely learning energy based objective intuitively appealing similarity previous successful le principled training method mrfs like contrastive divergence idea algorithm appealing look like could find nice place literature however submission current form ready publication experiment qualitative generated sample obviously indicative high quality pointed elsewhere mathematical analysis currently demonstrate tightness variational bound case learned transition operator evaluation using annealed importance sampling estimate held likelihood necessary assuming analysis repaired ability directly parametrize transition operator interesting strength method explored experiment contrasted standard energy based modeling look like promising idea review question already raised important technical point help strengthen future submission,3
654.json,much like underlying idea convinced execution current state primary concern expressed review question think addressed specifically think choice make forward reverse trajectory almost pathologically mismatched thus make variational bound extremely loose high variance claim tightness bound appendix relies assumption transition distribution obeys detailed balance learned transition distribution obey detailed balance therefore tightness claim appendix hold section briefly discus idea learning energy function rather directly learning transition distribution think excellent case could choose mcmc transition operator obey detailed balance energy function appendix beyond step experimental visually impressive suspect primarily driven mismatch generative inference trajectory concern review question also note suspect term dropped training gradient optimizing variational bound likelihood really really really report compare likelihood competing method detailed comment written based previous version first paragraph difficult follow mode spurious mode spurious mode mcmc mcmc chain ideally mcmc mcmc mcmc ideal often best last bullet could make temperature infinite last step case last step sample directly prior posterior prior exactly using energy function great especially many mcmc transition operator obey detailed balance le prone suffer forward backward transition mismatch primary concern technique alpha depend temperature never specified last paragraph section note also depends theta backpropagating full chain dropping term gradient equilibrium thermodynamics note noneq also increase noise variance distance data increase right left mislabeled label pane many walkback step,4
654.json,proposes kind generative based annealing process transition probability learned directly maximize variational lower bound likelihood overall idea clever appealing think need quantitative validation better discussion relationship prior work term prior work raise closely related algorithm share much mathematical structure proposed method reason sufficient mention passing related work section method relationship variational walkback need discussed detail understand correctly proposed method essentially extension raise transition probability learned rather fixed based existing think interesting worthwhile extension relationship existing work need clarified analysis appendix seems incorrect derives formula ratio prior posterior probability formula hold assumption constant temperature case ratio large temperature varied analysis neal applies answer different main selling point method optimizes variational lower bound likelihood even accurate estimate obtained using importance sampling ought easy report likelihood estimate method wonder estimate reported lot prior compare mnist addition natural baseline raise check ability learn transition actually help think basic idea sound willing raise score issue addressed revised version minor comment recognized obstacle training undirected graphical model training requires sampling mcmc chain inner loop training example seems like unfair characterization since standard algorithm usually take single step mini batch method discussed related work missing citation method justified term carving energy function right direction point sure actually happening point method optimize lower bound likelihood therefore learn globally correct allocation probability mass,3
480.json,review title verbose accurate title current overall better title probably somewhere overall approach interesting three technique task skip diagonal connection internal label kind data available make sense found hard understand interpret explanation discussion helpful earlier question benefit including explanation worthwhile briefly mentioning relationship diagonal connection emerging term similar idea skip connection skip seems accurate regardless draw network whereas diagonal make sense certain visual layout response comment discussion leading le segmentation action bout corresponding discussion section like assumed bout refers action event certain understood correctly bout last minute given readership think inappropriate define thing explicitly response comment behaviour last minute millisecond interesting curious know classification accuracy relates time scale behaviour mistake long term behaviour realize tell part story behaviour long term duration also different short term characteristic many behaviour easy classify accuractely despite long term easy investigate comment hard investigate probably worth point although something might want look future response comment scaling human behavior agree principle adding conv layer directly sensory input right thing seriously usually pretty work actually work sure aware indeed sure much experiential detailed understanding limitation work presented nice system demonstrated handle spatiotemporal trajectory claim made correspond consider adjusting rating depending future revision,6
746.json,holding review hoping missing detail code,3
746.json,basic idea contribution nice worth pursuing powerful divide conquer algorithm design strategy learn better program task sorting planar convex hull however execution idea convincing need polishing acceptance right proof concept feel make great workshop contribution main concern method presented currently easily applicable task typically demonstration program induction input output example well known task serf purpose proving generic learning machine able solve well known task useful task generality contribution however present learning machine hand tailored chosen task essentially demonstrates enough engineering hardcoding recurrency structure designing problem specific rule supervision lower recurrency level partially trainable sorter convex hull solver found contribution relatively hard understand high level idea mixed level trick required work clear either model operate much actually learned much designed answer question make mixing description trick required solve task make thing even confusing believe much accessible instead promising general solution clearly stated challenge faced possible solution highlight proof concept partially trainable implementation important divide conquer paradigm explicit reasoning complexity induced program solution generic enough applicable unknown problem network require trick specific problem writing style picture method general fall back level detail specific task,3
746.json,find extremely hard read main promise train model combinatorial search procedure especially dynamic programming learn split merge present methodology supposed make form scale invariance property scarcely motivated problem approach relevant however general research direction fruitful important much readable start clear formal problem formulation followed schematic view overall flow description part supervised part also tabular form sample various kind problem solved method could listed beginning motivation clear description central paradigm motivate rest concrete manner instead quite chaotic switching level high level detail problem formulation solution somewhat random hard parse order split merge phase seem make discrete choice hierarchical manner training explain discrete choice backpropagated network unbiased manner case general direction exciting frustrating read present form spent several hour without manage achieve clear mental image presented piece together revise score improved greatly readability perspective think require major rewrite,2
603.json,considers code completion problem given partially written source code produce distribution next token sequence token interesting important problem relevance industry research propose lstm sequentially generates depth first traversal surprisingly improve previous approach brittle conditioning mechanism bielik still simply augmenting previous work lstm based conditioning enough contribution justify entire direction greatly improve contribution include considering distinct traversal order change predictive accuracy way dealing token ultimate goal improve code completion great beyond simply neurifying previous method comment last sentence related work claim method examine limited subset source code aside vague statement accurate model described bielik maddison tarlow principle condition part already generated difference work lstm learn condition flexible increase complexity computation denying prediction experiment interesting number prediction accuracy accurate predict think also interesting accurate ground truth clearly model trained ignore loss worse overall worse token,4
603.json,study problem source code completion using neural network model variety model presented simple variation lstms adapted peculiarity data representation chosen code represented sequence nonterminal terminal pair terminal allowed empty another minor tweak option deny prediction make sense context code completion probably better make prediction unsure come next empirically show performance worse previous work predicting terminal better predicting nonterminals however find split terminal nonterminals strange clear takeaway surely simple proxy care often system going suggest next token actually appears code compute report single number summarize performance overall flavor lstms existing dataset amazing also issue writing could improved total think enough contribution warrant publication iclr detailed comment find ntnt strange predicts nonterminal terminal independently conditional upon hidden state discussion related work need reworking example bielik generalize work listed start section maddison citation wrong,3
603.json,overall direction promising several serious issue affect novelty validity incorrect claim related work affecting novelty work first explore deep learning approach automatic code completion toward deep learning software repository also us deep learning code completion cited code completion statistical language model pldi cited incorrectly also code completion recurrent neural network phog independent javascript representation learning applied language python oopsla submission automatically extract feature high precision cited baseline structured generative model natural source code incorrect citation icml also linear condition context claimed submission us comparable prediction task terminal symbol type prediction made simpler used phog state oopsla thus claimed point improvement substantiated particular javascript type node however phog oopsla prediction considers type also whether right sibling child node necessary predicting tree fragment instead sequence node however make prediction harder considered lead label increase comparing state state however basic phog cited probabilistic code decision tree oopsla appeared submission deadline iclr,3
551.json,summary propose histogram based state representation differentiable motion model observation update state tracking observation linear gaussian noise used motion neural network used learn measurement track robot state hallway arena positive show encode prior knowledge state transition architecture assumption observation learned purely data better accuracy baseline limited training data negative motion simplistic response earlier question generic feed forward neural network could used complicated motion however novelty framework clear proposed couple neural network learn motion observation model observation simplistic dimensional observation proposed generic feed forward network technical novelty clear histogram based representation scalable also highlighted hence proposed approach cannot applied complicated setting figure compare state estimation accuracy baseline lstms clear accuracy lstm saturated larger scale experiment training data sample note sample efficiency desirable property also discussed section expect model prior knowledge work better small number sample model assume structure experiment larger number sample insightful,3
551.json,propose time series discrete state robotics application think proposed method simplistic useful presented form state space dimensionality topology exactly matched experiment displacement transition linear action observation dimensional seems quite behind current state embed control watter state representation learned directly pixel furthermore compare method except lstm also feel like must prior work combining hmms think necessary relate work literature,2
443.json,proposes online variant segment segment transducer allows circumvent necessity observing whole sentence making target prediction mostly build previous work allowing additionally leverage independent prior target hypothesis like language grammar sentence length strong point well written interesting idea combining various source information bayesian framework seqseq model handling something online manner typically make thing difficult trying definitely interest community strong experimental section strong though complete weak point weak point improve computational complexity tillmann proposal hence algorithm found difficult apply scenario input long already take account rather constrained alignment latent variable baseline combine direct bias contribution channel obvious algorithmic constraint included minor comment related first weak point elaborate clue work conceptually different work tillmann except course fact connectionist discriminative model derive particular conditional probability sensitive different choice hyper parameter naively search search space something clever comment detail auxiliary direct definitely interest crucial correct choice pruning variable make markovian assumption first order markovian assumption typo table chanel channel last apology late review,6
443.json,proposes neural noisy channel input sequence pair based previous work segment segment neural transduction ssnt noisy channel difference sequence sequence complete sequence observed beforehand ssnt handle problem elegantly performing incremental alignment prediction however present anything particular novel ssnt ssnt still applicable reverting input output sequence said unidirectional lstm used encoder instead bidirectional lstm think difference minor decoding algorithm presented appendix relatively experimental study comprehensive strong however important baseline number missing experiment give number us direct bias give direct bias number even better although using direct make sense mathematically however work pretty well practice rescore smooth prediction deep speech speech recognition english mandarin baidu example think also explain noisy channel much better direct table couple minor question clear direct experiment ssnt sequence sequence training complexity great computational cost still expensive long input sequence example paragraph document level modeling speech sequence well written overall still interesting channel always great interest general public,5
455.json,introduces energy based generative adversarial network provides theoretical empirical modeling number image datasets including large scale version category imagenet know energy based gans ebgan introduced bengio proposed version make number different design choice first away entropy regularization term bengio introduced ensure discriminator converged energy function proportional density data optimum implies discriminator proposed scheme become uniform convergence discussed theoretical section however introductory text seems imply otherwise could recover meaningful score function trained energy function discriminator clarified second version ebgan setting includes innovation introduction hinge loss value function auto encoder parametrization energy function innovation empirically justified disappointing really good empirical supporting argument made support introduction significant contribution theoretical analysis energy baesd formalism showing optimum corresponds nash equilibrium impressive empirical large image standard straight style model achieve theoretical seem solid make nice contribution regarding quantitative table seems appropriate bold ebgan line seems statistically indistinguishable form rasmus though mentioned bold typically indicates state think could much stronger novel contribution energy based setting thoroughly explored ablation experiment said think already become contribution building including least iclr submission think accepted publication iclr,6
455.json,proposes novel extension generative adversarial network replaces traditional binary classifier discriminator assigns scalar energy point generator output domain discriminator minimizes hinge loss generator attempt generate sample energy discriminator show nash equilibrium condition yield generator match data distribution assuming infinite capacity experiment conducted discriminator taking form autoencoder optionally including regularizer penalizes generated sample high cosine similarity sample minibatch pro well written topic interest many set stage exploration wider variety discriminator currently used training gans theorem regarding optimality nash equilibrium appear correct thorough exploration hyperparameters mnist experiment semi supervised show contrastive sample generator improve classification performance con relationship work broaden scope discriminator generative network provide contrastive sample energy based made clear visual inspection alone difficult conclude whether gans produce better sample gans lsun celeba datasets difficult ass effect regularizer beyond visual inspection inception score computed vanilla specific comment unclear reconstruction loss necessarily produce different gradient direction confusing pulling away abbreviated seems strange inception trained natural image used compute score mnist using mnist trained compute inception style score seems appropriate figure little variation across histogram figure enlightening appendix proof theorem unclear nash equilibrium system exists typo minor comment abstract probabilistic gans probably traditional classical gans theorem nash equilibrium exists several paper presented overall concern related work experimental evaluation section feel novel enough well justified optimality proof quality generated sample springenberg jost tobias unsupervised semi supervised learning categorical generative adversarial network arxiv preprint arxiv taesup yoshua bengio deep directed generative model energy based probability estimation arxiv preprint arxiv,6
793.json,summary proposes surprisal driven feedback training recurrent neural network feedback next step prediction error network input network shown result language modeling task contribution introduction surprisal driven feedback feedback error previous time step question point fully clear whether used ground truth label test surprisal feedback part assume since claim misprediction error additional input criticism really badly written rethink organization equation presented bptt necessary main text could moved appendix justification convincing enough experimental lacking single dataset provided although claim sota enwiki paper hypernetworks better result achieve claim wrong requires ground truth label test however assumption really limit application technique limited application le rule conditional language modeling task high level review pro simple modification seems improve interesting modification con need test label writing assume access ground truth label test experimental lacking,3
793.json,proposes previous error signal output layer additional input recurrent update function order enhance modelling power dynamic system rnns make erroneous assumption test label information given real world application except application mean language modelling task experiment right task test approach also comparing model test error signal inference time unfair cannot test label information observed hold online prediction problem experiment conducted dataset reporting state result unfortunately true already four paper reporting better number reported task however author cite understand came paper manuscript updated final decision size still missing without information hard judge contribution proposed trick,2
793.json,proposes leverage surprisal signal specifically author us error corresponding previous prediction extra input current timestep lstm general idea suprising driven feedback interesting online prediction task simple enough idea seems bring significant improvement however current form important flaw overall writing could improved particular section composed mostly equation forward backward propagation feedback feedback lstm however author provides analysis along equation therefore clear insight author try express section addition feedback evaluated experimental section clear feedback described experimental evaluation limited dataset enwik explored think necessary idea different datasets feedback lstm see consistent improvement also author claim state enwik hypernetwork already cited achieves better table hypernetworks author compare method last prediction error extra signal argue comparison dynamic evaluation fair feedback lstm us prediction error extra input forward prop dynamic evaluation backprop network change weight accordingly also propagate prediction error leverage extra supervised information prediction error summary pro interesting idea seems improve performance con writing weak evaluation dataset compare approach last timestep error signal,2
402.json,interesting algorithm seems clear problem well recognized strong plausible approach hyperparameter optimization based smbo struggled make good convergence training present fresh look smbo alternative least thought reviewer pointed much overlap previously published successive halving algorithm still excited cautiously optimistic simple alternative smbo first advance search skeptical practitioner since case random search grid search,7
402.json,present hyperband method hyperparameter optimization trained gradient descent iterative scheme build successive halving random search approach jamieson talwalkar address tradeoff training fewer model longer amount time many model shorter amount time effectively idea perform multiple round successive halving starting exploratory setting round exponentially decreasing number experiment granting exponentially resource contrast recent paper topic approach rely specific underlying learning curve therefore make fewer assumption nature seem show approach highly effective often providing several factor speedup sequential approach overall think good contribution hyperparameter optimization literature relatively simple implement seems quite effective many problem seems like natural extension random search methodology case early stopping seems like hyperband useful problem random search expected perform well computational budget sufficiently constrained squeezing absolute best performance feasible near optimal performance sufficient personally like plot figure enough method time converge order optimal near optimal really sure agree randomx baseline useful comparison demonstrates benefit parallelism sequential method virtually method also parallel extension think randomx shown also like smacx spearmintx tpex also think worth seeing forth hyperband fare baseline,6
394.json,propose conceptually simple method regularisation recurrent neural network idea related dropout instead zeroing unit instead respective value preceding time step element wise certain probability overall well written method clearly represented issue raised reviewer review question phase related work complete probably best currently available matter regularising rnns experimental section focus comparing method current sota benchmark synthetic problem experiment focus sequence discrete value additional experiment also show sequential jacobian higher long term dependency dropout case overall bear great potential however point raised review question like experiment feature complete hyper parameter search proper selection process standard community done especially author count seems indicate necessary resource available want repeat point table show validation error reliable estimator testing error respective data thus overfitting selection process serious concern zoneout seem improve much task zoneout investigated well mathematically analysis form gradient unit time step unit time step interesting especially necessarily zero dropout also question whether zoneout variational interpretation spirit yarin work obvious treat zoneout resnet framework dropout incremental part overall little effort done answering question zoneout work well even though literature bear plenty starting point analysis data set used symbolic great ground covered continuous data dynamical system obvious whether transfer right away extreme amount trick published currently improved training zoneout stand nice idea simple implement however delivers experiment convince provide convincing theoretical insight either consequently reduces epsilon improvement great text mediocre experimental evaluation little theoretical insight,6
681.json,find general direction work promising opinion three main drawback motivation overall idea seem reasonable derivation convincing mathematically experiment limited presentation need significant improvement writing wording general poorly structured point sometimes difficult follow proposed idea overall organization need improvement connection section properly established could significantly improved simply writing fully convinced motivation proposed linearity described page argue waldspurger suggests higher order nonlinearities might beneficial sparsity unless missing something work seems suggest general case higher order nonlinearities neglected could please comment hand adding second order term descriptor seems interesting direction long stability small variation preserved shown experimentally experimental section rather limited stronger thorough numerical evaluation presented opinion show convincingly clear advantage proposed method standard implementation scattering transform order show merit proposed approach really helpful directly compare running time compression rate question show empirically proposed higher order nonlinearity produce sparser representation complex modulus minor issue proof section preceded clear statement form proposition hadamart hadamard valid validation nonzeros coefficient nonzero coefficient figure difficult understand please provide detail figure supposed show comparison standard implementation scattering network seem comparison figure please explain please verify reference first reference state mallat,3
681.json,overview work seems promising believe compared baseline precisely described explained signal processing point view pro descriptor fast implementation con lack rigor long accordingly content computational gain algorithm clear work compared obvious baseline scattering transform detail con section author motivates scattering transform defines contraction space relies geometric feature nonlinearity used scattering network complex modulus piecewise linear real modulus piecewise linear complex modulus shape bell interpreting could clarify omega introduced could give precise reference page claim higher order nonlinearity refers instead usually done scattering network section motivation linearity clear first linearity might potentially increase variance architecture since depends higher moment think fair analysis compute numerically normalized variance divided averaged norm sanity check besides prove energy decreasing possible argue architecture similar scattering transform precise mathematical foundation required since setting different permutation relevant variability notion sparsity whole sometimes refers number value either norm mathematically small value even still value compute graph figure bird dataset might ratio instead clarity wavelet defined morlet wavelet,3
619.json,consider simple optimization technique consisting adding gradient noise specific schedule test method number recently proposed neural network simulating computer logic memory network neural programmer neural random access machine network question optimization studied extensively standard network study specific class model therefore welcome consistently show better optimization property adding noise training procedure issue clear whether proposed optimization strategy permit learn actually good model simply better noise comparison obtained literature desirable example mnist experiment section optimization procedure reach favorable scenario average accuracy level approximately still actually learned interesting problem representation linear probably reach similar accuracy understand architecture specially designed difficult optimize layer interesting consider scenario depth actually beneficial solving problem,3
619.json,present simple method adding gradient noise improve training deep neural network first appeared arxiv year many innovation area improving training deep neural network time batch normalization rnns layer normalization normalization propagation mention compare method particular state however recent work applying batch normalization recurrent network laurent shown promise improving generalization ability recur rent architecture focus work statement simply incorrect thoroughly explored cooijmans establish batch normalization effective rnns proposed method extremely simple similar numerous training strategy previously advocated literature result contribution incremental best could significant sufficiently strong empirical supporting particular variant however discussed multiple training strategy algorithm literature empirically compared unfortunately fairly seriously date appropriate publish iclr,3
619.json,propose noise gradient computed optimizing deep neural network stochastic gradient based method show multiple data set indicate method counteract parameter initialization especially beneficial training complicated architecture method tested multitude different task architecture convincing accompanied confidence interval understand experiment must taken long like include situation gradient noise help situation seem much optimization initialization tool employed quantity experiment variety model provide quite convincing evidence effect gradient noise generalizes many setting always convincing section method helped significantly optimal training scheme used example mnist good compared state since method simple hoping theoretical argument usefulness said experimental investigation importance annealing procedure comparison effect gradient stochasticity comparison weight noise provide additional insight well written cite relevant prior work proposed method described clearly concisely expected given simplicity proposed idea original acknowledge similar algorithm used training pretty much identical simulating langevin dynamic goal finding single optimum mind rather approximating expected value work evaluation tool model become bigger complex despite lack novelty method think valuable method easy implement seems useful complicated hard initialize important others field know suspect many people least method variety architecture task method useful suggests many people also repertoire optimization trick pro idea easy implement method evaluated variety task different model interesting experiment compare method similar approach investigate importance annealing scheme well written con idea original clear theoretical motivation analysis convincing,6
530.json,understanding relation object important task domain like vision language robotics however model trained real life datasets often exploit simple object property relation based identify relation animal bigger size typically predator small size animal prey model predict relation without necessarily understanding given difficulty task controlled setting required investigate neural network designed actually understand pairwise object relation current take significant step answering question controlled dataset also multiple experiment presented validate relation learning ability proposed relation network dataset proposed ensures relation classification model succeed learning relation object exploiting predator prey like object property present thorough experiment validate claim truly learn relation object particular ability force simple linear layer disentangle scene description latent space permuted description interesting clearly demonstrates learns object relation shot experiment demonstrate ability convincing manner requires understand relation represent abstract label assign label future sample relationship graph suggestion permutation invariant well since work pair object ensure invariant order object pair need operate pair object order identify pairwise interaction however practical application complicated group interaction ternary interaction person riding bike wear helmet require operate pair every possible subset object scene generally pairwise edge based approach scalable larger number object mention deep network sufficiently large number parameter large enough training capable matching performance interesting point could true practice investigated effect trying identify minimum capacity training example required match performance provided setup help quantifying significance practical application limited example word task could benefit another plot performance different amount training sample simulation setup current great first step towards analyzing relation learning ability still clear transfer real life datasets strongly encourage experiment real life datasets like coco visual genome hico stated review stage minor terminology object scene description used refer abstract entity misleading reader object detection domain computer vision could clarified early introduction minor like show ability generalize unseen category quite interesting could moved main draft completeness proposes network capable understanding relationship object scene ability thoroughly investigated series experiment controlled dataset currently evaluated simulated dataset quite promising could translate real life datasets well,6
530.json,proposes relation network order pairwise interaction object visual scene straight forward first shared weight applied pair object finally prediction created operates summing linear function pair object experimental evaluation done synthetic dataset generated architecture hand crafted title claim much delivers discovering object relation important task however discover object relation instead object represented hand coded ground truth attribute small trivial relationship discovered relative position discovering object relationship tackled several decade computer vision cite compare technique body literature typically refer contextual model proposed architecture help object detection scene classification work presence noise missing detection accurate detection estimate complex texture work attribute object estimated real image convinced experiment done real scene case indoor scene datasets nyuv scenenn chen cvpr text image correference could used outdoor scene kitti relationship car pedestrian cyclist could also serve benchmark without showing real scene tackle problem simple much current context model pairwise relationship object mrfs deep net,2
530.json,proposes relation network relation input entity object relation network built stage first lower level structure analyzes pair input entity pair input entity structure next output lower level structure aggregated across input pair simple used input higher level structure basic version structure multi layer perceptrons mlps overall interesting approach understanding relation among entity core idea clear well motivated pooling technique induce invariance used learn relation idea build pooling structure spatial temporal average pooling focus pairwise relation current pairwise approach could potentially extended higher order interaction modulo scaling issue experiment scene description image verify efficacy relation network baseline used incapable modeling structured dependency present task interesting know pooling operator across object pooling data augmentation permutation effective training mlps task regardless proposed novel effective handling relation show promise higher level reasoning task,6
475.json,present training deep generative model discrete hidden variable using reparameterization trick applies particular like architecture show architecture achieves state density modeling performance mnist similar datasets well written exposition thorough precise several appendix justify various design decision detail wish paper field take degree care exposition likelihood quite strong especially given competitive algorithm based continuous latent variable probably main thing missing experiment separate contribution architecture inference algorithm comparable architecture trained vimco algorithm applied previously published discrete architecture concerned variance gradient general formulation algorithm comment variance derivative think response convincing problem well engineering principle smoothing distribution probably worth pointing since problem seems likely occur unless user aware proposal widely separated normal natural distribution consider actually work gradient something commonly done autodiff framework another concern many sequential operation needed inference note actually general boltzmann machine distribution take form autoregressive variable processed time section mention possibility grouping together variable distribution elaborated detail appendix solution requires decomposing joint product conditionals applying cdfs sequentially either seems like stuck handling variable sequentially might expensive minor second paragraph section need reference appendix,8
475.json,interesting handle reparameterization vaes discrete variable idea introduce smoothing transformation shared generative recognition leading cancellation second contribution introduce prior autoregressive connection generative recognition model whole package becomes entangled complex hard figure cause claimed good performance experiment study contribution separately nice framework become little complex problem nice software delivered used plug play mode overall rich idea think great contribution conference,7
567.json,develop learn subspace multiple view data point neighborhood similar view similarity measured distribution neighbor pair view motivation natural criterion information retrieval like idea preserving neighborhood relationship across view retrieval task nice learned space different dimensionality different view however empirical validation seems preliminary revised iclr submission revision welcome think still need work order publishable current form could good match workshop track experiment small data set example train test mnist task real task point focusing efficiency presumably computation requirement keep considering larger data set however clear conclusion drawn apply realistic data set considering wealth work done multi view subspace learning application real task hard contribution without showing applicable realistic setting minor point claim information retrieval based approach exist think overstated example contrastive loss hermann blunsom multilingual model compositional distributed semantics related information retrieval natural compare presentation sloppy number vague point confusing wording example term dependency get used rather colloquial get confusing time since used technical context using technical definition information retrieval task analyst vague quite grammatical probability analyst inspected item next pick inspection well defined discussion divergence quite follow reasoning relationship cost miss help make precise perhaps drop divergence pretty well motivated anyway penalty added used instead confused stated iteratively find component pair note defined iterative operation need typically solved rather projection found done apply nonlinear dimensionality algorithm algorithm quite follow task case image patch stock price minor comment typo figure font small difference measure different measure since hence grammatical feature based view view external neighborhood,3
588.json,applies convnet based object detection technique detection weather event climate data additionally exploring effect using unsupervised autoencoder style objective term pro application object detection technique extreme weather event detection problem unique knowledge well written describes method well including survey related work best make convolution unsupervised learning relatively unexplored detection literature aspect validated shown produce least small performance improvement purely supervised approach con benefit convolutional architecture unsupervised learning little underwhelming semi result result strange semi worse base result expect aspect give slight improvement base result given using together give best perhaps thorough enough hyperparameter search case acknowledge provide potential explanation however reviewer pointed criterion true positive loose relative standard criterion hand visualized figure typical overlap criterion could reasonable domain detector seem localize event well enough system could used expedite human review climate image extreme event still useful also report higher overlap threshold minor probably squared norm square rather norm minor table semi supervised model parameter corresponding supervised one decoder layer overall well written applies interesting underutilized technique relatively unique domain striking ablated appropriately shown beneficial final version nice higher overlap threshold,5
588.json,edit thoughtful author response addressed major concern github link data code really helpful reproducing looked carefully great revision addressed many issue including additional upgrading rating recommend acceptance proposes apply deep net perform detection localization extreme weather event simulated weather data problem related object detection computer vision input image multichannel spatial weather data video temporal version data output bounding spatial temporal localization weather event class label weather event type differs standard object detection input multiple heterogenous channel labeled data scarce simple quite reasonable deep proposed task based similar approach computer vision proposal based system popular vision currently particular faster rcnn proposed approach simple fine starting point little innovation part detection system noted valid application idea computer vision task hand propose supervised approach ground truth bounding location label used semi supervised approach additionally incorporates reconstruction loss regularization case loss fairly standard reasonable confusing semi supervised loss actually label used supervised loss additionally incorporates reconstruction loss hence semi supervised loss actually stronger make terminology confusing easy follow notation sloppy example equation state loss weighted combination reconstruction error bounding regression loss actually combination supervised unsupervised loss lsup lunsup lrec defined although assume lrec lunsup fairly technical nevertheless minor issue fixed also reference figure biggest concern though experimental single figure table shown figure table metric defined mean average recall versus version shown supervised semi supervised moreover number seem place without consistent pattern supervised better seemingly much strong semi supervised thing unclear many event actually training testing data importantly good absolute term regardless experiment fairly sparse ablation study discussion lacking also unclear future researcher able reproduce experimental setting commitment open source data reproduce experiment critical future minor classification loss objectness loss never seen used together like normally objectness used stage object proposal system first stage class agnostic proposal given second stage cropped class specific classifier applied strongly suspect removing objectness loss impact since classification loss provide strictly stronger supervisory signal regardless fairly standard choice justified experimentally overall borderline believe valuable apply computer vision technique domain little work community said expertise type data possible deep learning technique routinely used climate science literature suspect though overall little novelty algorithmic side equation section commonly used literature reconstruction loss improve data sparse setting interesting experimental inconclusive experimental validation generally insufficient reproducibility future research difficult unless data open sourced overall think good start improved experiment careful writing think could eventually make decent,5
422.json,present variational inference based method learning nonlinear dynamical system unlike deep kalman filter proposed method learns state space force latent state maintain information relevant prediction rather leaving implicit observation experiment show proposed method better able learn meaningful representation sequence data proposed dvbf well motivated part presentation clear experiment show interesting illustrative example think contribution interesting potentially useful recommend acceptance svae method johnson deserves discussion sentence devoted since method seems pretty closely related like dvbf svae imposes markovianity assumption able handle similar kind problem understand important algorithmic difference svae network predicts potential whereas dvbf network predicts innovation tradeoff section say latter interest solving control related task clear follows reason svaes meet desideratum mentioned introduction since svae code publicly available could probably compare experiment confused role uncertainty principle could estimate transition parameter maximum likelihood fitting point estimate done instead integrated part marginal likelihood interpret giving flexibility different dynamic different sequence case distribution depend data rather data independent,6
422.json,proposes standard svgb sequential setting like several previous work however proposes clear state space constraint similar linear gaussian model markovian latent space conditional independence observed variable given latent variable however case linear assumption well motivated goal meaningful latent variable experiment interesting still completely convinced regression figure namely could obtain angle velocity state using function powerful linear function also watter included rereading sure understand coordinate combined checkerboard said figure well motivated resulting novel enough bouncing ball experiment quite convincing especially prediction problem fully determined initial velocity position,5
658.json,argued response reviewer comment interpreted encoders decoder well interesting perspective could potentially worth however current draft convincing respect talking updated improved version require minor revision make point apparent significant major rewrite seems beyond done experiment also pointed reviewer rather unstructured difficult much insight probably also list reviewer flaw issue experiment given detailed comment reviewer seems little additional value essence simply deliver point promise concerned sense suggest clear reject conference dataset employed mnist considered dataset pretty much purpose day sense dataset choice helping many people might want convince safe,2
571.json,proposes approach boosting generative model based likelihood ratio estimate approach evaluated synthetic data well mnist dataset task generating sample semi supervised learning idea boosting generative model proposed method interesting reviewer find experiment unconvincing following reason bagging baseline section seems refitting dataset raising probability power alpha renormalizing make peaked clear good baseline please know misunderstood procedure sample generation experiment section us slowly converging markov chain seen similarity plot seems unlikely therefore resulting sample stationary distribution qualitative evaluation using seems necessary section choice alpha seem quite arbitrary happens obvious choice alphai made seems hard infer anything semisupervised classification reported baseline seems perform well boosted model work mostly clearly written reviewer know original,4
434.json,show work rnns used lstm operator applied hidden hidden input hidden contribution separately experiment conducted show lead improved generalisation error faster convergence well written idea well presented data set consequently statistical assumption used limited continuous data autoregressive generative modelling hyper parameter nearly constant experiment ruled picked favor method judging text different learning rate could lead equally fast convergence vanilla lstm concluding experiment flawed sufficiently support claim exhaustive search hyper parameter space could rule,6
526.json,develops theoretical guarantee convergence training error result quite general cover training wide range neural network model idea approximate training loss linear approximation since linearity variable thus convex plug developed literature online learning good novelty using taylor approximation thus greatly simplifying analysis behaviour however problem main result theorem clear taylor optimum converge noticed upper bound path dependent appendix try claim taylor optimum indeed converges proof buggy proof lemma proved difference sequential taylor optimum approaching note actually weaker cauchy sequence insufficient guarantee convergence lefthand side equation denote review equivalent training error upper bound average error sufficient guarantee convergence training error neither take gradient descent example thus minibatch whole training convergence training error infty convergence necessary sufficient imply convergence training error another concern theorem minor compared problem mentioned achieve sqrt rate algorithm pick particular learning rate larger smaller learning rate order lead significantly worse regret experiment learning rate picked according theorem overall good motivation good novelty could developed good problem buggy proof mentioned think ready publish,2
526.json,adopts taylor approximation neural net separating convex convex component optimization enables bound training error taylor optimum regret theorem nice theoretical result applicable popular deep net empirical study back theoretical claim,6
463.json,look train significant label noise present good main method proposed first latent variable training require algorithm alternating estimating true label maximizing parameter given true label second directly integrates true label simply optimizes pro examines training scenario real concern dataset carefully annotated con mnist synthetic hard tell translate real datasets comment equation expensive happens training imagenet class nice well recover corrupting distribution parameter using either integration method overall however idea novel previous cited paper tried handle noise label think make better either demonstrating state dataset known label noise demonstrate method reliably estimate true label corrupting probability,4
463.json,addressed erroneous label problem supervised training problem well formulated presented solution novel experimental justification limited effectiveness proposed method hard gauge especially scale proposed method large number classification target whether still effective example interesting whether proposed method better training le high quality data figure seems data proposed method tends behave well noise fraction threshold dramatically degrades passing threshold analysis justification behavior whether chance expected method useful,6
674.json,proposes provide theoretical explanation deep convolutional neural network invertible least going back certain intermediate layer image considering invertibility single layer assuming convolutional filter essentially correspond incoherent measurement satisfying opinion interesting direction research ready publication feel treatment sufficiently towards explaining phenomenon deep neural network even reading response feel minor variation standard compressive sensing sparse reconstruction incoherent measurement deep neural network fundamentally different single layer deep part make forward task work note significant deterioration applied recursively therefore best theory explains partial invertibility single layer single layer approximately invertible surprising cascade layer theoretical analysis phenomenon useful believe must beyond analyzing single compressive measurement type layer explain much theory hold cascade entirely possible sparse recovery theory break beyond single layer invertibility end property caused correlation weight different layer word tell current individual layer whether fact step towards explaining invertibility whole network,3
674.json,propose theoretical framework analyze recoverability sparse activation intermediate layer deep network using theoretical tool compressed sensing relate computation performed particular recovery algorithm iterative hard thresholding present proof necessary condition recoverability hold also show detailed empirical evidence hold practice well written present angle current architecture work well give brief sufficient review fundamental compressed sensing present main result relating feed forward network surprising result progress naturally detailed experimental section introductory analysis beginning section particular delivers gist method work approachable simple math common theoretical paper increasing complexity experiment done small step show nice progression artificial distribution realistic experiment aspect improved first although treatment relu linearity sufficient assumed little discussion pooling linearity present problem well discussion inverted pooling switch needed relationship feed forward net algorithm assumes tied weight might worthwhile mention result stronger case rnns case design although might obvious might help reader briefly note reconstruction algorithm meant applied layer sequentially basing activation layer back propagation order finally filter coherence measure must defined either mathematically proper reference,6
674.json,summary study invertiblity convolutional neural network random reconstruction algorithm similar proposed layer wise inversion network clarity confusing standard notation deep learning comment make simplification analysis make based compressive sensing framework linearity relu dropped simplification random gaussian weight instance know preserve distance relu applied metric change instance kernel,4
731.json,method introduces binary encoding level dbow document embedding method mikolov binary encoding consists sigmoid trained parameter inserted standard training stage embedding document encode binary vector obtained forcing sigmoid output binary output embedding vector component binary vector used compact storage fast comparison document pro binary representation outperforms semantic hashing method salakhutdinov hinton experimental approach sound compare experimental setup salakhutdinov hinton since meantime document representation improved mikolov also combine representation show benefit binary dbow con insertion sigmoid produce binary code training process incremental explanation abstract difficult follow expert detail comparison efficient indexing method used image retrieval missing large scale indexing embedding vector derivation inverted multi index probably interesting binary code babenko lempitsky efficient indexing billion scale datasets deep descriptor cvpr detailed comment section motivation producing binary code given also experimental section could give timing usage number show benefit binary embeddings figure enough space include information representation parameter training objective characteristic size dropout particular clear embedding lookup linear projection cannot merged single smaller lookup table presumably intermediate training objective prevents length binary code tied dimensionality word embeddings section experimental setup salakhutdinov hinton specify whether difference setup similarity inferred code code compared using hamming distance binary code perform well despite lower capacity mean smaller size real vector plot could dropped space needed section could argue transferring wikipedia anything else cannot called transferring since wikipedia purpose include topic lexical domain section specify real vector compared distance inner product specify performance large embedding vector without filtering binary code equivalently perf code size hamming,5
731.json,present method represent text document paragraph short binary code allow fast similarity search retrieval using hashing technique real valued paragraph vector mikolov extended adding stochastic binary layer neural network architecture method binarizing final activation compared simply adding noise sigmoid activation encourage discritization binarizing activation forward pas keeping real valued backward pas straight estimation present encouraging using straight estimation newsgroup text datasets using binary code plus side application presented interesting important exposition clean clear however novelty approach limited machine learning standpoint literature binary hashing beyond semantic hashing krizhevsky binary autoencoders explained important baseline missing real valued paragraph vector learned first converted binary code using shelf hashing method random projection charikar kulis darrell gong lazebnik norouzi fleet given lack novelty missing baseline recommend current publication iclr conference proceeding moving forward suitable conference applied side comment believe practical perspective easier first learn real valued paragraph vector quantize indexing said approach proposed perform better like empirical comparison proposed approach simpler stage quantization method suggested estimating propagating gradient stochastic neuron bengio discussing straight estimation alternative argues length binary code cannot longer bit longer code suitable document hashing quite right given multi probe hashing mechanism example mult index hashing norouzi hashing similarity search survey wang survey related work binary hashing quantization seem ignore extensive work done binary hashing,4
731.json,work proposes learn short binary code paragraph vector allow fast retrieval document experiment show superior semantic hashing approach simple technically interesting code size loss compared continuous paragraph vector seems moderate asks reader refer salakhutdinov hinton baseline number think placed easy reference simplicity could show precision recall proposed semantic hashing also seems semantic hashing show twice size english seems comparable interesting many binary bit required match performance continuous representation comparison continuous dbow trained bigram also make fair comparison figure show loss using real binary dbow seems user needed high quality ranking retrieval stage could afford extra space computation better standard dbow obtain continuous representation stage minor comment first line introduction sheer sheer line bottom word embeddings word embeddings table code size refer dbow number element continuous vector line bottom line section cover wide cover wide,5
361.json,address problem predicting learning curve difference prior work learn neural network generalizes across hyperparameter setting bayesian neural network sghmc demonstrate proposed approach effective extrapolating partially observed curve well predicting unobserved learning curve various architecture seems promising bayesian optimization love experiment evaluates relative advantage proposed method thought way handle learning rate decay perhaps could algorithm random subset data extrapolate thinking evaluation measure addition practice care promising make sense evaluate accurately method identified best minor comment font small almost illegible hard copy please increase font size legend ax figure figure seem line line overlapping case,6
361.json,proposes bayesian neural network architecture predicting value learning curve training machine learning model exploratory ultimate goal method bayesian optimization system experiment limited assessing quality prediction build previous work domhan however work incorporates information tested hyperparameter setting rather extrapolating single learning curve also explores mcmc method inference sgld sghmc tell either tested domhan well performance seems overall positive particularly initial phase curve little information case expected sharing knowledge across curve help regime seem tested might informative curve training mostly fully observed might case sharing information really help something concern approach timing stated train network take second worst case epoch little hour spent training bayesian network trivial fraction several hour take train tuned bayesian network make many separate prediction shown figure interesting accurate individual piece example bound asymptotic value learning curve since mostly predicted accuracy value tend minor question comment figure ax read validation accuracy figure describe lastseenvalue although seems self explanatory good explicit bottom left figure used anywhere else baseline figure table predicting final value curve every value along curve conditioned previous value basis function sufficiently capture flexibility learning curve basis function help hurt,6
418.json,work introduces novel method training gans displacing simultaneous unrolling inner optimization minmax game computational graph clearly written explains justification well problem attacked significant important approach novel however similar idea tried solve problem unrelated gans first quantitative experiment section attempt find best generate training example done using bfgs claim able find generator generate particular training example demonstrated step gans able generate many training example unrolled gans however find experiment unreasonable able find certain generates certain sample guarantee particular mode high probability fact identity function potentially beat model proposed metric cantor proof equivalence power real space applies smaller dimension well realistically possible generate image generator finding specific certain exists generate sample prove generator missing mode prof generator similar enough identity function able generate possible image metric thus measuring something potentially tangential diversity mode dropping another problem metric showing optimization able find specific training example prove exist harder find comparison might showing unrolled gans smoother function step gans thus easier optimize second quantitative experiment considers mean pairwise distance generated sample data sample first number likely small case mode dropping argue number closer indication generated sample diverse data metric convincing distance measured pixel space could generating garbage still perform well metric quantitative even though method optimizing diversity sanity check score quality inception score performance useful another metric consider training using approach mnist dataset concatenation mnist digit easily identifiable mode demonstrate able generate mode equal probability perfect metric either arguably much better metric metric used iclr submission,6
418.json,present approach tackling instability problem present generative adversarial network general idea allow generator peek ahead discriminator evolve decision boundary time premise information prevent generator collapsing produce sample single mode data distribution well written clearly motivates attack important open issue experiment well carried strongly support presented idea pursued approach substantially elegant current existing hack commonly used make gans work practice however three main issue partly doubt success method resolved clear candidate acceptance entirely convinced effect cannot obtained following procedure simply train discriminator extended number step updating generator number equivalent unrolling step used current experiment generator updated undo update discriminator update step instead briefly glanced response reviewer seems imply tried something similar setup stopping gradient flow appropriate point although think exactly equivalent tried reproduce simple mnist example using fully connected network instead generator without much success even unrolling discriminator step generator still engages mode seeking behavior train could either implementation peculiarity generator batch normalization anywhere latter entail dependence proposed approach specific form discriminator generator discussed code found,6
789.json,attempt learn markov chain estimate probability distribution latent variable eased generate sample data distribution current form acceptable following reason quantitative evaluation include sample generative however insufficient judge performance comment description unclear indulge charity interpret must mean mean true posterior generative typically vaes variational approximation true posterior trying sample true posterior comment using additive noise input seem like reasonable idea justification done approach learn transition operator usually amenable data augmentation based semi supervised learning encourage improve testing semi supervised learning benchmark,2
623.json,investigates hessian small deep network near training main result many eigenvalue approximately zero hessian highly singular mean wide amount theory apply overall point deep learning algorithm singular undercut many theoretical important already made watanabe almost learning machine singular focus growing body work investigating phenomenon general reference could fleshed much variety prior work examined hessian deep learning dauphin identifying attacking saddle point problem high dimensional convex optimization nip work amari others experimentally hard tell small sized network considered might translate much larger network seems likely behavior much larger network different reason optimism though fact clear bulk outlier behavior emerges even network characterizing behavior simple system valuable overall feel preliminary likely interest fleshed attacking important problem better situating related literature experiment sufficient size reveal large scale behavior relevant practice,2
623.json,analyzes property hessian training objective various neural network data distribution study particular eigenspectrum hessian relates difficulty local convexity optimization problem several interesting insight discussed local flatness objective function well study relation data distribution hessian somewhat lacking aspect described effect presented general tested specific setting without control experiment mathematical analysis example regarding concentration eigenvalue zero figure unclear whether concentration effect really caused training increasing insensitivity local perturbation consequence specific choice scale initial parameter figure complexity data defined clear whether fully overlapping distribution hessian become zero considered complex simple data plot legend label unreadable printed format plot figure range axis image hessian matrix figure render properly printed format,3
336.json,refreshing openai taken time resurrect classic heuristic like sampling dropout pixelcnn sort technique like pixelcnn probably hold missing key needed eventually decent originally created image cifar real life data set engineering streamlining welcome general public especially helping avoid expensive cluster gpus deepmind afford sense openai fulfilling mission grateful thus welcome addition hope find appears experimental conference anyway conceptual level hope openai talented team stop competing contrived contest improve basis point certain obscure likelihood instead focus bigger picture problem example almost year later class conditional cifar sample left figure column class horse still inferior sample figure reference forgive beating dead horse resolution sharpness improved good engineering nobody general public take sample seriously despite claim forward deepmind team pixelcnn fully generative neural rigorously defined section reference merely perturbative vain boltzmann machine procrustean experience lost decade boltzmann machine time perhaps come think fundamental le heuristic,8
766.json,proposes pedestrian detection method using fast rcnn framework batch normalization edgeboxes used collect pedestrian proposal instead selective search used original fast rcnn method proposed method evaluated inria dataset pro proposed method show good performance state con lack novelty fast rcnn variant fasterrcnn,2
766.json,apply commonly used fast rcnn detection system pedestrian detection edgeboxes object proposal incorporate batch norm network shown inria pedestrian datasets reasonable state shown caltech pedestrian standard modern dataset used evaluate pedestrian detection perhaps importantly novelty detection system described standard application fast rcnn pedestrian detection implementation state novelty work edgeboxes used fast rcnn seem aware recent development object detection including faster rcnn,1
770.json,proposes incorporate knowledge base fact language modeling thus time step word either generated full vocabulary relevant entity demonstrate effectiveness generated dataset wikifacts aligns wikipedia article freebase fact also suggest modified perplexity metric penalizes likelihood unknown word high level like motivation named entity word usually important downstream task difficult learn solely based statistical occurrence fact encoded could great supply however find difficult follow detail mainly section think writing need much improved cannot find symbkey voca copy defined confusing seems average previous fact embeddings necessary make clear enough flstm used notion fact embeddings also clear understand taken concatenation relation entity object entity anchor topic fact learn embedding special relation entity embeddings transe generating word entity fact description sound strange generate symbol position first entity multiple word necessary keep order also might helpful incorporate prior information example common mention obama entity barack obama,5
770.json,proposes evolution upon traditional recurrent language model give capability deal unknown word done pairing traditional rnnlm module operating able copy fact generate unseen word shown efficient much better plain rnnlm dataset writing could improved beginning section particular hard parse similar effort recently like pointer sentinel mixture model merity attempt overcome limitation rnnlms unknown word usually adding mechanism copy longer past history proposal current different interesting bring knowledge another source language harder need leverage large scale able train conveniently nice architecture appears sound writing make hard fully understand completely give higher rating comment cope dependency freebase updated anymore likely unseen word making going freebase performance standard benchmark like penn tree bank long train compare standard rnnlm importance knowledge context initialized fact embedding first word word fact description chosen prediction copied encoded generation history following prediction embedding unknown word word happens michelle example section embedding dictionary want predict next word,5
770.json,address practical problem generating rare unseen word context language modeling since language follows zipf approach limit vocabulary computation reason hence rare word often mapped token rare word especially important context application question answering proposes language modeling technique incorporates fact knowledge base thus ability generate potentially unseen word also release dataset aligning word freebase fact corresponding wikipedia description first selects fact based previously generated word fact based selected fact predicts whether generate word based vocabulary output symbolic word latter trained predict position word fact description overall could rewriting especially notation section experiment well executed definitely good heat map insightful comment contribution much stronger showed improvement practical application question answering although clearly mention technique could applied improve section unclear refer entity topic make text little confusing since topic also associated something abstract case topic always freebase entity really necessary predict fact every step generating word word many distinct fact average choose generate sentence intuitively natural language sentence describe fact entity fact generation step could avoided adding latent variable decides fact generated also faster equation make hard decision choose fact trained every word need annotated corresponding fact might always realistic scenario domain social medium text learning position embeddings copying knowledge word seems little counter intuitive sequence knowledge word follow particular structure like word always last name obama also nice compare char level inherently solves unknown token problem,5
320.json,proposes novel approach learning visual servoing based iteration main contribution bilinear dynamic predicting next frame feature based action current frame formulation servoing function learns weight different feature channel elegant method optimizing bellman error learn function pro good exploring different way connect action frame representation predict next frame feature argue favour locally connected bilinear strike balance computation expressive ability con make good argument different choice liked experimental comparing approach fully connected convolutional locally connected dynamic pro idea weighting different channel capture importance obejcts different channel seems effective treating error across channel equally also validated experimentally unweighted performance suffers consistently solving bellman error difficult problem learning approach current present solid optimization scheme based observation scaling function parameter affect best policy chosen enables elegant approach opposed typical optimization scheme gamma minu fixed con however liked difference iterative approach hold second term fixed experimental overall find experimental unsatisfying given small scale simulation however lack benchmark domain need recognized also pointed review section idea modifying need experimentally validated current form clear whether modified perform better original version overall contribution solid term technical novelty problem formulation however could stronger experiment suggested earlier bolster claim,6
320.json,investigates benefit visual servoing using learned visual representation propose first learn action conditional bilinear visual feature obtained trained policy derived using linearization dynamic multi scale multi channel locally connected variant bilinear presented since bilinear predicts dynamic step ahead proposes weighted objective incorporates long term value current policy evaluation problem addressed using fitted value approach well written mathematically solid conceptually exhaustive experiment also demonstrate benefit using value weighted objective important contribution also seems first outline trust region fitted iteration algorithm trained visual feature also shown help empirically generalization overall recommend benefit many researcher robotics however context conference find contribution specifically representation problem limited show trained representation useful consider learning proportionally speaking spends time control problem representation learning also policy representation fixed value approximated linear form using problem specific feature make le valuable perhaps le aligned think iclr,7
320.json,summary proposes tackle visual servoing specifically target following using spatial feature map convolutional network trained general image classification task combine bilinear model step dynamic visual feature map multiple scale reinforcement learning algorithm learn servoing policy policy learned minimizing regularized weighted average distance feature predicted aforementioned visual dynamic contribution controlled experiment simulation quantifying usefulness trained deep feature visual servoing clear performance benefit respect many sensible baseline including one using ground truth bounding box principled learning multi scale visual feature weight efficient trust region fitted iteration algorithm handle problem distractors good sample efficiency thanks choice function approximator based step visual feature dynamic open source virtual city environment benchmark visual servoing suggestion improvement complex benchmark although environment synthetic experiment benefit greatly complex visual condition clutter distractors appearance motion variety environment richness diversity least realism diversity object appearance could vastly improved using larger number model including realistic diverse one obtained google sketchup instance populating environment distractor car traffic parked important main desired quality approach robustness visual variation representation learning although improvement already significant current synthetic experiment interesting measure impact training also fine tuning convnet possibly needed better generalization challenging visual condition also allow measure benefit deep representation learning visual servoing relevant iclr representation learning although method straightforwardly adapted mention briefly reproducibility formalism algorithm clearly explained slightly overwhelming mass practical trick implementation detail described varying level detail throughout appendix grouping simplifying reorganizing exposition implementation detail help better probably consist summarizing important one section link open source implementation method completeness typo learning relative recent addition applied directly learn conclusion spite aforementioned limit experiment interesting solid part thanks excellent reply review question subsequent improved revision lead believe capable following significant extent aforementioned suggestion improvement thus leading even better,6
459.json,proposed nice framework leveraging tucker tensor train rank tensor factorization induce parameter sharing multi task learning framework nice appealing however well studied problem considers simple task different classification clear really need deep learning simple datasets comparison existing shallow necessary show benefit proposed method particular deep dataset ignore basis speculation clear proposed framework really superior simple regularization like nuclear norm idea nuclear norm regularization also extended deep learning gradient descent popular method,4
459.json,proposed deep multi task representation learning framework learns cross task sharing structure every layer deep network tensor factorization knowledge sharing approach removed requirement user dened multi task sharing strategy conventional approach experimental indicate approach achieve higher accuracy fewer design choice although factorization idea exploited past task think applying interesting thing want point saving parameter rank factorization conventional layer weight size also reduced used recent neural network explored first earlier work cited speech recognition community huang deng gong cross language knowledge transfer using multilingual deep neural network shared hidden layer ieee international conference acoustic speech signal processing ieee,7
662.json,proposed dynamic neural turing machine overcomes rigid location based memory access used original main contribution introducing learnable addressing curriculum learning using hybrid discrete continuous attention proposed empirically evaluated facebook babi task shown improvement original pro comprehensive comparison feed forward controller recurrent controller encouraging curriculum learning hybrid discrete continuous attention con weak baseline hyper parameter engineering table comparing reported table graf hybrid computing using neural network dynamic external memory fact baseline graf better proposed controller maybe worthwhile reproduce using hyper parameter setting table could potentially lead better performance section hard follow overall clarity need improvement,5
662.json,introduces variant neural turing machine graf value stored continuous discrete mechanism control memory quite complicated seem require trick work overall seems different term appear cost function many different hack required learn hard understand justification trick sophisticated choice code available plan release afaik evaluated problem babi task achieves performance slightly vanilla lstm much worse different memory augmented model proposed last year term writing description quite hard follow describing different block independently optimization trick regularization equation hard read using standard notation softplus overloading notation write similar equation different way example compared equation scalar vector arrow instead equal overall hard together piece code available afraid enough detail able reproduce number finally performance babi task quite poor compared memory augmented model,3
399.json,proposes method significantly increasing number parameter single layer keeping computation even le current sota model idea based using large mixture expert small network adaptively activated gating network idea seems intuitive main novelty designing gating network encouraged achieve objective utilizing available expert importance distributing computation fairly across load additionally introduces technique increasing batch size passed expert hence maximizing parallelization gpus experiment applying proposed approach rnns language modelling task show beat sota significantly le computation result selectively using much parameter machine translation show number parameter beat sota incurring half effective computation several comment believe better presentation currently page long opinion find section crux need better motivation intuitive explanation example equation deserves description currently devoted additional space easily regained moving detail experiment section architecture training detail appendix curious reader experiment section better organized finishing experiment completely moving also glitch writing section missing important reference conditional computation,6
399.json,describes method greatly expanding network size term number stored parameter context recurrent applying mixture expert recurrent layer shared time step process feature timesteps time effective batch size increased factor number step thus even sparsely assigned expert expert used large enough batch input remain computationally efficient another second technique redistributes element within distributed also described increasing expert batch size experiment performed language modeling machine translation task showing significant gain increasing number expert compared well explicitly computationally matched baseline system area fall short presenting plot statistic real computational load system behavior loss term employed balance expert explored experiment section nice effect along effect increasing effective batch size measurement loss course training compared count histogram distribution expert batch size overall think well described system achieves good using nifty placement overcome otherwise might disadvantage sparse computation small comment like entirely clear whether datapoints coincide left right plot line point left right also nice color matched corresponding line,6
376.json,performs important service exploring clear systematic performance trainability characteristic neural network architecture particular basic motif recently popular pro address important question many others liked know answer computational resource thoroughly attack nice google resource help community work appears done carefully believed basic answer arrived typical training environment lstms reliable basically grus answer seems fairly decisive practically useful course real answer complicated little summary subtlety discussed nicely insistence strong distinction capacity trainability help nicely clear misconception reason gated architecture work much easily trainable somewhat lower capacity vanilla rnns hard task benefit better trainability outweigh cost mildly lower capacity point near equivalence capacity equal number parameter useful make clear importance tuning something sometimes gotten lost vast flow paper architecture idea quantifying fraction infeasible parameter diverge nice practical problem everyone working network often addressed text clearly written con work ugrnns rnns seems preliminary think clearly shown recommended generality least want better statistic significance difference performance quantifying figure layer panel high standard declaring architecture useful make ugrnns contribution seem le important really mind though guess point really novel first place totally fine though know iclr area chair think give short shrift detail algorithm setting tuner internal parameter us batched bandit expected improvement acquisition function matern kernel feature scaling automatic relevance determination performed optimizing kernel give good reference expect actually trying replicate involves missing detail found figure hard read first mostly panel small detail choice visual cleanliness neuroscience reference bit synapse seems little throw away connection experimental neuroscience tenuous rate well explained guess discussion seems gratuitous maybe couched slightly le strong term nothing really strongly shown agreement computational architecture neuroscience perhaps could something like wonder anything coincidence bit result numerically similar bit measurement neuroscience,7
726.json,summary develops generalization dropout using information theoretic principle basic idea learning representation input predicting must choose carry least amount information long predict idea formalized using information bottleneck lagrangian lead optimization problem similar derived variational dropout difference information dropout allows scaling factor associated divergence term encourages noise amount noise added made parameterized function data function optimized along rest experimental cifar mnist show small improvement binary dropout strength highlight important conceptual link probabilistic variational method information theoretic method showing dropout generalized using formalism arrive similar model presentation excellent experimental cluttered mnist impressive weakness cifar figure seem validation unless axis label typo clear test used make hard compare reported springenberg well literature quality theoretical exposition high quality figure give nice qualitative assessment however experimental section made better example matching cifar reported springenberg trying improve using information dropout clarity well written easy follow originality derivation information dropout optimization problem using lagrangian novel however final quite close variational dropout significance general interest researcher representation learning highlight alternative think latent variable information bottleneck however unless shown achieve significant improvement simple dropout wider impact likely limited overall present insightful theoretical derivation good preliminary experimental section improved minor comment suggestion expecially especially trough probably minus sign missing expression figure error bar might good idea figure well please consider comparing figure activity standard trained binary dropout similar filtering happening already,5
726.json,interesting connection made dropout tishby information bottleneck vaes specifically classification split face inference prior classifier optimizing objective data lambda lambda information bottleneck formed lambda control upper bound number bit traveling objective equivalent objective downweighted posterior prior encoder take input decoder predicts related work section discussed sufficiently section better remind definition mutual information connection vaes section interesting unfortunately mnist cifar great since method potentially flexible form dropout slightly disappointing unclear cifar seem substantially worse originally reported architecture unclear version beta used figure overall think theory presented promising however lack sufficiently convincing experimental encourage experiment prove significant improvement least cifar perhaps larger problem,5
458.json,overview introduces biasing term theoretical example yield solution approximately equal lower generalization error come computational cost estimating gradient biasing term iteration stochastic gradient langevin dynamic approximating mcmc sample partition function modified gibbs distribution cost equivalent adding inner loop standard algorithm minibatch pro review distills many theorem past decade suggest promising forward increasing generalizability deep neural network generally well written well presented interesting discussion eigenvalue hessian characterize flat minimum promising mathematical argument suggest generalization error bounded motivating research area con point suggested rebuttal claim given abstract experiment competitive baseline demonstrate entropy lead improved generalization potential accelerate training appear supported current experiment comment discussion section experiment entropy comparable generalization error always lower cross entropy loss clear reconcile claim similarly claim accelerated training convincingly supported present version vanilla requires single forward pas minibatches epoch parameter update method requires forward pass epoch number langevin update require minibatch sample could fact mean worse computational complexity reach point remark note single epoch defined number parameter update required dataset clear answer objection factor additional computation required inner loop sgld iteration sgld appears introduces potentially costly tradeoff must carefully managed user previous point suggest could attention magnitude claim example introduction read actively biasing towards wide valley aid generalization fact optimize solely free energy term obtain similar generalization error original loss function according value reported mnist generalization error using free energy term partition function modified gibbs distribution equivalent using loss function corresponds setting equation cifar used another contribution characterization optimization landscape term eigenvalue hessian generalization error associated flat local extremum helpful interesting found plot clear useful another reviewer already pointed high level similarity flat minimum hochreiter schmidhuber responded already adding paragraph helpfully explores difference however similarity also carefully identified mentioned includes detailed theoretical analysis could helpful future work area independently discovered similar approach training generalizable network clear assumption eigenvalue made section appendix affect application result real world problem magnitude need chosen correspond measurable characteristic dataset little mysterious current version,7
458.json,note earlier version review almost identical present earlier version available arxiv found,5
458.json,introduces regularization term encourages optimizer search flat local minimum reasonably loss instead seeking sharp region loss motivated empirical observation local minimum good generalization performance tend flat shape achieve regularization term based free local energy proposed gradient term tractable closed form solution obtained performing monte carlo estimation using sgld sampler experiment show evidence flatness good local minimum also performance proposed method comparison adam optimizer well clearly written enjoyed reading connection concept free energy optimization framework seems interesting motivation pursuing flatness also well analyzed experiment wondering first term correct guess also wondering experiment evaluation performance char lstm text generation already used flatness experiment think adding experiment various model application deep architecture seqseq make author claim persuasive also found mixed usage terminology free energy free entropy confusing,6
458.json,present principled approach finding flat minimum motivation seek minimum better generalization ability idea original loss function term exploit width depth objective function fact regularization term interpreted gaussian convolution exponentiated loss therefore introduced regularization term essentially gaussian smoothed version exponentiated loss smoothing obviously tends suppress sharp minimum overall developing regularization term based thermodynamics concept interesting couple concern want clarify rebuttal reporting generalization performance experiment report number epoch showing proposed algorithm reach better generalization fewer epoch plain number epoch take line algorithm total number epoch line combined former fair comparison multiply number epoch line number iteration take approximate langevin dynamic seems obtain little gain plain proposed algorithm approximates smoothed exponentiated loss smoothing refer convolution gaussian wondering compare simpler idea smoothing original loss dropping exponentiation difference motivation thermodynamics interpretation deeper proposed scheme lends accurate approximation achieves better generalization bound term smoothness smoothing cost function without exponentiation allows simpler approximation monte carlo integration instead mcmc section,8
321.json,edited score present method hierarchical using stochastic neural network introduced using information theoretic measure option identifiability additional reward learning diverse mixture policy nice result comparison strong baseline directly combine intrinsic reward sparse reward show supposedly smooth reward solve task besides argument made difficulty long term credit assignment benefit hierarchical abstraction possible explanation might diversity requirement imposed policy training assumed baseline case wonder shed insight improving baseline proposing hierarchical policy learning hierarchical rep option critic paper nice visualization present promising direction strengthened possibly addressing following point limited diversification policy concatenation bilinear integration allow minimal differentiation policy first hidden weight problem tested task essentially require locomotion policy minimal diversification limitation obvious task ideal policy diverse thus interesting apply harder locomotion domain ideal policy similar manipulation solving task state different solving another state limitation hierarchical policy manager network trained policy fixed furthermore time step policy fixed requires intrinsic reward learned policy good solving stream task nice discussion handling case ideally connecting hierarchical policy learning intrinsic unsupervised reward seem domain specific supervised reward seems unavoidable,6
321.json,like setting presented several criticism question failure work richness behavior complex expected approach issue diversity skill could discovered looking random variable denoting grid agent currently situated space discretized happens expanding first point approach work complicated embodiment link swimmer instead think important ass generality approach claim recently heess independently proposed learn range skill training environment useful downstream task similar framework however training setup requires goal specified comparison intrinsic reward signal agent training phase construction requires minimal domain knowledge entirely agree reward proposes also quite hand crafted specific seemingly limited control task,7
622.json,show spin glass technique introduced choromanska analyze surface loss deep neural network applied deep residual network interesting contribution seems similar one choromanska thus novelty seriously limited main theoretical technique described already introduced main theoretical mentioned fact already proved also lot assumption choromanska path independence assumption weight distribution,2
622.json,summary study resnets theoretical formulation spin glass conclusion resnets behave ensemble shallow network start training examining magnitude weight path specific length change training scaling parameter assumption increase causing behave ensemble deeper deeper network clarity somewhat difficult follow heavy notation perhaps notation overloading summary proof main text might helpful specific comment proof lemma sure sequence beta come follows resnet structure used somewhat different normal multiple layer skipped analysis used layer skipped seems like skipping mostly affect number path certain length experiment supporting scale increase practice interesting sure theorem necessarily proving link theoretically however particularly given simplifying assumption start section,6
675.json,first want point really long like page long without supplementary material iclr official page limit nice reviewer shoe take undue advantage rule page addition conventional page limit doubling page quite unfair review proposes artificial dataset sequence learning call artificial artificially generated original mnist dataset smallish dataset real image handwritten digit addition dataset propose train recurrent network using schedule length sequence call incremental learning experiment show proposed schedule better schedule data furthermore also show proposed schedule better intuitive schedule verify ablation study proposed dataset following issue find anything novel proposed incremental learning schedule nothing natural thing learning sequence similar idea already tried number including bengio ranzato piece work ablation study conduct tease verify indeed improvement performance curriculum used furthermore test hypothesis single dataset propose artificially generated real sequential dataset language modeling technique work scenario fact quite positive language modeling vocabulary size huge performance gain close reported convinced value artificial dataset already many real world sequential dataset available including text speech finance area exactly dataset bring table super clear another dataset thing almost felt dataset created sole purpose making proposed idea work much better shown experiment datasets said long significant part length collection experiment completely related main message instance experiment section completely unrelated story true transfer learning experiment section,2
675.json,submitted proposes learning sequence predictor line incremental learning curriculum learning easier sample presented first complexity increased training particularity complexity defined length sequence given training premise longer sequence harder learn since need complex internal representation targeted application sequence prediction primed prefix tested single dataset extract mnist idea interesting worth reading also many interesting aspect evaluation part perform several ablation study rule side effect test proposed learning strategy compared strategy however biggest concern still evaluation tested method single dataset standard derived mnist given general nature claim order confirm interest proposed algorithm need tested datasets public datasets different application long trimmed significantly transfer learning part prediction classification different story clear connection main contribution presentation organization could improved quite sequentially written sometimes read like student report loss given long unnumbered equation page better explained provide explanation term make clearer different symbol mean learning supervised variable prediction observation data ground truth name table correspond description section,4
675.json,present thorough analysis different method curriculum learning major issue dataset used seems specific necessarily justified mentioned anonreviewer great experiment standard task also really understand performance ffnn model good please elaborate last comment however well written comparison described method interesting probably apply datasets well long page please reduce move appendix section method described extremely similar described reinforcement learning neural turing machine zaremba,4
730.json,proposes analysis three method applied traditional lstms monte carlo test time averaging average pooling residual connection show method help enhance traditional lstms sentiment analysis although well written experiment section definitely dead point firstly although show improvement traditional lstms state secondly purpose take extension strong baseline research experiment adequate datasets used quite similar though different statistic thus suggest carry experiment diverse task like lstm search space odyssey besides extension really novel,4
730.json,present three improvement standard lstm architecture used many neural model monte carlo averaging embed average pooling residual connection modification trivial implement definitely interest researcher experimenting deep learning said concerned experiment residual connection seem consistently help performance vertical residual help lateral residual hurt imdb opposite fundamentally need task sentiment analysis quite sure focus text classification task using lstm encoder could conceivably benefit modification great huge variety task like really make much stronger point experiment included thorough analysis interesting need task convince modification generalize think ready publication,4
730.json,agree reviewer application area limited agree overall sentiment evaluate effectiveness recent technique area conjunction recurrent network advertises method list method improving recurrent baseline performing experiment however fails shown generalize task effectiveness method need shown across wide variety task intend replace traditional baseline general rather specific subset application like desire evaluate many recent technique many replication experiment towards strong point however whether synergy enhancement sentiment analysis cannot interesting whether generalize across wide variety task,4
360.json,supervised learning significant advance occurred framework semi supervised learning adopted used weaker approach unsupervised learning infer property distance measure smoothness regularizer could used small number labeled example approach rested assumption smoothness manifold typically attempt stretch analogy reinforcement learning although analogy somewhat incoherent label equivalent reward function positive negative reward mean positive negative label still make worthwhile attempt explore notion semi supervised clearly important area deserves attention term labeled mean typical framework reward function unknown confusing term unlabeled mean situation reward unknown technically controlled markov process classical transfer learning setup agent attempting transfer learning source labeled target labeled reward function known learned policy known source semi supervised setting target unlabeled source labeled unlabeled basic approach inverse infer unknown label attempt construct transfer restriction made linearly solvable mdps technical reason experiment reported using three relatively complex domain using mujoco physic simulator work interesting opinion reviewer work fails provide simple sufficiently general notion semi supervised sufficiently wide interest community remains done future interim work sufficiently interesting problem certainly worthwhile study,5
360.json,formalizes problem setting subset available mdps access reward name setting semi supervised reinforcement learning ssrl reference semi supervised learning access label subset dataset provide approach solving ssrl named semi supervised skill generalization build framework maximum entropy control whole approach straightforward amount algorithm partial label alternate iteratively estimating reward function parametrized fitting control policy using reward function provide experiment task obstacle link reacher link reacher vision half cheetah mujoco well written overall clear appendix provides context think implementation detail missing able fully reproduce experiment provide code link inverse reinforcement learning seems done correctly however reference policy policy learning instance seems samp term equation could benefit variance reduction lambda precup retrace lambda munos experimental section convincing appreciate precision small discussion sentence extensively test generalization capability policy learned method measure performance wide range setting superset unlabeled labeled mdps number different scenario replacement superset union case explain better poor oracle obstacle link reacher reinforce sentence obstacle task true reward function sufficiently shaped learning unlabeled mdps hence reward regression oracle method perform poorly correction page tuple tuple overall think good sound personally unsure parallel reference previous work complete thus confidence score intended,6
360.json,proposes study problem semi supervised distinguish labelled mdps provide reward unlabelled mdps associated reward signal underlying simple since aim simultaneously learning policy based reinforce entropy regularization technique also reward used inverse reinforcement learning feedback unlabelled mdps experiment made different continous domain show interesting well written easy understand based simple efficient idea simultaneously learning policy reward resulting algorithm exhibit interesting property proposed idea quite obvious first one propose test experiment could made stronger mixing continuous discrete problem convincing,7
527.json,proposes extension multiplicative apply reparametrization trick weight matrix lstm proposes interesting trick none seems crucial instance propose multiply output gate inside activation function order alleviate saturation problem logistic sigmoid hyperbolic tangent also share across inference different gating unit cell state candidate brings time increase number parameter lastly variant rmsprop additional hyper parameter schedule across training time nicer apply trick baseline model show improvement regard trick architectural modification lstm trick combined performance great expect apply batch normalization layer normalization zoneout model issue applying regularization optimization technique fourth paragraph section connect dynamic evaluation fast weight misleading find hard connect dynamic evaluation variant fast weight fast weight test error signal claim dynamic evaluation us error signal gradient update weight potentially increase effectiveness also limit scope conditional generative modelling output observed predicted afraid tell assumption misleading never assume test label information given inference time test label information evaluate generalization performance application label information test time stock prediction weather forecasting however many application instance machine translation know best translation unlike weather forecasting also fair apply dynamic evaluation baseline model well compare score achieved proposed mlstm quality work novelty good either performance proposed oftentime worse method better dynamic evaluation coupled together however dynamic evaluation improve method well ilya generating text recurrent neural network icml,3
570.json,extends mostly work bilstm bilstm attention proposed following way train topic specific word embedding using approach similar paragraphvec leveraging topic title information provided data considers multiple unit answer selection problem sentence selected answer section another selected supplemental section single answer selection problem studied mechanism used retain coherence different part answer inspired attention mechanism introduced practical presented interesting main innovation rather limited,3
435.json,heuristic improve gradient descent image classification simple effective look like workshop track demonstration algorithm limited task cifar theory support know generalize task working dnns find observation opposite experience particular architecture combine wide variety layer type embedding gating found adam type technique outperform simple momentum save searching right learning rate type layer adam work well combined poliak averaging fluctuates batch another revision substantially improved content including experiment another cifar workshop track modified breakthrough work recommendation longer appropriate therefore improved rating,6
435.json,interesting investigation learning rate schedule bringing idea restarts often overlooked deep learning thorough study trivial datasets outcome fully conclusive good approach novel enough warrant publication thank revising based concern typo flesh flush,6
435.json,describes speed convergence sudden increase otherwise monotonically decreasing learning rate several technique presented clear parameterized method proposed evaluated cifar task concept easy understand chose state model show performance algorithm relevance go beyond image classification pro simple effective method improve convergence good evaluation well known database con connection introduction topic unclear hard read line bound maybe best setting tmult clearer baseline also seem converge remark loss surface tmult helpful also understanding relationship network depth performance method value analysis,6
659.json,describes recurrent transducer us hard monotonic alignment step discrete decision taken either emit next symbol consume next input token moderately novel similar architecture proposed speech recognition,4
589.json,investigates modeling graph sequence propose graph convolutional recurrent network grcn extends convlstm data unregular graph structure timestep replace convolution graph convolutional operator defferrad propose variation grcn graph convolution applied input data graph convolution applied input data previous hidden state evaluate approach different task video generation using movingmnist dataset world level language modelling using penntreebank movingmnist show grcn improves upon convlstm however evaluate layer convlstm report better layer also good grcn nice evaluate gcrcn setting well show improvement grcn relatively convlstm grcn task seems relatively weak compared recent work video pixel network kalchbrenner contradicts claim shown good performance case video prediction conclusion penntreebank experiment author compare lstm without dropout however zaremba still seems different reported zaremba report test perplexity large regularized lstm table outperforms score grcn also following work variational dropout zoneout since improve upon zaremba difference experimental setting nice directly comparable previous work pro interesting con overall proposed contribution relatively incremental compared defferrad weak grcn relatively previous work experiment convince grcn advantage,3
589.json,address problem modeling temporally changing signal graph signal node change function input hidden state neighborhood size hyperparameter approach follows closely generalized arbitrary graph structure rather fixed grid using graph convolution defferrard strict generalization graph formulation treat edge equally conv kernel built directionality show moving mnist penn tree bank language modeling task experiment decent concern proposed exceptionally novel technical perspective usually mind case provided make deficiency thorough experimental evaluation clear write interesting insight pro con approach respect previous model case lean towards case experiment section rather terse light interpretation fully date latest penn tree bank language modeling know hotly contested well known dataset surprised comparison zaremba expect multiple writing clear make sufficiently strong attempt compare model provide insight comparison proposed work better particular unless mistaken word probability function neighborhood graph width graph example suppose sample word part graph information propagate part graph along edge also clear achieve reasonable moving mnist cannot distinguish direction moving edge state provide satisfying insight work pixel know turn next frame wish thought presented clearly summary somewhat weak technical contribution experiment section thorough insight sparse,3
423.json,work brings multiple discriminator result multiple discriminator useful stabilizing main problem stabilizing seems gradient signal discriminator motivation using multiple discriminator reduce effect think work indicates direction promising however think consider result approach enforce discriminator gradient improving generative adversarial network denoising feature matching show advantage multiple discriminator,5
423.json,extends framework accommodate multiple discriminator motivate point view multiple discriminator tackle task equivalent optimizing value function using random restarts potentially help optimization given nonconvexity value function multiple discriminator help overcome optimization problem arising discriminator harsh critic generator receiving signal multiple discriminator le likely receiving poor gradient signal discriminator main idea look straightforward implement practice make good addition training toolbelt convinced extension gmam evaluation metric without evidence game converging even approximately hard make case discriminator tell something meaningful generator respect data distribution particular inform mode coverage probability mass learning curve figure look convincing provide good evidence increasing number discriminator stabilizing effect learning dynamic however seems like figure along figure also show unmodified generator objective stable even discriminator case even necessary discriminator train generator using unmodified objective overall think idea presented show good potential like extended analysis line figure datasets think ready publication update rating revised following discussion,6
531.json,proposes generative video composed background object sprite optimization performed framework proposal outer product softmaxed vector resulting delta like composed convolution interesting achieve translation image differentiable parameter seems attractive alternative complicated differentiable resamplers used stns translation needed made comment regarding part text especially experiment clear experimental section particular seems rushed alluded given even appendix extremely novel exotic proposal showing synthetic experiment could excused however though novelty method disappointing even attempt trying tackle problem real data suggest example aerial video taken drone platform since planar assumption make probably hold case also suggest another pas proof reading missing reference unfinished sentence caption aforementioned issue experimental exposition,3
531.json,present approach modeling video based decomposition background sprite latent hidden state exposition think approach sensible main issue lacking experiment synthetic datasets find graphic inspired question investigating interesting think clear work introduces useful machinery modeling general video think appropriate workshop contribution current form,3
474.json,discussion looked previous work rover based hand helpful better understand current hand needed better understand current overall think like work familiar chorale probabilistic approach gram find quite hard follow various part extensive notation help clarity think idea approach good certainly worth publishing worth pursuing sure current form iclr appropriate venue incidentally issue application think music application appropriate problem necessarily approach next suggestion sense long form journal publication actually give space necessary fully explain idea provide clearer running example needed provide necessary background appropriate readership provide necessary background previous system perhaps demonstrating second dataset show generality approach short conference seems dense format giving project description merit possible focus aspect system might work good suggestion exactly revised substantially though cannot suggest detail within appropriate page count consider raising score think effort better invested turning long clearer journal submission addendum based discussion revision revised score,5
618.json,present improved formulation aiming separate geometric transformation inherent feature network estimate transformation filter given input image work based solid technical foundation motivated plausible rationale value work practice subject question relies assumption input image subject transformation certain group locally transformation constitute real challenge practice state cnns resnet already quite resilient local deformation component state limited experiment cifar seem provide strong argument computational cost discussed,4
618.json,work applies steerable frame various task convolutional neural network location invariant operator traditionally applied provide detailed overview steerable frame followed experimental section applies dynamic steerable network small machine learning problem steerability conceptually useful even though evaluation performed small task reason task evaluated piece wise pose invariance needed subset task fact simply using overcomplete base sort feature processing improves already highly optimized resnet densenet architecture quite interesting achievement edge detection relatively hard baseline selected dynamic filter network already attempt achieve position invariant filter fact dsfn improves performance task verifies regressing parametrization steerable filter yield better regressing filter directly last experiment apply network video classification using lstms show improved performance increased capacity network general quite interesting work even though offer ground breaking mainly sense performing experiment larger task theoretically interesting show promising minor issue suggestion related lstm experiment order exact useful include information total number parameter network estimate pose also increase number parameter possible provide detail back propagation done steerable filter edge detection experiment useful provide standard baseline similar number parameter simply useful location variant filter task last sentence second paragraph page missing verb also maybe unnecessary hyphenation convnet incorrect multiple place probably hyphenation conv,6
395.json,propose software package probabilistic programming taking advantage recent successful tool used deep learning community software look promising potential transform work probabilistic modelling community allowing perform rapid prototyping iterate idea quickly composability principle used insightfully extension inference example going beyond inference simple implement using existing deep learning tool make software even compelling however important factor whether practical real world case demonstrated sufficiently submission many example code snippet given evaluated dirichlet process mixture example figure important proposed black inference tool really work snippet example figure converge optimised real data convince community practicality package necessary demonstrate empirically currently evaluated various inference technique difficult implement using pure presentation presentation could improved example could signalling explained page used without explanation could mention example given thereafter also suggest explain preface layer implemented handled example useful discus value optimised value change inference performed even section clear majority experiment time reported table difficulty around convergence encountered analytical entropy inference issue become difficult diagnose inference automated tool diagnose provided toolbox give sensible experiment bottom page time reported difficult inference work full control computational graph structure sampler extremely insightful give table comparing performance time predictive likelihood various inference tool model benchmark intend difficulty probabilistic modelling benchmark evaluate compare many model sensible caffe ecosystem exist benchmark large portion community working imagenet example datasets compare dpmm example minor comment table suggest compare turner alpha equivalent hellinger distance concluded value performs best sure alpha chosen handle discrete distribution figure xreal defined figure suggest highlighting figure comma instead period rized page conclusion software development presented quite exciting glad pushing towards practical accessible inference current form though forced give submission score,4
395.json,introduces edward probabilistic programming language built tensorflow python supporting broad range popular contemporary method probabilistic machine learning quality edward library provides extremely impressive collection modern probabilistic inference method easily usable form provides brief review important technique especially representation learning perspective combined experiment implementing various modern variational inference method accelerated first experiment variational inference valuable clear link complete code reproduce provided experiment look except characterising stan hand optimised implementation seems unfair code clearly hand optimised specific hardware configuration think anyone doubt quality implementation please ruin picture unsubstantiated sensationalist claim instead current suggest comparing head head stan single core separately reporting extra speedup gain parallelisation number also help reader estimate performance method hardware configuration clarity general clearly written easy read numerous code example helpful also difficult sometimes unclear missing helpful could provide clearly link machine readable companion jupyter notebook great even text html easier copy paste like complete runnable code example originality edward library clearly unique collection probabilistic inference method term main threat novelty come previous publication group main refers tran cover similar material although different perspective unclear published submitted somewhere significance seems likely edward profound impact field bayesian machine learning deep learning comment draw clear distinction specialised language including stan turing complete language edward seems unfair believe stan also turing complete additionally proof provided support turing completeness edward,7
680.json,summary proposes neural machine translation translates source target text manner character character learn morphology encoder decoder hierarchical decoder provide compelling various bilingual corpus different language pair well written competitive compared baseline literature review think well written like analysis presented clean precise idea using hierarchical decoder explored cite paper mainly application mainly application several existing component character level task sense good made code available online however contribution general point view still limited request size model table failure case failed translate correctly overview review pro well written extensive analysis various language pair convincing experimental con complicated mainly architecture engineering application bringing together various well known technique much novelty proposed potentially slower regular model since need operate character instead word us several rnns serban sordoni bengio courville pineau hierarchical neural network generative model movie dialogue arxiv preprint arxiv,5
680.json,update reading response revision dated removed comment insufficient comparison past work title update score main reason score novelty proposal hgru matrix basically achieve effect whether continue character level state using word level state seems solution specific symbolic framework like theano used tensorflow however problem language like matlab luong manning used torch well written good analysis especially like figure however think little novelty work title learning morphology nothing specifically enforced learn morpheme subword unit example maybe constraint weight figure detect morpheme boundary additional objective like used though clear constraint incorporated cleanly moreover surprised litte comparison brief mention given work luong manning train deep layer word character model achieves much better english czech bleu compared bleu achieved think hgru thing complicated term presentation read correctly hgru basically either continue character decoder reset using word level state boundary done luong manning even make efficient decode target word morpheme level good know speed proposed iclr submission perhaps different analysis character based learns adding additional layer encoder minor comment annotate figure minh thang luong christopher manning achieving open vocabulary neural machine translation hybrid word character model,4
680.json,present first neural translation system operates purely character level another,6
403.json,neural turing machine related external memory model demonstrated ability learn algorithmic solution utilizing differentiable analogue conventional memory structure particular approach provide mechanism shifting memory access head linked memory current read position relevant work us differentiable version turing machine tape controller output kernel softly shift head allowing machine read write sequence since soft shift typically smear focus head controller also output sharpening parameter compensates refocusing distribution premise work notice emulates differentiable version turing tape particular reason constrained follow topology turing tape instead propose memory stored point manifold shift action form group memory point different relationship another rather constrained mathematically elegant empirically test model shift group acting rotation group acting sphere overall well communicated novel idea primary limitation limited impact approach certainly mathematically elegant even likely beneficial specific problem problem structure match group structure clear significantly contributes building model capable general program learning instead likely make already complex slow even slower general seem memory topology problem specific therefore learned rather specified baseline used comparison simple even sharpening approach solving problem head distribution becoming smeared also comparison successor provides general approach linking memory based prior memory access minor issue footnote page misleading regarding linkage matrix explicitly excludes identity controller keep head position gating following link matrix figure page difficult follow,6
403.json,proposes memory access scheme based group action ntms pro well written novel addressing scheme extension seems work slightly better normal ntms interesting theory novel addressing scheme based group con lantm seems slightly better normal result table confusing source code available difference property normal become clear said lantm better differentiable provide robust relative indexing scheme also differentiable also provide robust indexing scheme said head discrete actually space already continuous become clear meant test real world task task comparison extension sparse access memory,5
403.json,summary formalizes property required addressing indexing memory augmented neural network well pair addressing read write operation proposes framework group addressing space experiment algorithmic task reported review summary brings unity formalism requirement memory addressing maintaining differentiable memory proposal provide generic scheme build addressing mechanism comparing proposed approach value network unbounded number memory cell lack incentive reuse index might reveal impractical detailed review read well appropriate relevance related work unified presentation memory augmented network clear brings unity field proposed approach introduced clearly powerful give tool reused reading article appreciate growing memory mentioned drawback stressed discussion impact efficiency scalability needed,7
546.json,thank interesting perspective neural approach approximate physical phenomenon describes method extrapolate given dataset predict formula naturally occurring function like sine cosine multiplication pro approach rather simple hence applied existing method major difference incorporating function input done successfully seems even though good interpolation fails extrapolate data correct function great idea basis function like sine cosine make approach explicit con page claim entirely correct restriction well approximate equality hold real value although claim correct predicting correct solution within certain limit experiment involve variable interesting neural approach model hundred variable another looking linearity like sine cosine multiplication basis function data linear combination function able learn weight division linearity predicting expression equation seems unlikely hence wondering possible make sure architecture universal approximator suggested edits page seems typographical error expression compared predicted formula figure,6
792.json,inspired analysis effect label similarity hinton proposes soft target regularization iteratively train network using weighted average exponential moving average past label hard label target argument loss claim prevents disappearing label similarity early training yield competitive regularization dropout without sacrificing network capacity order make fair comparison dropout dropout tuned carefully showing performs better dropout regularization particular value dropout table demonstrate convincing advantage possible dropout performs better reasonable tuning cross validation baseline architecture used experiment belong recent state method thus yielding significantly lower accuracy seems also experiment setup involve data augmentation also change augmentation clear number epoch small number like without putting convergence test therefore significance method convincingly demonstrated empirical study label similarity could calculated using softmax final layer rather using predicted label advantage dropout clear figure dropout without cross validation regularizing enforcing training step keep label similarity interesting idea novel significant pro provides investigation regularization label similarity training con empirical support intuitive claim regarding proposed procedure iterative version unstable practice,2
511.json,present algorithm approximating solution certain time evolution pdes present interesting learning based approach solve pdes idea alternate sampling point space time generating solution sampled point regressing space time function satisfy latter solution sampled point hopefully generalize beyond point actually find proposed algorithm interesting potentially useful practice classic grid based simulation pdes often expensive practical curse dimensionality hence learning solution pdes make sense practical setting hand point simply running gradient descent regression loss function work differentiablity show studied pdes therefore think proposed idea actually interesting approach learning solution presence differentability indeed challenging setup numerically solving pdes motivates problem time evolution operator applied spatial derivative application control thery think direct interest problem machine learning community even deep learning community example,6
511.json,approximating solution pdes approximators hard particular general discontinuous differentiable solution making particularly tricky unless underlying process diffusion case term make everything smooth worse direct correlation small residual well performing policy tsitsiklis beard todorov forget lot work properly cited example inadequate reason think scale anything useful bunch typo range kutta anything submitted wrong venue learned representation using iclr resubmit adprl sorry terseness despite rough review absolutely love direction research anything solve harder control problem people take notice,2
507.json,introduces dataset evaluate word representation task considered called outlier detection also known word intrusion identify word belong semantically related word task proposed camacho collados navigli evaluation word representation main contribution introduce dataset task covering language dataset generated automatically wikidata hierarchy entity instance category considered belonging cluster outlier sampled various distance tree several heuristic proposed exclude uninteresting cluster dataset developing good ressources evaluate word representation important task dataset introduced might interesting addition existing one however hard reviewing concerned lack discussion comparison existing approach besides word similarity datasets particular believe interesting discus advantage evaluation dataset compared existing one word analogy proposed evaluation also seems highly related entity typing discussed overall believe introducing ressources evaluating word representation important community however ambivalent submission entirely convinced proposed dataset clear advantage existing ressources also seems existing task entity typing already capture similar property word representation finally might relevant submit lrec iclr,4
507.json,describes benchmark word representation spotting build upon idea recently presented repeval workshop able collect significantly larger amount example relying existing ontology although innovation relatively incremental important step defining challenging benchmark general purpose word representation human able perform task almost flawlessly given adequate domain knowledge experiment show current embeddings fall short technical contribution thorough dataset construction appears logical correlation analysis convincing like accepted iclr,7
507.json,first praise generating releasing data socially useful task algorithm generate cluster language data semantic similarity brings point point using algorithm scalable release small data roughly order magnitude data set released semeval task recent year expected something order magnitude larger hand checked small subset cluster found ambiguous probably removed mechanical turk scale pretty well post facto filter cluster using effect imagenet created million item evaluating data paper tricky issue make data good publishable number medium sized data set released every year semeval designed address task people find interesting know data exactly address task propose task trying address idea semantic similarity multiple data set thrown since semeval wish included comparison show particular task data combination better suited analyzing semantic similarity existing data set final note seem well suited iclr data set indirectly useful evaluating word embeddings hence representation learn much glove empirically le good semantic similarity embeddings true interesting first proposal word cluster stand task context human evaluation topic model jonathan chang jordan boyd graber chong wang sean gerrish david blei reading leaf howhumans interpret topic model neural information processing system deserves citation think,5
696.json,aim consolidate recent literature simple type reading comprehension task involving matching question answer found passage explore type structure learned model propose modification reading comprehension datasets daily mail simpler side generally involve chain reasoning multiple piece supporting evidence found datasets like mctest many model proposed task break model aggregation reader explicit reference reader show aggregation reader organize hidden state predicate structure allows mimic explicit reference reader experiment adding linguistic feature including reference feature existing model improve performance appreciate naming writing make clear aggregation reader specifically learning predicate structure well inclusion dimensionality symbol space think effort organize categorize several different reading comprehension model broader class useful field producing many model landscape unclear concern predicate structure demonstrated fairly simple clear provides insight towards development better model future since explicit reference reader need learn daily mail dataset little headroom left demonstrated chen desire dramatic improvement performance mentioned discussion section probably cannot achieved datasets complex datasets probably involve multi inference discus message scattered hard parse could benefit focus think explosion various competing neural network model task contribution like attempt organize analyze landscape valuable might better suited conference journal tacl,5
696.json,aim provide insightful analytic survey recent literature reading comprehension distinct goal investigating whether logical structure predication rephrased response arises many recent model really like spirit appreciate effort organize rather chaotic recent literature unified theme aggregation reader explicit reference model overall quality writing great section especially nice read also happy proposed rewording logical structure predication clarification detailed helpful think still slight mixed feeling contribution work first wonder whether choice dataset ideal first place accomplish desired goal concern dailymail dataset chen clear whether dataset support investigation logical structure interesting kind maybe bound rather lack logical structure second wish discussion predication shed practical insight dataset design design better tackle reading comprehension challenge sense helpful could make precise analysis different type reading comprehension challenge type logical structure lacking various existing model datasets point specific direction community need focus,5
696.json,proposed analyze several recently developed machine reader found machine reader could potentially take advantage entity marker given marker point entity usually like analysis paper found argument proposed clear like experiment stanford reader show entity marker fact help stanford reader found rather interesting however found organization overall message quite confusing first feel want explain behavior definition structure however sure successful attempt still clear structure make reading section frustrating also sure take home message mean entity marking used model design model also entity reference time role linguistic feature linguistic structure overcome reference issue overall feel analysis interesting feel benefit focused argument,4
415.json,author proposed simple effective technique order regularized neural network obtained quite good technique show effective applied even state topology welcome regularization technique used applied easy task initial configuration still best known,6
415.json,proposes regulariser cnns penalises positive correlation feature weight affect negative correlation alternative version penalises correlation regardless sign also considered refers local global respectively find confusing general term mean plethora thing experimental validation quite rigorous several experiment conducted benchmark datasets mnist cifar cifar svhn improvement demonstrated case improvement seem modest baseline already competitive pointed case raise question statistical significance though global regulariser mnist interesting main novelty seems leaving negative correlation alone interesting exactly much difference make main concern ambiguity stemming fact sometimes discus activation sometimes filter weight refers feature however already said address somewhat ignores interaction choice nonlinearity seems like could important especially goal obtain feature activation uncorrelated done applying penalty weight data agnostic also ignoring nonlinearity believe already mentioned response reviewer question addressed think important definitely need discussed response answer question role bias point perfectly possible combine proposed technique multi bias approach really point rather latter example challenge idea feature positively correlated redundant seems assumption work built upon current intuition okay correlated feature long wasting capacity case multi bias seeing weight shared across set correlated feature dichotomy regularisation method reduce capacity described introduction seems arbitrary especially considering weight decay counted among former proposed method counted among latter think much depends one definition capacity clearly weight decay actually reduce number parameter overall work perhaps incremental seems well executed convincing even particularly ground breaking,6
550.json,proposes modified objective mapped representation corrupted input pushed closer representation uncorrupted input thus borrows denoising stochasticity contractive auto encoders objective compare representational closeness appears rather incremental common collapse representation avoided additional external constraint tied weight batch normalization normalization heuristic appreciates added paragraph discussing point usual remediation raised earlier question think deserve proper formal treatment note external constraint seem arise information theoretic formalism articulated cast doubt regarding validity completeness proposed formal motivation currently exposed extra regularization information theoretic perspective remains unclearly articulated interpretation lambda strength experimental front empirical support approach weak experiment synthetic small scale data modified test error mnist larger original time expect precise setting lambda original performance still within displayed error modified unclear whether improvement actually statistically significant,3
550.json,work introduced form regularization denoising autoencoders explicitly enforces robustness encoding phrase input perturbation author motivates regularization term minimizing conditional entropy encoding given input modifier denoising autoencoders evaluated synthetic datasets well mnist along regular auto encoders denoising autoencoders work fairly similar several existing extension auto encoders contractive auto encoders author include comparison experiment section need polishing detail provided help understand figure section,3
550.json,proposes additional term denoising autoencoder objective term well motivated introduces asymmetry encoder decoder forcing encoder represent compressed denoised version input propose avoid trivial solution introduced term using tied weight normalized euclidean distance error trivial solution occurs scaling magnitude code encoder back decoder proposed auto encoder scheme similar host auto encoders literature time evaluate proposed scheme data distribution well mnist although work well motivated certainly seems like empirically unproven incremental improvement idea,4
317.json,present framework solve problem amortized inference adopts learned affine projection layer ensure output consistent also proposes three different method solve problem minimizing cross entropy generally great however still several comment proposed amortized inference novel different previous method combined framework obtain plausible good compared another based method photo realistic single image super resolution using generative adversarial network question arise formulation add latest state using affine projection architecture constraint need corresponding image pair training however training affine projection layer still need image pair mean merely transfer training procedure training affine projection present many framework including natural image imagenet author also provide conventional test dataset perform fair comparison previous work size nature image presented limited framework perform well image larger size encounter input arbitrary size normal noise term latent space better illustrated learning distribution author noise vector overall provides framework solid theoretical analysis idea novel author explore many method though still exist question like necessity experiment needed think work provide good inspiration community,7
317.json,sincere apology late review argues approach super resolution amortised estimation projection step keep consistent dependency proposed experimentally verified obtain better throughout three different method solve resulting cross entropy problem proposed tested summary good well written presented experimental sufficient present well chosen example real world application understanding contribution field super resolution novel part specific training gans appeared different variant elsewhere also discussion believe relevant future work super resolution finding based training yield visually appealing suggests work domain manuscript proof read typo worth fixing,8
747.json,proposes method interior gradient analysing feature importance deep neural network interior gradient gradient measured scaled version input integrated gradient integral interior gradient scaling factor visualization comparing integrated gradient standard gradient real image input inception show integrated gradient correspond intuitive notion feature importance motivation qualitative example appealing lack qualitative quantitative comparison prior work baseline simply standard gradient presented reference qualitative comparison cite numerous work deeplift layer wise relevance propagation guided backpropagation attack problem feature importance lack comparison method major weakness believe publication without comparison review question articulated concern answered,2
602.json,summary propose multi gated attention model interaction query document representation answering cloze style question document representation attended sequentially multiple hop using similarity query representation using product scoring attention function proposed method improves upon daily mail datasets comparable dataset state pro nice idea heirarchical attention modulating context document representation task specific query representation presentation clear thorough experimental comparison latest comment overall system present number architectural element attention multiple layer multi query based attention context gated attention encoding query vector layer independently important breakdown gain performance factor ablation study presented section help establish importance gated attention however clear much multiple hop gated attention contribute performance important specialized query encoder layer understanding better help simplify architecture token represented using clear crucial performance proposed method significant performance drop absent reader although change reader could affect performance hence clear much main idea gated attention contributes towards superior performance proposed method,5
602.json,present interesting idea iteratively weighting word representation document hence coded representation well simple multiplication operation correctly pointed operation serf filter reduce attention le relevant part document hence leading better performance modeling close state cloze style task deserve even higher score following limitation could addressed better interesting conceptually simple though significant increased computational overhead architecture proposed specific task improvement main idea gated attention le significant comparing reader reader latter includes number engineering trick adding character embedding using word embedding trained larger corpus glove well small improvement modeling using token specific attention also wish shed light role number hop play intuitively empirically feel insight could obtained deeper analysis impact different type question example,5
481.json,investigate phenomenon adversarial example adversarial training dataset imagenet final conclusion still vague raise several noteworthy finding experiment well written easy follow although still concern comment good contribution worth publish pro first time literature proposed concept label leaking although effect becomes significant dataset large carefully handled future research work along line using ratio clean accuracy adversarial accuracy measure robust reasonable compared existing work literature con although conclusion based experiment imagenet title seems little misleading consider section main contribution note section section specific large scale dataset thus emphasizing large scale title introduction seems improper basically conclusion made based observing experimental test performed verify hypothesis without conclusion seems rushy example dataset imagenet infer conclusion large scale datasets,5
481.json,well written divided part adversary training imagenet empirical study label leak single multiple step attack transferability importance capacity part think training without clean example make reasonable imagenet level experiment explaining harnessing adversarial example batchnorm important training large scale part look like extension work inception suggest experiment training without clean sample part experiment cover variable adversary training lack technical depth depth capacity experiment explained regularizer effect training label leaking novel transferability experiment fgsm careful observe special mnist fgsm example find augmentation effect number make grey part image make number look like number although effect hard observed complex data cifar imagenet related observation fgsm example transferable part raise many interesting problem guess lack theoretical explanation overall think empirical observation useful future work,5
710.json,presented unsupervised approach automatic segmentation bioacoustic data applied existing approach hierarchical dirichlet process hidden markov model task originality work investigation approach task argue difficult namely bioacoustic segmentation provide evidence difficult task explaining exist consensus among human expert done however provide convincing approach successful fails many case replicate correct segmentation defined baseline human expert addition clarity writing extremely poor including many grammatical error awkward sentence,3
655.json,update thank author comment point still suitable publication leaving rating untouched proposes transfer learning method addressing optimization complexity class imbalance main concern following quite hard read typo unusual phrasing loose terminology like distributed transfer learning meaning fine tuning softmax meaning fully connected deep learning meaning base neural network still sure detail actual algorithm right caption figure table informative jump back forth understand number image mean understand conventional transfer learning refer fine tuning fully connected layer judging figure case essential compare proposed method regime convolutional layer also updated comparison present comment review question question considers case better reduce notation clutter question still clear mean distributed transfer learning figure supposed highlight difference conventional approach fine tuning fully connected layer think softmax conventional term fully connected layer diagram follows base number convolutional filter every layer order obtain distributed ensemble need connect reason filter index make sense probably misinterpreting figure could revise diagram make clearer overall think need significant refinement order improve clarity presentation thus cannot accepted,2
593.json,investigates deep generative model multiple stochastic node give meaning semi supervision methodological point view nothing fundamentally novel similar semi supervised work kingma although work sometimes latent node complex extension fairly classical auxiliary variable trick used make sure inference network trained data point supposing fact latent variable observation tilde observation observed uninformative unobserved alternatively separate inference used learn generative throw inference observed inference used exercise approximate complex simpler effectively inferring target data collected strong although simple datasets overall well written interesting lacking term methodological advance minor feel title general content personally agree strong contrast made deep generative model graphical model deep generative model graphical model typically learned interpretable classical graphical model multiple stochastic variable exclusive graphical model draw deep kalman filter recurrent word tructure problematic seems concerned disentangling semanticizing latent representation generative supervision debatable whether model structure,5
439.json,good well written present simple effective approach predict code property input output pair experiment show superiority baseline speedup factor order magnitude solid gain domain program limited work trying idea difficult task using neural net augment search good starting point right approach instead generating full complex code threshold acceptance,5
439.json,present approach learn generate program instead directly trying generate program propose train neural estimate attribute condition search procedure interesting approach make sense building generative program complex task faster computation time shown experimental section respect baseline including enumeration setup small program length instruction found clear proposed approach scale larger program perhaps many attribute still advantage metric time find single program whose execution result input output pair given input however mentioned generic program best program rank list program program result correct execution could show experiment setting still useful proposed approach challenge realistic scenario second experiment show length program training time different length test time however shown program finished could show finding program missing analysis type program difficult often nnet wrong affect speed failure mode proposed method proposed length representation input output pair average pooling final representation however average pooling make sense make sense combine prediction decoder encoder learning execution seems difficult program small might going difficult longer program setting seem reasonable summary interesting tackle problem outside area expertise might miss something important,6
706.json,introduce prior approximate posterior family variational autoencoders compatible reparameterization trick well capable expressing multiple mode also introduce gating mechanism prior posterior show improvement word document modeling dialogue response generation original abstract overly strong assertion unimodal latent prior cannot multimodal marginal intz response cannot possibly capture complex aspect data distribution critical restriction assertion unimodal latent prior necessary multimodal observation false sensible motivation piecewise constant prior posterior example think sort regularized autoencoder code constrained fill part prior latent space sphere packing argument made filling gaussian prior gaussian posterior code space although explore much hypercube based tiling latent code space sensible idea stated found message quite sloppy respect concept multi modality type multimodality play multimodality observed marginal distribution captured deep latent gaussian multimodality prior make sense situation mnist digit could prior mode corresponding latent code digit class multimodality posterior given observation final type multimodality harder argue except allows expression flexibly shaped distribution without highly separated mode believe flexible posterior approximation important enable fine grained efficient tiling latent space think need multiple strong mode interested experiment demonstrating otherwise real world data think clear different type multi modality part analysis demonstrate one also found unsatisfactory piecewise variable analysis show different component multi modal prior corresponding different word rather separation gaussian piecewise variable mention earlier question found surprising learned variance mean gaussian prior help dramatically nvdm likelihood powerful network transforming latent space make scale invariant explicitly separating contribution reimplemented base prior posterior interpolation learned prior parameter strengthen experiment overall strong improvement text modeling task nvdm seem hard understand like ablation analysis difference proposed fact adding constant component help document modeling interesting nice qualitative analysis prior mode represent also surprised posterior mode highly separated interesting explore corresponded ambiguous word sens experiment dialog modeling mostly negative quantitatively observation piecewise constant variable encode time related word gaussian variable encode sentiment interesting especially since occurs set experiment actually quite interesting interested seeing analysis case like analysis sort word encoded different prior mode whether correspond group similar holiday day conclusion think piecewise constant variational family good idea although well motivated experimental good document modeling without ablation analysis baseline hard small modification nvdm fact nvdm performs better interesting though better motivate need different type multi modality demonstrate sort thing actually captured introduces interesting variational family show performs better task motivation analysis clearly focused demonstrate broadly applicable family also good experiment standard datasets like mnist even without absolute likelihood improvement method yielded interpretable multiple mode valuable contribution,3
643.json,propose simple modification online dictionary learning inspired neurogenesis propose step atom addition atom deletion order extent online dictionary learning algorithm algorithm mairal extension help adapt dictionary changing property data online adaptation interesting even quite simple overall algorithm quite reasonable always described sufficient detail example threshold condition neuronal birth death supported strong analysis even resulting algorithm seems perform well quite extensive experiment overall idea nevertheless interesting even completely generally well written pretty easy follow analysis however quite minimal could interesting study evolving property dictionary analyse accuracy following change data still nice work,6
643.json,like thank detailed response clarification work proposes training scheme online sparse dictionary learning assumes stationary flow incoming data goal challenge learn online manner capable adjusting incoming data without forgetting represent previously seen data proposed approach deal problem incorporating mechanism adding deleting atom dictionary procedure inspired adult neurogenesis phenomenon dentate gyrus hippocampus main innovation baseline approach mairal neuronal birth represents adaptive increasing number atom dictionary neuronal death corresponds removing useless dictionary atom neural death implemented including group sparsity regularization dictionary atom group corresponds column dictionary promotes shrink zero atom useful keeping controlled increase dictionary size believe strong side connection adult neurogenesis phenomenon opinion nice feature well written easy follow hand overall technique novel although exactly equivalent similar idea explored neural death implemente elegantly sparsity promoting regularization term neural birth performed relying heuristic measure well dictionary represent incoming data depending level stationarity incoming data presence outlier could difficult still adaptive dictionary size interesting could also cite reference selection literature particular idea used automatically selecting dictionary size believe work address online setting still relevant reference instance ramirez ignacio guillermo sapiro framework sparse coding dictionary learning ieee transaction signal processing,4
751.json,present heuristic avoiding large negative reward already experienced distilling event danger well written including rather poetic language heuristic evaluated domain think order properly evaluate well known benchmark atari atari seems particularly since game full catastrophe sudden death reviewer favourite quote imagine self driving periodically pedestrian order remember undesirable child learn adjust behaviour without actually stab someone catastrophe lurking past optimal shave,4
751.json,topic keeping around highly rewarding dangerous state important studied extensively literature review comment mention compared expected sarsa really like extensive baseline accepting also increasing amount literature using reward replay buffer deep agent jaderberg reinforcement learning unsupervised auxiliary task blundell charles free episodic control narasimhan language understanding text based game using deep reinforcement learning could perhaps reinforce agent avoid revisiting catastrophic state overall approach presented principled instance catastrophe directly provided signal learner instead separate,3
791.json,proposes unsupervised training objective based patch contrasting visual representation learning using deep neural network particular feature representation patch image encouraged closer different image distance ratio positive training pair optimized proposed method empirically shown effective initialization method supervised training strength training objective reasonable particular high level feature show translation invariance proposed method effective initializing neural network supervised training several datasets weakness method technically similar exemplar network dosovitskiy cropping patch single image taken type data augmentation comparable data augmentation positive sample exemplar dosovitskiy experimentally misleading reported based fine tuning whole network supervision however table exemplar convnets dosovitskiy unsupervised feature learning network finetuned labeled sample classifier trained upon feature therefore comparison fair suspect exemplar convnets dosovitskiy achieve similar improvement fine tuning without comparison head head comparison without fine tuning based architecture except loss experimental fully convincing regarding comparison autoencoder zhao interesting compare large scale setting shown zhang icml augmenting supervised neural network unsupervised objective large scale image classification training alexnet time consuming latest titan level gpus proposed method seems useful natural image different patch image similar,4
791.json,proposed self supervised loss formulated using siamese architecture encourages patch image closer feature space contrasting patch taken different random image loss similar spirit doersch iccv isola iclr workshop seems proposed loss actually simplified version doersch iccv make spatial offset freely available self supervised signal natural image intuitively seems self supervised problem posed method strictly simpler therefore le powerful aforementioned work like discussion comparison approach nevertheless proposed method seems effective achieving good empirical using simple loss though implementation detail provided effect patch size overlap sampled patch important measure taken avoid trivial solution,5
791.json,present novel unsupervised pretraining deep convolutional network setting though likely applicable fully connected net well method spatial constrasting building triplet patch input image learning presentation assigns high score patch coming image score patch diferent image method simple enough surprised tried least according previous work submission comment usage section worth defining mathematically kind probability talking taking part probability replaced another word like know method using batch statistic section sampling unless simply mean sample possible triple batch number patch sampled algorithm hyper parameter rather tried value think missing detail like patch size whether played think important hyper parameter quite impressive cifar maybe much cifar expect train imagenet cifar build better representation considered interesting piece work obvious application seems relatively straightforward implemenent think liked understanding spatial contrasting actually learns empirical study effect various parameter choice patch size attempt beating state cifar,6
590.json,proposed machine learning called dynamic reader machine reading comprehension task compared earlier system proposed able extract rank answer candidate given document many recent model focusing building good question answering system extracting phrase given article seems different aspect unique work convolution dynamic chunking convolution network often used modeling character based word embeddings curious effectiveness representing phrase therefore wish could analysis effective compare convolution framework alternative approach lstm comparison important us gram gram gram information convolution network clear gram information still needed lstm model dynamic chunking good idea similar idea proposed recent paper kenton also target dataset however like analysis dynamic chunking approach good approach representing answer chunk given representation chunk constructed first word representation generated convolution network sure ability representation capture long answer phrase character base embedding previous trained model interesting could show advantage disadvantage using linguistic feature compared character embeddings short several good idea proposed lack proper analysis make difficult judge important proposed technique,4
590.json,summary propose reading comprehension question answering system recent task answer question either single token span given text passage first encodes passage query using recurrent neural network attention mechanism calculates importance word passage respect word question encoded word passage concatenated attention resulting vector encoded three convolutional neural network different filter size gram used capture local feature candidate answer selected either matching pattern answer training choosing possible text span certain length candidate answer three representation gram representation compatibility representation question representation calculated score combined linearly used calculating probability candidate answer right answer question method tested squad dataset outperforms proposed baseline overall judgment method presented interesting motivated point example explained attention mechanism beneficial concatenate original passage encoding attention weighted one contribution moderately novel proposing mainly attention mechanism convolutional encoding fact combining question passage score compatibility became fairly standard procedure model detailed comment equation still understand sentence best function concatenate hidden stat fist word chunk forward last word backward word chunk passage answer gave response clarify point,5
355.json,describes approach taken train learning agent game doom propose number performance enhancement curriculum learning attention zoomed centered frame reward shaping game variable post training rule inspired domain knowledge enhancement together lead clear demonstrated competition curriculum learning clearly help learning increasingly difficult setting nice result overfitting harder class learned probably curriculum health speed conclude adaptive curriculum better stable pure however stretch given graph pure learn harder show result graph back claim show clear benefit post training rule goal solve problem like shooter make significant contribution show technique practical solving problem ultimately improving performance kind task still excited mainly relies heavily many source domain knowledge quite pure reinforcement learning problem relatively unsurprising maybe novel problem though sure realistically draw conclusion figure current form recommend increase resolution actual metric determine fuzziness clarity image something concrete arrow already resolution image added rebuttal still high image figure link trust accepted,5
355.json,basically applies spatial navigation task first time applied navigation fact original reported experiment although experimental great sure additional insight warrant conference might make sense workshop graph constructed using single hyper parameter sweep think report many random initialization make comparison robust overall main idea curriculum really novel make real system,3
355.json,solid applies doom enhancing collection trick vizdoom competition think fair expect competition aspect overshadow scientific approach justifying every design decision isolation fact decent latter concern remained unanswered anonreviewer addition citation list rather thin example reward shaping rich literature incrementally difficult task setup dating back least mark ring work also complementary work game asking direct comparison give reader sense context place,6
375.json,proposes simple reweight word embedding simple composition function sentence representation also show connection weighting scheme previous work comment technical detail word discourse confusing sure whether word discourse discourse vector frequent discourse meaning justification related syntac sure thie line mean fact discovered detecting common component existing embeddings section computing sentence embedding explanation sentiment table,6
621.json,tackle task music generation orderless nade task fill note given roll timesteps pitch randomly mask pitch trained predict missing note follows orderless nade trained sampling normally follows ancestral sampling procedure ordering defined output run current input sample output according order add output next input continues procedure output sampled point sampling strategy instead suggest strategy us blocked gibbs sampling approach blocked gibbs strategy instead mask input randomly independently sample repeat procedure point strategy make sure sampling chain mix well happen large however since sample independent large give incoherent sample thus follow annealed schedule making smaller time eventually reduce ancestral sampling giving global structure sample conduct variety experiment involving normal metric human evaluation find blocked gibbs sampling outperforms sampling procedure well written great main problem read uria know much learned work context iclr submission submitted computational music conference clear accept however iclr enough novelty compared previous work build upon orderless nade established blocked gibbs sampling annealing scheme basically exact used thus main novelty application music domain finding method work better sampling music good contribution tailored working music domain found also hold domain like image cifar tiny imagenet text document generation change mind accept iclr even trying musical domain bach chorale useful however stand experiment convincing enough,4
621.json,present distribution four part bach chorale using convolutional neural network furthermore address task artificial music generation sampling using blocked gibbs sampling show distribution seems appropriate data hand also analysis proposed sampling scheme analogy gibbs sampling human music composition interesting sure evaluation though since reported likelihood directly comparable previous work difficulty judging quality quantitative human evaluation like data direct comparison model nade bach perform also find question piece music prefer stronger test question piece musical really know musical mean worker finally think bach chorale interesting musical piece deserve subject analysis find hard judge well modelling approach transfer type music might different data distribution nevertheless conclusion believe exciting interesting task produce trivial musical data,5
733.json,present anomaly based host intrusion detection method lstm used system call sequence averaged sequence likelihood used determine anomaly attack also compare ensemble method baseline classification well written idea clearly presented demonstrates interesting application lstm sequential modeling hids problem overall novelty limited considering major technical component like lstm ensemble method already established contribution proposed ensemble method need evaluation also possible ensemble idea baseline,4
733.json,propose using lstm sequence system call perform network intrusion detection nids idea using neural network general nids idea using sort sequence system call nids published idea using lstms nids published operates count gram system call rather sequence processing seem heavy overall proposed system work well proposed nids system check portability good side adding state nids well matched iclr learn representation many people thrown lstm sequence problem therefore think threshold iclr wish submit security conference reference debar herve monique becker didier siboni neural network component intrusion detection system research security privacy proceeding ieee computer society symposium ieee creech gideon jiankun semantic approach host based intrusion detection system using contiguousand discontiguous system call pattern ieee transaction computer staudemeyer ralf applying long short term memory recurrent neural network intrusion detection south african computer journal,4
733.json,novel approach anomaly detection considered task intrusion detection based system call sequence system call sequence regarded language multiple lstm language model trained ensembled diversity ensemble achieved choosing different hyper parameter lstm combination done averaging transformation likelihood really like fact attack data used training like ensemble approach high level drawback following might simple answer expert field relaying system call seems weak attacker access normal sequence system call fool system interleaving malicious system call normal one artificially raise likelihood sequence line covering anomaly detection task rnns used added introduction give better idea novelty approach,7
533.json,present novel approach surprise based intrinsic motivation deep reinforcement learning clearly explain difference recent approach intrinsic motivation back method broad class discrete continuous action domain present tractable approximation framework ignores stochasticity true environmental dynamic approximates rate information gain somewhat similar schmidhuber formal theory creativity intrinsic motivation exploration bonus added trpo generally better standard trpo however appreciated thorough comparison recent work intrinsic motivation instance bellemare recently achieved significant performance gain challenging atari game like montezuma revenge combining exploration bonus however montezuma revenge presented experiment comparison significantly improve strength,5
533.json,provides surprise based intrinsic reward method reinforcement learning along practical algorithm estimating reward idea similar previous work intrinsic motivation including vime work intrinsic motivation positive method simple implement provide benefit number task however almost always outmatched vime proposed method consistently best proposed perhaps consistent surprisal unfortunately asymptotically equal true reward claim massive speed numerical measurement show vime slower initialize significantly slower iteration otherwise perhaps analysis clarify claim overall decent simple technique perhaps slightly incremental previous state,5
564.json,explore idea incorporating skip connection time rnns even though basic idea particularly innovative proposal merge information current hidden state different pooling function evaluated different model compared popular text benchmark point experiment feature prediction task nice model domain modelling conditional distribution sensory input data audio video given insight pointed reviewer feel comparison model fair sota change quickly hard place experiment complete picture claimed help long term prediction think lack corresponding analysis pointed earlier question mine claimed lstm train slow hard scale match personal experience prevalence lstm system production system google baidu microsoft clearly speaks like basic idea point make think ready publication,3
564.json,proposes idea looking step backward modelling sequence rnns proposed previous hidden state also look back step also proposes different way aggregate multiple hidden state past reviewer issue firstly writing requires improvement introduction abstract wasting much space explain unrelated fact describe already well known thing literature statement written misleading instance explains among various neural network model recurrent neural network rnns appealing modeling sequential data capture long term dependency sequential data using simple mechanism recurrent feedback say rnns cannot actually capture long term dependency well rnns appealing first place handle variable length sequence temporal relationship symbol sequence criticism lstms hard accept say lstms slow slowness hard scale larger task know company already using gigantic seqseq model production lstms used building block system indicates lstms practically used large scale setting secondly idea proposed incremental field previous work propose direct connection previous hidden state however previous work aggregation multiple number previous hidden state importantly fails deliver proper analysis whether main contribution actually helpful improve problem posed architecture said handle long term dependency better however rigorous proof intuitive design architecture help understand work better design architecture speaking high level seems like maybe helpful mitigate vanishing gradient issue linear factor always good practice least page analyze empirical finding thirdly baseline model used weak plenty model trained tested word level language modelling task using penn treebank corpus contains outdated model cannot fully agree statement best knowledge best performance training condition day based method usually score term test perplexity lower achieved zhang architectural complexity measure recurrent neural network nip,2
732.json,decent accuracy number hard argue acceptance given following motivation based incorrect assumption paragraph vector work unseen data numerous basic formatting bibtex citation issue lack novelty another standard directed like word bigram,3
732.json,feel structured around shortcoming original paragraph vector namely alleged inability infer representation text outside training data reasonably sure case unfortunately basis premise work presented longer hold render subsequent discussion void recommend rejected encourage revisit novel aspect idea presented turned different type going forward,2
657.json,present trick compress wide shallow text classification based gram feature trick include using optimized product quantization compress embedding weight pruning vocabulary element hashing reduce storage vocabulary minor component focus model large vocabulary show reduction size model relatively minor reduction accuracy problem compressing neural model important interesting method section well written good high level comment reference however machine learning contribution marginal experiment convincing mainly focusing benchmark commonly used implication state text classification model unclear optimized product quantization approximating inner product particularly novel previous work also considered reduction size come pruning vocabulary element method proposed pruning vocabulary element simply based assumption embeddings larger norm important coverage heuristic taken account machine learning point view proper baseline solve problem relaxed binary coefficient embedding vector learn coefficient jointly weight regularizer coefficient used encourage sparsity practical point view believe important baseline missing simply us fewer vocabulary element based subword unit,4
657.json,proposes series trick compressing fast linear text classification model clearly written quite strong main compression achieved product quantization technique explored application within neural network compression literature addition gong work cited worth mentioning quantized convolutional neural network mobile device cvpr,5
728.json,framework semi markov decision process sdmps long used skill learning temporal abstraction reinforcement learning proposes variant called semi aggregated formalism samdp defined clearly enough merit serious attention approach quasi heuristic explained example rather clear definition work also lack sufficient theoretical rigor simple experiment proposed using grid world demonstrate skill grid world served purpose long enough reinforcement learning time retire realistic domain routinely used used well,3
728.json,present method visualization analysis policy observed trajectory policy produce method infers higher level skill cluster state result simplified discrete higher order state action transition matrix used analysis modeling interpretation construct semi aggregated propose combining idea creating semi mpds agregrated mdps method consists choosing feature state clustering skill inference reward skill length inference selection method demonstrated small grid world problem trained agent playing atari game correctly identify tool mean interpretibility method important analysis deployment method real world application particularly true robotics high consequence system result presented method high level transition matrix body literature looking hierarchical method lower level skill combined higher level policy presented method similar result advantage presented method come structure analyzes already trained agent interesting benefit emphasizing difference contrasting broader body literature build propose combining idea existing idea semi mpds agregrated mdps using modified mean state clustering appears novelty presented method limited stronger explicitly stated contribution combining existing method better highlighted practical utility method evaluation section made stronger analytical precise evaluation showing full strength method difficult read improve readability semi aggregated section include precise description method narrative build intuition welcome addition existing narrative algorithm formula applicable included well self contained example background occam razor principle included reduce number acronym particular similarly sounding acronym define acronym using clear contribution contrast relevant literature specific benefit presented method typo formatting mistake distracting reading approach reverse engineering hierarchy learning high level transition matrix interesting promising perhaps method used outperform single network approach using input specialized hierarchical trainer learn complex behavior optimally possible large network approach unfortunately fall short novelty precision clarity,3
682.json,propose novel energy function rbms using leaky relu activation function hidden unit analogous relu unit feed forward network leaky relu rbms split input space combinatorial number region region defines truncated gaussian contribution proposing novel sampling scheme leaky much shorter markov chain initializing sample leaky yield standard multi variate normal visibles slowly annealing dimension similar scheme shown outperform estimating partition function experiment performed cifar svhn interesting believe interest iclr community theoretical contribution strong introduce proper energy formulation relu rbms also novel sampling mechanism improvement estimating partition function unfortunately experimental somewhat limited baseline notably absent including bernoulli visible leaky relu hidden allowed evaluate likelihood standard binary datasets stand performance cifar svhn improved leaky relu recent generative model based auto regressive model comparison unfair certainly limit wider appeal community furthermore issue costly projection method required guarantee energy function remain bounded covariance matrix region fair leave future work given contribution limit appeal pro introduces energy function leaky relu activation function introduces novel sampling procedure based annealing leakiness parameter similar sampling scheme shown outperform con somewhat date missing experiment binary datasets comparable prior work missing baseline cost projection method,5
378.json,overview work proposes link trajectory probability reward defining appreciated reward suggests linear relationship trajectory reward probability exploited measuring resulting mismatch action sequence appreciates reward probability increased method simple modification well known reinforce method requiring extra hyperparameter intuitively provides better exploration mechanism epsilon greedy random exploration method tested algorithmic environment compared entropy regularized reinforce double learning performs equally better baseline especially complex environment remark focus introduction algorithmic task double edged sword interesting domain test hypothesis benchmark method time distracts reader generality proposed method introduction reward sparse section task reward correct emission time step corrected episode reward section discussed move mention section approach seems quite sensible range logpi urex sure understand agree experimentation choice alternative grid search random search bergstra bengio illustrate better hyperparameter robustness allow explore number experiment opinion interesting approach policy gradient sure tackle important question agent explore ambivalent claiming algorithm robust hyperparmeters simply performs better selected hyperparameter range really show performs well amount time hyperparams range could ment need different hyperparameters devil advocate matching logpi obvious choice implies strong prior reward factor lie space policy point failure correct wrong length trajectory grows reward expected grow linearly short way reward le explored long way getting reward creating imbalance unless reward shaped shorter trajectory reward case task might good also compare method explicitly trying explore better value function prioritized experience replay schaul risk repeating play major role method little analysis effect experiment methodology reasoning clearly explained think communicates message well message novel albeit minor modification well known algorithm well motivated think welcome addition literature concerning exploration experiment chosen accordingly seem reflect hypothesis realize tyranny extensive experimentation scarcity time think benefit cleverer experimentation well demonstrating explicitly impact method exploration reading convinced measuring mismatch trajectory observed reward probability given current policy clever well motivated thing think could convincing empirical argument even task,7
378.json,proposes algorithm based reinforce aim exploring appreciate action sequence idea compare probability sequence action current policy estimated reward action current policy estimate reward provide higher feedback thus encouraging exploration particular sequence action urex tested algortihmic problem show interesting property comparison standard regularized reinforce ment learning interesting well defined well explained know urex original certainly useful community drawback restrict evaluation algortihm algorithmic problem specific easy test proposed onto standard problem clearly help make article stronger greatly encourage task,6
689.json,us tensor build generative model main idea divide input region represented mixture model represent joint distribution mixture component tensor restricting tensor efficient decomposition train convolutional arithmetic circuit generate probability input class label providing generative input label approach seems quite elegant completely clear choose specific architecture choice relate class joint distribution represent even choice somewhat heuristic overall framework provides nice controlling generality distribution represented experiment simple synthetic example missing data somewhat limitation convincing could include experiment real world problem contained missing data issue must known element input missing somewhat limit applicability could experiment problem relating netflix challenge classic example prediction problem missing data spite limitation experiment provide appropriate comparison prior work form reasonable initial evaluation little confused input missing data handled experimentally introductory discussion impression generative built region patch image believe marginalize missing region however missing data consists randomly missing pixel seems every region missing information appropriate marginalize missing pixel specifically equation represents local region ensuing discussion show marginalize missing region done subset region missing also seems like summation equation following equation could quite large time also schizophrenic extent applicable beyond image motivation probabilistic mostly term image experiment state state inpainting algorithm method limited image want compare method restricted image convincing experiment outside image domain also clear proposed network make translation invariance widely assumed much success cnns come encoding translation invariance weight sharing invariance built network expect work well challenging image domain minor point carefully proofread give example first page significantly lesser significantly le provenly provably,6
689.json,provides interesting generative model address classification missing data problem tensorial mixture model proposed take account general problem dependent sample nice extension current mixture model sample usually considered independent indeed reduced conventional latent variable model much love idea behind feel pitiful sloppiness presentation missing notation flaw technical derivation going technical detail high level concern follows joint density sample modeled tensorial mixture generative interpretation decomposition decomposition prior density tensor clear interpretation product mixture model sample independent however interpretation seems flawed elaborate detailed technical comment employ convolution operator compute inner product realizable zero padding invariance structure advantage compared feed forward neural network lost however sure much affect performance practice author could comment little sample complexity method given complexity liked idea much iclr submitted present technical detail well sloppiness notation read technical detail arxiv version pointed technical typo like point reference equation one arxiv generative figure flawed theta vector length product vector well defined obvious dimension term side equation equal fact tucker decomposition instead multiplication ldots ldots theta theta ldots theta mean multi linear operation tensor ldots mode projected onto theta suspect special case diagonal gaussian mixture model typo could derive third last equation page might understand example claim reduces product mixture accurate first equation page right product operation equal product operation similarly equation second equality hold unless special case however true might typo good could also suspect correct typo performance mnist might improved overall like idea behind much suggest technical typo accepted,4
328.json,explores ability nonlinear recurrent neural network account neural response property otherwise eluded ability model multilayer trained imitate stimulus response mapping measured actual retinal ganglion cell response sequence natural image performs significantly better especially accounting transient response conventional model work important step understanding nonlinear response property visual neuron recent shown response even retinal ganglion cell response natural movie difficult explain term standard receptive field model present important challenge field even work starting point work seen light challenge course tease apart perhaps could pruned simplified part critical performance nice analysis nevertheless result good first start think important people know confused called movie understanding essentially sequence unrelated image shown stated frame rate think must refer refresh rate monitor right guess deviation even stronger show actual dynamic natural scene real movie expect even profound effect potentially much informative,7
328.json,clearly written nice straightforward result rnns good predictive model neuron firing rate retina hand primary scientific contribution seems confirm approach work particular stimulus locked task gain using seemed relatively modest taught anything biology hand along concurrent work mcintosh introducing neural network modeling field currently using prove effective think interesting applying framework like neuron input shorter discretization time scale suspect followup work building proof concept increasingly exciting minor comment understand role bin epoch throughout rather alternating epoch pas data better axis scale,6
484.json,investigates fact deep network perform well practice modifying geometry pooling make polynomially sized deep network provide function exponentially high separation rank certain partitioning previous work showed superiority deep network shallow activation function relu pooling mean pooling current activation function conv pooling multiplication node value although experimental considered scenario actually general reasoning problem hard therefore drawback significant current contribution add reasonable amount knowledge literature study convolutional arithmetic circuit show address inductive bias pooling adjust bias interesting contribution give intuition deep network capture correlation input variable size polynomial correlation exponential worth note although tried express notation definition carefully successful helpful elaborate definition expression conclusion sense make accessible,6
484.json,address question function well suited deep network opposed shallow network basic intuition convincing fairly straightforward pooling operation bring together information information correlated efficiently used geometry pooling region match correlation brought together efficiently shallow network without layer localized pooling lack mechanism combine correlated information efficiently theoretical focused convolutional arithmetic circuit building prior theoretical make interesting technical notion separability sense measure degree function represented composition independent function separability measured relative partition input appropriate mechanism measuring complexity function relative particular geometry pooling operation many technical notion pretty intuitive although tensor analysis pretty terse easy follow without knowledge prior work sense comparison deep shallow network somewhat misleading since shallow network lack hierarchical pooling structure example shallow convolutional network relu pooling really make sense since occurs whole image seems really analysis effect pooling pooling example clear deep without pooling efficient shallow network work clear much theoretical depend product pooling might extended common pooling even theoretical difficult derive case simple illustrative example might helpful fact prepare longer version journal think could made intuitive could simple example function efficiently represented convolutional arithmetic circuit pooling structure fit correlation perhaps showing also could represented convolutional network relu pooling also appreciate explicit discussion depth deep network affect separability function represented shallow network local pooling difference deep shallow perhaps mostly pooling pooling however practitioner find deep network seem effective deep network convolutional layer pooling explicitly discus whether provide insight behavior overall think attack important problem interesting convincing really get heart depth important theoretical limitation arithmetic circuit comparison shallow network without localized pooling,5
580.json,summary present simple linear dynamic language modeling linear dynamic greatly enhance interpretability well provide potential improve performance caching dynamic common sequence overall quantitative comparison benchmark task underwhelming unclear consider common dataset considered single dataset hand present number well executed technique analyzing behavior many impossible linear overall recommend accepted despite provides interesting read important contribution research dialogue feedback could improved shortening number analysis experiment increasing discussion related sequence model experiment compelling whereas sort feel like showing reader fit data well particularly important property trust fit data well since reasonable perplexity lstms grus great language modeling data rigid combinatorial structure nested parenthesis nice compared linear method sort data scared negative interesting linear method substantially better task definitely discussion belanger kakade related work different motivation fast scalable learning algorithm rather interpretable latent state dynamic simple credit assignment future prediction given past hand also linear dynamic look singular vector transition matrix analyze broadly useful reader discussed directly comparison came openreview discussion recommend folding example useful emphasize bias vector correspond column kalman gain matrix last thing regarding corresponds kalman filtering also kalman smoothing state vector inferred using future addition past observation could something similar said matrix sparse convex combination dictionary matrix parameter sharing could provide even interpretability since character represented dimensional weight used combine dictionary element could also provide scalability word level problem,5
580.json,summary propose input switched affine network character level language modeling kind without pointwise nonlinearity switching transition matrix bias based input character motivated intelligibility since allows decomposition output contribution kappa term basic linear algebra probe network regarding reviewer quite sure understood main idea argument expert language model intelligibility interpretability read paper similar premise closest related work familiar deconvnet insight vision cnns think original novel work work high quality well written clearly result work found section projecting readout subspace computational subspace interesting meaningful main hesitation part isan analysis entirely convincing isan trained small task text clear whether strong char larger scale task analysis section provide much real insight learned network caveat towards isan architecture proposed form really small vocabulary character based language modeling general large vocab discrete input continuous input analysis many cute plot idea quantity look much concrete insight clear analysis specific isan idea generalize general nonlinear rnns seems quantity kappa analysis rest meaningful elaborating wrote question example input letter revenue spot character massively positively impact logit seems quite meaningless meaning influence character look switching matrix prior using previous state interesting produce following metric kappa seem meaningful remark relates last paragraph even though list con longer recommend accept specifically originality work case make vulnerable critique work well motivated well executed inspire many interesting investigation along line,6
580.json,present character language gain interpretability without large loss predictivity contribution characterize experimental investigation cute insight recall multi class logistic regression allows apportion credit prediction input feature feature raised probability correct class others lowered point sufficiently simple architecture linear apportion credit prediction among element past history pro quite well written read nice simple architecture still respectably easy imagine using classroom assignment easy implement student could replicate investigation influence network prediction present nice visualization section also describes computational benefit caveat predictive accuracy figure say isan near identical performance architecture appears true comparing largest model explanation appears smaller parameter size still beat usual metric perplexity word people usually report performance reduction traditionally considered good dissertation assumed average char word converting cross entropy char perplexity word addition known whether family remain competitive beyond situation tested explanation tried character based language modeling char dataset extremely high best model contrast word based trained word get trained word get number copied cited,5
738.json,finding applying sparsity backward gradient training lstms interesting seems incomplete without proper experimental justification validation loss reported definitely insufficient proper testing commonly reported evaluation criterion need included support claim degradation applying proposed sparsity technique also actual justification gain term speed efficiency make much stronger,3
738.json,present observation possible utilize sparse operation training lstm network without loss accuracy observation novel although surprising knowledge must state familiar research fast implmentations minor note lstm language wordvec layer simply linear embedding layer wordvec name specific directly character level language model present central observation clearly however much convincing well known dataset experiment used graf sutskever actual training validation test performance reported main observation certainly interesting think sufficient subject full conference without implementation simulation benchmarking promised speedup multiple task example gain affected various architecture choice present interesting technical report like detailed future,4
738.json,contribution training lstms many intermediate gradient close zero flat shape tanh sigmoid nonlinearities origin show rounding small gradient zero matrix sparsity training training character level lstm language model sparsification significantly change final performance argue sparsity could exploited specialized hardware improve energy efficiency speed recurrent network training novelty thresholding gradient induce sparsity improve efficiency training novel result knowledge missing citation prior work explored precision arithmetic recurrent neural network language model hubara quantized neural network training neural network precision weight activation,3
684.json,term strategy ambiguous could please explain formal term strategy discounted return time reward time could author compare method learning vague using many term different meaning without clarifying diversion output given state action pair always function definition value state action long policy deterministic output always different learning description specify policy mentioned data generation part based approach learning curve iteration give useful information final clearly nothing comparable previous work tested three game vague using informal language sometimes misusing common term experiment small scale even scenario performing clear based approach,1
734.json,considers case multiple view data learned probabilistic deep neural network formulation make linear unlike make inference difficult therefore framework invoked inference show maximum likelihood estimation based linear latent lead canonical correlation direction linear case dnns clear least present analysis solution canonical direction analysis hence find stretch refer type contrast dcca dccae taking canonical correlation feature account inside objective provide interpretation bach jordan probabilistic interpretation canonical correlation analysis technical report also significant body related work linear multi view model discussed example probabilistic linear multi view model also extended bayesian case common private space variational deep learning case gaussian process latent variable model human pose estimation mlmi learning shared latent structure image synthesis robotic imitation nip damianou manifold relevance determination icml damianou lawrence deep gaussian process aistats utility bringing together element multi view modeling vaes seems like obvious idea best knowledge done actually potentially useful however question proper extending multiple view convince work well multiple view using shown straightforward construction specifically vcca seem promote state term actually overall vcca private seems quite posed dimensionality manually tuned exhaustive search actual provide consinstent encouraging private common variable avoid learning redundant information relying dropout seems quite solution fact seems dropout rate quite crucial perhaps good performance might achieved tuning might flickr better without changing seems quite difficult optimize reason purely experimental point view vcca private seem promote either course expect published beat previous baseline seems extension multiple view interesting idea deserves investigation efficiently another issue approximate posterior parameterized view make le useful generic multi view since misbehave task classification classification main objective compare proper classification feedforward neural network plot nice overall convinced merit attaching multiple view however convince proposed achieve practical connection method multiple view bottom line although interesting need little work,4
775.json,address question often overlooked reinforcement learning locomotion experiment biggest point critique difficult draw conclusion reason beyond experiment consider single neural network architecture single reward function example torque controller limited policy network suggestion vary number neuron show hold different state representation trained pixel data current form term deeprl seems arbitrary positive side well structured easy read experiment sound clear easy interpret definitely interesting line work beyond extension argue considering realistic physical constraint actuator constraint communication delay real robot could greatly improve impact work,5
325.json,present method training generative iterative denoising procedure denoising process initialized random sample crude approximation data distribution produce high quality sample multiple denoising step training performed setting markov chain slowly blend proposition current denoising real example data distribution using chain current denoising updated towards reproducing changed better sample blending process clearly written considers interesting approach training generative model intrigued simplicity presented approach really enjoyed reading proposed method novel although clear tie recent work aiming denoising model sampling distribution work sohl dickstein recent work using daes generative model think general direction research important proposed procedure take inspiration perspective generating sample minimizing energy function transition along markov chain successful potentially sidestep many problem current procedure training directed generative model convergence mode coverage problem generative adversarial network problem modeling multi modal distribution arise restrictive approximate inference paired powerful generative said another method seems promising addressing issue also superficially similarity presented work idea combining hamiltonian monte carlo inference variational inference entirely convinced method presented able perform better mentioned although might simpler train similarly although agree using mcmc chain generate sample like procedure likely costly convinced procedure least also work reasonably well simple mnist example general direct comparison different inference method using mcmc chain like procedure nice understand perhaps scope thing expected however direct comparison procedure sohl dickstein term sampling step generation quality directly related major point good although general method explained well training detail missing importantly never mentioned alpha omega assuming omega increase mentioned experimental setup also unclear alpha affect capability generator intuitively seems reasonable small alpha many step ensure slow blending distribution clear necessary point procedure break assume alpha work generator magically denoise sample relatively uninformative draw mention figure caption denoising produce good sample step might also artifact training small alpha least priori reason experiment carried infusion chain generating chain shown complicated data distribution unfortunate feel interesting look good evaluating respect several different metric bound likelihood nice well unfortunately current approach come theoretical guarantee unclear choice alpha procedure work whether deeper connection mcmc sampling energy based model eye subtract value perhaps worth short sentence conclusion minor point second reference seems broken figure start epoch result contains little information perhaps useful show complete training procedure axis scale explanation regarding convolutional network make sense write structure improved gans unlike generates sample fixed length random input thus suppose really generator fully connected network followed convolution rather several stage convolution followed fully connected layer convolution choice parametrizing variance sigmoid output unit somewhat unusual specific reason choice footnote contains error allow allows information little information force network force page error page error operator learn markov chain monte carlo variational inference bridging salimans diedrik kingma welling icml update copied response believe response clarifies open issue strongly believe accepted conference remaining issue acknowledge architecture generator likely highly optimal might hamper performance method evaluation however subtract main point thus keeping score clear accept want emphasize believe published case review process form threshold high overall inflated review score,6
598.json,describes system speech recognition us linear conditional random field framework convnet estimate node potential transition score provided trained scalar value convnet acoustic computes score letter reduces need expert knowledge training system test time score word level language convnet node potential learned letter letter transition score word insertion penalty combined find best scoring word hypothesis trained audio waveform power spectrum mfcc feature using conditional maximum likelihood estimation experiment librispeech corpus show achieves test clean librispeech using mfcc feature using power spectral feature using waveform pro interesting convnet trained scratch using conditional maximum likelihood perform reasonably well speech recognition system english us graphemic letter based acoustic model instead phonetic model promising research direction con missing context prior work deserves cited addition paper already mentioned various comment also aware another interspeech zhang towards speech recognition deep convolutional neural network,3
517.json,sincerely apologize late arriving review proposes frame problem structure estimation supervised classification problem input empirical covariance matrix observed data output binary decision whether variable share link sufficiently clear goal clear everything well described main interesting point empirical experimental section approach simple performs better previous learning based method observation interesting interest structure discovery problem rate specific construction supervised learning method reasonable attempt attempt approach problem much technical novelty part algorithmic contribution method invariant data permutation could possible target technical contribution make claim technical part said method well constructed well executed good precisely state theoretical part well rather straight forward like claim written little surprise statement summary make interesting observation graph estimation posed supervised learning problem training data separate source sufficient learn structure novel unseen test data source practically relevant hand empirical stronger method hand practitioner interested structural discovery side constraint interpretability deriving method discussion conclusion understand consider future work good first step could stronger also stand already,5
517.json,proposes method learning graphical model combined neural network architecture sparse edge structure estimated sampling method introduction problem graphical lasso selection however proposed method still implicitly includes selection proposed method sparse prior include hyper parameter tune hyper parameter tuning equivalent problem section therefore understand real advantage method previous method advantage proposed method another concern unorganized algorithm first sigmai sampled sampled sigma sigma different sigmai furthermore construct sigma finally simple question input data sampled data used algorithm definition receptive field proposition proposition,4
350.json,interesting architecture accumulates continuously corrects mistake video sequence clarity video generated seems helpful towards understanding information flow network nice link hyperparameters optimized kitti underperforms finn outperforms previous state mathieu clear different train test sequence moment since standard benchmark really exist video prediction author pick favorite underperforming finn walking video disappointing,5
350.json,learning physical structure semantics world video without supervision area computer vision machine learning investigate prediction future image frame inherently unsupervised help deduce object structure property case single object pose category steering angle supervised linear readout step enjoyed reading clear interesting proposes original network architecture prednet video frame prediction produced promising synthetic natural image moreover extensive experimental evaluation analysis provide put solid ground others compare weakness link predictive coding better explained used motivation prednet idea proposed method learning implicit object make scene vague fetched sound great minor comment next number labeled training example interesting much unsupervised training data used train representation,7
448.json,familiar enough mean field technique judge soundness willing roll minor point presentation speaking evolution travel network could give reader helpful intuition confusing immutable input vector introduced variable represent called evolution interpreting analysis network trainable information pas training step whatever reason perturb weight information start pas without subsequently perturbing weight stop information passing perhaps could clarified definition training algorithm comment central claim previous work initializing neural network promote information flow glorot bengio,7
448.json,expands recent mean field approximation deep random neural network study depth dependent information propagation phase dependence influence drop extremely well written mathematical analysis thorough numerical experiment included underscore theoretical overall stand paper thoroughly analysis training performance deep net,8
673.json,hierarchical memory fixed learned hierarchical experimental section layer softmax layer show mips mips mips mean mips best adopt approximated mips worse even original method need exact mips seems proposed method robust,4
673.json,proposes algorithm training memory network large memory training model traditional way using soft attention mechanism memory slot slow also harder train dispersion gradient proposes mips algorithm memory choose subset memory slot attention applied since cost exact mips full attention propose approximate mips faster compute inferior performance artifact using mips cannot learn memory slot hence trained kept fixed entire training experimental section show efficacy using mips using simplequestions dataset exact mips performance full attention approximate mips deterioration performance quite clearly written easy understand think idea proposed super convincing number issue mips algorithm force memory fixed rather limiting constraint especially problem dataset require multiple hop training compounded reasoning entirely sure usefulness technique furthermore exact mips sample complexity full attention achieve speedup approx mips expected significant drop performance motivates idea proposing solution eliminate heuristic used prune memory however section using multiple heuristic make training work agreed used heuristic data dependent still feel like kicking road heuristic concerned experimental convincing first speed comparison second compare method mips fast nearest neighbor search flann,4
366.json,propose variant compare standard inference scheme online also evaluate different prodlda sure proposed topic modeling literature though general like direction look promising experimental however confound inference make hard understand significance furthermore discus hyper parameter selection known significantly impact performance topic model make hard understand proposed method expected work maybe generate synthetic datasets different dirichlet distribution ass proposed method recovers true parameter figure prior posterior text talk sparsity whereas axis read topic proportion confusing section clear mean unimodal softmax basis consider dirichlet dimensional simplex concentration parameter alpha alpha make multimodal softmax basis still multimodal none number include error bar statistically significant minor comment last term equation error reconstruction accuracy negative reconstruction error perhaps idea using inference network much older helmholtz machine,5
366.json,comparison nvdm look unfair since user introduces couple trick dirichlet prior batch normalisation high momentum training nvdm convincing experimental design explore effect trick separately neural variational inference,4
537.json,proposes approach generating synthetic training data deep network based rendering model learning additional transformation adversarial training approach applied generating barcode like marker used honeybee identification demonstrate classifier trained synthetic data generated proposed approach outperforms training limited real data training data hand designed augmentation topic using machine learning particular adversarial training generating realistic synthetic training data interesting important proposed method look reasonable written well downside experiment limited fairly simple widely known domain honeybee marker classification sure important task order demonstrate general applicability method allow comparison existing technique experiment standard realistic datasets helpful overall recommend acceptance encourage perform experiment datasets appreciate added baseline manually designed transformation strengthens reviewer point interesting analyze restricting fixed transformation necessary transformation important perhaps provide guideline designing set transformation complicated scenario tone claim method improvement previous work whereas previous work relied real data training using trained model mixing real generated data able train dcnn scratch generated data performed well tested real data fair comparison domain studied work much simpler studied previous work comparison appropriate,5
537.json,submission proposes interesting match synthetic data real data type architecture main novelty parametric module emulate different transformation artefact allow match natural appearance several point raised discussion proposed method driven previous model traditional approach perform mentioned effect like blur lighting background could also potentially modelled upsamling network directly predicts image assume blur lighting modelled convolution transformation extend convolution spatial transformer network answer partially address point proposal submission seems parameterised module trained match real data distribution remains unclear generic parameterisation also neural network done regular gans benefit introducing stronger unclear using render engine generate initial sample appearance limited novelty compare traditional data augmentation technique noise dropout transformation linking kera code data augmentation readily available could tested imagedatagenerator reply plenty augmentation used detail going provided appendix appreciated information directly included revision procedure could directly checked right remains point uncertainty different stage phi effect performance important one evaluate effect hand tuning transformation stage learning great also include including excluding stage completely also reporting much initial jittering data help interesting idea limited novelty concern evalations comparison outlined addition success single dataset task shown task interesting seems challenging overall remains make weak recommendation acceptance,5
496.json,proposes multiscale recurrent neural network layer different time scale scale fixed variable determined neural network method elegantly formulated within recurrent neural network framework show state performance several benchmark well written question extend bidirectional,7
496.json,proposes modified architecture multiple layer higher layer passed lower layer state flush operation predicted consisting passing state reseting lower layer state order select three operation time step propose using straight estimator slope annealing trick training empirical visualization illustrate modified architecture performs well boundary detection pro well motivated exceptionally well composed provides promising initial learning hierarchical representation visualization thorough experiment language modeling handwriting generation annealing trick straight estimator also seems potentially useful task containing discrete variable trade flush operation innovative con couple case fully deliver empirical computational saving given hierarchy beyond single level data contains separator space seem demonstrated unclear whether better downstream performance hierarchical information architecture change acting regularization something could hopefully addressed,6
316.json,address problem achieving differential privacy general scenario teacher trained disjoint subset sensitive data student performs prediction based public data labeled teacher noisy voting found approach altogether plausible clearly explained adding discussion bound tightness theorem appreciated simple idea adding perturbation error count known differentially private literature nicely used elegantly applied much broader convex setting practical context number differentially private related paper generality approach clear improvement predecessor clarity writing make method worth publishing,8
377.json,address question utilize physical interaction answer question physical outcome question fall popular stream community understanding physic moved step worked experimental setup prior physical property rule us deep reinforcement learning technique address problem overall opinion interesting attempt idea without clear contribution experimental setup quite interesting goal figure block heavier block glued together pushing pulling object around without prior also show reasonable performance task detailed scenario experiment interesting contribution unclear main question result bring insight scenario interesting focused physical experiment different potentially easier learning playing game atari word task really different typical popular task excited showed insight experiment learned representation currently discus factual outcome example describes experimental setup much performance agent could achieve could probably dissect learned representation discus experimental linked human behavior physical property law overall rating think could deeper analysis however recommend acceptance merit idea following detailed question directly impacting overall rating page assume agent prior knowledge physical property object law physic hence must interact object order learn answer question property must interact object order learn property also learn observation figure right missing axis label page relating bandit interesting formal approach based page make distinguishing heaviest block difficult confused small mass make task harder unless really close machine possible distinguish even pixel difference speed network architecture page since agent exhibit similar performance using pixel feature conduct remaining experiment section using feature observation since agent substantially faster train least showing correlation performance instance level rather average performance even think conclusion throughout paper felt many conclusion difficulty based particularly chosen training distribution example agent really know instance difficult really depend empirically learned distribution training sample indicates object word hard easy matter much unless thoroughly tested various type distribution baseline approach,5
377.json,investigates question gathering information answering question direct interaction environment sense closely related active learning supervised learning fundamental problem exploration exploitation consider specific instance problem physic domain learn information seeking policy using recent deep method mostly empirical explores effect changing cost information discount factor structure learned policy also show general purpose deep policy gradient method sufficient powerful learn task proposed environment knowledge novel well task formulation section valuable community environment open sourced expression latent structure dynamic used throughout text connection bandit mentioned section therefore seems aspire generality approach quite fully ground proposed approach formally existing framework provide completely example approach formalize concept question answer make question difficult quantify difficulty define cost information unit bit scalar reward semantics pomdp kind consider define discounted state action space important problem structure interaction labeling reward paragraph section worth expressing directly definition labeling action occur labeling phase transition reward function specific structure positive negative lead absorbing state notion phase could perhaps implemented considering augmented state space tilde phase,6
377.json,purport investigate ability agent perform physic experiment environment infer physical property object environment problem well motivated indeed inferring physical property object crucial skill intelligent agent relatively little work direction particularly deep also well written architectural theoretical contribution none claimed main novelty come task application using recurrent task simulate agent interacting environment infer physical property object specifically task considered moving block determine mass poking tower fall determine number rigid body composed course represent limited cross section prerequisite ability agent understand physic thing since comparison different simpler agent task difficult determine task selected challenging mentioned review question heavier task seems quite easy actuator fact simply must learn take difference successive block position directly encoded feature experiment thus particularly surprising agent solve proposed task main claim beyond solving proposed task related physic simulation agent learn different strategy task balance cost gathering information cost making mistake cost gathering information implemented multiplying reward value gamma somewhat interesting behaviour hardly surprising given problem setup item highlight approach learning physical object property interaction different many previous approach visual cue however also note novel explored work agrawal think crucial discus approach detail potentially along removing le relevant information related work section specifically highlight proposed task interesting compared example learning move object towards certain position poking discern level contribution must following question much task contribute previous work goal agent learn property object interaction much agent task contribute understanding agent interact environment learn physical property object difficult know exactly concern outlined convinced answer significant extent particular since proposed agent able essentially solve task clear task used benchmark advanced agent used babi like task another possible concern pointed reviewer description extremely concise nice example diagram illustrating input output time step ease replication overall important make progress towards agent learn discover physical property environment contributes direction however technical contribution rather limited thus clear extent push forward research direction beyond previous work mentioned nice example discussion future agent learn physic interaction speculation difficult version task proposed approach fit picture edit score updated comment,6
377.json,present interesting experimental finding state deep reinforcement learning method enable agent learning latent physical property environment formulates problem agent labeling environmental property interacting environment based action applies deep reinforcement learning evaluate whether learning possible approach jointly learns convolutional layer pixel based perception later layer learning action based reinforcement signal mixed opinion written clearly present interesting experimental finding introduces formulates problem potentially important many robotics application simultaneously suffers lacking algorithmic contribution missing crucial experiment confirm true benefit pro introduces problem learning latent property agent environment present framework appropriately combine existing tool address formulated problem try reinforcement learning image input fist like actuator action lead direct application robot con lacking algorithmic contribution applies existing tool method solve problem rather developing something extending approach essentially training lstms convolutional layer using previous asynchronous advantage actor critic tower experiment probably important setting fist pixel missing setting receiving pixel input using fist actuator continuous space setting closest real world robot thus important confirm whether proposed approach directly applicable real world robot however figure missing setting reason behind lack comparison baseline method without explicit baseline difficult agent really learning aspect proposed approach benefitting task instance tower task agent randomly pushing hitting tower using fist number time passively observing consequence produce label perform compared approach approach fixed action policy everything else perform compared full deep reinforcement learning version,6
663.json,introduces large scale multi product classification system consists three module image architecture text decision level fusion policy tried several fusion method including policy taking input text image probability choose either average prediction training experimental show text alone work better image multi fusion improve accuracy small margin little surprising feature level fusion work worse text alone writing clear useful practical experience learning large scale however lean toward rejecting following dataset reported mentioned releasing walmart dataset going really hard reproduce without dataset technical novelty limited decision level fusion policy investigated previous method performance gain also limited,4
663.json,tackle problem multi modal classification text image pro interesting dataset application con rather lacklustre showing mild improvement compared oracle improvement perhaps insight whether incorrect decision humanly possible help significance could explored intermediate architecture feature fusion class probability without finetuning feature fusion reported evaluation standard datasets comparison previous work policy learnt given input class probability network perform better mean,4
663.json,present system approach combine multiple modality perform classification practical scenario commerce general find proposed approach sound solid novelty feature fusion decision time fusion standard practice multi modal analysis rest offer surprise implementing approach seems better venue focus production system seems iclr focus research novel algorithm theory,3
419.json,introduces blend idea generative topic model recurrent neural network language model evaluate proposed approach document level classification benchmark well language modeling benchmark seems work well also analysis topic learned ability generate text overall clearly written code promised others able implement approach potentially major question address topic model make exchangability word assumption discussion generative story topicrnn explicitly discus whether assumption also made surface appears since sampled using document topic vector know practice come recurrent observes clear clean exposition generative relates actually done generating sequential text section clear topic generate word without using seems inconsistent generative specification need shown made clear complete topic allows linear interaction topic vector theta seems like might required keep generative tractable seems like poor assumption expect topic representation rich interaction language create nonlinear adjustment word probability document please discussion modeling choice exists possible future work could modify assumption explain assumption might imagine figure color difficult distinguish,5
462.json,explores important angle adversarial example detection adversarial image utilization trainig robust network take competition adversary model level present appealing evidence feasibility robustifying network employing detector subnetwork trained particularly purpose detecting adversary terget manner rather making network robust adversarial example jointly trained primary detector system evaluated various scenario including case adversary generator access generated generic show good improvement approach present well motived thorough analysis back main message writing clear concise,6
383.json,introduces reinforcement learning framework designing neural network architecture time step agent pick layer type corresponding layer parameter filter order reduce size state action space used small design choice strength novel approach automatic design neural network architecture show quite promising several datasets mnist cifar weakness limited architecture design choice many prior assumption possible number convolution filter fully connected layer maximum depth hard coded dropout method demonstrated tabular learning setting unclear whether proposed method work large state action space overall interesting novel approach neural network architecture design seems worth publication despite weakness,5
383.json,learn deep architecture small vision problem using learning obtain solid sota limiting certain type layer competitive everything else good know well performs allowing complex structure much convincing real size task imagenet,5
340.json,present unsupervised image transformation method map sample source domain target domain major contribution lie require aligned training pair domain based gans make work unsupervised setting decomposes generation function module encoder identify common feature space domain decoder generates sample target domain avoid trivial solution proposed additional loss penalize feature difference source sample transformed sample pixel difference target sample generated sample present extensive experiment transferring svhn digit image mnist style transferring face image emoji style proposed learning method enables unsupervised domain transfer could impactful broad problem context present careful ablation study analyze effect different component system helpful understanding transferred image visually impressive quantitative also show image identity preserved across domain degree interesting show domain text image addition face identity also great interest analyze well facial attribute preserved mapping target domain,6
340.json,update thank running experiment explanation manuscript addressed concern updated score accordingly work aim learning generative function map input source domain target domain given representation function remain unchanged accepting input either domain criterion termed constancy proposed method evaluated visual domain adaptation task relatively easy follow provided quite extensive experimental datasets constancy main novelty work seems counter intuitive force function starting restricted function might already lost information face dataset learned optimize performance certain task external dataset clear input source target domain recovered applying equation also function learned particular task mind experiment representation function learned identify digit source svhn dataset identity face dataset result procedure repeated perform domain adaptation domain different task recognizing expression instead identity insight baseline method proposed equation perform poorly figure show visual comparison style transfer proposed method clear though method better possible apply style transfer generate emojis photo repeat experiment shown table,5
356.json,present method synthesize string manipulation program based input output pair focus restricted class program based simple context free grammar sufficient solve string manipulation task flashfill benchmark probabilistic generative called recursive reverse recursive neural network presented assigns probability program parse tree bottom pas presented synthetic dataset microsoft excel benchmark called flashfill problem program synthesis important recent interest deep learning community approach taken based parse tree recursive neural network seems interesting promising however seems complicated unclear several place detail negative side experiment particularly weak seem ready publication based experimental positive realized method obtains accuracy flashfill benchmark presented input output example performance degrades input output example used surprising came hypothesis explain phenomenon problem indicating either code severe shortcoming useful program synthesis need applicable many input output example complicated program require many example disambiguate detail program given shortcoming experiment convinced ready publication thus recommend weak reject encourage address comment resubmit general idea seems promising comment unclear several place probability distribution normalized given nature bottom evaluation potential enumerate different completion program compare exponentiated potential restrict applicability long program enumeration completion get prohibitively slow input output pair program instead better section clear elaborate potentially including example input output representation supposes fixed number input output example across task task regarding experiment could present baseline flashfill benchmark based previous work method applicable short program based choice number instruction program considered correct identical test program considered correct succeeds held input output pair using program sample report accuracy best program recall first filter program based training input output pair evaluate program selected well beyond recommended limit page please consider making shorter,4
614.json,introduces time dependent recommender system based point process parametrized time dependent user item latent representation later modeled coupled autoregressive process representation user item change interacts item user function user item representation time called coevolution autoregressive process called recurrent also incorporate heterogeneous input experiment performed several datasets compared different baseline several contribution modeling recommendation parametrized point process parameter dynamic modeled latent user item representation optimization algorithm maximizing likelihood process different technical trick seem break intrinsic complexity evaluation experiment time dependent recommendation nip describes similar continuous time coevolution similar evaluation difference lie detail point process latent factor dynamic slightly different modeling approach argument exactly know make perform better proposed nip choice process parametrization quite similar justification choice specific form point process paper tried form well remark applies form dynamical process linearity used modeling latent user item vector limited sigmoid function probably change much linear evidence role linearity note inconsistency paper concerning evaluation introduce criterion exactly evaluate item recommendation mentioned time predicts item user interact mean next item user interact time time prediction relevant metric recommendation comparison complexity execution time different method helpful complexity method apparently proportional item user complexity limit method overall quite nice look technically sound albeit many detail missing hand mixed feeling similarity nip make better work convincing marginal extension previous work convinced either evaluation criterion evidence used large datasets,5
614.json,proposes method time changing dynamic collaborative filtering comment main idea build upon similar previous work group author wang major difference appears change latent factor author describes bptt technique train author introduced time prediction metric evaluate effectiveness time dependent however need condition given user item pair interesting consider metric example switching time user change another item jointly predict next item switching time summary improves existing work time dynamic recommender system time prediction metric interesting open interesting discussion evaluate recommender system time involved also comment,5
614.json,seek predict user event interaction item particular point time roughly speaking contribution follows model evolutionary process user preference toward item able incorporate external source information user item feature process proposed generative able estimate specific time point event occur able account linearity following review question understand combination novel aspect fully generative process sampled certainly nice though course generative process like regular regression estimate specific time point sure practice relevant distinction part appeared combination previous work though combination part certainly pass novelty quite followed issue mentioned review discussion requires multiple interaction userxitem pair order user interacts business multiple time slightly unusual setting compared temporal recommender system work question extent whether problem setting restrictive said take point subsample yelp data keeping user hundred event mean left biased sample user base issue technically nice experiment include strong baseline report good performance,5
478.json,torn seeing mpeg dataset reference curvature scale space brought mind saying worth worth well question mpeg dataset benchmark saturated long quite surprising submission modern conference brought question representation said main purpose connect theory differential geometry curve computational engine convolutional neural network fair enough agree seemingly different field deserve credit connecting give benefit doubt worth approach pursue using siamese configuration make sense adaptation deep convnet framework signal reasonable extent invariant based method made smoothed filtered representation coupled nonlinearities sensible revisit problem using convnets mind seeing accepted since different mainstream worry narrow audience iclr still care type shape representation,5
478.json,show contrastive loss siamese architecture used learning representation planar curve proposed framework able learn representation comparable traditional differential integral invariant evaluated example generally well written show interesting application siamese architecture however experimental evaluation show rather preliminary many choice validated biggest concern choice negative sample network basically learns distinguish shape different scale instead recognizing different shape well known fact order achieve good performance contrastive loss careful hard negative sampling using easy negative lead inferior thus underlying reason choice negative unfortunately discussed furthermore miss thorough quantitative evaluation concentrate showing particular example instead measuring robust statistic multiple curve invariance noise sampling artifact general show interesting first step direction however clear whether experimental section strong thorough enough iclr conference also novelty proposed idea limited siamese network used many year work show applied different task,4
400.json,propose drnn neural decoder tree structure like architecture since clear improvement traditional approach information flow direction parent sibling desirable tree structure probability distribution tree boundary last sibling leaf avoids special ending symbol larger size putting thing learn parameter shared symbol test drnn using task recovering synthetic tree recovering functional program better traditional method like seqseq model think recovering synthetic tree task satisfying reason surface form already containing topological information make task easier figure number node grows even number large performance drop dramatically sure simple baseline capture topological information surface string much worse drnn case seems show full potential since length information flow long think experiment interesting think task difficult tree structure information important task example seqseq parsing vinyals possible drnn proposed decoder side think task like show potential drnn convincing architecture like better traditional alternative,5
457.json,great deal ongoing interest compressing neural network model line work focused using precision representation weight even bit however approach accompanied significant impact accuracy proposes iterative quantization scheme network weight quantized stage largest weight absolute value quantized fixed unquantized weight adapt compensate resulting error experimental show extremely effective yielding model weight essentially reduction accuracy bit accuracy decrease slightly substantially better achieved quantization approach overall clear technique aware novel experiment thorough compelling recommend acceptance could another second pas writing style grammar also description pruning inspired partitioning strategy could clarified somewhat chosen splitting ratio seems referenced figure caption main text,7
457.json,idea reasonable gradually original weight compressed weight compressing part fine tuning rest everything seems fine look good question addressed improve good incorporate answer mainly pruning method compared fairly outperforms good better explain encoding method question clear made make mistake question computation bit misleading fact used variable length encoding average close bit represented value represented bit first needed distinguish remaining bit represent different value power,6
482.json,light detailed author response update manuscript raising score reiterating support think among strongest traditional applied deep learning work iclr receive great deal interest attention attendee describes modern deep learning approach problem predicting medication taken patient period time based solely upon sequence code assigned patient time period problem formulated multilabel sequence classification contrast language modeling multiclass classification propose standard lstm architecture embedding layer handle sparse categorical input similar described related work choi experiment using cohort patient record find model outperform strong baseline including random forest well common sense baseline difference performance recurrent model appear large enough significant given size test strength important problem point value proposition ehrs widely adopted throughout combination legislation billion dollar incentive federal government included accurate record fewer medication mistake benefit largely failed materialize seems like major opportunity data mining machine learning well written lucid introduction motivation thorough discussion related work clear description experiment metric interesting qualitative analysis empirical solid strong rnns convincing baseline contrast recent related paper including lipton kale iclr relatively small choi mlhc omitted many obvious baseline discussion thorough thoughtful right kidney code embedding promising result weakness make several unintuitive decision related data preprocessing experimental design foremost among choice full patient sequence instead truncated patient sequence end randomly chosen time point necessarily invalidate somewhat unnatural explanation difficult follow reducing potential impact also reduces potential advantage chosen metric seem appropriate expert trouble interpreting absolute relative performance beyond superficial score invest space explaining level performance metric necessary useful real clinical setting whether gap various model significant even informal sense proposes nothing novel term method serious weakness method conference like iclr think strong enough empirically sufficiently interesting application warrant acceptance regardless thing make competitive example potential hypothesis higher capacity model prone overfitting noisy target investigate perhaps looking kind error make final comment piece clinical work huge weakness lack ground truth label missing medication model trained tested data noisy label training right huge problem provided label noise random even class conditional problem testing though seems like could skew metric assumption label noise systemic seems unlikely given data recorded human clinician case shown appendix lend credence assertion case actual medication received probability hunch clinical reviewer view great skepticism need creative evaluation invest time money labeling data really prove work worth hope accepted think great interest iclr community however borderline whether willing fight acceptance address reviewer critique particular dive question overfitting imperfect label provide insight might willing raise score lobby acceptance,7
363.json,proposed compare aggregate task require semantically comparing text sequence question answering textual entailment basic framework apply convolutional neural network aggregation element wise operation comparison attentive output lstms highlighted part comparison compare several different method matching text sequence element wise subtraction multiplication operation demonstrated achieve generally better performance four different datasets weak point incremental work lack innovation qualitative evaluation subtraction multiplication comparison function perform varied kind sentence interesting,5
363.json,proposes compare aggregate framework performs word level matching followed aggregation convolutional neural network compare different comparison function evaluates four datasets extensive experimental reported compared various published baseline well written overall detailed comment page line including including benefit preprocessing attention step provide without figure hard read printed hard copy please enhance quality,6
363.json,present general approach modeling natural language understanding problem distinct textual input question source text aligned approach soft attention first used derive alignment token text comparison function us resulting alignment represented pair attention query attention derive representation aggregated single vector output computed present overall modeling strategy made work quite well offer detailed empirical analysis comparison component work timely language understanding problem kind major open issue threshold addressable representation learning method work present general approach straightforward reasonable show yield good work border incremental relative earlier work parikh contributes enough substantial way strongly recommend acceptance detail least implemented problem longer sequence everything snli sensitive word order empirically competitive insensitivity place strong upper bound performance make clear seems salient enough warrant brief mention introduction discussion section understand correctly attention strategy based closely general bilinear strategy luong earlier bahdanau work probably cite former directly relevant reference strategy since risk overfitting large number parameter using version input dimension smaller output dimension tensor probably note submultnn look like strategy sentence level matching lili cite reason parameter preprocessing question answer could require different thing weighted highly,7
498.json,introduces dropout latent variable leveraging formulation analyze dropout inference define network output training instance dropout used every training sample test expected dropout value used scale node output introduce notion expectation linearity derive bound inference mild assumption furthermore propose sample based inference regularizer present analysis accuracy model expectation linearization constraint compared without relatively minor issue view dropout seems applicable probabilistic model whereas dropout generally applicable deep network however expect regularizer formulation dropout effective even probabilistic model dropout page defined please define page mentioned proposed regularizer standard dropout network achieve better monte carlo dropout used seems case mnist dataset cifar table also appears dropout achieves best performance across task method course expensive procedure comment computational efficiency various dropout procedure accuracy quite valuable couple typo input input defined right place paragraph overall good think accepted discussed conference,6
358.json,proposes hierarchical generative lower level consists point within datasets higher level model unordered set datasets basic idea double variational bound higher level latent variable describes datasets lower level latent variable describes individual example hierarchical modeling important high impact problem think explored deep learning literature pro shot learning look good expert area idea using double variational bound hierarchical generative well presented seems widely applicable question training statistic network minibatches subset example used using minibatches actually give unbiased estimator full gradient used example example statistic network want pull example dataset certain feature treat characterization seems graphical right side figure statistic network trained minibatches able learn characterization given minibatch missing example dataset using minibatches opposed using example dataset train statistic network seems like limit expressive power suggestion hierarchical forecasting electricity sale could interesting practical case type,7
358.json,sorry late review technical problem openreview prevented posting present method learning predict thing set data point method hierarchical version layer consists abstract context unit summarizes dataset experiment show method able learn learn acquiring ability learn distribution small number example overall nice addition literature shot learning method conceptually simple elegant seems perform well compared recent paper shot learning proposed method simpler based unsupervised representation learning clearly written pleasure read name overly grandiose relative done proposed method seem much common statistician unless mean someone think statistic experiment well chosen shot learning seem pretty solid given simplicity method spatial mnist dataset interesting might make good benchmark input figure seem pretty dense though method able recognize distribution fewer sample nitpick point figure seem correspond meaningful point claimed text release code,7
749.json,take task figuring design pattern current deep architecture namely theme recurring literature distributed representation deep architecture aspect particularly valued firstly excellent review recent work made realize many thing missing secondly community service aspect helping someone start figure coordinate system deep architecture could potentially important introducing another trick trade submission however think work still half done even though working project great idea properly firstly sure choice pattern made maxout instance pattern many nonlinearities prelu relu stand ground something general strive simplicity similarly pattern vague increase symmetry backed statement noted special degree elegance fractalnet lead design pattern applied architecture applies anything fractalnet pattern phrased weird name cover problem space guess stand dataset augmentation train backed single reference unless relate regularization text preceding overtrain connection description train provided training network harder problem improve generalization harder problem mean add additional term regularizer harm unexperienced reader confusing regularization something sound like overfitting exact opposite furthermore extension proposed section seem tune particular could figure taylor series network stem design pattern proposed rest whether text another architecture innovation importantly design pattern deployed practice think network concrete mention propose freeze drop path variant symmetry consideration drop path application increase symmetry pattern freeze drop path symmetric drop path expressed concretely intuitive guess second really part applying pattern understanding first missing appreciated like revised version table design pattern axis deep network another breakdown network applies design pattern part previous work also covered cryptic language minimal explanation taking place alternative work useful,3
460.json,study policy learning actor critic experience replay important challenging problem order improve sample efficiency reinforcement learning algorithm attack problem introducing truncate importance weight modified trust region optimization combining retrace method combination technique performs well atari mujoco term improving sample efficiency main comment technique contribute performance gain experiment could carried evaluate separate gain trick helpful,6
460.json,introduces actor critic deep approach experience replay combine truncated importance sampling trust region policy optimization also proposes method called stochastic duelling network estimate critic continuous action space method applied atari game continuous control problem yield performance comparable state method mentioned beginning main contribution work lie combining truncated importance sampling retrace trust region policy optimization stochastic duelling network improvement work well beneficial future work however improvement appears quite incremental moreover acer framework seems much complex fragile implement compared standard deep learning prioritized replay appears perform well atari game atari domain still money prioritized replay simplicity thirdly improving sample efficiency deep laudable goal really goal pursued problem setting sample efficiency important unfortunately evaluates sample efficiency atari continuous control task domain domain sample efficiency important thus clear proposed method acer generalize problem really care sample efficiency technical aspect need clarification retrace assume compute recursively starting trajectory please comment clear derive approximation double tilde sign missing section argued give lower variance estimate action value function bias correction term state us replay memory frame across thread comparable size previous work however thread much smaller compared earlier experiment atari game example million experience replay transition used prioritized experience replay schaul huge impact performance model acer competing model order properly ass improvement acer previous work need also experiment larger experience replay memory comment please move section appendix moreover using small value lambda reduce variance occasional large importance weight still cause instability think meant using large value lambda mention squared error used missing subscript beginning hard understand stochastic duelling network please rephrase part please clarify sentence compare different agent adopt metric median human normalized score game figure bottom please label vertical ax,5
460.json,look several innovation deep evaluates effect solving game atari domain read like laundry list researcher latest trick written clearly enough lack compelling message expect work interesting people already implementing deep method probably much attention broader community claim suggest approach stable sample efficience expected theoretical analysis respect property empirical claim help clarify abstract proposed innovation based sound method particularly nice approach working discrete continuous domain reasonably complete empirical nice confidence interval plot also really tease apart effect various innovation harder understand impact piece really intuition example acer outperforms also clear matching discrete task state continuous task good coverage related literature nice work draw attention retrace including theoretical characterization,5
773.json,present repurposing rectified factor network proposed earlier biclustering method seems potentially quite interesting serious problem presentation quality method relies mainly technique presented nip mostly experimental procedure clarified especially table seem depend critically upon sparsity reported cluster explain sufficient detail sparsity hyperparameter determined clarity style writing terrible completely unacceptable scientific publication text look like industry white advertisement objective scientific complete rewrite needed considered publication specifically reference company using method must deleted additionally table essentially unreadable recommend using figure cleaning table removing engineering notation reporting number become general figure preferred primary mean presenting text table included supplementary information originality novelty work appears limited method mostly based nip experimental evaluation appears least partially novel example detection similar hochreiter without comparison significance strongest claim based strong empirical performance benchmark problem however unclear useful others code available detail implementation le complete furthermore method depends many specific tuning parameter whose tuning method fully defined leaving unclear guarantee generalisation good performance,3
773.json,clarity novel contribution section difficult understand notation seemed inconsistent particularly still confident understand used originality novelty come applying including relu linearity dropout training problem biclustering sound like good idea significance proposed algorithm appears useful tool unsupervised data modelling make convincing argument significant previous state fabia widely used method outperforms address practical difficulty method quality experiment high quality comment introduction claim method much faster fabia rectified unit allow gpus clear work many biclusters supported method look like number biclusters used method experiment introduction claim using dropout training increase sparsity bicluster assignment seems like reasonable hypothesis claim supported better argument experiment deep deep us relu dropout,4
745.json,overall idea interesting well written well motivated however think ready publish iclr following reason related representation learning suitable general machine learning data mining conference proposed approach work small class model cannot apply popular formulation logistic regression neural network unclear want specific type formulation like linear regression compare method linear programming approach also unclear need develope parallel algorithm linear regressio problem relatively easy solve unless data next comment dataset used relatively small used proving concept datasets considered solved second using single core hogwild suitable sparse dataset asynchronized nature data sparse proposed approach slightly better worse hogwild dense dataset unclear need symsgd instead simply parallelizing gradient computation using gpus together experiment convincing,3
745.json,describes correction technique combine update multiple make statistically equivalent sequential technique comment proposed method novel interesting allow update corrected even update delayed proposed theory applied square loss setting linear update rule making somewhat limited much interesting iclr community technique applicable general objective function setting deep neural network resulting technique requires book keeping dimensional reduced combiner matrix cause computation term complexity argue overhead canceled simd support symbolic update however normal update might also benefit simd especially dataset dense overall even though practical value work limited technique specifically correction rule proposed could interest people scaling learning encourage author extend method case linear objective function could make interesting iclr community,5
518.json,proposes amortized version stein variational gradient descent svgd method neural network trained mimic svgd dynamic applies method generative adversarial training yield training procedure discriminator interpreted energy based probabilistic criticism presentation time energy spent setting table method claimed widely applicable scope empirical evaluation narrowed single specific setting view either fall short goal showing widely applicable proposed method spends much time setting table steingan enough time evaluating consequence empirical insufficient justifying approach proposed another reviewer pointed dcgan becoming outdated benchmark comparison qualitatively steingan sample look significantly better dcgan sample except celeba dataset particular case dcgan sample appear one presented original come quantitatively dcgan beat steingan small margin imagenet inception score steingan beat dcgan even smaller margin cifar inception score also opinion testing accuracy score convincing evaluation metric true measure amount information captured simulated image set sensitive information useful discrimination task general modeling task instance score likely completely blind information present background image reason outlined think ready publication iclr,3
518.json,propose amortized svgd amortized form prior work svgd particle variational method maximally decrease divergence update amortized svgd done training neural network learn dynamic apply idea train energy based model admit tractable unnormalized density svgd main difference addition repulsive force prevents degeneracy encouraging probability mass spread location outside mode able still strong enough entropy like term high dimension curious understanding previous work problem experiment data set experimental apply kernel hidden representation autoencoder seems similar kernel approach work well otherwise however unlike autoencoder part fixed break much proposed motivation criticism prior work must autoencode onto dimensional space putting effort autoencoder change iteration applying method unlike previous literature us inference network amortized svgd approach seems fact slower amortized approach must make actual update regressing perform update previous approach like perform local inference updating inference network parameter least partially performing local inference seems quite costly training recommend rejected provide comprehensive experimental expecially around influence autoencoder incremental update versus full update training time amortized amortized approach current promising unclear given many knob playing reference swersky zemel generative moment matching network presented international conference machine learning,3
670.json,learning unsupervised state representation using multi task reinforcement learning propose novel approach combining gated neural network multitask learning robotics prior evaluated approach simulated datasets showed promising clearly written theoretically sound positive gating enable learning joint representation multi task learning extended single task prior work combining multiple type loss learn strong representation coherence proportionality causality repeatability consistency separation negative parameter choice arbitrary parameter limiting multi task learning different individual task rather sharing transferring knowledge task experiment could conducted using standardized simulation tool openai make easy compare recommend consider standardized picking parameter evaluate standard high dimensional datasets,5
670.json,build upon method jonschkowski brock learn state representation multiple task rather single task research direction learning representation multiple task interesting largely unexplored approach learn different representation task different policy task task detected automatically built neural network state proposed method orthogonal multi task learning though goal learning solve multiple task interesting helpful discussion point discussed review question phase reference multi task learning work policy distillation actor mimic iclr appropriate well method proposes jointly learn task classifier state representation learner using differentiable gating mechanism control flow information proposes task coherence prior gating mechanism ensure learned task classifier temporally coherent introducing structure enables method improve performance standard multitask approach evaluation involves experimental scenario first involves controlling car drive around track task detecting task easy learned state representation linear observation evaluates performance policy learned proposed approach show sufficient comparison demonstrate usefulness approach standard multitask second navigation scenario state representation qualitatively shown resulting control policy learned state representation comparison since multi task state representation learning approach useful also learn control better also evaluate control comparison first experiment without evaluation experiment incomplete lastly publication venue like iclr method evaluated thoroughly wider range demonstrate generality approach show method applies complex task theory method scale experiment demonstrate handle realistic scenario scaling beyond mnist level image real image higher dimensional control task evaluating method complex scenario important unexpected issue come trying scale scaling straight forward running experiment including straight forward summary pro con con approach necessarily share information across task better learning requires learning different policy task experimental evaluates learned policy multi task state representation experiment realistic scenario environment high dimensional control problem pro approach enables using network multiple task often true transfer multi task learning approach novel learn single policy multiple task including task coherence prior ensures task classification meaningful experimentally validated task task show improvement baseline approach thus rating higher included evaluation control policy navigation included another challenging compelling scenario lastly minor comment question think could improved important approach could approach combined state representation learning approach approach autoencoder experiment additional useful comparison evaluate performance single task setting controlling upper bound well policy able perform learned multi task policy reach level performance upper bound tighter known position baseline also useful right observation baseline eventually reach performance approach useful know approach simply speed learning significantly enables better performance aliasing issue image higher resolution image,4
719.json,present alternative supervising training neural network without explicitly using label link link information available pair example pair network trained used supervise presentation clear writing improved design choice explained power function used step approximating distribution section consider uniform distribution understand using different prior break assumption nothing known class however practical situation proposed setting work useful also exist large body work semi supervised learning training based similar idea overall think work clarified improved good venue,4
719.json,explores technique classless association milder unsupervised learning know class label exactly prior example belong class proposed stream architecture neural network stream process example class simultaneously stream rely target pseudo class cluster index output intermediate representation forced match statistical distribution uniform case trained step obtains current statistical distribution given output vector step update weight architecture given pseudo class experimental organized mnist exhibit better performance compared classical clustering algorithm term association accuracy purity provide comparison supervised method proposed architecture expectedly performs worse promising basic motivation architecture apparently relies unlabeled data agreement pseudo label generated stream hard follow motivation proposed architecture hidden detail trying achieved matching distribution using pseudo target perhaps statistical distribution class assumed uniform extend prior even case assume know prior current setup need justification interesting example class mnist rotated mnist background mnist hard guess different example stream feel like found interesting approach classless association extended lot many problem good catch like idea future extensive experiment large scale datasets task current version lack theoretical motivation convincing experiment definitely recommend presented iclr workshop point typo figure second line caption necessity equation clear batch size enormous compared classical model explanation uniform clarified course simplest prior pick word good completeness typo page second paragraph line,4
719.json,look correct still convinced experimentation performed perhaps another experiment challenging data welcome honestly find clear motivation work however could potential interested presented conference,5
471.json,extends neural conversational model batch reinforcement learning setting idea collect human scoring data response dialogue however score expensive thus natural policy learning training base policy unsupervised data deploying policy collect human score learning line score overall contribution modest extending policy actor critic application dialogue generation approach well motivated written clearly easy understand main concern primary dataset used restaurant recommendation small conversation fact several order magnitude smaller datasets used literature twitter ubuntu dialogue corpus dialogue generation surprising chatbots additional structure able generate reasonable utterance small dataset able similarly small restaurant dataset mostly directly dialogue state surface form rather embedding representation context thus remains seen approach also result improvement much unsupervised data available reference tsung hsien milica gasic nikola mrksic lina rojas barahona stefan ultes david vandyke steve young network based trainable task oriented dialogue system arxiv preprint arxiv,5
471.json,author propose policy actor critic algorithm batch setting improve chat bot approach well motivated well written except intuition batch version outperforms line version comment clarification regarding batch online setting artificial experiment instructive real world experiment performed thoroughly although show modest improvement,6
471.json,discus batch method setup improve chat bot provide nice overview setup using present algorithm similar previously published line setup problem make comparison online version explore several modeling choice find writing clear algorithm natural extension online version constructive remark comparison constant state value function artificial experiment difference real life task good understand discussion option artificial task seems like giving constant value function unfair advantage update weight layer like state value function section sentence last defined last sentence missing stochastic case section last paragraph significant significantly different,7
778.json,need clear table original compression applied task case like compression applied state net float representation important instance network mnist imagenet lower feasible float representation probably imposible restricted representation,4
778.json,explores quantization method weight activation need training method reach compression ratio experience speed well written clearly expose detail methodology major criticism three fold compared many pruning method described section performance method difficult judge alone second several compression scheme involving pruning training vector quantization seem achieve much higher accuracy compression ratio speed hence practical application running network power memory device method seem much suited advantage given method possibly reducing time take compress network thus unclear particular taking trained network starting point quantized subsequently fine tuned might take much longer process method given maybe quantify finally much speed memory reduction seems arise three fully connected layer particular last speed convolutional layer comparably small making wonder well method work convolutional network inception architecture deep compression compressing deep neural network pruning trained quantization huffman coding,3
739.json,proposes algorithm polynomial feature expansion matrix reduces time complexity standard method factor density sparse matrix main contribution work significant enough experiment incomplete convincing background problem sufficiently introduced reference introduction part overall three paper cited decade many relevant paper cited recent literature experiment part weak claim time complexity algorithm improvement standard method factor experiment still large proposed method standard explain likely language implementation convincing fairly compare method course need implement programming language experiment environment higher degree feature expansion empirical experiment show advantage proposed method minor problem listed section notation clearly defined section typo efter algorithm titled input output clearly listed figure meaning colored area described standard deviation quantile running time many run algorithm used generate ribbon many detail experimental setting missing,2
611.json,proposed learn embeddings user item using deep neural network recommendation task resulting method minor difference previous neural network also used recommendation task experiment since proposed method dualnets item feature comparison unfair,4
611.json,provides minor improvement deeprs major improvement come coupling user item factor prediction motivation clear improvement architecture minor think author improve discus impact introduction coupling might make stronger specifically conduct isolate experiment change loss architecture gradually coupled network final proposed coupled network demonstrate importance coupling another important missing part seems time complexity since coupled much costly generate recommendation discussion impact real world usage added overall think improved accepted,4
611.json,response review question strong especially question dataset density dataset subsampled responded subsampling common recommender system work including paper cited particularly strong justification subsampling good idea particular answer question look without subsampling think question could easily answered directly especially given goal dealing cold start issue heavily emphasized seems sample data reduce sparsity review question seem answered satisfactorily contribution propose user item embedding method mean learning complex linear interaction user item fairly similar recent work deep though network formulation difference overall reasonably together make contribution important area though still shortcoming addressed namely evaluation unusual recall result reported though usually evaluation seen recommender system research least performance measure rmse reported completeness even strong given contribution fairly simple standard recommender system task shame unusual data sample taken case possible report competing method using exactly data used exactly error measure fairest comparison possible without hard tell much performance improvement really method better versus choice datasets choice loss function,3
597.json,methodologically interesting based methodological contribution vote acceptance however sweeping claim clearly beating existing baseline shown hold local search method solving instance optimality second compared clearly suboptimal method seeing clear dominance local search method find irresponsible left figure line local search referring obviously poor implementation google rather local search method everyone us example nip figure used talk sure anymore think narrative rnns also clearly perform better local search course people figure like purpose clearly avoid misconception right course action upon realizing real strength local search make local search line optimal showing method still worse proper local search chose leave figure still suggesting method better local search probably even think course mislead many superficial reader people outside deep learning must look like sensational obviously wrong claim thus vote rejection despite interesting method update rebuttal change torn hand well written think method interesting promising even like improve future point view clear accept hand using extremely poor baseline making method appear sensationally strong comparison course many iteration reviewer question anonymous feedback come method inferior state fine expected along problem seem want true make statement find greedy approach time efficient percent worse optimality statement true well known community typically quite trivial percent worse optimality hard interesting push last percent side note probably stop found optimal solution like method finding local optimum anytime algorithm even mean find optimal solution millisecond solution percent suboptimal even faster nevertheless since claim toned course many iteration starting feel positive reading section knapsack solving version reviewed least stated simple heuristic yield optimal solution simple heuristic expknap employ brand bound linear programming bound pisinger minknap employ dynamic programming enumerative bound pisinger exact solution also optained quantizing weight high precision performing dynamic programming pseudo polynomial complexity bertsimas demir version went show simple heuristic already optimal like method revision december however paragraph along optimal expknap minknap seems dropped instead introduced poor baseline method random search greedy likely effort find method optimal easy instance personally find pointless present random search nobody like comparing mnist decision stump better surprising greedy interesting however dropping strong simple heuristic expknap minknap entire discussion appears unresponsible since resulting table version suggests method better baseline course column bold number one approach find responsible hide better baseline also least tool solver google tried seems support knapsack directly,5
554.json,present deep eligibility trace combine drqn eligibility trace improved training algorithm evaluated problem single hyper parameter compared topic interesting adding eligibility trace update novel family algorithm explored deep written clearly related literature well covered experiment make promising much stronger investigative experimental crucial contain wider range problem different hyper parameter setting comparison vanilla drqn deepmind implementation well state method,3
574.json,present large scale visual search system finding product image given fashion item exploration interesting nice discussing challenge operating domain proposed approach address several challenge however several concern main concern comparison even mention work done tamara berg group fashion recognition fashion attribute automatic attribute discovery characterization noisy data eccv matching street clothing photo online shop iccv retrieving similar style parse clothing tpami difficult show contribution novelty work without discussing comparing extensive prior enough detail attribute dataset collection process source image clean product image real world image annotation done instruction annotator given annotation collected understand data statistic example proprietary kind qualitative detail important understand contribution others compare work missing baseline table compare simpler method method described text present interesting exploration concern need addressed ready publication,3
574.json,introduces pratical large scale visual search system fashion site us recognize multi label attribute us state faster rcnn extract feature inside region interest technical contribution clear approach used standard state method novelty applying method multi label recognition task available method using binary model changing cross entropy loss function comparison method simple baseline order sequential prediction clear either seems attribute form tree hierarchy used order sequence well written either reported internal dataset release dataset,2
333.json,work proposed simple strong baseline parametric texture synthesis empirical experiment sample generated baseline composed multi scale random filter sometime rival based multi layer trained filter concluded texture synthesis necessarily depend deep hierarchical representation learned feature map work indeed interesting insightful however conclusion needed testified especially deep hierarchical representation firstly generated sample single layer perfect much worse parametric method besides based seems better inpainting task figure last least hierarchical instead lot filter different size handle multi scale efficiently,6
333.json,framework gatys demonstrated correlation statistic empirical gram matrix deep feature response provide excellent characterisation visual texture investigates detail kind deep shallow network work well framework main finding shallow net consisting single filter bank random weight work surprisingly well simple regular texture produce visually superior complex data adapted filter one network like broadly contains interesting informative discussion strength limitation method texture synthesis figure show optimisation image respect shallow filter bank result texture image lower loss optimising objective directly imputed difficulty optimising highly linear cost function reasonable explanation supplementary material show better optimisation obtained initialising based optimisation shallow network optimisation useful complement original experiment main limitation systematically compare different method quantifiable objective trivial define image statistic allow simply generate exact copy reference texture hence good visual quality trivial statistic also shallow instead capture texture distribution measuring well method meet challenge remains open problem hence empirical seem confirm intuition simple statistic good enough texture synthesis term quality diversity compared complex statistic difficult conclusively confirm case indicate diversity could measured term entropy reasonable acknowledge answer question difficult practice furthermore still account aspect problem namely visual quality also suggest perform psychophysical assessment practical addressing problem deem material future work overall since evaluation image generation hard problem think still sufficient strength warrant publication iclr still form psychophysical assessment useful confirm intuition present obtained inspecting figure supplementary material,7
648.json,present semi supervised algorithm regularizing deep convolutional neural network propose adversarial approach image inpainting discriminator learns identify whether inpainted image come data distribution generator time learns recognize object image data distribution experiment show usefulness algorithm feature learned discriminator result comparable better object recognition performance reported state datasets overall proposed idea seems simple effective regularize cnns improve classification performance,5
648.json,proposes method incorporate super resolution inpainting framework semi supervised learning using discriminative feature larger image core idea novel usefulness discriminative feature semi supervised learning already established previous work catgan dcgan salimans however good actually getting semi supervised framework working larger image pascal datasets using proposed context conditioning approach achieves state datasets think provide baseline pascal dataset important compare contribution context conditioning idea standard using semi supervised learning applied version pascal dataset table trouble training vanilla pascal even image size mentioned explained concerned specially almost match baseline seems like hacky improve upon core idea great compare dataset even downsampled pascal dataset,5
648.json,rebuttal thanks reporting alexnet fact great mention interesting understand happens fact fact still disturbing moreover claim look wrong light example suggests gain stem method rather better architecture since discrimination real fake painting closely related target task object classification extracting feature representation suitable filling surprising able exceed performance pathak pascal classification statement possibly part updated think cannot published current form perhaps revision initial review demonstrates application generative adversarial network unsupervised feature learning show representation learned discriminator conditional trained image inpainting performs well image classification side effect fairly convincing inpaintings produced proposed method combine existing idea using discriminator feature learner radford performing unsupervised feature learning image inpainting pathak therefore conceptual novelty limited plus side implement idea well demonstrate state good pascal although pascal experiment incomplete overall borderline mode gladly raise score address concern regarding experiment experimental evaluation pascal quite satisfactory comparison prior work unfair network architecture used different architecture used existing method alexnet great hide fact understand willing simply method alexnet architecture although commenters asked experiment strongly support claim current reasoning thought reasonable current model making difference clear convincing great better architecture lead better also important properly compare prior work related topic doersch also tried using architecture possible compare another question comparing noroozi favaro eccv also like address comment richard zhang qualitative inpainting incomplete comparison previous method instance pathak missing impossible compare different version proposed method different image used different variant realize little space main show many shown supplementary material quantitative missing currently inpainting interesting picture look much could,4
330.json,discus method computing vector representation document using skip gram style learning mechanism added regularizer form global context vector various bit drop none individual component proposed believe combination fashion appreciated detailed analysis behaviour section main downside submission relative weakness empirical front arguably interesting task sentiment analysis classification likewise waste page projection rather space analysis disappointed reduced evaluation agree reviewer concerning soft baseline think accepted interesting algorithm nicely composed efficient reasonable assume reader might idea presented,6
330.json,present framework creating document representation main idea represent document average word embeddings data dependent regularization favor informative rare word forcing common word close experiment sentiment analysis document classification show proposed method lowest error rate compared baseline document embedding method like motivation finding best encode document vector offer significant technical contribution technique main selling point simplicity speed proposed method reason like good task convinced best learn document representation trained minimize classification error trained language final hidden state representation average hidden state widely used method represent document bidirectional lstm concatenate final hidden state document representation think useful know proposed method compare approach task document classification sentiment analysis,5
330.json,proposes learning document embeddings constituent word embeddings jointly learned randomly dropped corrupted training none piece particularly novel result efficient learning algorithm document representation good empirical performance joint training word document embeddings idea idea enforcing document represented word embeddings part joint learning word phrase representation autoencoders lebret collobert furthermore corruption mechanism nothing traditional dropout input layer coupled wordvec style loss training method offer little novelty front hand efficient generation time requiring average word embeddings rather complicated inference step docvec moreover construction embedding capture salient global information document capture specifically information aid local context prediction simple performance sentiment analysis document classification quite encouraging overall despite lack novelty simplicity efficiency performance make worthy wider readership study recommend acceptance,6
756.json,proposes feed forward neural network learn similarity preserving embeddings also proposed idea represent vocabulary word using word given context first considering related work proposed approach brings marginal novelty especially context encoders small improvement wordvec experimental setup provide convincing visualization standard benchmark evaluation word vector,1
756.json,introduces similarity encoder based standard feed forward neural network generating similarity preserving embeddings approach utilized generate simple extension cbow wordvec transforms learned embeddings average context vector experiment performed analogy task named entity recognition offer reasonable intuitive argument feed forward neural network generate good similarity preserving embeddings architecture approach novel tell nothing vanilla neural network trained similarity signal slightly original idea context embeddings augment expressive capacity learned word representation course using explicit contextual information idea especially task like word sense disambiguation efficient parametric estimation multiple embeddings word vector space neelakantan also cited specific method used original know evaluation method convincing corpus used train embeddings smaller ever used practice unsupervised semi supervised embedding learning performance analogy task say little benefit method larger corpus mentioned comment expect gain le significant global context statistic brought conec also picked wordvec training argument made claim extrinsic evaluation important real world application good experiment however embeddings trained small corpus convinced observed benefit persist trained larger corpus overall believe offer little novelty weak experimental evidence supporting claim cannot recommend acceptance,2
486.json,introduces method semi supervised learning graph exploit spectral structure graph convolutional implementation proposed algorithm limited complexity shown scale well large dataset comparison baseline different datasets show clear jump performance proposed method technically fine clear algorithm seems scale well different datasets compare favorably different baseline algorithm simple training seems easy concerning originality proposed algorithm simple adaptation graph convolutional network defferrard semi supervised transductive setting clearly mentioned could better highlight difference novelty reference also comparison family iterative classifier usually compare favorably performance training time regularization based approach although mostly used inductive setting reference family method mention complex filter could learned stacking layer limit architecture hidden layer comment interest using layer graph classification reference iterative classification qing lise getoor link based classification icml gideon mann andrew mccallum generalized expectation criterion semi supervised learning weakly labeled data journal machine learning research david jensen jennifer neville brian gallagher collective inference improves relational classification proceeding tenth sigkdd international conference knowledge discovery data mining joseph pfeiffer jennifer neville paul bennett overcoming relational learning bias accurately predict preference large scale network proceeding international conference world wide international world wide conference steering committee stephane peter ludovic denoyer patrick gallinari iterative annotation multi relational social network advance social network analysis mining asonam international conference ieee,6
412.json,present novel layer wise optimization approach learning piecewise linear nonlinearities proposed approach train piecewise linear cnns layer layer reduces problem latent structured well studied literature addition present improvement bcfw algorithm used inner procedure overall interesting however unfortunately experiment convincing pro best knowledge proposed approach novel provide nice theoretical analysis well written easy follow con although proposed approach applied general structured prediction problem experiment conduct simple multi class classification task make work le compelling test accuracy performance cifar reported look right accuracy best reported existing work often report example,4
412.json,proposes approach optimizing objective cnns proposed method us wise optimization step optimizes parameter layer fixing parameter layer insight large class cnns optimization problem particular formulated optimizing piecewise linear function function optimization happens optimization problem commonly encountered latent structural connection allows borrows idea latent structural literature particular concave convex procedure learn parameter cnns overall well written traditional cnns structural svms almost separate research communties connection cnns latent structural interesting might bridge facilitate transferring idea camp course proposed method also limitation limited layer wise optimization nowadays layer wise optimization essentially coordinate descent algorithm really competitive strategy learning cnns choose layer wise optimization already lose something term optimizing objective since using coordinate descent instead gradient descent course also gain something since guarantee coordinate descent step always improve objective clear loss gain balance focues improving optimization objective however know better objective necessarily correspond good overfitting although backprop standard learning always improve solution objective unlike proposed method might good thing since prevent overfitting goal learning better solution optimization problem first place optimization problem merely proxy learn good generalization ability experiment weak cifar used small dataset today standard cnns typically used large scale datasets imagenet clear whether conclusion still hold applied imagenet compare crippled variant without batch normalization dropout although mention reason want focus optimization mentioned earlier designed purely obtain best solution optimizes objective goal reasonably optimize objective preventing overfitting comparison purely term optimization meaningful first place,5
691.json,present environment called retro learning environment reinforcement learning focus super nintendo claim interface support many others including benchmark given standard algorithm super nintendo game using rivalry metric environment generally standardized evaluation method like public data set competition long history improving quality machine learning research example past year atari learning environment turned standard benchmark comparison algorithm sense could worthy contribution field encouraging challenging domain research said main focus presenting framework showcasing importance challenging domain experiment existing algorithm show reward shaping policy shaping bias toward going right super mario help learning domain knowledge help obvious rivalry training interesting idea training different opponent learner overfits opponent forgets play game oddly get evaluated well game also part describes scientific especially rivalry training le polished disappointing excited hoping significant scientific contribution accompany environment clear necessary publication also clear iclr right venue work contribution mainly code example mloss could better jmlr associated journal track accompanying paper,4
691.json,present valuable collection video game benchmark extendable framework establishes initial baseline reward structure many possible game implemented mean extract score incremental reward structure github repo look like plan rivalry training weaker component probably emphasised le topic vast body uncited multi agent literature well studied problem setup avoid controversy recommend claiming novel contribution topic think really invented method train agent enabling train several opponent benchmarking technique agent evaluation enabling compete rather playing game instead explain established single agent multi agent baseline benchmark suite definition function predicts score game given current state selected action incorrect read something like estimate cumulative discounted reward obtained state starting action following certain policy minor inside target network different parameter theta reference missing year reference point corresponding published paper instead arxiv version,6
687.json,forward sincere effort investigate fundamental nature learning representation neural network topic great interest importance field propose simplistic pruning algorithm essentially monitor performance decay function unit pruning interesting idea could potentially instructive though total think achieved first find introduction pruning lengthy particularly novel surprising example necessary preamble section pruning algorithm sensible though overly simplistic approach course matter effective addressing question however looking contribution make interesting pithy novel take pruning opinion second relevant overall rating section deeper scratching surface figure offer much beyond expected decay performance percentage neuron removed gain value experiment particularly deep covering problem mnist convince draw lesson broader story neural network generally third essential algorithmic architectural mathematical insight expect heavily experimental paper,2
687.json,enjoy reading introduction background particular reminding reader popular paper late early idea proposal straight forward remove neuron based estimated change loss function packpropagation estimate either first second order backpropagation expected first order method worse second order method turn worse brute force method however many reason think work appropriate iclr much stronger comprehension weight decay algorithm relation bayesian prior mentioned think work regime require least comment furthermore many statement text necessarily true particular light deep network modern regularization method example state accurate method call brute force however assumes effect neuron independent might case serial order removal necessarily best also still think unnecessarily long idea could delivered much compressed also think writing section enough point included,2
444.json,edit revision made thorough address many concern also easier understand recommend latest version acceptance increased score present interpreting lstm model notable opaqueness particular propose decomposing lstm prediction task importance score word used generate pattern used find answer simple matching algorithm wikimovies dataset extracted pattern matching method achieves accuracy competitive normal lstm show power proposed approach really like motivation interpreting lstms definitely still work progress high performance pattern matching surprising however several detail pattern extraction process clear evaluation conducted specific task prediction made every word recommend current form weak accept hope clarify approach believe proposed method potentially useful researcher comment please introduce detail specific task applying model section clear point answer entity within document softmax predicting value word answer vector mean transforming hidden state dimensional vector binary prediction performance pattern matching change different cutoff constant value question whose answer entity could proposed approach used prediction made every word extension sentence level sentiment classification,6
444.json,work proposes pattern extraction method understand trained lstm learnt allow implementation hand coded algorithm performs similarly lstm good shown dataset architecture unclear well approach generalize however seems useful understand debug model question wikimovies seem generated template pattern matching approach likely work well however experiment clear extend type task answer free form text substring document required produce continuous span original document approach also seems deficiency handle word type number entity name encoded embedding word description algorithm seems approach requires entity detector mean approach unable determine reached entity decomposition output lstm manual pattern matching explicit year annotation used seem show automatic method unable deal word type also good attention baseline addition gradient based baseline minor comment seem undefined reference seem section instead table similarly section shown section paragraph section adam adam,6
556.json,first like thank putting much work necessary somewhat tedious topic think somewhat standard conference detailed comment definitely love version published issue ironed also agree many point raised reviewer repeat major point previous section minimum deep network loss function part decent said previous section theory show minimum strong assumption practical proof minimum vary quality implies probably need take many precaution avoid minimum practice minimum decent task finding decent minimum quickly reduced task finding minimum quickly first reviewer pointed never guaranteed practice actually reach local minimum could always region objective function algorithm make essentially progress final error level practice actually depend significantly many factor optimization algorithm learning rate schedule initialization weight presence unsupervised pretraining whether neuron added eliminated training therefore task optimizing neural network reduced finding minimum quickly figure like figure suggests diagnosed exactly transition phase happened think also concept fast decaying error followed slow decaying error simple enough reader understand without dedicated graph minor point presentation brace positioned lower figure blue brace brace join horizontally please careful misuse transient phase minimization phase concept section talk transient minimization phase optimization however diagnosing algorithm reach minimization phase seem think minimization phase simply part optimization process error decrease slowly afaik case minimization phase optimization algorithm enters vicinity local minimum approximated second order taylor expansion even occur verify example learning rate small enough change algorithm training point seem arbitrary minimization phase reached epoch decided stop training dataset experiment dataset cifar please replicate least dataset many figure unclear figure following information relevant network used dataset used learning rate used batch norm whether figure show train test validation error easy reader ascertain information figure beginning section algorithm find different minimum significant finding however obvious update taken algorithm vary wildly keep mind exponentially large number minimum probability different algorithm choosing minimum essentially zero sheer number true even shift learning rate slightly different random seed minibatch generation lack confidence interval value figure limited unclear plot change random seed changed information single weight initialization single minibatch sequence figure used infer confidence interval around plot figure might look like think confidence interval still shown least subset configuration presented lack information regarding learning rate question mark left open regarding change different learning rate used even tell chose learning rate interval gave section lack information regarding absolute distance interpolated point figure interpolate three trained weight configuration however interpolated point apart highly significant point close together hump mean point brittle apart hump minor point lstm fixed network architecture like layer type lstm equivalent also multiple version specify used font size legend upper triangle table small write best viewed zoom table caption pretend somehow fix problem personally prefer legend unreadable legend,3
556.json,dedicated better understanding optimization landscape deep learning particular explored different optimization algorithm thus also characterizes behavior algorithm heavily us approach goodfellow find hard understand contribution example surprising different algorithm reach different solution starting initialization useful build basic intuition also receive clear answer question posed reviewer regarding clarifying finding contribute future work optimization deep learning find fundamentally missing example probably plenty way modify approach goodfellow similar work come interesting visualization method deep learning question helpful term designing better algorithm gaining intuition optimization surface look like general interesting though fairly confident better journal conference interesting instructive even sanity check plot eigenspectra solution recovered algorithm order critical point recovered,3
556.json,provides extensive analysis error loss function different optimization method presentation well done informative experimental procedure clarified sufficiently well theoretical evaluation like crucial wide range application help better understand improve convergence behavior given system pro important analysis good visualization con describes mostly observation optimum vary different method however attempt explain happens solve aside batch norm font small,5
612.json,proposes method future frame prediction based transformation previous frame rather direct pixel prediction many previous work proposed similar method response state previous work deterministic proposed also handle multimodality asked could test method using frame input predicting transformation output able quantify importance using transformation input output since first work us transformation input also dismissed suggestion saying frame input output future frame produce blurry misunderstanding suggestion currently seem valid novel contribution work compared previous work,4
612.json,describes approach predict unseen future frame video given known past frame approach based contrast related paper work space affine transformation instead pixel flow said another network take input affine transforms describe motion patch past frame likewise output affine transforms predict future patch motion make simplifying hypothesis namely sequence frame modeled accurately enough patch affine framework unreasonable paper optical flow community based similar hypothesis flow smoothly varying affine field instance locally affine sparse dense matching motion occlusion estimation leordeanu epicflow edge preserving interpolation correspondence optical flow revaud optical flow semantic segmentation localized layer sevilla lara method state give hint validity kind approach addition also seems reasonable reformulate prediction task predicting motion rather predicting pixel indeed patch affine motion space considerably smaller image space making problem much tractable amenable high resolution video agree point also find suffer important flaw specifically choice comparing previous approach term pixel prediction error seems convenient least clear evaluation metric imperfect reason completely dismiss quantitative comparison previous work frame output network moving digit datasets figure look definitely compared paper chose suspicious newly proposed metric pose several problem first action classification evaluated state approach task second metric actually evaluate network claimed next frame prediction instead evaluates another network never trained distinguish real synthetic frame accurately classify action predicted frame find proxy metric weakly related supposed measured adition really make sense train network something else final task evaluated affine motion patch estimated explained problem solved globally treating patch independently pretty vague manner estimating motion patch akin solving optical flow still active subject research therefore important flaw lie potentially erroneous etimation motion input network video made available clear motion wrongly estimated sometimes since entire approach depends input find important discus aspect motion estimation failure impact network also patch affine hypothesis hold patch large enough cover several object contradictory motion appears case video even ignoring weird proxy evaluation part network still trained network trained minimize difference noisy ground truth output affine transforms instead minimizing loss actual output space frame pixel exact ground truth available true loss pixel lead blurry type loss exist instance gradient loss introduced mathieu shown solve issue noted minimizing loss transformation space affine parameter harder intepret introduces unexpected artifact motion often largely underestimated obvious figure hard tell difference input output frame proposed approach sufficiently compared previous work particular approach closely related spatio temporal video autoencoder differentiable memory taraucean iclr also output prediction motion space experimental compare comparison optical flow unfair first approach brox year second really fair assume constant flow frame least basic extrapolation could done take account flow pair input frame last overall approach compared challenging baseline disagree answer gave reviewer question denote ground truth frame predicted frame asked video,2
632.json,contribution summarized transgaussian similar idea transe model subject object embeddings parameterization gaussian distribution naturally adapted path query like formulation along entity relation representation trained transgaussian lstm attention built natural language question aiming learning distribution normalized though relation question answering experiment generated worldcup dataset focusing path query conjunctive query overall think gaussian parameterization exhibit nice property could suitable completion question answering however detail main experimental convincing enough writing also need improved comment major comment main concern evaluation strong either knowledge base completion based question answering many existing competitive benchmark webquestions experimenting tiny wordcup dataset convincing moreover question generated template question even sure need apply lstm scenario much stronger demonstrate effectiveness benchmark conjunctive query current assumes detected entity question could aligned relation take conjunction assumption might always correct necessary justify real datasets named gaussian attention kind think closely related well known attention mechanism related embedding literature minor comment find figure confusing first orange block denote relation second denote every single word question maybe make clearer besides entity recognition usually still need entity linker component link text mention entity,4
632.json,present extension previous work using embeddings modeling knowledge base performing centered around multivariate gaussian likelihood instead inner product score attention supposed allow control attention dealing spread dense centered around quite complicated supplementary material make might clearer make separate paper completion another like idea controlling spread attention make sense however feel convincing enough justify compared usual inner product several reason ablation experiment separate different piece study influence separately interesting point sense table appendix need particular canonical experiment comparing gaussian interaction inner product useful experiment existing benchmark completion help agree difficult find perfect benchmark good idea propose worldcup come addition experiment existing data table appendix page compare transe transgaussian task link prediction wordnet seen fixing point simple setting existing benchmark unfortunately transgaussian perform well compared simpler transe along poor transgaussian single table indicate training transgaussian seems pretty complex hence question actual validity architecture,3
438.json,like demonstration including learning auxiliary task interfere task even help also surprising deep network deep structure allows learn first good representation world base solution specific goal even early representation course depend task performance clear common first stage sensory representation like need edge detection thus training additional task least increase effective training size course unclear adjust make fair comparison could included insight change representation without auxiliary training still strongly disagree implied definition supervised even self supervised learning definition unsupervised learning without external label matter come human example expensive machine used train network task solved later without expensive machine call self supervised method label predicted used bootstrap parameter learning case using externally supplied label clearly supervised learning task,4
438.json,relatively novel work proposes augment current model adding self supervised task encouraging better internal representation proposed task depth prediction loop closure detection task assume environment well position information prior well suited large variety task pertaining navigation robotics extensive experiment suggest incorporating auxiliary task increase performance large extent learning speed additional analysis value function internal representation suggest structure discovered without auxiliary task specific environment task work provides additional proof using input data addition sparse external reward signal help boost learning speed well learning better internal representation original clearly presented strongly supported empirical evidence small downside experimental method maybe shown picking run hard judge whether better suited particular hyperparameter range chosen simply robust hyperparameter setting maybe analysis performance function hyperparameters help confirm superiority approach baseline suspicion adding auxiliary task make robust hyperparameters another downside dismiss navigation literature sympathize limit number thing experimental comparison literature proven insightful measuring quality learned representation,6
678.json,first like apologize delay reviewing summary work explores several experiment transfer training specific reading comprehension reader artificial well populated dataset order perform another target dataset understand several experiment transfer learning sure trained artificial dataset tested small target datasets section trained artificial dataset like fine tuned example target dataset tested remaining target example several model trained using different set fine tuning example tested performance randomly intialized fine tuned model section trained artificial dataset like made embedding component encoder component alternatively component reset random initialization test importance training component fine tuned example target dataset tested remaining target example section think make thing difficult follow fact test composed several task sometimes reported mean performance across task sometimes performance task sometimes mean performance several model report standard deviation also could better explain mean best validation interesting unpretentious work clarity presentation could improved maybe simplifying experimental setup interesting conclusion think reported section nuanced difference datasets exposed minor unexplained acronym benfits subsubset,5
678.json,proposes study transfer learning context story system presented short story answer question study system trained answer question dataset eventually used answer question another dataset mostly negative transfer seems almost existant centered around presenting negative indeed main hypothesis transferring datasets attention reader turn impossible need small portion labeled data target dataset meaningful performance negative could fine bringing value sharp analysis failure mode reason behind might indicate research direction follow however much answer review question actually start give insight typing seems transferred instance impact syntax different babi gutenberg book news article word entity ngrams distribution overlap datasets unfortunately much take away,3
678.json,work investigates performance transfer learning resource rich setup booktest daily mail corpus resource babi squad benchmark setting experiment show poor improvement shot learning however exposed training instance improvement observed claim made require comprehensive analysis criticize babi resource real world scenario babi designed unit test representing many natural language phenomenon thus claim related babi weak evidence questioning transfer learning high resource resource real world scenario highly recommend using recently proposed real world scenario importantly work explain improvement using transfer learning remotely address hypothesizing knowledge transfer encoded embeddings also considering related work claim bring marginal novelty still central work,2
697.json,considers grassmannian optimize skip gram negative sampling sgns objective learning better word embeddings clear proposed optimization approach advantage existing vanilla based approach neither approach come theoretical guarantee empirical comparison show marginal improvement furthermore idea projector splitting algorithm applied numerous occasion machine learning problem reference vandereycken matrix completion sepulchre matrix factorization computational cost approach carefully discussed instance expensive always perform efficient rank update therefore rank update requires operation computational cost iteration proposed approach,3
697.json,present principled optimization method sgns wordvec proposed method elegant theoretical perspective sure tangible benefit approach example using riemannian optimization allow converge faster alternative evaluation show dramatic advantage sgns difference word similarity benchmark within range hyperparameter effect improving distributional similarity lesson learned word embeddings levy theoretical connection riemannian optimization nice though might useful understanding related method future,5
398.json,answer question whether chaotic behaviour necessary ingredient rnns perform well task question sake propose architecture designed chaos subsequent experiment validate claim chaos necessary refreshing instead proposing another incremental improvement start clear hypothesis test might base future design principle rnns downside experiment conducted task known demanding dynamical system perspective nice traversed data set find data chaos actually necessary,7
398.json,think provide interesting direction understanding maybe constructing recurrent model easier interpret clear direction lead think could interesting starting point future work worth exploring,6
767.json,present method adaptively setting step size treating learning rate action whose reward change loss function method presented popular adaptive first order method training deep network adagrad adam rmsprop interesting difficult ass true apple apple manner specific comment computational overhead actor critic algorithm relative algorithm plot wall time optimization presented even though success method like adagrad wall time performance number iteration single learning rate learned accurately compare popular first order method train separate parameter similar popular first order method adaptively change learning rate parameter since learning stationary process algorithm assume stationary environment expect algorithm work learning learning rate figure proposed method compare something like early stopping actor critic method overfitting le simply worse optimization,4
767.json,proposes using actor critic algorithm training learning rate controller supervised learning proposed method outperforms standard optimizers like adam rmsprop experiment conducted mnist cifar main concern lack comparison similar recently proposed method learning step size controller robust neural network training daniel learning learn gradient descent gradient descent andrychowicz work daniel quite similar also proposes using policy search method rep clear downside approach work prior knowledge stated thing second concern experiment number reported method surprisingly example rmsprop table table suggest method tuned properly reinforces need comparison standard architecture previously reported example baseline used better architecture like resnet simplicty network network list,3
767.json,question response mention compare work learning learn gradient descent gradient descent goal current work work quite different work form optimization algorithm case bayesian hyper parameter optimization method multiple hyper parameter work tune hyper parameter network architecture used experiment cifar quite outdated performance much poorer work published last year comparison valid claim advantage method state network architecture claim still hold setting discussed extra cost hyper parameter optimizers justified method could push sota multiple modern datasets summary general idea actor critic network meta learner interesting idea particular application proposed seems practical value reported limited hard draw conclusion effectiveness method,2
566.json,proposes perform active learning using pool selection deep learning mini batch using approximation bayesian posterior several term turn approximated maximum likelihood estimation bayesian inference approach active learning various approximation generally theoretical framework interesting difficult follow written poor english sometimes painful read alternative active learning strategy technique need described detail hand proposed approach complex approximation benefit detailed structured presentation another dataset plus datasets concern gray digit usps arguably somewhat similar,5
566.json,quality initiate framework incorporate active learning deep learning framework mainly addressing challenge scalability accompanies training deep neural network however think well polished quite grammatical typing error clarity need major improvement term clarity motivation introduction difficult active learning deep architecture could better explained tied explanation section example motivated need mini batch label query never mention section describe main methodology related work section although appearing systematic thorough little detached main body related work section survey literature help reader locate work relevant literature highlight pro con perspective maybe could shorten explanation related work directly related spending time discussing comparing work related current work graf originality significance proposed active learning training framework idea treat network parameter optimization problem bayesian inference problem proposed previously graf formulate active learning problem sampling informative data informativeness defined variational free energy depends fisher information reconcile computational burden computing inverse fisher information matrix proposed technique approximate seems novel think initiate interesting direction adapts deep learning label expensive problem active learning need improved term presentation,5
566.json,introduces mechanism active learning convolutional neural network cnns calling deep seeing seem hidden layer filter active learning criterion greedy selection scheme based variational free energy series approximation sometimes hard read many grammatical error sloppy notation place page line used never introduced overall give accepting score weak grammatical error accepted fixed final version optimally native speaker topic interesting appears succeed goal showing proof concept active learning cnns datasets surprised uncertainty sampling curriculum learning added method break usps particular uncertainty sampling well fact better method mnist apparently horribly usps explanation useful question necessary first sample larger subset subset select using active learning merely done reason computational efficiency actually somehow improve instrumental worse done,5
338.json,investigates cold start problem review spam detection first qualitatively quantitatively analyze cold start problem observe enough prior data user realistic scenario traditional feature fail help identify review spam instead turn rely abundant textual behavioral information existing reviewer augment information user specific propose neural network represent review reviewer learnt word embedding jointly encoded behavioral information experiment make comparison traditional method show effectiveness strength well organized clearly written idea jointly encoding text behavior interesting cold start problem actually urgent problem several online review analysis application knowledge previous work attempted tackle problem meaningful present reasonable analysis proposed also available downstream detection model weakness experiment author window width filter module author window width example width extract unigram feature trigram together detail previous work related work section specifically description help reader understand task clearly also typo corrected making purchase decision making purchase decision devoted explore devoted exploring sufficient behavior sufficient behavior business trip business trip abundant behavior information abundant behavior reviewer provide reviewer provides feature need take much feature need take much historical review historical review utilizing embedding learning utilizing embedding learning experiment prof experiment prove general discussion good accepted,4
553.json,strength nice solid piece work build previous study productive well written clear weakness possibly avoid relatively empty statement example task identify word used similarly across context scoring function specified give high score term whose usage similar across context educational study annotation drawn data similar different general discussion first section sure much done interesting method seemed reminiscent previous method used past year measure similarity albeit statistical twist conceptually vein section however describes interesting valuable piece work useful future study topic retrospect background provided section useful necessary support experiment section short work described useful others working area worthy presentation minor comment word punctuation missing word annotation used ppmi sgns skipgram negative sampling mikolov word vector released hamilton unclear multiple method refers word detected multiple method ccla,3
553.json,propose general framework analyzing similarity difference term meaning representation different context strength framework proposed generalizable applied different application accommodate difference notation context different similarity function different type word annotation well written easy follow weakness concern term experiment evaluation us qualitative evaluation metric make harder evaluate effectiveness even validity proposed method example table compare result hamilton using different embedding vector listing word changed hard tell quantitatively performance ccla issue also applies experiment comparative lexical analysis context word meaningful word practitioner actually care without addressing evaluation issue find difficult claim ccla benefit downstream application,2
251.json,delf mathematical property skip gram explaining reason success analogy task general superiority additive composition model also establishes link skip gram sufficient dimensionality reduction liked focus explaining property skip gram generally found inspiring read much appreciate effort understand assumption affect affected composition operation used perform respect think worthwhile read community main criticism however linguistically rather naive compositionality operation take word return another meaning extremely strange word course composed produce vector away correspond concept space still meaning productivity exist otherwise compositionality linguistic term simply refers process combining linguistic constituent produce higher level construct assume constraint apart vague debatable notion semantic transparency implication composition take place set also wrong ordering matter hugely ugar cane cane sugar well known shortcoming additive composition another important aspect pragmatic factor make human prefer certain phrase single word particular context opposite naturally changing underlying distribution word large corpus instance talking male royalty rather king prince usually implication regard intent speaker perhaps highlighting gender difference mean equation matter divergence modification hold noise data fundamental linguistic process point addressed section completely sure comment nutshell think present composition flawed convinces indeed happens skip gram think interesting contribution part sufficient dimensionality reduction seems little disconnected previous argument stand afraid able fully follow argument grateful clarification response understand well argument skip gram produce word neighbour follow exponential parametrisation categorical distribution unclear whether actually reflects distribution corpus opposed happens pure count based fact skip gram performs well despite reflecting data implement form need make assumption underlying form data fair resulting representation optimised task geometrical regularity important regardless actual pattern data kind denoising going minor comment abstract unusually long could think shortened para starting think misconstrued circularity firth observed occurrence effect correlated similarity judgement judgement cognitive process trying statistical method occurrence effect vector space word representation sense thing modelling underlying linguistic process direct observation pair wise similarity break circularity better model kind judgement human known make think paraphrase better word ynonym given comparing word unique lexical item para starting interesting actually zipfian distribution long tail fairly uniform probably worth pointing analogy relation hold well practice requires ignore first returned neighbour analogy computation usually observed term para starting find intuitive synonym paraphrase anything involving woman subtraction involved analogy computation precisely straightforward composition operation involves implicit negation last tiny general comment usual write mean probability word given context actually context target word make reading little harder perhaps change notation literature claim arora work understand vector composition strong instance work paperno baroni explaining success addition composition method weighted vector paperno baroni whole le part composition affect value distributional semantic vector computational linguistics thank response hope accepted,3
178.json,describes extension word embedding method also provide representation phrase concept correspond word method work fixing identifier group phrase word concept denote concept replace occurrence phrase word identifier training corpus creating tagged corpus appending tagged corpus original corpus training concept phrase word set taken ontology since domain application biomedical related corpus ontology used researcher also report generation test dataset word similarity relatedness real world entity novel general nicely written technique pretty natural though substantial contribution scope contribution limited focused evaluation within biomedical domain discussion generated test resource could useful resource could true interesting contribution small technical problem probably matter mathematical expression implementation technical problem want define calculation good idea thought think natural could defined rather ranking entire vocabulary equation define probability quite easy show even size vocabulary infinite need change explanation take talk probability small correction line concept concept,1
178.json,summary present embedding word phrase concept vector space us ontology concept mapped phrase phrase found text corpus treated atomic symbol using us essentially skip gram method train embeddings word atomic phrase also concept associated proposed work evaluated task concept similarity relatedness using umls yago backing ontology strength question addressed phrase similar semantically close furthermore phrase compositional nature proposes plausible train phrase embeddings trained embeddings shown competitive better identifying similarity concept software released could useful biomedical researcher weakness primary weakness novel essentially tweak skip gram furthermore full presented seem best table mayo datasets choi baseline substantially better similar trend seems dominate table larger umnsrs data proposed best competitive previous simpler model chiu general discussion say us known phrase distant supervision train embeddings however clear supervision understand correctly every occurrence phrase associated concept provides context train word embeddings supervision traditional sense identifying concept text predictive task terminology confusing notation introduced section never used rest beta control compositionality phrase word quite surprising essentially equivalent saying single global constant decides compositional phrase surprising part actual value beta chosen cross validation table wikinyt zero basically argues compositionality experimental setup table need explanation say data label similarity relatedness concept entity however concept phrase mapping really many many phrase word vector used compute similarity seems concept vector table approximate method approximate concept average phrase best performing clear need concept ontology instead could started seed phrase,1
178.json,present method jointly embed word phrase concept based plain text corpus manually constructed ontology concept represented phrase apply method medical domain using umls ontology general domain using yago ontology evaluate approach compare simpler baseline prior work mostly intrinsic similarity relatedness benchmark existing benchmark medical domain mechanical turkers generate general domain concept similarity relatedness dataset also intend release report comparable prior work strength proposed joint embedding straightforward make reasonable sense main value mind reaching configurable middle ground treating phrase atomic unit hand considering approach applied concept composed several representative phrase describes decent volume work including development additional contribution form evaluation dataset several evaluation analysis performed weakness evaluation reported includes intrinsic task mainly similarity relatedness datasets note evaluation known limited power predicting utility embeddings extrinsic task accordingly become recently much common include least extrinsic task part evaluation embedding model similarity relatedness evaluation datasets used presented datasets recording human judgement similarity concept however understand correctly actual judgement made based presenting phrase human annotator therefore considered phrase similarity datasets analyzed medical concept evaluation dataset mini mayosrs extremely small pair larger superset mayosrs little larger pair reported relatively human annotator agreement medical concept evaluation dataset umnsrs reasonable size based concept represented single word represented human annotator mentioned make relevance dataset questionable respect representation phrase general concept note quite extensively fine tune hyperparameters datasets report compare prior work make reported analysis questionable suggest method superb prior work achieved comparable prior work required much manual annotation think argument strong also large manually constructed ontology also manually annotated dataset used prior work come existing clinical record require dedicated annotation general missing useful insight going behind reported number treat relation phrase component word hand concept alternative phrase similar type compositional relation however different nature mind deserves dedicated analysis example around line expect analysis specific relation phrase component word perhaps reason reported behavior dominant phrase another aspect absent could strengthen work investigation effect hyperparameters control tradeoff atomic compositional view phrase concept general discussion mentioned weakness recommend reject submission encourage consider improving evaluation datasets methodology submitting minor comment line context concept line phrase overlap handled line believe dimension also terminology negative sampling matrix confusing us embeddings represent context positive instance well line regarding observed phrase completed clear word trained joint text imply last word phrase considered target word make sense notation equation confusing using instead line pedersen missing reference section line find fine grained similarity scale human annotation line newly introduced term string confusing suggest keep using phrase instead line task exactly used hyper parameter tuning important find even appendix table hard trend instance behaves rather differently either alone interesting development trend respect hyper parameter line missing reference table,1
288.json,analyzes story ending last sentence sentence story corpus built story cloze task mostafazadeh proposes based character word gram classify story ending also show better performance story cloze task proper distinguishing right wrong ending prior work whereas style analysis interesting area show better prior work story cloze task several issue first define style also need restructured instance section actually mix experiment clarified question comment right quite difficult reader follow data used different experiment data discussion refers detail data used necessary order ass claim subtle writing task imposes different style author line many story looking written many different person many story person description post analysis coherence pair story written person judged coherent neutral chosen confirm case perhaps claim justified experiment however understanding experiment compare original right original wrong writer convinced line correct simply stated without justification instance five frequent word chosen frequent word also thesis table puzzling bar legend category character gram tune development frequent feature chose among frequent word need justify choice especially link choice style feature reflecting style understand section design task connects rest perhaps lost training test set refer difficult understand differs previous work reconcile line suggest real understanding text required order solve task approach terminology right wrong ending coming mostafazadeh choice term exactly right wrong ending mean right coherent right morally good took quick look could find exact prompt given turkers think need clarified first paragraph section story cloze task line understandable question comment table original story differ coherent incoherent description corpus seems turker first sentence original story write sentence ending story right provide coherent ending sentence ending story wrong provide incoherent ending find last sentence incoherent story incoherent shoe kathy find great kathy like buying shoe wonder many turkers judged coherence story ending variable judgement criterion used judge story coherent incoherent also turker judge coherence right wrong ending making relative judgement absolute judgement huge implication rating line randomly sample original set mean line virtually sentence quantify table could weight feature line compared ending existing task turkers ending task line made sure pair ending written author true right wrong pair original pair according description line shorter text span text unclear line published,1
288.json,strength promising topic different writing style finishing story could appeal discourse pragmatic area participant weakness suffers convincing thorough discussion writing style implication experiment discourse pragmatic example regarding style could sought answer following question implication starting incoherent story sentence proper noun sign topic shift implication ending story coherently past tense verb clear study deceptive language similar short long answer current study liked complete comparison term cognitive load mental state appears somewhat vague insufficient discussion coordinator line onwards benefit thorough discussion issue role coordinator short story discourse general coordinator differ term genre story coordinator seem make sufficiently clear target reader research language teacher crowd sourcing experiment designer need revision term organization repetition throughout text also abbreviation table clear general discussion revised particularly term theoretical standpoint implication discourse pragmatic response reviewer comment indicate willingness update clarify issue related experimented however liked stronger commitment incorporating implication study discourse pragmatic area,1
699.json,divide keyphrases type absent phrase phrase match contiguous subsequence source document present phrase phrase fully match part text used based generative model discussed copy keyphrase prediction copy mechanism predict already occurred phrase strength extraction phrase absent current document interesting idea significant research interest easily understandable copy current context idea deep recurrent neural network already used keyphrase extraction show good performance also interesting proper motivation justify copy deep recurrent neural network weakness discussion required convergence proposed joint learning process copyrnn reader understand stable point probabilistic metric space obtained otherwise tough repeat evaluation process show current system extract present absent kind keyphrases evaluated baseline contains present type keyphrases direct comparison performance current system state art benchmark system present type phrase important note local phrase keyphrases also important document experiment discus explicitly interesting impact copy based automatic extraction local present type phrase impact document size keyphrase extraction also important point found published reference performs better sufficiently high difference current system inspec hulth abstract dataset reported current system us document training publication held training baseline publication used training baseline additionally topical detail dataset scientific document used training copy also missing affect chance repeating current system capture semantics based model better compare system also capture semantics even strong baseline compare performance current system suggestion improve example given figure seems absent type phrase actually topical phrase example video search video retrieval video indexing relevance ranking define domain domain topic document case interesting helpful evaluating absent type keyphrases identify topical phrase entire corpus using relate document high ranked extracted topical phrase using normalized google distance similar effort already applied several query expansion technique relate document query matching term absent document reference zhiyuan peng yabin zheng maosong clustering find exemplar term keyphrase extraction proceeding conference empirical method natural language processing page zhang wang gong huang keyphrase extraction using deep recurrent neural network twitter proceeding conference empirical method natural language processing,3
699.json,strength novel particularly like ability generate keyphrases present source text weakness need explicit whether evaluated model trained tested data set exposition copy mechanism quite clear convincing general discussion present supervised neural network approach keyphrase generation us encoder decoder architecture first encodes input text us attention mechanism generate keyphrases hidden state also advanced variant decoder attention mechanism condition keyphrase generated previous time step interesting novel think ability generate keyphrases source text particularly appealing main concern evaluation evaluated model trained amount data evaluated test set clear example data section line say supervised baseline evaluated cross validation comment mostly clearly written easy follow however part unclear absent keyphrases think need distinguish usage meaning consistent model frequent word vocabulary section line section line suppose word vocabulary line mean absent word keyphrases speaking wondering many keyphrases fall outside unknown word line also ambiguous think probably clearer model generate word present source text long appear somewhere else corpus vocabulary exposition copy mechanism section mechanism specific locality attention basic however find explanation intuition misleading understand correctly copy mechanism conditioned source text location match keyphrase previous time step maybe higher tendency generate gram seen source text figure argument sophisticated attention probably make copyrnn better overall former particularly better absent keyphrases model perform equally well present keyphrases word embeddings initialized,3
676.json,strength well written clear well designed figure reader easily understand methodology even figure predicting binary code directly clever reduce parameter space error correction code work surprisingly well really surprised bit achieve bleu parameter reducing technique described work orthogonal current existing method weight pruning sequence level knowledge distilling method restricted neural machine translation used task long output vocabulary weakness annoying point relatively large dataset aspec best proposed still bleu point lower softmax even larger dataset like french english million sentence even larger similarly performance language pair maybe mention http arxiv speed decoding speed bleu loss le general discussion describes parameter reducing method large vocabulary softmax applying error corrected code hybrid softmax bleu approach orignal full vocab softmax quick question hidden dimension size model could find experiment setup bit achieve bleu surprisingly good however could increase number bit increase classification power small plenty room bit computation time even change another thing counter intuitive predicting binary code actually predicting rank word interpret embeddings seems semantic relation word rank powerful remembers data,3
318.json,strength proposed model shown lead rather substantial consistent improvement reasonable baseline different task word similarity word analogy serf demonstrate effectiveness model also highlight potential utility incorporating sememe information available knowledge resource improving word representation learning contributes ongoing effort community account polysemy word representation learning build nicely previous work proposes idea improvement could interest community applying attention scheme incorporate form soft word sense disambiguation learning procedure weakness presentation clarity important detail respect proposed model left poorly described detail otherwise generally read fairly well however manuscript need improved accepted evaluation word analogy task seems unfair given semantic relation explicitly encoded sememes point detail general discussion stress importance accounting polysemy learning sense specific representation polysemy taken account calculating sense distribution word particular context learning procedure evaluation task entirely context independent mean ultimately vector word least evaluated instead word sense disambiguation sememe information used improving learning word representation need clarified clear sememe embeddings learned description seems assume existence sememe embeddings important understanding subsequent model model require training sememe embeddings unclear proposed model compare model consider different sens sememes perhaps baseline example sufficiently described emphasis instead soft hard word sense disambiguation stronger inclusion baseline based related work reasonable argument made proposed model particularly useful learning representation frequency word mapping word smaller sememes shared set word unfortunately empirical evidence provided test hypothesis interesting look deeper aspect also seem explain improvement much since word similarity data set contain frequent word pair related point improvement gain seem attributable incorporation sememe information word sense disambiguation learning procedure mentioned earlier evaluation involves context independent word representation even method allows learning sememe sense specific representation aggregated carry evaluation task example illustrating hownet figure entirely clear especially modifier computer say model trained using best parameter exactly determined also unclear optimized randomly chosen target word observation finally motivation setting,3
477.json,tldr compare wide variety approach towards word modelling language modelling show modelling morphology give best modelling pure character precision experiment show biggest benefit towards word modelling gained word typically exhibiting rich morphology noun verb comprehensive experiment justify core claim strength comprehensive overview different approach architecture towards word level modelling numerous experiment designed support core claim best come modelling morpheme introduce novel form word modelling based character gram show outperforms traditional approach wide variety language splitting language examined typology examining effect model various typology welcome introduction linguistics world language modelling analysis perplexity reduction various class word russian czech particularly illuminating showing character level morpheme level model handle rare word much gracefully light could something much language modelling requires understanding semantics much requires knowing various morphosyntactic effect weakness character gram lstm seems little unmotivated character gram well reviewer guess character gram roughly correspond morpheme especially semitic language made report gram opposed addition roughly possible distinct trigram latin lower case alphabet enough almost constitute word embedding table consider observed trigram many distinct observed trigram think meaningfully claim examining effectiveness character level model root pattern morphology dataset unvocalised thus pattern root pattern appreciate finding transcribed arabic hebrew vowel challenging half typology reduplication seems different kind phenomenon three strictly morphological typology indonesian malay also exhibit various word affix used reduplication lexical process sure splitting linguistic typology justified general discussion structured clearly easy read puzzled chose dimensional character embeddings dimensionality embedding greater size vocabulary number character alphabet surely getting anything extra read author response opinion altered little still think strength weakness already discussed hold,3
564.json,describes straightforward extension left right beam search order allow incorporate lexical constraint form word sequence must appear output algorithm shown effective interactive translation domain adaptation although proposed extension simple think make useful contribution formalizing also interesting know cope well unordered constraint associated alignment information seem potential application technique beyond one investigated example improving ability handle compositional construction area still might traditional main weakness experiment somewhat limited interactive simulation show method basically work difficult sense well instance many case constraint incorporated acceptable manner large bleu score increase indirect evidence similarly adaptation compared standard fine tuning baseline relatively inexpensive autodesk corpus despite weakness think decent contribution deserves published detail given common usage pbmt coverage vector potentially misleading term appropriate data structure seems likely coverage table also give indication number constraint source sentence test corpus allow calibration bleu gain,3
335.json,work describes gated attention based recurrent neural network method reading comprehension question answering method employ self matching attention technique counterbalance limited context knowledge gated attention based recurrent neural network processing passage finally pointer network signal question attention based vector predict beginning ending answer experimental squad dataset offer state performance compared several recent approach well written structured explained know mathematics look also good opinion interesting work useful question answering community wondering plan release code approach perspective miss information technology used implementation theano cuda cudnn useful reader appreciate could perform test statistical significance highlight even quality finally know space constraint evaluation including additional dataset validate work,3
270.json,present purpose built neural network architecture textual entailment based three step process encoding attention based matching aggregation variant based treernns based sequential bilstms sequential outperforms published ensemble tree better still clear well motivated impressive everything solidly incremental nonetheless recommend acceptance major issue like discussed response suggest several time system serve baseline future work especially helpful meaningful claim could said task could argue unusually simple elegant think really major selling point architecture symmetric way seem like overkill compute attention across sentence direction separate inference composition aggregation network direction presumably nearly double time really necessary asymmetric task done ablation study present full sequential esim ensemble tree based present tree based minor issue think barker jacobson quote mean quite want mean context making specific settled point direct compositionality formal grammar probably better general claim widely accepted principle compositionality vector difference feature also appeared prior work since give redundant parameter take vector input matrix multiplication exactly equivalent take different matrix parameter learning related reason using feature still make sense worth commenting implement tree structured component major issue speed scalability typo klein manning figure standard tree drawing package like tikz qtree produce much readable parse tree without crossing line suggest using thanks response still solidly support publication work groundbreaking novel place surprising enough bring value conference,3
270.json,proposes stanford natural language inference snli dataset build sentence encoding model decomposable word level alignment parikh proposed improvement include performing decomposable attention output bilstm feeding attention output another bilstm augmenting network parallel tree variant strength approach outperforms several strong model previously proposed task tried large number experiment clearly report one work hyperparameter setting one serf useful empirical study popular problem weakness unfortunately many idea work seem useful beyond scope particular dataset used claim proposed network architecture simpler many previous model worth noting complexity term number parameter fairly high reason help empirical gain extend datasets well term ablation study help well tree variant effect removing inference composition minor issue method used enhance local inference equation seem similar heuristic matching function used natural language inference tree based convolution heuristic matching want cite first sentence section unsupported claim either need citation need stated hypothesis work novel empirical study rigorous part could useful researcher working similar problem given strength changing recommendation score read response,2
266.json,compare different way inducing embeddings task polarity classification focus different type corpus find necessarily largest corpus provides appropriate embeddings particular task effective consider corpus subcorpus higher concentration subjective content found latter type data also referred task specific data moreover compare different embeddings combine information task specific corpus generic corpus combination outperforms embeddings drawn single corpus combination evaluated english also le resourced language catalan strength address important aspect sentiment analysis namely appropriately induce embeddings training supervised classifers polarity classification well structured well written major claim made sufficiently supported experiment weakness outcome experiment predictable method employed simple found hardly idea neither significant lesson reader learns embeddings sentiment analysis main idea focusing task specific data training accurate embeddings already published context named entity recognition joshi addition made incremental nature find experiment inconclusive apparently statistical signficance testing different classifier carried table various classifier configuration produce similar score case statistical signficance testing really give proper indication whether difference meaningful instance table left half reporting wonder whether significant difference wikipedia baseline combination furthermore doubt whether signficant difference different combination either using subj wiki subj multiun subj europarl table improvement focusing subjective subset plausible general however wonder whether real life particular situation resource sparse helpful selection opinionfinder processing step possible language english equivalent tool fine grained datasets functionality could learnt fact experiment catalan information considered prof minor detail line discussion dataset confusing thought task plain polarity classification also refer opinion holder opinion target information relevant experiment carried mentioned line variation explain well motivated first need effective simple appending line subjective information isolated configuration assume employ opinionfinder however explicit mention line definition variable properly match formula equation find equation line similar line unclear precise task carried take opinion holder target consideration response thank much clarifying remark follow explanation regarding incorporation opinion holder target though overall change score since think work lack sufficient novelty thing raised response insufficient submission incremental nature,1
657.json,strength thoroughly written discus approach compared approach aware finding somewhat limited regarding mean value weakness minor orthographical mistake repetive clause general benefit section shortened allow extension section main goal laid clearly enough result ambivalence goal general discussion table column wide figure especially greatly benefit column width easy understand first read major improvement could achieved straightening content,2
483.json,strength first neural network based approach argumentation mining proposed method used pointer network multi task learning outperformed previous method experiment datasets weakness basically application argumentation mining although combination multi task learning task novel novelty enough long publication lack qualitative analysis error analysis also major concern general discussion besides weakness mentioned well motivated although three characteristic described strong motivation directional lstms attention mechanism describe problem solved discus experiment much problem solved figure difficult understand self link link output decoder link decoder lstm input figure equation also figure abbreviation defined equation strange calculate probability component type probability calculated experiment understand joint performed microtext corpus clear whether blstm trained joint task objective study discourse parsing using attention mechanism describe difference study minor issue related related floating able able raised recommendation score reading convincing author response strongly recommend discus improved example well detail feature ablation,2
769.json,proposes method building dialogue agent involved symmetric collaborative task agent need strategically communicate achieve common goal like interested much data driven technique used dialogue management however concerned approach proposes actually specific symmetric collaborative task task represented graph operation finding intersection object people know section introduce symmetric collaborative dialogue setting however dialog studied clark wilkes gibbs explored cognition walker furniture layout task journal artificial research line domain rich slot value semantics however domain based attribute value pair domain could semantics represenation based attribute value pair first order logic section hard follow often refer figure find example helpful example section point dialogue represent anyone went columbia,2
723.json,nice morphological segmentation utilizing word embeddings present system us word embeddings measure local semantic similarity word pair potential morphological relation global information semantic validity potential morphological segment type well written represents nice extension earlier approach semantically driven morphological segmentation present experiment morpho challenge data three language english turkish finnish language exhibit varying degree morphological complexity system trained wikipedia text show proposed morse system delivers clear improvement score english turkish compared well known morfessor system used baseline system fails reach performance morfessor finnish note probably result richness finnish morphology lead data sparsity therefore reduced quality word embeddings improve performance finnish language similar degree morphological complexity could consider word embeddings take account word information example article dblp journal corr caor author kris marek title joint word embedding word morphology journal corr volume year http arxiv timestamp biburl http dblp trier journal corr caor bibsource dblp computer science bibliography http dblp article dblp journal corr bojanowskigjm author piotr bojanowski edouard grave armand joulin tomas mikolov title enriching word vector subword information journal corr volume year http arxiv timestamp biburl http dblp trier journal corr bojanowskigjm bibsource dblp computer science bibliography http dblp critique existing morpho challenge data set example many instance incorrectly segmented word material moreover note segmentation data historically valid example segmentation business busi ness segmentation longer semantically motivated provide data consisting semantically motivated segmentation english word form english wikipedia show morse deliver highly substantial improvement compared morfessor data conclusion think well written present competitive interesting task semantically driven morphological segmentation accompany submission code data definitely value submission,3
723.json,continues line work applying word embeddings problem unsupervised morphological segmentation soricut stn proposed method morse applies local optimization segmentation word based orthographic semantic rule heuristic threshold value associated strength present multiple way evaluate segmentation hypothesis word embeddings useful also type method english turkish data set convincing clearly written organized biliography extensive submission includes software testing english morse three small data set used expriments weakness idea quite incremental based mostly work soricut however main problem concern meaningful comparison prior work analysis method limitation first proposed method provide sensible segmenting compound based section method segment compound using terminology method considers either constituent affix unsuprisingly limitation show especially highly compounding language finnish limitation indicated discussion section introduction experiment seem assume otherwise particular limitation modeling compound make evaluation section quite unfair morfessor especially good segmenting compound ruokolainen morse seems segment accident thus wonder morfessor segment much larger proportion semantically compositional compound fair experiment include equal number compound segmented constituent another problem evaluation concern hyperparameter tuning hyperparameters morse optimized tuning data apparently hyperparameters morfessor recent version morfessor kohonen grnroos single hyperparameter used balance precision recall segmentation given morse outperforms morfessor precision recall many case affect conclusion least mentioned important detail evaluation missing morpheme level evaluation method described referred moreover table seems compare different evaluation set morfessor base inference method seem official morpho challenge evaluation llsm narasimhan us aggregated data morpho challenge probably including development training set morse evaluated morpho challenge development might affect conclusion difference score rather large definitely mentioned software package seem support training testing included english general discussion put quite focus issue segmenting semantically compositional compound problematic way first mentioned proposed method seem provide sensible segmenting compound second finding level lexicalized base form freshman morpheme smallest meaning bearing unit fresh different task different case example former sensible phrase based latter unsupervised segmentation method morfessor typically target latter critizing method different goal confusing finally certainly semantic compositionality compound decision always somewhat arbitrary unfortunately many gold standard including morpho challenge data set tend also inconsistent decision section mention computational efficiency limitation million input word form provide detail bottleneck collecting transformation support set cluster actual optimization problem computation time scale discussion mention benefit morse approach adaptability stemmer ability control precision recall need small number gold standard segmentation tuning true also many morfessor variant creutz lagus kohonen grnroos misleading true morfessor work usually fine completely unsupervised method extension provide least much flexibility morse mathias creutz krista lagus inducing morphological lexicon natural language unannotated text proceeding international interdisciplinary conference adaptive knowledge representation reasoning akrr espoo finland june miscellaneous abstract maybe mention minimally supervised method unsupervised typical extent excluding hyperparameter tuning section mentioned somewhere empty string section mentioned specific variant implementation morfessor applied experiment section doubt increasing size input vocabulary alone improve performance method finnish language morphologically complex never encounter even possible inflection word form data mention derivation compound encourage improving format data set using something similar data set example using separator multiple analysis confusing make impossible format language reference many proper noun abbreviation title written lowercase letter narasimhan missing publication detail,3
723.json,strength find idea using morphological compositionality make decision segmentation quite fruitful motivation quite clear well structured weakness several point still unclear case rule ambiguity treated null example general discussion inference stage seems suboptimal approach limited known word general discussion present semantic aware method morphological segmentation method considers set simple morphological composition rule mostly appearing plus suffix prefix approach seems quite plausible motivation behind clear well argumented method utilizes idea vector difference evaluate semantic confidence score proposed transformational rule previously shown various study morpho syntactic relation captured quite well word analogy vector difference hand also shown case derivational morphology much le regularity inflectional performance substantially drop gladkova search space inference stage although tractable still seems optimized rule matching sky system first need searhc though whole radd probably quite huge possible substitution limited known word exist rule clear rule transformation orthographically semantically completely different treated instance consider suffix hand used verb transforms agentive noun play player hand could also used adjective producing comparative form instance older consider bigger versus digger mentioned quite irregularity derivational morphology suffix might play various role instance might also represent patiental meaning like looker merged single rule cluster exploration similarity threshold measure affect performance presented,3
467.json,present self learning framework learning bilingual word embeddings method us embeddings source target language seed lexicon step mapping learning bilingual lexicon induced learning step repeated using lexicon learning mapping process stop convergence criterion strength seed lexicon directly encoded learning process binary matrix self learning framework solves global optimization problem seed lexicon explicitly involved role establish initial mapping embeddings guarantee convergence initial seed lexicon could quite small correspondence small size seed lexicon appealing mapping language large bilingual lexicon good evaluate framework respect quality word embeddings language least language scarce language resource word embeddings language could differ structure coverage think could simulated basis available data training corresponding word embeddings different subcorpora language,3
543.json,update rebuttal appreciate taking time clarify implementation baseline provide evidence significance improvement report clarification definitely included camera ready version much like idea using visual feature language looking forward seeing help difficult task future work strength thinking chinese japanese korean character visually great idea weakness experimental show incremental improvement baseline choice evaluation make hard verify central argument visual feature improve performance processing rare unseen word detail baseline missing make difficult interpret make hard reproduce work general discussion proposes computer vision technique cnns applied image text improve language processing chinese japanese korean language character might compositional evaluate simple text classification task assigning wikipedia page title category show simple representation character outperforms based representation combination visual representation standard encoding performs better visual alone also present evidence visual feature outperform encoding rare word present intuitive qualitative suggesting learns good semantic embeddings character think idea processing language like chinese japanese visually great motivation make sense however entirely convinced experimental evaluation quite weak hard whether robust simply coincidental prefer rigorous evaluation make publication ready statistically significant indicate author response support accepting ideally prefer different evaluation entirely specific comment section paragraph lookup never explicitly embeddings whether tuned backprop visual embeddings clear baseline implemented baseline tuned task specific visual embeddings even concerning since make performance substantially le comparable entirely understand chose evaluate classifying wikipedia page title seems real argument using visual ability generalize rare unseen character focus task directly evaluating machine translation word agree language conceptualized visually character composition important evaluation highlight weakness standard approach make good case need visual feature table improvement statistically significant might fault found figure difficult understand since main probably want present clearly contribution obvious understand rank axis measure rare word think frequency rarest word furthest left since visual intersects axis left lookup mean visual better ranking rare word model intersect point axis evaluated title trained data author response helpful could summarize information figure supposed show concise fallback fusion show performance different threshold seems edge case threshold might representative technique generally simple traditional experiment unseen character nice idea presented afterthought liked eval direction classifying unseen word maybe translation figure people speak chinese,3
447.json,strength main strength incorporation discourse structure attention allows learn weight given different edus also clear provides good explanation used finally evaluation experiment conducted thoroughly strong state baseline weakness main weakness strongly support main claim discourse structure help text classification even unlabeled variant performs best outperform state provides minimal gain hurt legal bill domain approach particularly full variant seems data greedy real solution provided address beyond simpler unlabeled root variant general discussion general feel like good first shot incorporating discourse structure based classification fully convince style structure significantly boost performance task given also costly build parser domain needed legal bill domain described wish explored least mentioned next step making approach work particular face data sparsity example defining task independent discourse embeddings possible discourse parsing could incorporated main task optimized jointly good work wish pushed little given mixed,2
369.json,describes method improving step translation using deep learning presented chinese spanish translation approach seems largely language independent setting fairly typical step first step translates morphologically underspecified version target language second step us machine learning fill missing morphological category produce final system output inflecting underspecified form using morphological generator main novelty work choice deep classifier second step also propose rescoring step us select best variant overall solid work good empirical classifier model reach high accuracy clearly outperforming baseline svms improvement apparent even final translation quality main problem lack comparison straightforward deep learning baseline specifically structured prediction problem address independent local decision followed rescoring step unless misunderstood approach sequence labeling task rnns well suited bidirectional lstm network trained used standard sequence labeling setting reading author response still think baseline including standard lstm framework independently local label case clarified better response problem using rnns standard justify better compare approach final scoring step entirely clear rescore best sentence feature searching weighted graph single optimal path need explained clearly current impression produce graph look best path generate inflected sentence path nothing else select best variant sure reading addressed response report larger word embeddings lead longer training time also influence final attempt explain adding information source sentence hurt seems counter intuitive number information entirely lost sometimes appreciate thorough discussion final version perhaps couple convincing example contains number typo general level english sufficient presentation minor correction context application context application case faced step case divided step markov markov task based direct translation task based direct translation task provided corpus task provided corpus phrase based system dramatically phrase based approach investigated different feature set feature word source information word source correspondant corresponding class gender classifier class number classifier layer input consists consists extract relevant sigmoid output rather tanh layer produce information word consists empty sentence longer word empty sentence sentence longer classifier trained classifier trained aproximately approximately coverage raise coverage exceeds unless misunderstand descendant order descending order cuadratic quadratic multiple place best best rescoring step improves rescoring step comparable comparable,1
369.json,present method generating morphology focusing gender number using deep learning technique morphologically simplified spanish text proposed approach us classifier reassign gender number token necessary compared approach learning algorithm evaluated machine translation chinese spanish translation direction recently task generating gender number rarely tackled morphology generation method usually target evaluated morphologically rich language like german finnish however calling work presented morphology generation overselling proposed method clearly deal gender number given fact rule handcrafted specific task think method straightforwardly applied complex morphology generation morphologically rich language relatively clear section presenting proposed method work done design method think interesting impact various task however evaluation part work barely understandable many detail done done missing evaluation cannot know proposed method brings improvement state method experiment cannot replicated furthermore analysis obtained provided since half page still available possibility provide information make clear evaluation work lack motivation think deep learning especially improve gender number generation state method word contribution used wisely obvious real contribution detail abstract mean unbalanced language section claim main contribution deep learning deep learning task contribution section claim neural machine translation mentioned neural approximation achieve state recommend remove claim discus since junczys dowmunt last iwslt presented corpus showing outperforms around bleu point section wrote using language pair main contribution using language pair contribution nonetheless think nice machine translation focus improving machine translation english number provided table computed preprocessing remove sentence longer token precise obtain development test set provide experiment currently replicable especially section wrote used moses default parameter default parameter moses depending version provide number version used section mean hardware cost table detail provided regarding obtain value chose value given classifier accuracy precisely data train test classifier data used section understood experiment properly used simplified spanish cannot find text simplify spanish train classifier system section method better classification algorithm say nothing performs compared state method least precise chose classification algorithm comparison furthermore rule impact generally explain high accuracy method implement classification algorithm must provide cite framework used experiment guess trained phrase table simplified spanish must precise chose meteor metric like bleu evaluate must provide explanation choice particularly appreciate bleu evaluation meteor must mention version used meteor largely changed since cited version last paraphrase meteor score statistically significant section future work mentioned simplify morphology present simplification morphology think choosing word misleading typo ensambling cuadratic style plain text citation rewritten like toutanova built toutanova built place caption table table space table caption used template must prepared generally suggest read submission instruction provided website greatly help improve also important information regarding reference must provide paper reference response thank response wrote rule added post processing mean apply compute classification apply computing still wondering impact wrote spanish simplified shown table answer question obtain simplification exactly rule software reader need reproduce approach classification algorithm presented table state need cite furthermore table tell deep learning give best classification tell approach better state approach machine translation need compare approach state morphology generation approach described related work designed machine translation much convincing opinion,1
484.json,considers synergistic combination based speech recognition technique attention based seqseq network combination fold first similarly multitask learning used train joint seqseq cost second novel contribution score seqseq ensembled decoding beam search seqseq rescored main novelty using auxiliary training objective originally proposed also decoding strength identifies several problem stemming flexibility offered attention mechanism show combining seqseq network problem mitigated weakness incremental improvement since model trained output well ensembled however nice simple change offer important performance improvement system general discussion spent explaining well known classical system description core improvement better decoding algorithm start appear description nonstandard maybe either presented standard explanation expanded typically relation deterministic character sequence corresponds blank expanded form also unsure last transformation,2
715.json,strength task simple best squad single evaluation comparison weakness analysis error detailed comment general discussion present method directly querying wikipedia answer open domain question system consist component module query fetch wikipedia article module answer question given fetched wikipedia article document retrieval system traditional system relying term frequency model ngram count answering system us feature representation paragraph consists word embeddings indicator feature determine whether paragraph word occurs question token level feature including soft feature capturing similarity question paragraph token embedding space combined feature representation used input direction lstm encoding question work word embeddings used used train overall classifier independently start span sentence within paragraph answer question system trained using different open domain datasets squad webquestions modifying training data include article fetched engine instead actual correct document passage overall easy follow interesting question system accuracy individually document reader performs well beat best single model squad explains significant drop table mention instead fetched test using best paragraph accuracy reach still significantly squad task presumably error large neural network matching isnt good learning answer using modified training includes fetched article instead case training testing done document understanding task analysis whats going provided training accuracy case done improve fair allude conclusion think still need part provide meaningful insight understand interested treating pure machine comprehension task therefore want rely external source freebase could helped entity typing interesting tying back first question error highly relevant topical sentence mention could entity typing helped also refer quase similar system related work quase also open domain system answer using fetched passage relies instead wikipedia,2
579.json,strength well motivated tackle interesting problem clearly written structured accompanied documented code dataset encouraging weakness limited completely deterministic hand engineered minimization rule relevant literature neglected sound thorough experimental evaluation general discussion tackle practical issue system redundant uninformative inaccurate extraction proposed approach dubbed minoie designed actually minimize extraction removing overly specific portion turning structured annotation various type similarly ollie minie state system clausie test publicly available datasets showing effectively lead concise extraction compared standard approach time retaining accuracy overall work focus interesting perhaps underinvestigated aspect sound principled clearly written sufficiently detailed accompanied supplementary material neat java implementation main concern however entirely static deterministic rule based structure minie even though understand handful manually engineered rule technically best strategy precision approach typically hard scale term language recent trend faruqui kumar falke word think contribution somehow fall short novelty substance proposing pipeline engineered rule mostly inspired system clausie reverb instance really appreciated attempt learn minimization rule instead hard coding furthermore completely ignore recent research thread semantically informed nakashole moro navigli delli bovi traditional extraction augmented link underlying knowledge base sense inventory wikipedia wikidata yago babelnet contribution relevant term related literature fact text fragment constituent explicitly linked knowledge base reduce need minimization rule section example bill right provided line pipeline proper entity linking module recognize automatically phrase mention registered entity regardless shape subconstituents also underlying sense inventory seamlessly incorporate external information collocation multi word expression used section chance rely wordnet wiktionary compile dictionary collocation finally remark experimental evaluation despite claim generality minie choose experiment clausie underlying system likely optimal match interesting improvement minie consistent also system order actually ass flexibility post processing tool among test datasets used section included recent benchmark stanovsky dagan reported also comparison system included textrunner woie kraken reference manaal faruqui shankar kumar multilingual open relation extraction using cross lingual projection naacl tobias falke gabriel stanovsky iryna dagan porting open information extraction system english german emnlp ndapandula nakashole gerhard weikum fabian suchanek patty taxonomy relational pattern semantic type emnlp andrea moro roberto navigli wisenet building wikipedia based semantic network ontologized relation cikm andrea moro roberto navigli integrating syntactic semantic analysis open information extraction paradigm ijcai claudio delli bovi luca telesca roberto navigli large scale information extraction textual definition deep syntactic semantic analysis tacl gabriel stanovsky dagan creating large benchmark open information extraction emnlp,2
71.json,strength state clearly contribution beginning provide system dataset figure help illustrating approach detailed description approach test approach performance datasets compare published work weakness explanation method paragraph detailed mention work repeated corresponding method section committed address issue final version readme file dataset committed readme file general discussion section mention example dbpedia property used feature mean property used subset latter please list response explain detail point strongly believe crucial list feature detail final version clarity replicability section lample lstm might beneficial input word embeddings similarly lample figure source language english since mention translated english response stated correct figure based section seems topical relatedness implies feature domain dependent helpful much domain dependent feature affect performance final version performance mentioned feature mentioned response related work make strong connection florian work emphasize supervised unsupervised difference proposed approach still supervised sense training however generation training data involve human interference,3
71.json,strength impressive resource fully automatic system particularly suitable cross lingual learning across many language good evaluation within outside wikipedia good comparison work employed manual resource weakness clarity improved general discussion present simple effective framework extract name language link english importantly system fully automatic particularly important aiming learn across large number language although trivial able context provide evaluation within outside wikipedia particularly like work context previous work us manual resource good scientific practice glad refrain worry look good clarity improve easy write quite complex process large scale resource generates however well organized many point felt reading long list detail encourage give better structure example happy better problem definition high level motivation beginning example better exposition motivation decision contribution part admire effort already made think done even better important deserves clearer presentation like think provides important resource like presented,3
67.json,strength knowledge lean language independent approach weakness peculiar task setting marginal improvement wemb waste space language always clear general discussion seems quite similar add marginal improvement contains quite redundancy related work uninformative figure figure figure useful description short might better task look somewhat idiosyncratic useful already method give hypernym given word seems presuppose figure first star connected conjunction last start disjunction output dark star three input white star line appears suggest threshold tuned test data wemb poorly explained line part text puzzling make sense section titled combined manually built hierarchy dashed line mean,1
31.json,update author response major concern optimization hyperparameter numerous addressed important considering report folded cross validation explanation benefit method experimentally confirmed difference evaluating fold example quite unconvincing summary present complex neural detecting factuality event mention text combine following complex traditional classifier detecting event mention factuality source source introducing predicate sip bidirectional attention based lstm learns latent representation element different dependency path used input us representation lstm performs output prediction detect specific underspecified case another predict actual factuality class methodological point view combining reasonably familiar method bilstm fairly complex however take text sequence word embeddings input rather hand crafted feature different dependency path combining factuality concept source sip clue usage hand crafted feature somewhat surprising coupled complex deep evaluation seems tainted report folded cross validation report optimized hyperparameters finally convincing considering complexity amount preprocessing required extraction event mention sip clue macro average gain rule based baseline overall performance seems modest best looking micro average proposed outperform simple maxent classifier generally well written fairly easy understand altogether find informative extent current form great read tier conference remark keep mentioning lstm combined properly actually mean properness manifest improper combine model find motivation justification output design rather weak first argument allows later addition cue manually designed feature kind beat learning representation advantage using deep model second argument design tackling training kind hand wavy experimental support claim first motivate usage complex architecture learning latent representation avoiding manual design feature computation define manually designed feature several dependency path lexical feature input notice discrepancy lstms bidirectional also attention already become standard various task thus find detailed description attention based bidirectional lstm unnecessary present baseline section also part generates input thus think calling baseline undermines understandability reported originate fold however contains numerous hyperparameters need optimized number filter filter size cnns optimize value reporting folded cross validation allow fair optimization hypeparameters either optimizing hyperparameters optimizing value test unfair notice value application grammatically underspecification dimension polarity certainty option easily think case clear event negative specified whether absence event certain probable possible language style great degree great degree unusual construct either great extent large degree event cannot describes network detail shown figure shown figure detail,2
31.json,comment author response thank clarifying unclear step framework reference facet still find pipeline particularly interesting contribution state marneffe used additional annotated feature system fair comparison implement system annotated information factbank marneffe feature cited predicate class requires dependency parser vocabulary list roser saur thesis general class event might referring factml event class admit particularly clear work sure could clarify continue find combined properly obscure agree using lstm respectively appropriate valuable seem imply prior work improper combination must proper thank reporting separate lstms path curious combination le effective case experiment kind alternative structure deserve reported introduces deep neural technology task factuality classification defined factbank performance exceeding alternative neural model baseline reimplemented literature strength clear presentation sophisticated factuality classification evaluation show attentional feature bilstm clearly provide benefit alternative pooling strategy also exceeds performance traditional feature based linear given small amount training data factbank kind highly engineered seems appropriate interesting bilstm able provide benefit despite little training data weakness main concern work regard apparent departure evaluation procedure prior literature failure present prior work strong baseline novelty feel work original engineering deep neural net factuality classification task work valuable approach particularly novel proposal step supervised framework line particularly interesting given factbank always described term facet assuming correct interpret step referring facet work cite saur present much earlier weaker system baseline consider qian ialp work compare former work developed timebank portion factbank evaluated held acquaint timebank section present work report held marneffe system also chosen baseline feature implemented present system evaluated pragbank corpus alternative representation factuality proposed prabhakaran emnlp evaluation therefore somewhat lacking comparability prior work also important question left unanswered evaluation effect using gold standard event sip given famed success bilstms little feature engineering somewhat disappointing work attempt consider minimal system employing deep neural net task instance dependency path candidate event plus modifier path inclusion heterogeneous information bilstm interesting feature deserved experimentation order input permuted delimiters used concatenating dependency path instead strange second nsubj chain line sippath rspath cuepath input separate lstm combined attentional feature evaluated together bilstm component might worth reporting whether beneficial component could benefit providing path information word could benefit character level embeddings account morphology impact factuality tense aspect proposed future work lacking specificity seeing many question raised number related task consider applying general discussion class classifying event please state parameter mean properly term clear work consider improper chain form defined anywhere citation repetition nsubj example line seems unusual feature lstm learn worth footnoting classified separately distance surface distance many sip cue perhaps table table good augmented count embedded author event percentage removed necessary fold given small amount training data surely fold useful substantially increase training cost clear benefit come psen increase significant substantial affect overall substantially significance across metric drop precision recall clear sentence trying table corpus size seems report significant figure column except micro seems unsurprising rspath insufficient given task respect input encode information interesting performance sippath alone claim precise understanding marneffe evaluates pragbank factbank minor issue english usage application applicable think mean relevant relative displayed simple source unclear sure mean basline mean pipeline,2
31.json,proposes supervised deep learning event factuality identification empirical show outperforms state system factbank corpus particularly three class main contribution proposal attention based step deep neural event factuality identification using bidirectional long short term memory bilstm convolutional neural network strength structure perfectly well organized empirical show convincing statistically significant performance gain proposed strong baseline weakness detail following weakness novelty relatively unclear detailed error analysis provided feature comparison prior work shallow missing relevant paper several obscure description including typo general discussion impactful state novelty explicitly presenting first neural network based approach event factuality identification case please state crystallize remaining challenge event factuality identification facilitate future research better provides detailed error analysis regarding table dominant source error made best system bilstm impact error basic factor extraction table overall performance factuality identification table analysis presented section like feature ablation study show useful additional feature stronger compare prior work term feature feature explored word unclear whether main advantage proposed system come purely deep learning combination neural network unexplored feature feature comparison missing relevant paper kenton yoav artzi yejin choi luke zettlemoyer event detection factuality assessment expert supervision proceeding conference empirical method natural language processing page sandeep soni tanushree mitra eric gilbert jacob eisenstein modeling factuality judgment social medium text proceeding annual meeting association computational linguistics page understandable example given illustrate underspecified modality underspecified polarity reason first definition underspecified relatively unintuitive compared class probable positive second example helpful understand difficulty detection reported line among seven example corresponds explanation quite limited illustrate difficulty minor comment several obscure description including typo shown explanation feature section somewhat intertwined thus confusing section coherently organized separate paragraph dedicated lexical feature sentence level feature stating feature comprises feature lexical level sentence level introduce corresponding variable beginning moving description embeddings lexical feature line first paragraph presenting last paragraph relevant source identification separate subsection detection title section baseline misleading understandable title basic factor extraction basic feature extraction section extract basic factor feature baseline system event factuality identification presented neural network architecture convincing describes beneficial attention mechanism task table seems show factuality statistic source table informative along table also show factuality statistic author embed table effective highest system performance respect combination source factuality value shown boldface section say auxwords describe syntactic structure sentence whereas section say auxiliary word reflect pragmatic structure sentence claim consort well neither seems adequate summarize useful dependency relation mark task seems another example support effectiveness auxiliary word explanation thin compared auxiliary word ensure line event event line detail detail line section section make specific line cent research cent research cent study search uncountable noun line factbank factbank,1
66.json,strength tackle explored task obvious practical application well written motivated weakness method validation user study several weakness discussion investigates various method generate memorable mnemonic encoding number based major system opposed method rely system encode sequence method proposed work return single sequence instead candidate selected improve memorability since memorability ambiguous criterion optimize explore various syntactic approach short likely sentence final us template sampled form nice structure gram language fill slot template proposed approach well motivated section existing tool place approach context previous work security memorability point showing password based mnemonic phrase offer best world term security random password memorability naive password solid motivation appease reader initially skeptical importance feasibility technique term proposed method baseline gram model unsurprisingly generate encoding table show indeed chunk sentence produce shorter sentence short digit relevant additional characteristic method replacement template seems simple gram number digit trigram reweighing could perform well evaluation weaker rest main concern time memorization setting seems inadequate test framework mnemonic technique meant recall repeated memorization exercise single priming event thus informative setting user reminded number encoding daily period time buffer period test recall also closely resemble real life condition technique used password memorization term difference long term recall recognition interesting explanation former method performed similarly latter sentence performs better could likely word officiate example provided make encoding hard remember easy spot case somewhat defeat purpose approach also useful reader provided appendix example digit encoding user presented study better sense difficulty recall quality encoding suggestion nice provide background major system familiar suspect might many audience included come logic behind digit phoneme map,2
128.json,proposes neural network architecture represent structural linguistic knowledge memory network sequence tagging task particular slot filling natural language understanding unit conversation system substructure node parse tree encoded vector memory slot weighted substructure embeddings time step additional context labeling strength think main contribution simple flatten structured information array vector memory connected tagger additional knowledge idea similar structured syntax based attention attention node treelstm related work includes zhao textual entailment natural language inference eriguchi machine translation proposed substructure encoder similar dcnn node embedded sequence ancestor word architecture look entirely novel kind like simple practical approach compared prior work weakness convinced empirical mostly lack detail baseline comment ranked decreasing importance proposed main part sentence embedding substructure embedding table baseline model treernn dcnn originally used sentence embedding easily take node substructure embedding clear used compute part us rnns chain based knowledge guided difference knowledge guided addition knowledge vector memory input seems completely unnecessary separate weight rnns advantage using increase capacity parameter furthermore hyper parameter size baseline neural network comparable number parameter also think reasonable include baseline input additional knowledge feature head word comment sensitivity parser error comment computing substructure embeddings seems natural compute attention word reason static attention word guess knowledge acting like filter mark important word reasonable include baseline suggest input additional feature since weight word computed inner product sentence embedding substructure embedding embeddings computed mean node phrase similar whole sentence get higher weight leaf node claim generalizes different knowledge think substructure represented sequence word seem straightforward constituent parse knowledge finally hesitating call knowledge misleading usually used refer world external knowledge knowledge base entity whereas really syntax arguably semantics parsing used general discussion proposes practical seems working well dataset main idea novel comment strength think takeaway importantly experiment convincing presented need clarification better judge post rebuttal address main concern whether baseline treernn used compute substructure embeddings independent sentence embedding joint tagger another major concern separate rnns give proposed parameter baseline therefore changing score,1
578.json,strength proposes neural semantic graph parsing based well designed transition system work interesting learning semantic representation dmrs capable resolving semantics scope underspecification work show scheme computational semantics benefiting transition based incremental framework resolve parsing cost weakness major concern give common introduction definition dmrs example even make little confused cannot anything special dmrs description little detailed think however upon space limitation understandable problem exists transition system parsing background hardly learn something seeing good general discussion overall interesting like dmrs semantic parsing much like much hope open source code datasets make line research topic,4
201.json,strength evaluating word bound context either dependency sentence ordering important useful reference community experiment relatively thorough though choice could justification used downstream task instead intrinsic evaluation weakness change objective function gbow somewhat justified dependency based context bound representation word available predicting context unclear exactly case deserves discussion presumably dependency context bound representation also suffer drawback ling unfortunately compare original objective definite weakness addition change match gbow without comparing original objective adding word vector trained using original gbow objective function justify change assuming show large change hyperparameter setting discussed played large role levy consider trying different hyperparameter value depend pretty heavily task simply taking good value another task work well addition unclear exactly trained section simple linear classifier section logistic regression average word vector input call neural word technically previous work also used name find misleading since logistic regression hence linear something call neural important know trained section trained know different conclusion result task changing general discussion evaluates context taken dependency par context taken word position given sentence word token relative position indicator useful community show researcher word vector trained using different decision emphasis improve main takeaway future researcher given really summarized start specifically abstract chunking bound representation outperform word representation dependency context work better linear context case addition simple text classification bound representation perform worse word representation seemed major difference different model context type small point improvement call unbounded context word lead confusion technique generalized word clarified easily distributional hypothesis distributed hypothesis citation comma instead semicolon separating deps capitalized consistently throughout usually appears deps also introduced something like dependency parse tree context deps typo different context affect performance word,3
554.json,strength present bayesian learning approach recurrent neural network language method outperforms standard dropout three task idea using bayesian learning rnns appears novel computationally efficient bayesian algorithm interest community various application weakness primary concern evaluation report performance difference type architecture lstm vanilla character task comparing learning algorithm penn treebank task furthermore rmsprop psgld compared character dropout compared sgld dropout word language task inconsistent suggest reporting dimension architecture exact learning algorithm character word task useful know proposed bayesian learning approach portable across task data set state performance gain mainly come adding gradient noise averaging statement justified empirically arrive conclusion experiment without adding gradient noise averaging need done dropout sentence classification task language caption task since dropout specific sentence classification suggest reporting performance method three task allow reader fully ass utility proposed algorithm relative existing dropout approach sort order sample theta thetak sample higher posterior probability likely higher index report result randomly selecting sample additional alternative regular known expensive train evaluate useful compare training evaluation time proposed bayesian learning algorithm dropout allow reader trade improvement versus increase training time clarification theta refer estimate parameter based sample clarify theta mean context dropout dropconnect typo output rmsprop,3
104.json,strength outperforms align supervised entity linking task suggests proposed framework improves representation text knowledge learned jointly direct comparison closely related approach using similar input data analysis smoothing parameter provides useful analysis since impact popularity persistent issue entity linking weakness comparison align could better align used content window size vector dimension also clear whether includes entity link graph directed consists wikipedia outlinks adjacency defined undirected graph align context entity entity link entity different cannot tell much impact change learned vector could contribute difference score entity similarity task sometimes difficult follow whether mention mean string type particular mention particular document phrase mention embedding used appears embeddings learned mention sens difficult determine impact sense disambiguation order without comparison unsupervised entity linking method general discussion,2
104.json,strength good idea simple neural learning interesting performance altough striking finally large application weakness amount novel content clarity section present neural learning method entity disambiguation linking introduces good idea integrate entity mention sense modeling within smame neural language modeling technique simple training procedure connected modeling allows support large application clear formally discussion always level technical idea empirical evaluation good although striking improvement performance reported although seems extension yamada conll add novel idea releant interest weaker point prose always clear found section clear detail figure explained terminology somehow redundant example refer dictionary mention dictionary entity mention pair different text anchor type annotated text anchor quite close nature yamada least outline difference general observation current version test multiple embedding entity linking disambiguation task however word embeddings used task also process directly depending entity parsing coreference semantic role labeling show word embeddings provided proposed mpme method weaker simpler wordspaces semantic task involving directly entity mention read author response,3
387.json,strength tackle interesting problem provides knowledge novel reasonable learning combining cognitive feature textual feature sentiment analysis irony detection clearly written organized provided useful detail informative example plot convincing good comparing approach previous work weakness reading abstract expected approach significantly outperform previous method using gaze textual feature consistently yield best upon reading actual section however seems like finding mixed think helpful update abstract introduction reflect evaluating dataset sentiment analysis sarcastic utterance included better classifying sarcastic utterance sarcastic one understand movement data useful sarcasm detection obvious helpful sarcastic sentiment classification beyond textual feature general discussion contains interesting content approach seems solid novel little weaker anticipated abstract believe still interesting larger community merit publication,3
779.json,strength well written clear part experimental comparison well done experiment well designed executed idea using zero resource impressive weakness many sentence abstract place stuff much information single sentence could avoided always extra sentence clear could section actual method used could explained detailed explanation glossed trivial guess idea reading section alone test time need source pivot corpus well major disadvantage approach played fact mentioned could strongly encourage mention comment general discussion us knowledge distillation improve zero resource translation technique used similar proposed yoon innovative part zero resource translation compare prominent work field approach also eliminates need double decoding detailed comment line could avoided complicated structure simple sentence line johnson sota english french german english line evidence provided combination multiple language increase complexity please retract statement provide evidence evidence literature seems suggest opposite line line repeated first mentioned previous paragraph line figure,3
759.json,proposes joint salient phrase selection discourse relation prediction spoken meeting experiment using meeting corpus show proposed higher performance based classifier strength written easy read technical detail described fully high performance also shown experimental evaluation also show useful comparison related research field discourse structure analysis phrase identification interesting note performance evaluation phrase selection discourse discourse relation labeling summary generation application also application prediction consistency understanding team member also verified weakness jointly modeling salient phrase extraction discourse relationship labeling speaker turn proposed intuitive explanation interactivity usefulness considering fully presented general discussion based classifier comparative method experiment useful mention validity setting,2
562.json,present method relation extraction based converting task question answering task main hypothesis question generic vehicle carrying content particular example relation easier create seem show good performance though direct comparison standard relation extraction task performed strength technique seems adept identifying relation measure work well unseen question seen relation relatively well unseen relation describe method obtaining large training dataset weakness wish performance also shown standard relation extraction datasets impossible determine type bias data relation generated wikidata wikireading extracted wikipedia regular newswire newsgroups seems nist slot filling dataset good appropriate comparison comparison train relation detection generated data well compare approach general discussion found well written argued idea interesting seems work decently also found interesting zero shot method behaved indistinguishably single question baseline multiple question system,3
237.json,summary present empirical study identify latent dimension sentiment word embeddings strength tackle challenging problem unsupervised sentiment analysis figure particular nice visualisation weakness experiment particular thin recommend also measuring performance expanding number technique compared methodology description need organisation elaboration idea tested itemised insufficiently justified quite weak term reported accuracy depth analysis perhaps work need development particularly validating central assumption distributional hypothesis implies opposite word although semantically similar separated well vector space,0
86.json,present method translating natural language description source code constrained grammar programming language source code liked well written address hard interesting problem taking advantage inherent constraint show significant performance improvement strength address interesting important problem space constraint inherent output space incorporated well good evaluation comparison also showing different aspect impact performance clearly written weakness primary major issue evaluation metric accuracy bleu easy compute think give sufficiently complete picture accuracy easily miss correctly generated code trivial program functionality inconsequential change could accuracy functional correctness bleu sure well evaluates code perform significant change tree transformation without changing functionality understand bleu used seems particularly problematic given token level gram evaluation perhaps bleu applied asts reference code generated code level normalization asts really like evaluation testing functional equivalence reference generated code understandably difficult since test code written reference however even done random reasonably small subsample datasets think give much meaningful picture minor issue page para structural information help information flow within network network mean section action embedding action embedding vector simply vector actually trivial embedding action computed difference vector equation section preprocessing replace quoted string description django dataset case string need copied generated code handled also mentioned supplementary material infrequent word filtered handle case word describe variable name literal need code read author response,3
388.json,describes interesting ambitious work automated conversion universal dependency grammar structure call semantic logical form representation essence construct assigned target construction logical form procedure defined effect conversion working inside using intermediate form ensure proper nesting substructure encapsulating one evaluation carried comparing gold standard lambda structure measuring effectiveness resulting lambda expression actually delivering answer question set impossible describe adequately space provided taken care cover principal part still many missing detail love longer version particularly short changed nice learn type question handled answered correctly information useful gaining better insight limitation logical form representation lead main concern objection logical form representation fact real semantic essentially rather close rewrite dependency structure input good step toward semanticization including insertion lambda operator explicit inclusion dropped argument enhancement operation introduction appropriate type unit construction eventive adjective noun like running horse president many even simple aspect semantic either present least simply wrong missing quantification every number various form reference said negation modal change semantics interesting way inter event relationship subevent relationship event vacation nice traveling pain easily cheat treating item unusual word defining obvious simple lambda formula fact require specific treatment example number requires creation separate object representation canonical variable allowing later text refer bind variable properly another example person event differ person need representation symbol event plus coupling mapping another example able handle time even simply temporally indexing event state none immediately obvious added case show quantifier referential scoping trivial easy point missing thing unfair sense expected cannot allowed make obvious error disturbing assignment event relation strictly parallel verb noun syntactic role claim seriously broke window window broke window filling semantic role break simply correct cannot dismiss problem nebulous subsequent semantic processing really need adequate treatment even mind principal shortcoming work make break point whether fight accepted conference happier simply acknowledged aspect wrong worked future sketch saying perhaps reference framenet semantic filler requirement independent representation notation conversion procedure reasonably clear like fact rather cleaner simpler predecessor based stanford dependency also courage submitting neural work day unbridled giddy enthusiasm anything neural,2
12.json,describes rule based approach time expression extraction insight time expression typically short contain least time token first recognizes time token combination dictionary lookup regular expression match tagging information expands time segment either direction time token reach based heuristic rule finally merges time segment single time expression based another rule evaluation approach rule based based system data set show significant improvement strength well written clearly presented rule motivated empirical observation data seems well justified evidenced evaluation weakness underspecification make difficult reproduce detail general discussion section season thing ramadan month holiday season section benchmark datasets three datasets section example without time token helpful section given approach close ceiling performance since expression contain time token system achieved recall plan improve plan release full rule software used,3
214.json,strength macro discourse structure useful complement micro structure like release dataset helpful range application weakness providing comparison existed cdtb better primary secondary relationship mentioned however difference nuclearity unclear precisely defined experiment method clearly described general discussion,2
193.json,present first parser ucca recently proposed meaning representation parser transition based us transition designed recover challenging discontinuous structure reentrancies experiment demonstrate parser work well easy build representation existing parsing approach well written interesting important problem transition system well motivated seems work well problem also thorough experimental evaluation including varying classifier base parser neural linear also comparing best output could existing le expressive parsing formulation set strong standard ucca parsing also interesting researcher working expressive meaning representation complex transition system open question extent parser subsumes transition based parser could ucca transition scheme used case heuristic alignment necessary learn extra transition terminal reduce existing algorithm perhaps work better answering question crucial strong overall understanding could point interesting area future work read author response agree everything,3
193.json,response answer clarification question review summary describes transition based system ucca graph featuring terminal node reentrancy discontinuity transition already proposed transition aspect swap transition cope discontinuity transition stack allow multiple parent node best performance obtained using transition classifier feature based bidirectional lstms compare obtained performance state parser using conversion scheme bilexical graph tree approximation parser trained converted data used predict graph tree predicted structure converted ucca confronted gold ucca representation strength present quite solid work state transition based technique machine learning parsing technique well written formal experimental aspect described precise demonstrate good knowledge related work parsing technique shallow semantic representation weakness maybe weakness originality lie mainly targeted representation ucca really proposed parser detailed comment clarification question introduction line note discontinuous node could linked projectivity dependency framework maybe rather state difference phrase structure syntax dependency syntax section ucca scheme description alternative node unit corresponds terminal several unit clear mean something else node either terminal terminal terminal node child thus neither terminal several unit note movement action state totally appropriate since process neither movement action agentless transformation ucca guideline three term later state state process dichotomy process action movement relation evolves time line note contrast john mary trip john mary child felicitous relational noun child evokes underlying relation participant child john mary accounted ucca section concerning conversion procedure complete provide precise description conversion procedure supplementary material ease reading describe informally variant constituent dependency conversion procedure manning also interesting motivate priority order used define head edge chosen case several child label made explicit leftmost converted graph figure edge seem inverted direction john moved john gave confused upper bound remote edge bilexical approximation current description conversion allow quick idea kind remote edge cannot handled concerning comparison parser seem completely fair tune proposed parser default setting parser section line please better motivate claim using better input encoding section convinced alledged superiority representation terminal node although considered elegant choose head construction noted formally head label used bilexical dependency recover information,3
107.json,describes cross lingual named entity recognition employ conditional random field maximum entropy markov neural network based method addition propose method combine output method probability based ranking based method select best training instance cross lingual comparable corpus cross lingual projection done using variant mikolov proposal general easy follow well structured english quality also correct combined annotation interesting detailed comment wondering motivation behind proposing continuous word cbow variation give much detail parameter employed original continuous skip gram offering suggest include also cbow reader analyse improvement approach since decay factor surrounding embeddings suggest take look exponential decay used similarly previous comment like look difference original mikolov cross lingual projection frequency weighted projection contribution valuable reader method really superior proposed data selection scheme effective selecting good quality projection labeled data improvement significant conducted test statistical significance like know difference result work significant suggest integrate text section beginning section look cleaner also recommend move evaluation table evaluation section miss related work section introduction includes part information suggest divide introduction section evaluation quite short page conclusion section obtain state appreciate discussion analysis suggested reference iacobacci pilehvar navigli embeddings word sense disambiguation evaluation study proceeding annual meeting association computational linguistics,3
384.json,strength present approach fine grained extraction learning modifier interpretation motivation easy understand interesting task addition approach seems solid general experimental show approach increase number fine grained class populated weakness part hard follow unclear multiplied weight explained product often observed property weight property class addition also seems unclear effective introducing compositional model increasing coverage think major factor increase coverage modifier expansion seems also applicable baseline hearst interesting score hearst modifier expansion general discussion overall task interesting approach generally solid however since weakness described ambivalent minor comment confused notation example unclear stand seems sometimes represents class sometimes represents noun phrase understanding correct paragraph precision recall analysis area curve instead area precision recall curve despite paragraph title precision recall analysis reading response thank response fully satisfied response modifier expansion think modifier expansion applied hearst proposed method however wondering whether take account similar modifier improve coverage hearst actually since seems still unclear effective introducing compositional model keep recommendation,2
239.json,proposes framework evaluation word embeddings based data efficiency simple supervised task main motivation word embeddings generally used transfer learning setting evaluation done based faster train target approach us simple task evaluated supervised fashion including common benchmark word similarity word analogy experiment broad embeddings show rank tend task specific change according amount training data used strength transfer learning data efficiency motivation interesting directly relates idea using embeddings simple semi supervised approach weakness good evaluation approach propagates task specifically approach give rank embeddings like follow rank task like text classification parsing machine translation however approach assessed difficult trust technique actually useful traditionally done discussion injective embeddings seems completely topic seem understanding experimental section confusing section point analysis answer question worth fitting syntax specific embeddings even supervised datset large fail understand evaluation conclusion made still section manuscript say hint purely unsupervised large scale pretraining might suitable application bold assumption fail understand concluded proposed evaluation approach embeddings obtained shelf pretrained one control corpus trained limit validity evaluation shown manuscript need proofreading especially term citing figure right place figure page cited page general discussion think start interesting motivation properly evaluate approach good mentioned intrinsic evaluation approach expect study conclusion propagate task done lack clarity proofreading manuscript also hinders understanding future think vastly benefit extrinsic study controlled experimental setting using corpus train embeddings instance current state think good addition conference,1
444.json,present evaluation metric lyric generation exploring need lyric original similar style artist whilst fluent herent well written motivation metric well explained describe hand annotated metric fluency herence match automatic metric similarity whilst metric similarity unique interesting give evidence effective automatic metric correlation metric others used separately claim used meaningfully analyse system performance take word correlation hand annotated performance metric getting worse score baseline system evidence metric capture quality could strong baseline missing reference recent work looking automating herence using mutual information density addition reference style matching community missing dethlefs style matching work pennebaker,2
444.json,proposes present comprehensive evaluation methodology assessment automatically generated lyric similar target artist assessment generation creative work challenging great interest community effort fall short claim comprehensive solution problem assessment nature ultimately fall subjective measure generated sample convince expert generated sample produced true artist rather automated preocess essentially specific version turing test effort automate part evaluation optimization understand human ass artistic similarity valuable however specific finding reported work encourage belief reliably identified specifically consider central question sample generated target artist human annotator asked able consistently respond question mean either annotator sufficient expertise perform task task challenging combination proposed automatic measure also failed show reliable agreement human raters performing task dramatically limit efficacy providing proxy human assessment interannotator agreement expected task subjective idea decomposing evaluation fluency coherence component meant make tractable thereby improve consistency rater score evaluation metric cause concern limit viability general purpose tool specific question comment line line level evaluation prefered verse level analysis specifically coherence line line analysis limit scope coherence consequtive line style matching term assumes artist distinct style always operate style argue artist kanye west drake tupac notorious produced work multiple style accurate term might artist matching section central automated component evaluation existing verse similar rhyme density given limitation rhyme density well work even manual intervention described section description include many judge used study many case judge already know verse judging case test ass easy match style rather judge recall knowledge,1
501.json,strength generate dataset rephrased caption planning make dataset publicly available approached task advantage caption generation term metric easier straightforward evaluate problem choosing best caption accuracy metric instance caption generation requires metric like blue meteor limited handling semantic similarity propose interesting approach rephrasing selecting decoy draw decoy form image caption dataset decoy single image come caption image decoy however similar term surface bleu score semantics similarity lambda factor decide balance component similarity score think interesting employ paraphrasing support motivation task evaluation show system trained focus differentiating similar caption performs better system trained generate caption however showing system tuned particular task performs better task weakness clear image caption task suitable comprehension task author system better order argue system comprehend image sentence semantics better apply learned representation embeddings apply representation learned different system task comparison main worry essentially converge using existing caption generation technique bahdanau chen formula presented confusing formula seems decoy true caption employed loss term however make sense mention decoy second term hurt mode performance learn generate decoy well written text ambiguous make clear either formula text otherwise make sense learn generate true caption learning distinguish true caption decoy general discussion formulate task dual machine comprehension accomplish task challenging computer system solve problem choosing similar caption given image argue system able solve problem understand image caption beyond keywords also capture semantics caption alignment image semantics think need make focus chosen approach better caption generation opinion caption generation le challenging learning image text representation alignment formula wonder future possible make learn generate decoy adjusting second loss term include decoy negative sign something similar,2
33.json,strength proposes nice combine neural lstm linguistic knowledge sentiment lexicon negation intensity method simple effective achieves state performance movie review dataset competitive best model dataset weakness similar idea also used teng though work elegant framework design mathematical representation experimental comparison teng convincing comparison rest method reported implementation sentence level experiment report phrase level detail well explained discussion general discussion reviewer following question suggestion work since dataset phrase level annotation better show statistic time negation intensity word actually take effect example many time word nothing appears many time change polarity context section lstm used regularizers lstm used predict sentiment label claimed sentence level annotation since goal avoid expensive phrase level annotation however reviewer still suggest please report rebuttal phase possible parameter optimized could also fixed prior knowledge reviewer find specific definition experiment section learned fixed learned fixed value section suggested conduct additional experiment part dataset phrase negation intensity word included report dataset without corresponding regularizer convincing,3
331.json,strength detailed guideline explicit illustration weakness document independent crowdsourcing annotation unreliable general discussion work creates benchmark corpus concept based well organized written clearly supplement material sufficient question necessary treat concept extraction separate task hand many generic summarization system build similar knowledge graph generate summary accordingly hand increase node number concept becomes growing hard distinguish thus general summary readable determine importance concept independent document definition summarization reserve main concept document therefore importance concept highly depends document example given topic coal mining accident assume concept instance coal mining accident cause coal mining accident document describes series coal mining accident important comparison document explores coal mining accident happen significant therefore given topic concept impossible judge relative importance appreciate great effort spent build dataset however dataset like knowledge graph based common sense rather summary,2
433.json,describes deep learning based parsing creole singaporean english universal dependency implement parser based dozat manning neural stacking chen train english hidden representation english input singlish parser allows make much larger english training along small singlish treebank annotate show approach work better using english parser training parser small singlish data also analyze common construction approach improves parsing quality also describe evaluate stacked based chen discus common construction analyzed framework provide annotated treebank sentence annotated people inter annotator agreement strength obtain good experimental setup appears solid perform many careful analysis explore influence many parameter provide small singlish treebank annotated according universal dependency guideline propose sound guideline analyze common singlish construction method linguistically informed nicely exploit similarity standard english creole singaporean english present method resource language applying existing english method another language instead present method potentially used closely related language pair well motivated method selecting sentence include treebank well written easy read weakness annotation quality seems rather poor performed double annotation sentence inter annotator agreement term make hard ass reliable estimate fact slightly higher inter annotator agreement update rebuttal convincingly argued second annotator annotated example compute follow annotation guideline several common construction second annotator fixed issue reasonable longer consider real issue general discussion concerned apparently rather poor annotation quality data might influence overall liked think good contribution conference question annotated sentence mention sentence annotated compute inter annotator agreement mention annotated sentence inter annotator agreement case disagreement subsequently discus sentence disagreement table seem discourse relation almost many dobj relation treebank artifact colloquial language discourse thing considered discourse language table discourse particle discourse imported vocab latter perhaps separate table gloss helpful level comment interesting compared approach martinez http arxiv perhaps mention reference section word grammar slightly strange think replacing grammar syntactic construction make clearer convey line line think regarded variant extraposition agree analysis figure perhaps sentence line think dozat manning longer state perhaps replace high performing something like helpful provided gloss figure,3
18.json,strength well motivated approach clear description solid weakness nothing substantial comment general discussion describes method called attention attention reading comprehension first layer network compute vector query word document word resulting matrix query document since answer document word attention mechanism used assigning weight word depending interaction query word work deepen traditional attention mechanism computing weight query word separate attention using weight main attention document word evaluation properly conducted benchmark datasets various insight presented analysis well comparison prior work think solid piece work important problem method well motivated clearly described researcher easily reproduce apply technique similar task remark equation assuming iterating training referring previous equation please clarify avoid confusion wondering whether explored discussed initializing word embeddings existing vector google news glove reason believe general purpose word semantics useful task clear referring letting explicitly learn weight individual attention referring architecture specifically output indirectly affecting much attention applied query document word clarifying useful also think improvement validation rather table think weight local relatively higher task benefit adding le since included table think nice provide insight reader liked software released part submission typo right column effective expected effective expected typo right column appear much frequent appears frequently typo left column hard hard hard made hard make,4
619.json,present corpus annotated essay revision includes example application corpus student revision behavior analysis automatic revision identification latter essentially text classification task using classifier variety feature state corpus freely available research purpose well written clear detailed annotation scheme used annotator annotate corpus added value believe resource might interesting researcher working writing process research related topic also liked provided clear usage scenario corpus major criticism first could easily corrected case accepted second requires work statistic corpus absolutely paramount describe corpus information talking number document assume corpus document essay draft correct number token around word essay number sentence assume talking unique essay word word total correct take draft word probably substantial overlap draft table information included aforementioned figure correct talking small corpus understand difficulty producing hand annotated data think strength work sure helpful resource community whole perhaps resource better presented specialised workshop specialised conference language resource like lrec instead general conference like mentioned last paragraph like augment corpus annotation also willing include essay comment minor essay native native speaker potential application corpus native language identification unigram feature used baseline word unigram specific classifier used classifier redundant,1
588.json,strength empirically verifies using external knowledge benefit weakness real world application utilize external knowledge making better prediction propose rare entity prediction task demonstrate case however motivation task fully justified task important real world application benefit task lack convincing argument proposing task current reading comprehension task evidence correct answer found given text thus interested learning world causality example basic reasoning comparing reading comprehension rare entity prediction rather unrealistic human terrible remembering name mentioned task difficult large number rare entity however challenging task even difficult level exist predicting correct morphological form word morphologically rich language task obvious application machine translation example general discussion helpful characterize dataset detail figure table seems overlapping entity important feature noway predict blank figure word london peter ackoyd description said brutalizing neural network essential understand characteristic data cognitive process search right answer given lack characteristic dataset find baseline inappropriate first contenc natural choice first sigh however mentioned candidate entity rare embeddings entity unrealizable consequence expected contenc work well fairer embeddings initialized trained vector massive dataset expect sort similarity larnaca cyprus embedding space contenc make correct prediction table performance avgemb entity used compute vector modeling perspective appreciate chose sigmoid predictor output numerical score help avoiding normalization list candidate rare difficult learn reliable weight however sidestep technique exist pointer network representation blank included computed lstm bilstm pointer network give probabilistic interpretation propto opinion pointer network appropriate baseline another related note unbalanced negative positive label affect training training model make positive prediction number negative prediction least time higher find task rare entity prediction unrealistic dataset interesting learn reasoning process lead right answer word attends making prediction,1
96.json,summary introduces dataset sarcasm interpretation task system called sarcasm sign based machine translation framework moses dataset collected sarcastic tweet hashtag sarcasm interpretation human sarcasm sign built based moses replacing sentimental word corresponding cluster source side sarcasm cluster translation target side sarcasm sarcasm sign performs moses evaluation metric outperforms moses term fluency adequacy strength well written dataset collected proper manner experiment carefully done analysis sound weakness lack statistic datsets average length vocabulary size baseline moses proper small size dataset assumption sarcastic tweet often differ sarcastic interpretation little sentiment word supported data general discussion discussion give detail weakness half dataset sarcasm interpretation however show important information dataset average length vocabulary size importantly show statistical evidence support method focusing sentimental word dataset small tweet guess many word rare therefore moses alone proper baseline proper baseline system handle rare word well fact using clustering declustering sarcasm sign handle rare word sarcasm sign built based assumption sarcastic tweet often differ sarcastic interpretation little sentiment word table however strongly disagrees assumption human interpretation often different tweet sentimental word thus strongly suggest give statistical evidence dataset support assumption otherwise whole idea sarcasm sign hack read response change decision following reason wrote fiverr worker might take strategy spirit corpus based must built given data data must follow assumption built wrote bleu score moses sign generally considered decent literature number show anything sentence dataset short look table changed moses meaning even half time translation simply copying blue score higher score might achieved system explicitly address rare word system focus sentiment word true wondering whether sentiment word rare corpus system obviously handle addition rare word,2
818.json,thank author response address concern though much promise necessarily given space constraint precisely problem like revision able check drawback fixed change needed quite substantial experimental promise include undergone review accepted stage still sure simply leave make necessary change without reviewing round upgrade score express ambivalence like research extremely messy presentation strength topic creative purpose research really worthwhile aim extracting common knowledge text overcoming well known problem reporting bias fact people state obvious fact person usually bigger ball joint inference information possible extract text weakness many aspect approach need clarified detailed comment worry understand approach make knowledge object interact knowledge verb allows overcome reporting bias get quickly highly technical detail without clearly explaining overall approach good idea experiment discussion need finished particular discussion task tackled lower half table obvious experiment missing variant give much better first task variant second task variant tested indeed improve baseline general discussion need quite work ready publication detailed comment five dimension figure caption implies physical relation know physical relation implies figure trying look essentially extract lexical entailment defined formal semantics dowty verb could please explicit link literature dowty david thematic proto role argument selection language around explain insight approach joint inference piece information help overcome reporting bias value value please also consider work multimodal distributional semantics related work section following paper particularly related goal bruni elia distributional semantics technicolor proceeding annual meeting association computational linguistics long paper volume association computational linguistics silberer carina vittorio ferrari mirella lapata model semantic representation visual attribute please clarify contribution specific task approach commonsense knowledge extraction language long standing task clear grounded mean point section dimension choose explain term condition post condition relevant example full distribution item obtained crowd sourced ideal help figure really slower part seems like related distinction formal semantics stage level individual level predicate person throw ball ball faster person stage level true general ball faster people individual level guess related condition post condition issue please spell type information want extract definition determiner missing section action verb class pick choose verb pick explicitly tagged action verb levin action frame pick know whether frame generating table partition made frame verb reuse verb frame across partition also proportion given case agreement whereas count given case something missing threshold partition randomly rate general relationship knowledge dimension choose choose dimension annotate frame section factor graph please give enough background factor graph audience able follow approach substrate role factor factor graph different standard graph generally beginning section give higher level description work good idea class knowledge antecedent missing object first type talking object pair verb suddenly selectional preference factor seem crucial part introduce earlier case understand role also verb level similarity figure find figure totally unintelligible maybe text clearer interpretable maybe think whether find convey intuitively also make sure readable black white submission instruction define term message role factor graph need soft instead hard need provide detail maxent classifier train input data encoded also explain appropriate baseline skimp seed knowledge problem table reference table like thought sure example right sense entity larger revolution also larger stronger mentioned discus task inferring knowledge object also include incidentally better used terminology table latent verb mention object task antecedent missing reference checked format grice sorower capitalization verbnet reference bibliographic detail,2
726.json,present neural predicting query directly natural language utterance without going intermediate formalism addition interactive online feedback loop proposed tested small scale strength clearly written properly positioned enjoyed reading proposed tested shown perform well different domain academic geographic query flight booking online feedback loop interesting seems promising despite small scale experiment semantic corpus published part work additionally existing corpus converted format believe beneficial future work area weakness clarification section entity anonymization sure understand choice length span querying search engine progressively reduced line section benchmark experiment understand correctly feedback loop algorithm used experiment indeed case sure data augmentation occur annotated training data augmented paraphrase initial data template added also added gold training think surprising help much gold query diverse case think stated clearly addition think interesting performance vanilla without augmentation think reported table find evaluation metric used somewhat unclear accuracy measure correctness execution query retrieved answer text seem indicate line mention executing query alternatively query compared seems case dong lapata table done differently different system dong lapata number comparable addition text mention slightly lower accuracy best line table difference almost point accuracy observation based upon significance test performed think still impressive direct parsing wording changed difference performance seem significant line regarding data recombination technique used liang since technique applicable scenario well currently open question whether actually improve performance left future work something prohibiting technique section three stage online experiment several detail missing unclear technical background recruited user crowd worker recruited trained text say recruited user asked issue least utterance mean query overall total size initial synthesized training report statistic query measure lexical variability length complexity generated seems especially important first phase surprisingly well furthermore since scholar us nice attached submission allow review period section scholar dataset dataset seems pretty small modern standard utterance total main advantage process scalability hindered creation much larger dataset comparing performance possible another baseline newly created dataset compare reported accuracy obtained line evaluation interactive learning experiment section find experiment somewhat hard replicate involve manual query specific annotator example annotator last phase asked simpler question realise always problematic online learning scenario think effort made towards objective comparison starter statistic query mentioned earlier readily available mean ass whether happens second maybe objective held test problematic relies seen query scaling experiment suggested might mitigate risk third possible ass different baseline using online technique sure whether applicable given previous method devised online learning method minor comment line requires require footnote seems long consider moving content body text algorithm sure utterance refers guess query user think accompanying caption algorithm make reading easier line line mention anonymized utterance confused first reading understand correctly refers anonymization described later think better forward reference general discussion overall like given answer question raised like appear conference author response appreciate detailed response made please include detail final version,3
726.json,proposes simple attention based generating query natural language without intermediate representation towards employ data augmentation approach data iteratively collected crowd annotation based user feedback well query produced benchmark interactive datasets show data augmentation promising approach strength intermediate representation used release potentially valuable dataset google scholar weakness claim comparable state geoquery atis support general discussion sound work research could future potential semantic parsing downstream application done little disappointed claim near state accuracy atis geoquery seem case point difference liang necessarily think getting sota number focus significant contribution like provided tone claim addition question mean minimal intervention mean minimal human intervention seem case mean intermediate representation latter term used le ambiguous table breakdown score correctness incompleteness incompleteness query exhibit expertise required crowd worker produce correct query helpful analysis user question could generated figure little confusing could follow sharp dip performance without paraphrasing around stage table need little clarification split used obtaining atis number thank response,3
97.json,describes system assist written test scoring strength represents application interesting problem recognizing textual entailment important task written test scoring weakness anything novel consist application existing technology known problem approach described autonomous still need human actual scoring lack quantitative qualitative evaluation useful system making scorer easier scorer effective compared automatic score system contains multiple component unclear quality contributes overall experience need work writing language style rough several place also contains several detailed example necessarily value discussion evaluation classification baseline predicting frequent class general discussion find inspiring message apart announcing build system,0
395.json,strength propose sense show marginal improvement scws dataset significant improvement datasets weakness technical aspect raise several concern could clarify drawback first drawback state optimizing equation lead underestimation probability sense understand expected reward sense selection independent action action optimize relatively easy setting optimizing expected reward sequence action episodic task proven doable sequence level training recurrent neural network ranzato even challenging setting machine translation number action average sequence length word sense maximum action sequential nature make hard accept claim first drawback second drawback accompanied detail math appendix state update formula minimize likelihood likelihood negative note optimizers adam adadelta minimize function however common practice want maximize minimize since reward defined negative standard optimizer expected negative reward always greater often done many modeling task language minimize negative likelihood instead maximizing likelihood also claim likelihood reach also indicates likelihood reach infinity computational flow line likelihood infinity likelihood could also explain sense based learning horizon length transition state action markov property draw independently trouble relation learning sense mnih reward given environment whereas reward computed reward sense state action pair cross entropy cross entropy defined variable scalar computed distribution total number sens categorial variable dimension cross entropy computed could justify dropout exploration epsilon greedy exploration dropout often used regularization preventing overfitting know gain using dropout exploration regularization state value probabilistic estimation line elaborate variable distribution defined variable interpret distribution sens word however definition contain normalizing constant valid distribution also related value section threshold exploration chosen arbitrary number constrain sumz held allow creation sense beginning training epoch image beginning training unstable creating sens might introduce noise could comment general discussion justification omitting negative sample line negative sampling successfully wordvec nature task learning representation negative sampling however work well main interest modeling distribution sens word noise contrastive estimation often preferred come modeling distribution sense us collocation likelihood compute reward wonder approximation presented affect learning embeddings consider task specific evaluation sense embeddings suggested recent research evaluation method unsupervised word embeddings tobias schnabel igor labutov david mimno thorsten joachim problem evaluation word embeddings using word similarity task manaal faruqui yulia tsvetkov pushpendre rastogi chris dyer read response,1
19.json,strength introduced novel method improve zero pronoun resolution performance main contribution paper proposed simple method automatically generate large training zero pronoun resolution task adapted step learning process transfer knowledge large data specific domain data differentiate unknown word using different tag general well written experiment thoroughly designed weakness question regarding finding antecedent zero pronoun antecedent identified prediction pronoun proposed method matching head noun phrase clear handle situation head word pronoun prediction noun could found previous content system achieves great standard data curious possible evaluate system step first step evaluate performance prediction recover dropped zero pronoun word second step evaluate well system work finding antecedent also curious decided attention based neural network sentence provide reason helpful researcher minor comment figure instead general discussion overall great innovative idea solid experiment setup,3
706.json,strength idea task addressed beautiful original combining indirect supervision accepting resulting parse direct supervision giving definition make particularly powerful interactively building natural language interface programming language proposed wide range potential application weakness several typo language error text seems missing section could benefit careful proofreading native english speaker general discussion present method collaborative naturalization core programming language community user incremental expansion syntax language expansion performed interactively whereby user type command naturalized language either selects list candidate par provides definition also natural language user give intuitive definition using literal instead variable select orange make method applicable programmer grammar induced incrementally used provide candidate par read response,3
173.json,strength introduces document clustering approach compare several established method showing improves case analysis detailed thorough quite dense many place requires careful reading presentation organized clear impressed range comparison influential factor considered argument convincing work influence future approach weakness provide information availability software described general discussion need minor editing english typo line regardless size regardless size line resource resource line consist consisting line versionand version,3
768.json,address task lexical entailment detection context chess kind game given sentence containing word relevant major contribution dataset derived wordnet using synset exemplar sentence context relevance mask word vector accomplished elementwise multiplication feature vector derived context sentence logistic regression classifier masked word vector beat state entailment prediction ppdb derived dataset previous literature combined existing feature beat state point also beat baseline derived dataset although best scoring method dataset masked representation also introduces simple word similarity feature cosine euclidean distance accompany cross context similarity feature previous literature similarity feature together improve classification large amount feature present relatively small contribution task interesting work seems correct go incremental method producing mask vector taken existing literature encoding variable length sequence mean vector think used mask novel however excluding ppdb feature look like best result representation introduced specific point creation context dataset false resulting similar synset permuted example take word synset guaranteed exemplar context hypernym synset entailment context semantically close masked representation hurt classification context agnostic word vector row table well classifier learn ignore context agnostic feature make clearer similarity measure previous literature currently say previous used salient similarity feature informative reader clearer contribution masked vector similarity feature seems like similarity work understand intuition behind macro measure relates sensitive model change context change expect macro compare cross language task well motivated missing relevant citation learning distinguish hypernym hyponym julie weed daoud clarke jeremy reffin david weir bill keller coling read author response noted original review quick examination table show similarity feature make largest contribution improvement score datasets aside ppdb feature author response make point similarity include contextualized representation however similarity feature mixed including contextualized contextualized representation need teased acknowledged response neither table give using masked representation without similarity feature make contribution masked representation difficult isolate,1
355.json,strength present sophisticated application grid type recurrent neural net task determining predicate argument structure japanese approach explicit syntactic structure outperforms current system include syntactic structure give clear detailed description implementation particular close attention performance dropped argument zero pronoun prevalent japanese especially challenging respect multi sequence take predicate sentence account achieves best performance example detailed clearly written weakness really minor comment typo listed correction improve english fluency think worth illustrating point pred including context around predicate example accusative marker included verb pred string understand boldface table general discussion typo error propagation need multi predicate interaction solution solution single sequence single sequence multi sequence multi sequence example bread bread assumes independence assumed independence multi predicate interaction multi predicate interaction multi sequence multi sequence residual connection residual connection multi predicate interaction multi predicate interaction twice naist text corpus naist text corpus state result state read author response satisfied,3
355.json,proposes prediction model japanese task adopting english state zhou also extend applying framework grid rnns order handle interaction argument multiple predicate evaluation performed well known benchmark dataset japanese obtained significantly better performance current state system strength well structured well motivated proposed obtains improvement accuracy compared current state system also using grid rnns achieves slightly better performance proposed single sequential mainly improvement detection zero argument focus weakness best understanding main contribution extension single sequential multi sequential impact predicate interaction smaller ouchi previous work extends ouchi neural network modeling curious comparison,3
435.json,develops lstm based classifying connective us whether indicate causal relation intended guiding idea expression causal relation extremely diverse thus amenable syntactic treatment abstract representation delivered neural model therefore suitable basis making decision experiment altlex corpus developed hidley mckeown offer modest consistent support general idea provide initial insight best translate idea distribution includes tensorflow based model used experiment critical comment question introduction unusual like literature review full overview contains lead redundancy related work section follows guess open standard sort intro really work despite reviewing idea take stand causation expressed rather make negative point reducible syntax really told positive contribution except general final paragraph section extending found disappointing really clear theory causation assumed seem default counterfactual view broadly like david lewis causation modal sufficiency claim counterfactual condition added line following arrow need special kind implication work well known problem lewis theory http bcopley content uploads copleywolff comment elsewhere endorse counterfactual view theory assumed temporal constraint mentioned page understand comment regarding example line seem saying regard sentence false true causal link argument breakage remaining issue divide event event impact causal theory discussed leaving confused caption figure misleading since diagram supposed depict pairlstm variant bigger complaint diagram needlessly imprecise suppose okay leave part standard definition prose diagram clear consistent semantics empty circle input lstm box prose seems look layer glove layer many layer representation diagram precise pooling tanh layer softmax also clear lstm box represent seems like leftmost final representation directly connected layer suggest depicting connection clearly understand sentence beginning line model discussion intrinsically require padding guessing requirement tensorflow efficient training fine correct please understand final clause though issue even related question convenient encode causal meaning convenience issue relates directly causal meaning find independent lstms statedlstm somewhat better first feed second issue reminiscent discussion literature natural language entailment question whether represent premise hypothesis independently first feed second regard open question entailment need investigation causal relation really endorse sentence beginning line behaviour mean assumption relation meaning input event hold better encode argument independently measure relation argument using dense layer surprising since talking subpart sentence might share information hard make sense hyperparameters best performance across task compare line line example interpret attribute unpredictability model interact data section concludes saying connective system correctly disambiguate causal meaning whereas hidey mckeown might correct example suffice show substantiate point suggest making wide range example manifest ambiguity seeing often system delivers right verdict help address question whether lucky example table,1
142.json,explores probabilistic model gaussian process regress target variable post editing time rate quality estimation output well structured clear introduction highlight problem point estimate real world application especially liked description different asymmetric risk scenario entail different estimator reader familiar spends quite space reflect think worth effort introduce concept reader approach choice kernel using warping explained clearly easy follow general research question answered interesting well phrased however question suggestion discussion section intrinsic uncertainty evaluation post editing rate chosen prediction common value predict research nice justify choice made section understand first paragraph exactly trend nlpd rastically decrease warped three datasets indeed state want advance state given standard baseline feature nice show another point estimate existing work result table sense overall quality model related hard interpret nlpd value always tempted look table sense different prediction since whole point right thing great provide notion drastic reduce nlpd worth qualitative analysis actual example section nicely written explains intuitively overall like since point problematic point estimate difficult task general additional information confidence arguably important submission advance state provide novelty term modeling since used research question goal clearly stated nicely executed minor problem section underestimate underestimate figure caption line actually blue green blue stated caption certain toolkit used modeling great refer final,3
129.json,describes training data selection approach score rank general domain sentence using classifier comparison prior work using continuous gram based language model well done even though clear also compared bilingual data selection difference cross entropy motivation instead lstm first unclear strength argue certain section text sentence important others achieved however experimentally show whether combination representation important textual description semi supervised using trained embeddings clear detailed point important aspect however picture layer showing input combined worth thousand word overall well written parenthesis citation necessary citet citep line experiment evaluation support claim little concerned method determining number selected domain sentence line based separate validation validation data used also clear data hyperparameters model chosen sensitive model table really compare score different approach number sentence selected figure show approach still seems outperform baseline case comment interested experiment compare technique baseline domain data available development discussion section could feature example sentence selected different method support claim made section regard argument abstracting away surface form another baseline compare could work axelrod replace word tag reduce data sparsity whether wordvec embeddings provide additional advantage using source target classification score similar source target lewis moore data selection difference cross entropy reference work around line reasonable finally wonder could learn weight source target classification score extending bilingual parallel setting,3
124.json,proposed interesting idea using cognitive feature sentiment analysis sarcasm detection specifically movement pattern human annotator recorded derive feature claim first work include cognitive feature community strength generally well written easy follow interesting idea inspire research task weakness motivation using cognitive feature sentiment analysis well justified imagine feature help reflect reading ease helpful detecting sentiment polarity improvement marginal considering cognitive feature comparing although discussed feasibility approach section convinced especially example given section technique helpful scenario,3
124.json,introducing tracking feature sentiment analysis type cognitive feature think idea introducing tracking feature proxy cognitive load sentiment analysis interesting think discussion feature comparison feature set clear helpful also like feasibility approach addressed section wonder help evaluation datasets conflate different domain movie review corpus tweet corpus might improve prediction movie review resp tweet tweet resp movie review training also make easier interpret table seem rather compared state pang data look much better compared twitter data section overlapping snippet training data testing data datasets right even come source pang sentiment minor extra bold distracting maybe,3
166.json,proposes approach multi lingual named entity recognition using feature wikipedia relying cross lingual wikifier identifies english wikipedia article phrase target language us feature based wikipedia entry experiment show feature help monolingual case also interesting direct transfer setting english tested target language liked proposes feature named entity recognition conduct fairly thorough experiment show utility feature analysis resource latin language particularly interesting named entity wikipedia addition interesting entity affected proposed method proposed method strongly dependent success cross lingual wikifier additional step pipeline often error prediction error wikifier given poor performance direct transfer tamil bengali lexical feature added wonder possible regularize various feature class differently become reliant lexical feature,3
166.json,concerned cross lingual direct transfer model using recent cross lingual wikification general idea highly innovative creative really propose core technology contribution mostly incremental marries research path direct transfer downstream task parsing tagging recent development cross lingual wikification technology however pretty much liked built coherent clear story enough experiment empirical evidence support claim convincing still several comment concerning presentation work related work detailed description related work relates work kazama torisawa needed also required state clear difference related system another relied encyclopaedic wikipedia knowledge difference indeed given text stressed facilitate reading placing work context although argue decided leave tag feature still interesting report experiment tag feature similar tackstrom reader might overview supported empirical evidence regarding usefulness lack feature different language language universal available least section could contribute running example still exactly sure edited tsai roth work given description entirely clear since mention several time approach tackstrom nothman orthogonal combined proposed approach beneficial simply reported preliminary selection language using combination model flavour discussion along line although acknowledge also orthogonal approach comparing strong projection baseline even context show usefulness limitation wikification based approach dutch best training language spanish spanish best language yoruba statistical coincidence something interesting going paragraph discussing depth quite interesting although idea sound table convincing small improvement detected scenario statistical significance test reported table could help support claim minor comment sect projection also performed method require parallel data make model widely applicable even language parallel resource work peirsman pado naacl vulic moens emnlp exploit bilingual semantic space instead direct alignment link perform transfer several typo detected text gain quite careful proofreading first sentence section base sentence parsable page avoid traditional pipeline disambiguate every gram page,3
12.json,describes modification output layer recurrent neural network model enables learning parameter gold projected annotation resource language traditional softmax output layer defines distribution possible label multiplied fully connected layer model noise generation process resulting another output layer representing distribution noisy label overall strong submission proposed method simple elegant report good tagging eight simulated resource language truly resource language making small gold annotation large cross lingually projected annotation training method modular enough researcher working different problem resource scenario likely practical standpoint experimental setup unusual think circumstance need build tagger little token annotation evaluation darpa sponsored research project fairly rare better empirical validation proposed method plot tagging accuracy proposed method baseline varying size gold annotation plot help answer question hurt performance target language method plenty gold annotation amount gold annotation approximately method beneficial answer depend target language beyond cross lingual projection noisy label could potentially obtained source crowd sourcing different set gold annotation although additional potential impact exciting show cross lingual projection surprising proposed training objective give equal weight gold noisy label since setup assumes availability small gold annotated corpus informative report whether beneficial tune contribution term objective function line describes projected data pair word token vector representation tilde explicitly mention vector representation look like distribution cross lingually projected tag word type natural question whether approach still work construct tilde using projected tag token level rather aggregating prediction word type also since word alignment preserved clear construct tilde word never aligned line replace closing bracket opening bracket,3
91.json,investigates three simple weight pruning technique show pruning weight based magnitude work best retraining pruning recover original performance even fairly severe pruning main strength technique straightforward good also clearly written nice covering previous work weakness work isn novel application known technique kind neural application namely aren surprising clear practical significance since take advantage need sparse matrix representation trickier working fast speed main problem space work change picture since field evolving fast need describe generally better explaining care pruning suggestion dealing weakness pruning inform architecture change instance figure suggests might able reduce number hidden layer also potentially reduce dimension source target embeddings another suggestion make link pruning retraining dropout theoretically grounded application dropout recurrent neural networks arxiv detailed comment line softmax weights output embeddings preferable term misleading call dimension network specify parameter size integer multiple number logical constraint line cite bahdanau attention idea rather luong class uniform class distribution seem similar naturally similar consider dropping figure suggestion could hybridize pruning class blind class class uniform embeddings figure show perplexity pruning used section figure figure loss pertain training test corpus figure seems missing softmax weight found diagram somewhat hard interpret might better give relevant statistic proportion class removed class blind pruning various level line might want cite simple initialize recurrent network rectified linear units arxiv,2
18.json,present transition based graph parser able cope rich representation semantico cognitive annotation scheme instantiated ucca corpus start first exposing according cover semantic based annotation scheme graph based possibility token node multiple governor terminal node representing complex structure syntactic coordinate phrase lexical multiword expression allowing discontinuous element verb particules interestingly none principle tied semantic framework could also work syntax representation layer quickly position work first introducing larger context broad coverage semantic parsing annotation scheme choice ucca present set parsing experiment devoted phrase based parsing using stanford parser ucca constituency conversion devoted dependency parsing using ucca dependency conversion finally core proposal experiment showing transition based graph parser suitable direct parsing ucca graph found work interesting considering publication several concern regard methodology empirical justification claimed first propose parser semantically oriented scheme course respect work behind scheme made graph various level specified structural argument semantically oriented label process state nothing transition set treat specificity graph even transition related remote edge could handled one assuming difference label like adding affix example restrict problem graph parsing many work post semeval shared task almeda martin ribeyre proposed extension transition based graph parser adaptation higher nothing precludes data mostly specific feature template anchor scheme even though le influencial count feature unigram anyway mentioned graph parser available don understand couldn used baseline source comparison regarding phrase based experiment using uparse could also validated another parser fernandez gonzales martin produce lcfrs like parsing good uparse missing first introduced uparse scheme support abstract view syntaxico semantic structure treebanks important metric related shared task point field many system model data competing think lack comparison point model parser detrimental work whole found interesting crossing time term next think discussed conference conll note random order please introduce grounded semantic page phrase haven stick constituent tree rich node label propagater trace train parse berkeley parser could good baseline conversion surface dependency tree mind useless loose many information richer conversion schluter semeval used expand ucca graph contains implicit unit correspondent text provide example mentioned representation raise fact scheme doesn seem allow modelling quantifier scope information thus fully comparable syntax oriented scheme indeed abstract example probably underspecified semantic level pcedt much really informative scheme really parsable according score seems harder error analysis useful said principle devised could apply thing look probably need take place much wider clearer introduction trying argue parser parse ucca suitable semantic analysis semantic oriented scheme actually parsable trying dense borderline confusing http corentinribeyre project view dagparser http github andre martin turboparser http github andre martin turboparser tree master semevaldata,2
25.json,present approach word sens temporal information past present future atemporal problem using graph based semi supervised classification algorithm allows combine item specific information presence temporal indicator gloss structure wordnet semantic relation synset take account unlabeled data perform full annotation wordnet based training data labeled previous work using rest wordnet unlabeled data specifically take advantage structure label breaking task binary formulation temporal atemporal using data labeled temporal perform finer grained tagging past present future order intrinsically evaluate approach annotate subset synset wordnet using crowd sourcing compare system obtained state time tagger stanford sutime using heuristic backup strategy previous work obtain improvement around accuracy show approach allows performance higher previous system using labeled data finally perform evaluation resource existing task tempeval show improvement label well constructed generally clear approach seems sound well justified work development resource fine grained temporal information word sense level made available could used improve various task remark especially concerning setting experiment think information given task performed extrinsic evaluation section example could useful understand system trying predict feature describe entity pairs made clear pair feature especially entity attribute pair dimension lemma obtained automatically sentence describing label used confusing sure understand event document creation time event sentence event mean kind pair considered relation described relation beginning find unclear footnote relation relation ignored make mapping complex also score macro micro averaged finally ablation study seems indicate possible redundancy lexica entity quite close score clue behavior also question extrinsic evaluation optimized parameter algorithm parameter since also used within mincut framework optimized finally libsvm library used weka wrapper think reference libsvm included remark interesting number example label gold data figure given coarse grained label temporal atemporal finer grained also nice idea number word ambiguous temporal level word like present said caption table presented significantly better significancy test indicated neither value minor remark related work kind task performed filannino nenadic related work requires post calibration procedure need reference footnote clearer explain calibration related work differ ours table really small maybe remove parenthesis caption give score prec caption also reduced information table better represented using graph beginning tempeval reference table made clearer ordering score column paragraph atemporal atemporal,3
151.json,describes deterministic dependency parsing algorithm analysis behaviour across range language core algorithm rule defining permitted dependency based tag algorithm start ranking word using slightly biased pagerank graph edge defined permitted dependency stepping ranking word linked closest word maintain tree permitted head rule directionality constraint overall interesting clearly presented though seems differ slightly sogaard unsupervised dependency parsing without training question suggestion head rule table good analysis rule relation corpus example section fact always lead connected graph mentioned frequently occurs large component typically surprised head direction chosen using test data rather training development data given fast decision converges sentence major issue surprising choice breaking word pagerank score work impact performance significantly tie rare enough impact various type constraint head rule directionality distance lead upper bound possible performance system informative include oracle constraint show much hurt maximum possible score particularly helpful guiding future work term modify system minor obtain rank table table column different order found table arrangement clearer isolate contribution,2
151.json,present parse tree namely universal dependency treebanks relying using modified version pagerank give meaningful word opposed stop word idea interesting though closed done gaard personalization factor giving weight main predicate nice better take next level tell personalization solely used main predicate weight seems arbitrary regarding evaluation detailed analysis chart beneficial sometimes hard gist table finally interesting score tagging prediction mode able degradation parsing performance heavily correlated degradation tagging performance expect interesting increment work gaard small smaller issue main idea main idea,2
13.json,present task event entity linking propose sentential feature cnns place external knowledge source earlier method used train part first part learns event mention representation second part learns calculate coreference score given event entity mention well written well presented easy follow rather like analysis done corpus regarding argument sharing event coreference furthermore analysis size impact dataset great motivation creating dataset however major issue need addressed fail motivate analyze pro con using generating mention representation discussed chose comparison model straightforwardly given improvement make according various metric state point score need evidence architecture indeed superior clear novel idea tackling event linking sentential feature given using fashion classification task could explicitly point mainly compare existing continuous space method event linking choice method table thorough enough information regarding dataset collected major issue dataset limited number event type making constrained biased important know event type cover also help support claim section approach strongly tied domain semantic feature available approach depend resource restricted need show earlier method fail dataset succeed also enabling meaningful comparison future think making dataset publicly available minor issue liked performance without gold reference table well nice explore cannot augmented vanilla coreference resolution system specific example line shelf corenlp system readily link bombing somehow leveraged event entity linking baseline given relatively small size dataset think compelling requires testing available resource well motivates working entity event coreference simultaneously also believe testing eventcorefbank parallel essential table show pairwise feature quite effective signal feature engineering still crucial competitive least scale dataset wonder feature effective report current chosen else tried,2
683.json,proposes boosting based ensemble procedure residual network adopting deep incremental boosting method used mosca magoulas step block layer added network position weight layer copied current network speed training method sufficiently novel since step deep incremental boosting slightly adopted instead adding layer network version add block layer position start selected position merges layer accordingly hence slightly adopts empirical analysis data augmentation clear whether improvement ensemble disappear data augmentation also main baseline skip connection therefore negatively affect fair comparison argue involve state net since analysis focus ensemble approach however potential improvement ensemble compensated inherent feature variant boosting procedure computationally restrictive case imagenet training variant perform much better case therefore baseline include state net dense convolutional network hence current preliminary addition clear sensitive boosting selection injection point adopts net provides empirical analysis however contribution sufficiently novel empirical satisfactory demonstrating method significant pro provides preliminary boosting net con sufficiently novel incremental approach empirical analysis satisfactory,2
379.json,describes novel technique improve efficiency computation graph deep learning framework impressive speedup observed implementation within tensorflow content presented sufficient clarity although graphical illustration could useful work relevant order achieve highest performance neural network training pro significant speed improvement dynamic batching source code provided con effect large real world allow reader improvement better context presentation vizualisation improved,7
396.json,proposes image generation back ground generated first foreground pasted generating first foregound mask corresponding appearance curving appearance image using mask transforming mask using predicted affine transform paste image using amturkers verify generated image selected time naturally looking corresponding image figure ground aware image generator segmentation mask learn depict object constrained datasets bird thus method appears limited general shape datasets also argue architectural contribution potential merit nice multiple layer foreground occluding foreground ever generated layered figure ground aware,5
545.json,proposes learning compositional kernel machine ckms extends classic kernel machine constructing compositional kernel function using product network considers convnets nicely learned nonlinear decision function resort success classification compositional nature perspective motivates design compositional kernel function product implementation indeed interesting agree composition important convnets whole story convnets success essential difference convnets ckms kernel convnets learned directly data ckms still build feature descriptor believe limit representation power ckms recent deep convolutional network hierarchical kernel machine anselmi seems interesting experiment seem preliminary good promising ckms small norb quite important show competitive recent classification standard benchmark mnist cifar even imagenet order establish novel learning norb composition ckms seem better convnets classifying image dominant object suspect sparse feature great could show accuracy feature matching kernel svms detail experiment need clarification high probability sampling collection many image generated norb symmetry ckms show better performance convnets small data convnets seem converged could possible show larger dataset,4
545.json,propose method efficiently augment variant many virtual instance show promising preliminary interesting read thoughtful methodology partially unsupported potentially misleading claim pro thoughtful methodology sensible design choice potentially useful smaller datasets statistical structure nice connection product literature con claim scalability unclear generally succeed telling complete story property applicability proposed method experiment preliminary scalability claim particularly unclear repeatedly mention lack scalability drawback convnets appears proposed le scalable standard svms often handle much fewer training instance deep neural network appears scalability advantage mostly training set roughly fewer instance even method could scale training instance unclear whether predictive accuracy competitive convnets domain moreover idea operation simply creating virtual instance training point test point still somewhat daunting training instance testing instance scalability number training instance biggest drawback using svms gaussian kernel modern datasets scalability claim need significantly expanded clarified related note suggestion convnets grow quadratically computation additional training instance introduction need augmented detail potentially misleading convnets typically scale linearly additional training data general suffers greatly lack clarity issue presentation full story presented critical detail often missing moreover strengthen remove broad claim support vector machine svms eclipsed multilayer perceptrons ckms could become compelling alternative convnets reduced training time sample complexity suggesting ckms could eclipse convolutional neural network instead provide helpful precise information convnets multilayer perceptrons used well eclipsed svms different relative advantage based information presented broadly advertising scalability convnets misleading ckms scale datasets million training test instance seems scalability advantage limited smaller datasets asymptotic scalability could much worse general even ckms could scale datasets good predictive accuracy convnets application specific full disclosure precise strength limitation work greatly improve ckms robust adversarial example standard convnets virtual instance many approach make deep net robust adversarial example useful consider compare idea behind ckms also inherently specific kernel method considered looking using virtual instance similar deep network full exploration might idea worth least brief discussion text advantage svms gaussian kernel deep neural net achieve quite good performance little human intervention design choice however ckms seem require extensive intervention term architecture neural network insuring virtual instance created plausible manner particular application hand unclear general want create sensible virtual instance topic deserves consideration moreover unlike svms example gaussian linear kernel standard convolutional network quite general model ckms applied seem like svms kernel method highly tailored particular application case norb dataset certainly nothing wrong tailored approach help clear detailed presented idea applied making relevant design choice range different problem indeed good avoid potentially misleading suggestion early proposed method general alternative convnets experiment give insight advantage proposed approach limited sense property strength limitation proposed method need greater range datasets much larger range training test size comparison also quite limited gaussian kernel using convnet feature dataset hand light blue curve figure least well light blue curve also work could considered combine advantage kernel method deep network also claim approach help curse dimensionality sensible particularly explored also seems curse dimensionality could affect scalability creating useful virtual instance unclear work without feature even method adapted scale unclear whether useful convnets domain indeed experiment convnets essentially match ckms performance example probably perform better ckms larger datasets speculate experiment consider larger problem methodology largely take inspiration product network application context kernel approach reasonably original worthy exploration reasonable expect approach significant significance demonstrated quality high sense method insight thoughtful suffers broad claim lack full precise detail short like need specific detail full disclosure method applicable precise advantage limitation code helpful reproducibility,4
545.json,proposes learning framework called compositional kernel machine combine idea kernel method product network first defines leaf kernel element query training example defines kernel recursively similar product network shown evaluation done efficiently using trick positive think idea interesting instance based learning method kernel successful past replaced deep learning method convnet past year investigate unexplored area combine idea kernel method deep network case negative although idea interesting clearly preliminary current form simply advantage proposed framework convnet elaborate important claim faster learn convnet clear case convnet gradient descent learning faster also inference running time convnet depends network structure addition network structure also depends size training perspective seem scalable training size probably kind specialized data structure trick even fairly simple dataset like norb hard time understanding leaf kernel capturing example element correspond pixel intensity leaf kernel essentially compare intensity value pixel query image training image case comparing background pixel across image help recognition think probably help explain better current form part dense hard understand also entirely clear design architecture product function example seems fairly arbitrary experiment section probably weakest part norb small dataset today standard even small dataset proposed method slighly better clear whether table linear kernel linear suspect performance even higher kernel worse convnet proposed method show improvement convnet synthetic datasets norb composition norm symmetry overall think interesting idea current form preliminary work needed show advantage said acknowledge machine learning history many important idea seem mature first proposed took time idea develop,4
768.json,proposes modification convnet training feature activation linear classifier divided group pair feature across pair group encouraged statistical correlation instead discovering group automatically work proposes supervision call privileged information assign feature group hand coded fashion developed method applied image classification pro clear easy follow experimental seem show benefit proposed approach con proposes core idea group orthogonality privileged information introduces background feature suppression without much motivation without careful experimentation comparison ensemble full experiment imagenet partial privileged information setting impactful promising willing accept improved version however current version lack focus clean experiment first abstract intro focus need replace ensemble single diverse ensemble like feature hope boost accuracy requiring fewer flop le memory based introduction expect rest focus point experimental ensemble experimental evidence proposed approach able avoid speed memory cost ensemble also retaining accuracy benefit second technical contribution presented group orthogonality however idea background feature suppression introduced motivation given motivation require suppression introduction seems moreover experiment never decouple suppression unable understand work critical experimental flaw reading minor suggestion comment equation definition incorrect normalizing factor figure seems incorrect mask placement mask mask background allow pas,4
768.json,starting point work understanding neuron neuron fire background foreground region provides independent piece information subsequent decision give complementary viewpoint input subsequent layer thought performing ensembling expert combination within rather using ensemble network propose sensible method decorrelate activation intermediate neuron delivering complementary input final classification layer split intermediate neuron foreground background subset append side loss force zero background foreground pixel respectively demonstrate improve classification scale classification example fraction imagenet resnet rather layer compared vanilla baseline loss enjoyed reading idea simple smart seems effective concern firstly seems particular vision vision know masking feature training testing help,5
787.json,proposes semantic embedding based approach multilabel classification conversely previous proposal considers underlying parameter determining observed label rank rather observed label matrix rank however clear extent difference assumption significant model label instance draw multinomial distribution parametrized nonlinear function instance feature neural network proposed training algorithm slightly complicated vanilla backprop significance compared nnml particular large datasets delicious eurlex clear well written main idea clearly presented however experimental significant enough compensate lack conceptual novelty,3
787.json,present semantic embedding multi label prediction question pointed proposed approach assumes number label predict known said orthogonal question although think trying understand different basic softmax output trained step approach instead stochastic gradient descent seems reasonable given similarity compare basic baseline regarding sampling strategy estimate posterior distribution difference jean agree slightly different think definitely refer point difference last question called semantic embeddings usually term used show semantic meaning trained embeddings seem appear,3
380.json,address major shortcoming generative adversarial network lack mechanism evaluating held data work bigans address learning separate inference network propose change objective function optimal discriminator also energy function rather becoming uninformative optimal solution training objective requires gradient entropy generated data difficult approximate propose method based nearest neighbor based variational lower bound presented show data learned discriminator energy function closely approximates probability data complex data discriminator give good measure quality held data largest shortcoming practical issue around scalability nearest neighbor approximation accuracy variational approximation acknowledge also since entropy estimation density estimation closely linked problem wonder practical method egans equivalent form approximate density estimation exactly problem gans designed circumvent nonetheless elegant mathematical exposition alone make worthwhile contribution literature also quibble writing seems something missing sentence finally whose discriminative power sure mean title undersells make sound like making small improvement training existing rather deriving alternative training framework,7
553.json,combining storage processing capability interesting research topic data transfer major issue many machine learning task well written unfortunately address thing medium depth probably length constraint opinion journal depth discussion technical detail better target even though researcher took interesting approach evaluate performance system difficult grasp expected practical improvement approach focus specialized hardware tpus question come mind much expect beat latest greatest real task consider expert topic even though experience systemc,5
553.json,idea moving processing machine learning contained within data storage device intriguing offer potential power efficient computation rather specialized topic feel especially wide interest iclr audience describes simulation rather actual hardware implementation describes implementation existing algorithm comparison algorithm train test performance seem relevant since novelty algorithm single layer perceptron mnist call question practicality system since tiny neural network today standard understand thought could scale contemporary scaled network term number parameter storage bandwidth expert area evaluated depth,4
601.json,seem shelf life dataset decreased rapidly recent literature squad dataset heavily pursued soon online couple month best performance leaderboard reaching rather surprising taking account fact formal conference presentation dataset took place month emnlp reported machine performance time submission reasonable speculation dataset hard enough newsqa submission aim address concern presenting dataset comparable scale created different collection strategy notably solicit question without requiring answer turkers order promote diverse hard answer question another notable difference question gathered without showing content news article dataset make bigger subset daily corpus opposed much smaller subset used squad think newsqa dataset present effort construct harder large scale reading comprehension challenge recently research topic satisfying datasets without weakness think dataset present potential value compared available today said read like prepared hurry numerous small thing could done better result wonder quality dataset human performance squad measured lower reported squad think sort difference easily happen depending level carefulness annotator maintain human level carefulness even level reading comprehension think best explain reason behind difference possible perform careful measurement human performance anything think look favorable newsqa human performance level look difficulty dataset come mainly potential noise collection process implies performance could result necessarily difficulty comprehension reasoning incorrect answer given human annotator also sure whether design choice presenting news article soliciting question good imagine people might asking similar generic question enough context presented perhaps taking hybrid like suggest present news article sentence phrase randomly redacted question generator context full material front another encouraging turkers asking trivial question engage automatic system turkers must construct pair existing state system cannot answer correctly,5
314.json,deep using deep neural network function approximators algorithm number success solving large state space empirically driven work build approach introduces algorithm performs better novel environment sensory data allows better generalization across goal environment notably algorithm winner visual doom competition idea algorithm additional dimensional observation ammo health provided game engine supervised target prediction importantly prediction conditioned goal vector given learned current action trained optimal action current state chosen action maximises predicted outcome according goal unlike successor feature representation learning supervised relationship prediction current state next state number prior work predicting future state part goal driven function approximators review section contribution work focus monte carlo estimation rather dimensional measurement prediction parametrized goal perhaps importantly empirical comparison relevant prior work addition comparison visual doom show algorithm able learn generalizable policy respond without training limited change goal well communicated empirical compelling significant interest minor potential improvement approximation supervised training making policy assumption learns replay buffer monte carlo regression expectation remainder trajectory assumed follow current policy sampled episode generated prior version policy discussed algorithm us additional metadata information part sensory input worth predicting compared algorithm think limitation approach work well sensory environment measurement provided mentioned clearly,6
314.json,present policy deep method additional auxiliary intrinsic variable method special case universal value function based approach cite correct reference maybe biggest claimed technical contribution distill many existing idea solve navigation problem think contribution clearly stated abstract intro liked failure mode approach circumstance problem generalizing changing goal conceptual problem since policy method catastrophic forgetting agent dose repeatedly train goal distant past since main contribution integrate several idea show empirical advantage liked domain like atari maybe using intrinsic variable overall think show clear empirical advantage using proposed underlying formulation experimental insight might valuable future agent,7
528.json,present design decision terpret experiment learning simple loop program list manipulation task terpret line work bridge programming language machine learning community contrasted recent interest community program induction focus using design programming language reduce search space namely used structure control flow else foreach zipwithi foldli template immutable data reuse neural memory type tried penalizing typedness restricting search well typed program work better bird view stand make everything continuous perform gradient descent discretize thing perform structured heuristic guided combinatorial search liked relevant baseline lambda wished also included fully neural network program synthesis baseline admittedly succeed except simplest task think experimental task simple enough generating code succeed wished terpret available code reproduce experiment wonder otherwise interesting recommendation design programming language perform gradient descent based inductive programming hold perform harder task loop even though task already interesting challenging wonder much task biased search good subset constraint structuring control flow overall think good enough appear iclr expert program induction synthesis writing time hard follow instance naming scheme variant could summarized table information feature embeds introduction basis modern computing page training objective minimize cross entropy distribution output register point distribution probability mass correct output value want cater community large think better treat output classification problem correct output value give detail exactly type criterion loss cross entropy terpret probabilistic programming language program induction gaunt,6
656.json,explicitly design geometrical structure combining scattering network aid stability limited data performance well written contribution combining scattering cnns novel seem promising feel work missing piece scattering literature make useful practical application wish investigated effect stable bottom layer respect adversarial example done relatively straightforward software like cleverhans deep fool interesting first layer stability hybrid architecture increase robustness significantly tell fooling image related level geometry finding case interesting well proposed architecture evaluated real limited data problem strengthen improved generalization claim however admit cifar cifar difference already seems like promising indicator regard point addressed additional experiment happy raise score summary interesting approach presented might useful real world limited data scenario limited data look promising adversarial example investigated experimental section realistic small data problem addressed minor sota resnet table indeed fashion day typo tacke developping,6
713.json,deal important issue vanishing gradient quest perfect activation function proposed approach learning activation function training process find research interesting concerned premature long experimental section sure conclusion appear somewhat confused amount maybe could mean perhaps statement exceptionally high accepted need bold statement performance solid evidence opinion lacking approach either breakthrough reading convinced case theoretical section could made little clearer finally performance affected huge advantage relu fact formula simple thus costly evaluate pelu compare,4
713.json,proposes modification activation function neural network parameterizing trainable parameter layer parameter proposed effectively counter vanishing gradient main concern regarding related claim effectiveness pelu analysis section discus pelu might improve training combating gradient propagation issue imply improved generalization result model easier train however experiment seek demonstrate improved generalization performance could principle better inductive bias nothing optimization analysis none experiment designed directly support stated theoretical advantage pelu compared optimizing model response review question state claim section meant apply generalization performance fail true claim except flexibility claim agree better training lead better sample performance agree flexibility sometimes help network adapt inductive bias problem instead overfitting much weaker claim compared mathematical justification improved optimization selection learning hyperparameters state discussion openreview learning rate selected favorable relu pelu however guarantee unfavorable raise question regime constructed better performance pelu draw conclusion pelu better overall convinced experimental setup match theory experiment,3
586.json,overall feel status update best researcher field clear observation interesting remark scattered quantum progress study done neural minor remark term table found table confusing several column model technically rnns rnns translation wordvec highlight rnns characterized term length input sequence length input size step input output working memory basic question input presented character output retrieved filter input output encoded treated filter intermediate layer intermediate activation function interpretable digit able interpret filter implementing reliable multiplication carry algorithm looking intermediate value shed light usually working model fail pathological case identified table preliminary experiment input alignment interesting way seed effective attentional mechanism also suggests presently dealing general expression evaluation correct algorithm remark abstract improving memory efficiency neural seem overblown paragraph page describes improvement using whileloop instead unrolling graph using swapmemory host memory memory run short seem like good practice remarkable improvement efficiency fact likely slow training inference memory fact point trying many random seed convergence make wonder neural worth computational cost evaluated mean learning algorithm already well understood parsing evaluating exprs consider spending computational cycle training model multiple seed traditional search program space sampling lisp program something note curriculum strategy employed presented interesting read indication length someone might train sort leave reviewer impression despite stated extension neural remains unclear useful might practical problem,4
549.json,proposes sparse coding problem cosine loss integrated feed forward layer neural network energy based learning approach directional extension make proximal operator equivalent certain linearity crelu although unnecessary experiment show significant improvement baseline pro minimizing cosine distance seems useful many setting compute inner product feature required finding bidirectional sparse coding corresponding feed forward crelu linearity con unrolling sparse coding inference feed foward network class wise encoding make algorithm unpractical multi class case requirement sparse coding class show proposed method could outperform baseslines real world task,4
725.json,update revision provided reducing rating marginally acceptance address problem training stochastic feedforward neural network proposes transfer weight deterministic deep neural network trained using standard procedure including technique dropout batch normalization stochastic network topology initial mechanism described performing transfer involves rescaling unit input layer weight appropriate specification stochastic latent unit used pretraining employ relu nonlinearities initial experiment mnist classification generative task multimodal target distribution show simple transfer process work well used pretraining us sigmoid nonlinearities pretraining us relus tackle problem introduces simplified stochastic feedforward neural network every stochastic layer followed layer take expectation sample input thus limiting propagation stochasticity network modified process transferring weight pretraining simplified sfnn described justified training process occurs three step pretrain transfer weight simplified sfnn continue training optionally transfer weight full sfnn continue training transfer deterministic called continue training third step skipped simplified sfnn also used directly inference experimental mnist classification show simplified sfnn training improve deterministic baseline trained batch normalization dropout experiment generative task mnist half face database show proposed pretraining process improves test negative likelihood finally experiment cifar cifar svhn lenet network network wide residual network architecture show stochastic training step improve performance deterministic confusing refer multi modal task meant generative task multimodal target distribution multi modal task also refer learning task cross sensory modality audio visual speech recognition text based image retrieval image captioning recommend precise term generative task multimodal target distribution early introduction refer task multi modal task rest sake brevity easier read sfnn used refer singular stochastic feedforward neural network plural stochastic feedforward neural network case plural meant write sfnns table hidden layer sfnn initialized relu much worse test hidden layer sfnn initialized relu notation us superscript indicate layer index confusing reader naturally par squared number unit second layer transfer weight back simplified sfnn need perform sort rescaling undoes operation equation ncsfnn stand supplementary material pro proposed easy implement apply task mnist showing stochastic training produce deterministic called generalizes better trained batch normalization dropout quite exciting con reason outlined time hard follow cifar cifar svhn convincing baseline used dropout batch normalization shown minst demonstration similar result challenging task strengthen minor issue believed stochastic believed stochastic underlying success efficient training method underlying success efficient training method necessary order complex stochastic nature many real world task necessary complex stochastic nature many real world task structured prediction image generation memory network memory network model task furthermore believed sfnn furthermore believed sfnn using backpropagation variational technique reparameterization trick using backpropagation variational technique reparameterization trick several effort developing efficient training method several effort toward developing efficient training method however training sfnn still significantly slower however training sfnn still significantly slower training prior work line considered consequently prior work area considered instead training sfnn directly instead training sfnn directly whether trained parameter whether trained parameter fine tuning light cost cost fine tuning recent advance design training recent advance design training rather believed transferring parameter believed transferring parameter opposite direction unlikely possible opposite unlikely address issue propose address issue propose intermediate sfnn intermediate sfnn forward pas computing gradient backward pas forward pas computing gradient backward pas order handle issue forward pas order handle issue forward pas neal proposed gibbs sampling neal proposed gibbs sampling making sfnn equivalent making sfnn equivalent case us unbounded relu case us unbounded relu relu type gradient vanishing problem relu type mitigate gradient vanishing problem multiple mode outupt space multiple mode output space first hidden layer first hidden layer replaced stochastic replaced stochastic layer former significantly outperforms latter former significantly outperforms latter simple parameter transformation sfnn clear work general simple parameter transformation sfnn clearly work general special form stochastic neural network special form stochastic neural network like first layer first layer connection naturally lead efficient training procedure connection naturally lead efficient training procedure,4
660.json,noted figure left sometimes seems sufficient tune learning rate argument figure right good learning rate make adam fail guess selected note adam several time faster beginning always converges show since adam adam understanding define time hyperparameters similarly define directly behaviour show extraordinary parameterized better adam looking directly whether overestimated learning rate could argue automatically tune learning rate problem individually anyway,4
322.json,address problem allowing network change number unit used training done simple elegant well motivated unit zero input output weight added removed training group sparsity norm regularization used encourage unit weight zero main theoretical contribution show proper regularization loss minimized network finite number unit practice result guarantee resulting network training data initial experiment show seem case potential advantage approach learn number unit network ease burden tuning hyperparameters disadvantage approach maybe approach really solve problem network still several hyperparameters implicitly control number unit emerge including parameter control often unit added rapidly weight decay zero clear whether hyperparameters easier harder tune one standard approach fairness claim made training easier little disappointing seem case emphasize able train network fewer unit achieve comparable performance network trained parametrically potentially important smaller network reduce time testing power consumption memory footprint important mobile device particular however compare experimentally existing approach attempt reduce size parametrically trained network pruning trained network clear whether approach really competitive best current approach reducing size trained network another potential disadvantage proposed approach hyperparameters control number unit appear network training time therefore training might potentially much slower approach parametric approach fixed hyperparameters practice many parametric approach require method like grid search choose hyperparameters slow many case experience similar problem make choice hyperparameters relatively easy mean cost grid search always paid slowness approach endemic discus issue scale much larger network trained concern approach practical large scale network training slow general experiment helpful encouraging comprehensive totally convincing want experiment much larger problem convinced approach really practical widely useful overall found interesting clearly written make potentially useful point overall vision building network grow adapt life long learning inspiring type work might needed realize vision current remain pretty speculative,4
334.json,present succinct argument principle optimizing receptive field location size simulated make saccade respect classification error image data whose label depend variable size variable location subimages explains existence foveal area primate retina argument could improved using realistic image data drawing direct correspondence number receptive field size eccentricity retinal cell macaque face challenge identifying loss function biologically plausible supportive claim argument could also improved commenting timescales involved presumably density foveal center depends number saccade allowed inference process well size target image also impact overall classification accuracy classification error rate dataset remain stubbornly seems high working like seems overall argument supposes trained good classifier training strategy model work better differently raise question eye visual cortex work like one evolutionary pressure applying pressure training objective zooming power translation dataset target image size translation dataset target image different size zooming tailor made strange high classification rate dataset wonder maybe model trained potential undermine overall claim comparing attention model spatial transformer network draw irrelevant take main point address potential concern training well problem parameterization could easily fixed,5
524.json,concept data augmentation embedding space interesting method well presented also justified different task spoken digit image recognition comment comparison simple layer baseline throughout task clear whether gain maintain complex baseline used another comment augmented context vector used classification wondering compare using reconstructed input furthermore table input feature space extrapolation improves performance whether complementary,6
318.json,proposes extension gated graph sequence neural network including ability produce complex graph transformation underlying idea propose method able build modify graph structure internal representation solving problem particularly solving question answering problem author proposes different possible differentiable transformation learned training typically supervised fashion state graph given timestep particular occurence presented take sequence input iteratively update internal graph state final prediction applied solving task babi interesting approach really interesting since proposed able maintain representation current state complex graph still keeping property differentiable thus easily learnable gradient descent technique seen succesfull attempt continuous symbolic representation moreover seems general recent attempt made ymbolic stuff differentiable model memory network since shape state fixed evolve main concern trained providing state graph timestep done particular task babi cannot solution complex problem concern whole content perhaps best journal format conference format making article still difficult read density,8
573.json,proposes setting learn model seek information asking question order solve given task introduce task designed goal show possible train model solve task reinforcement learning motivation task proposed work existence game like battleship agent need question solve given task quite surprising actually consider game potential task explore beside hangman also completely clear task selected significant amount work dedicated past understand property game like navarro human solve interesting task proposed work distinguish one studied existing literature human perform particular cohen lake recently studied question game searching large hypothesis space asking question evaluate performance human computer believe really benefit similar study developing ability model actively seek information solve task interesting challenging problem task require agent select question finite clean informative possibility allows simpler analysis given agent perform cost reducing level noise appear realistic setting also show using relatively standard deep learning model reinforcement learning able train agent solve task intended validates empirical setting also exhibit limitation approach using relatively setting perfect information fixed number question simple interesting agent able perform well task absence baseline limit conclusion draw experiment example hangman experiment seems frequency based obtains promising performance interesting good baseline occurrence letter frequency character gram overall explores interesting direction research propose promising task test capability learn asking question however current analysis task limited hard draw conclusion good focus human perform task strong simple baseline task related natural language since motivation work rather solving relatively sophisticated model,3
420.json,focus attention neural language modeling major contribution propose separate value predict vector attention mechanism instead single vector function interesting extension standard attention mechanism used application well report short attention span sufficient language model surprising propose gram exploit fact novel model neural language modeling interesting message done thorough experimental analysis proposed idea language modeling task task convinced response review question minor comment reed freitas gulcehre added related work section well,6
532.json,final review writer responsive agree reviewer experimental setup wrong increased score still think lack experiment conclusive reader interested thing either getting insight understanding something better learn method better performance fall category fails prove throughout rigorous experiment summary lack experiment inconclusive believe proposed method quite useful hence conference level publication proposes train policy network along main network selecting subset data training achieving faster convergence le data pro well written straightforward follow algorithm explained clearly con section mention validation accuracy used feature vector training invalidates experiment training procedure using data validation dataset tested paper claim faster convergence rate tested multiple datasets network architecture show consistency especially larger datasets proposed method going le training data iteration shown much larger scaler datasets imagenet discussed detail review question claiming faster convergence compare learning curve baseline adam plain unfair comparison almost never used practice regardless black optimizer case could adam alone black optimizer work well better adam black,3
476.json,describes careful experimental study cifar task us data augmentation bayesian hyperparameter optimization train large number high quality deep convolutional network classification model hard target ensemble best model used teacher distillation framework student model trained match averaged logits teacher ensemble data augmentation bayesian hyperparameter optimization also applied training student model convolutional convolutional student model varying depth parameter count trained convolutional model architecture parameter count convolutional student also trained using hard target cross entropy loss experimental show convolutional student convolutional layer unable match student convolutional layer constraint number parameter student kept constant pro thorough well designed study make best existing tool answer question whether deep convolutional model need depth convolution build nicely preliminary caruana con difficult prove negative admit said study convincing possible given current theory practice deep learning section state logits unnormalized probability include partition function follow iclr citation style quoting template publication included sentence citation parenthesis hinton information otherwise citation parenthesis deep learning show promise make progress towards bengio lecun minor issue english usage typo cleaned final manuscript necessary training student model convolutional layer necessary training student model convolutional layer remaining image validation remaining image validation evaluate ensemble prediction logits sample save data evaluated ensemble prediction logits sample saved data detail hyperparamter optimization detail hyperparameter optimization trained deep model spearmint trained deep model spearmint best obtained accuracy fifth best achieved best obtained accuracy fifth best achieved size architecture three best model size architecture three best model clearly suggests convolutional critical clearly suggests convolution critical similarly hyperparameter opimizer point view similarly hyperparameter optimizer point view,7
499.json,well known limitation deep neural network parameter typically used example even though different example different characteristic example recognizing animal likely require different feature categorizing flower using different parameter different type example potential greatly reduce underfitting seen recent generative model image quality much better le diverse datasets however difficult different parameter different example typically train using minibatches relies using parameter example minibatch matrix multiplies fully connected network hypernetworks cleverly proposes around problem adapting different parameter different time step recurrent network different basic insight minibatch always include many different example time step spatial position computational issue involved using different parameter parameter modified different position based output hypernetwork condition time step hypothetically hypernetwork could also condition feature shared sequence minibatch expect method become standard training rnns especially length sequence training testing phase penn treebank highly competitive baseline sota result reported impressive experiment convolutional network le experimentally impressive suspect aiming achieve state settled achieving reduction number parameter might even worthwhile consider synthetic experiment completely different type image appended bird left flower right show hypernetwork help situation case convnets case hypernetworks help specific rnns seems case explicitly changing nature computation depending position sequence greatly improves generalization usual could learn store counter indicating position sequence hypernetwork could efficient capacity application time series forecasting modeling could interesting area future work,8
421.json,present novel unsupervised segmentation classification time series data recurrent hidden semi markov proposed extends regular hidden semi markov model include recurrent neural network observation latent class modeling observation category efficient training procedure based variational approximation experiment demonstrate effectiveness approach modeling synthetic real time series data interesting novel proposed method well motivated combination duration modeling hmms state observation model based rnns combination alleviates shortcoming standard hsmm variant term simplicity emission probability method technically sound demonstrated effective interesting method compare quantitatively based method ammar dyer smith nip crfs complex data likelihood though noted response phase still limitation regardless think merit using rnns class specific generative model clear,6
525.json,present online learning method learning structure product network algorithm assumes gaussian coordinate wise marginal distribution learns parameter structure online parameter updated recursive procedure reweights node network contribute likelihood current data point structure learning done either merging independent product gaussian node multivariate leaf node creating mixture node multivariate large fact dataset scaled larger datasets term number datapoints promising although number variable still quite small current benchmark tractable continuous density modeling neural network include nice real family model scaled large number datapoints variable intractable method like genmmn property main issue work iclr audience mainly specific datasets used deep learning generative modeling literature genmmn baseline also good choice bridge neural community parzen window based likelihood evaluation really meaningful better way evaluate likelihood annealed importance sampling discussed quantitative analysis decoder based generative model recommend simple type lower bound likelihood something like real neural network density model scalable large number observation well instance clear method scale well horizontally like evaluating feasibility modeling something like mnist interesting spns strength marginal also various type conditional query tractable performance evaluated compared interesting application could imputation unknown pixel color channel image currently high performing tractable despite disconnect iclr generative modeling literature algorithm seems simple intuitive convincingly work better previous state online structure learning think much better baseline continuous data genmmn attempting compare neural network approach product network could actually combined deep latent variable model observation posterior could powerful combination like model better known iclr probabilistic modeling community know enough make relevant reviewer expert spns however seems simple effective algorithm online structure induction scalability aspect something important much recent work learning representation think good enough publication although prefer many addition clearly bridge literature deep generative modeling,5
362.json,proposes approach learning custom optimizer given class optimization problem think case training machine learning algorithm class represent like logistic regression cleverly cast reinforcement learning problem guided policy search train neural network current location history onto step direction magnitude overall think great idea nice contribution fast growing meta learning literature however think aspect could touched make stronger first thought claim train method learn regularity entire class optimization problem rather learning exploit regularity given task distinction terribly clear example learning optimizer logistic regression seem claim learning randomly sampled logistic regression problem allow learn logistic regression convinced bias randomly sampled data case instance drawn randomly multivariate gaussians random mean covariance half drawn seems optimizer trained optimize instance logistic regression given specific family training input logistic regression problem general simple experiment prove method work generally repeat existing experiment test instance drawn completely different distribution even interesting change test distribution deviate training distribution comment choice architecture used layer hidden unit softplus activation specifically unit layer relus presumably prevent overfitting given limited capacity network look dimensionality input space increase beyond love kind policy network learns function using contour plot step look like random problem instance compared hand engineered optimizers overall think really interesting great methodological contribution main concern oversold problem still relatively simple constrained however demonstrate approach produce robust policy general problem truly spectacular minor note section using denote optimal policy currently problem considered noiseless state transition given action deterministic interesting noisy problem,6
335.json,page plus appendix present mathematical derivation infomax actual neural population noise original bell sejnowski infomax framework considered noise case shown natural image patch mnist dataset qualitatively resemble obtained method seems like interesting potentially general approach unsupervised learning however quite long difficult follow twist turn example introduction hierarchical confusing took several iteration understand going hierarchical probably right terminology like deep hierarchy decomposing tuning curve function different part recommend condense central message important step conveyed short order complete mathematical development supplementary document also look work karklin simoncelli highly related also infomax framework noisy neural population derive cell retina show condition orientation selectivity emerges,6
636.json,present approach compensating input activation variance introduced dropout network additionally practical inference trick estimating batch normalization parameter dropout turned testing well show dropout influence input activation variance scale initial weight accordingly achieve unit variance help avoiding activation output exploding vanishing shown presented approach serf good initialization technique deep network performance slightly better existing approach limited experimental validation small difference accuracy compared existing method make difficult judge effectiveness presented approach perhaps observing statistic output activation gradient training epoch multiple experiment better support argument stability network using proposed approach might consider adding validation considering backpropagation variance multiple occasion comparison drawn batch normalization believe much weight initialization technique presented approach good initialization technique sure better existing one,5
374.json,summary proposes gating mechanism combine word embeddings character level word representation gating mechanism us feature associated word decided word representation useful fine grain gating applied part system seek solve task cloze style reading comprehension question answering twitter hashtag prediction question answering task fine grained reformulation gated attention combining document word question proposed task fine grain gating help better accuracy outperforming state method dataset performing state approach squad dataset overall judgment proposes clever fine grained extension scalar gate combining word representation clear well written cover necessary prior work compare proposed method previous similar model liked ablation study show quite clearly impact individual contribution also liked fact shallow linguistic prior knowledge tag tag frequency used clever interesting syntactic feature helpful,6
661.json,introduce semi supervised method neural network inspired label propagation method appears exactly proposed weston cite optimized objective function exactly weston possible novelty propose adjacency matrix input neural network feature show success blogcatalog dataset experiment text classification neighbor according wordvec average embedding build adjacency matrix reported accuracy convincing compared zhang reported performance last experiment semantic intent classification custom dataset neighbor also found according wordvec metric summary propose application original weston rebrands algorithm name bring scientific novelty experimental section lack existing baseline convincing,2
661.json,proposes neural graph machine add graph regularization neural network hidden representation improve network learning take graph structure account proposed however almost identical weston clarified answer question thing previous work showed graph augmented training range different type network including rnns work range problem graph help train better network layer graph well layer cnns graph augmented training work variety different kind graph however point mentioned seems simply different application graph augmented training idea observation made application think therefore proper call proposed novel name neural graph machine rather making clear empirical study proposed weston different problem acceptable,3
495.json,main contribution construction approximate piecewise smooth function multilayer neural network us layer poly hidden unit activation function either relu binary step combination well written clear argument proof easy follow question great similar without binary step unit extent find binary step unit central proof example piecewise smooth function requires least poly hidden unit shallow network,6
616.json,work propose perhaps deterministic retrieval function replace uniform sampling train data training discriminator although like basic idea experiment weak essentially quantitative real baseline small amount especially convincing qualititative honestly hard review semblance normal experimental validation note happening curve,2
616.json,proposes generates latent representation input image optimizes reconstruction loss adversarial loss nearest neighbor bank image memory framework adapted three task image painting intrinsic image decomposition figure ground layer extraction qualitative shown three task think proposed potential merit particularly like fact seems reasoning image composite matching bank image somewhat similar segmenting scene matching image composite work nip however champion overall clarity evaluation could improved detailed comment believe fatal flaw quantitative evaluation approach least comparison prior work intrinsic image decomposition sirfs maybe benchmark intrinsic image wild dataset found writing vague confusing throughout instance memory database could mean number thing seems simply image imagination also vague page database input image argument show input image input contribution listed page tightened clear relevant memory retrieval informative adversarial prior mean seems inconsistent module memory database present fully convolutional discriminator could detail possibility provide cost function,4
641.json,proposes method called wild variational inference goal obtain sample variational approximate distribution without requiring evaluate density becomes possible consider flexible family distribution apply proposed method problem optimizing hyperparamter sgld sampler experiment performed mixture gaussian distribution bayesian logistic regression task contribution seems connect previous finding svgd concept inference network hyperparameter optimization sgld considered rather simple connection extension also toyish experiment enough convince reader significance proposed particularly wondering particle based method deal multimodality simple gaussian mixture case general also method seems still require evaluate true gradient target distribution posterior distribution seems computational problem large dataset setting experiment compare method number update step considering light computation sgld update think sgld make much update unit time proposed method particularly large datasets bayesian logistic regression dimension seems also quite simple experiment considering posterior close gaussian distribution also including hamiltonian monte carlo automatic hyperparameter tuning mechanism like turn sampler interesting written unclearly especially clear exact contribution compared previous work including work main message quite simple page spent explain previous work overall like suggest significant high dimension large scale experiment improve writing,2
641.json,propose variational method based theme posterior approximation tractable density first another iclr submission amortized svgd wang innovation using sgld inference network second nip ranganath minimizing stein divergence parametric approximating family innovation defining test function rkhs obtaining analytic solution inner optimization problem methodology incremental everything section essentially motivation background related work notion wild variational approximation already defined ranganath termed variational program useful comment difference section first interesting analytically solves maximum problem faced ranganath however requires kernel certainly scale high dimension equivalent practice chosen simple test function family properly scale high dimension require deeper kernel also learning parameter easier parameterizing test function family neural network begin ranganath section introduces langevin inference network essentially chooses variational approximation evolving sequence markov transition operator salimans trouble understanding could understand mean inference network none amortized usual inference network sense parameter given output neural network simple define global parameter sgld chain used across latent variable strictly worse make inference network variational approximation used salimans using different objective train experiment limited mixture gaussians posterior bayesian logistic regression none address problem might suspect high dimensional real data lack scalability kernel comparison salimans langevin variational approximation note runtime difficulty training minor comment clear understood previous work expressive variational family inference network example argue rezende mohamed tran ranganath require handcrafted inference network however assume neural network amortized inference none even require inference network perhaps mean handcrafted posterior approximation extent true however three mentioned algorithmic nature rezende mohamed main decision choice flow length tran size variational data ranganath flow length auxiliary variable space work well different problem also true variational objective admit intractable latter consider salimans motivation could better explained perhaps could clearer mean inference network also recommend term variational inference method based class approximating family black variational inference ranganath assumes mean field family term used literature mean variational method imposes constraint class,2
354.json,much review question main thing like strengthen review larger scale evaluation discussion hyperparameters test error reported snapshot ensemble useful report statistic performance individual ensemble member comparison mean standard deviation maybe best single member error rate,6
712.json,propose recurrent neural network approach constructing stochastic volatility financial time series introduce inference network based recurrent neural network computes approximation posterior distribution latent variable given past data variational approximation used maximize marginal likelihood order learn parameter proposed method validated experiment synthetic real world time series showing outperform parametric garch model gaussian process volatility quality method proposed seems technically correct exception equation inference filtering smoothing sense posterior depends value,4
342.json,work combine variational recurrent neural network adversarial neural network handle domain adaptation time series data proposed method along several competing algorithm compared healthcare datasets constructed mimic domain adaptation setting contribution work relatively small extends vrnn adversarial training learning domain agnostic representation experimental proposed method clearly performs competing algorithm however clear advantage coming difference proposed method dann using variational little insight provided could bring difference term performance drastic difference temporal dependency captured method figure detailed comment please provide detail plotted figure projection representation learned dann dann text section suggests later case surprising regular plot vrada think dominant latent factor encoded figure table baseline quite significant difference performance testing entire target including validation test vrada hand performs almost identical setting could please offer explanation please explain figure detail interpret axis figure ax figure right plot figure extremely regular comparing one left,4
342.json,update thank comment reading still think novel enough leaving rating untouched proposes domain adaptation technique time series core approach combination variational recurrent neural network adversarial domain adaptation last time step pro consider important application domain adaptation well written relatively easy read solid empirical evaluation compare method several recent domain adaptation technique number datasets con novelty approach relatively straightforward fusion existing technique lack motivation particular combination vrnn revgrad still believe comparable obtained polishing dann carefully penalizing domain discrepancy every step additional comment convinced discussion presented section think visualization firing pattern used support efficiency proposed method figure look suspicious hardly believe could produce regular structure degenerate synthetic real world data overall solid sure iclr standard,5
529.json,us combination likelihood reward based learning learn sequence model music ability combine likelihood reward based learning long known result unification inference learning first appearing literature formalism attias fixed horizon extended toussaint storkey general horizon setting toussaint pomdps generalised kappen rawlik paper introduced basic unification additional probabilistic data driven objective combined reinforcement learning signal part unified reward likelihood hence optimal control target unification probability getting reward probability policy action known data derived distribution thereby introducing interpretation secondary objective prior alternative approach stochastic optimal control setting natural given whole principle matching control objective inference objective policy objective still still contain term approach still differ approach though discussion optimal control good think elaboration history reward augmentation work valuable allow policy method compared directly like like motivation objective sensible could made clearer unification argument us take different approach variational achieving objective another interesting point discussion choice mean policy must cover problem generation well trained often underfit resulting action course number iteration move state data unsupported part space result longer confident quickly tends fairly random approach opposed obvious implement cannot mitigate without strong signal overcome tail distribution music smaller discrete alphabet likely le problem real valued policy density exponentially decaying tail discussion light issue valuable balance thing seems critical seems clear figure reward signal needed high push signal right range altogether music setting provides reasonable demonstration augmentation sequence additional reward constraint valuable demonstrates learning signal afaics compare learning signal technique instead comparator technique reverts treating prior term rather reward term leaving question whether particularly appropriate another interesting question discussion whether music theory reward could approximated differentiable mitigating need approach,4
600.json,proposes group sparse autoencoder enforces sparsity hidden representation group wise group formed based label supervision group hidden representation used reconstruction group sparsity penalty allowing learning discriminative class specific pattern dataset also propose combine group level individual level sparsity equation clarity group activation reconstruction true equation individual hidden representation reconstruction still using subset representation corresponding class equation miss summation wondering simple typo algorithm trainable seems group sparse whose input data feature extracted sequential cnns pretrained cnns comment follows furthermore group sparse autoencoder semi supervised method since us label information form group whereas standard sparse autoencoder fully unsupervised said surprising group sparse autoencoder learns class specific pattern whereas sparse autoencoder think fair comparison autoencoders combine classification objective function although claim learns group relevant feature figure convincing enough support claim example first contains many filter look like last column look like visual inspection observe improvement classification using proposed algorithm mnist experiment comparison baseline missing believe baseline sequential sequential sparse autoencoder addition control experiment required compare equation different value alpha beta missing reference shang discriminative training structured dictionary block orthogonal matching pursuit consider block orthgonal matching pursuit dictionary learning whose block projection matrix constructed based class label discirminative training,3
552.json,main objection work operates hypothesis becoming popular literature need gradient flow order solve long term dependency problem usual approach enforce orthogonal matrix absence nonlinearity unitary jacobians hence gradient vanish explode however hypothesis taken granted know true instead synthetic data empirical evidence strong enough convince hypothesis true issue thinking representational power restricting orthogonal matrix mean represent family function complex attractor forth forward without input eigenvalue larger also becomes really hard deal noise since attempt preserve every detail input rather every part input affect output ideally want preserve need task given limited capacity learn issue everyone focused solving preserved issue without worrying side effect like paper going jacobians eigenvalue show help realistic scenario complex datasets,3
381.json,present novel pruning filter convolutional neural network strong theoretical justification proposed method derived first order taylor expansion loss change pruning particular unit lead simple weighting unit activation gradient loss function performs better simply using activation magnitude heuristic pruning intuitively make sense like remove filter activation also filter incorrect activation value small influence target loss thoroughly investigate multiple baseline including oracle set upper bound target performance even though computationally expensive devised method seems quite elegant show generalizes well multiple task computationally feasible easy combine traditional fine tuning procedure also work clearly show trade offs increased speed decreased performance useful practical application also useful compare different baseline however method seems useful involve training network thus probably much faster suggestion maybe extended future towards also removing part filter convolution complicated need change implementation convolution operator lead speedup,8
440.json,propose novel using bayesian policy search stochastic dynamical system specifically minimize alpha divergence alpha opposed standard claim method first based system solve year benchmark problem familiar literature difficult ass claim seems technically sound feel writing could improved notation section feel dense terminology approximation introduced make hard follow writing could better structured distinguish novel contribution review prior work understand section correctly mostly review black alpha divergence minimization probably make sense move appendix nip showing promising using sghmc bayesian optimization bayesian optimization robust bayesian neural network springenberg could comment applicability stochastic gradient mcmc sgld sghmc setup comment computational complexity different approach section original data sense fair simulate data using another neural network evaluate problem,5
401.json,separate introspection neural network predict future value weight directly past history introspection network trained parameter progression collected training separate meta learning model using typical optimizer pro organization generally clear novel meta learning approach different previous learning learn approach con benefit thorough experiment neural network architecture geometry parameter space sufficiently different cnns fully connected recurrent neural network neither mnist cifar experimental section explained architectural detail mini batch size experiment included comparison different baseline optimizer adam strong addition least explain hyper parameter learning rate momentum chosen baseline method overall omission experimental detail current revision hard draw conclusive insight proposed method,6
397.json,proposes variational autoencoder discard information found irrelevant order learn interesting global representation data seen lossy compression algorithm hence name variational lossy autoencoder achieve combine vaes neural autoregressive model resulting latent variable structure powerful recurrence structure first present insightful bit back interpretation show latent code ignored also mentioned literature autoregressive part end explaining structure data latent variable used propose complementary approach force latent variable used decoder first make sure autoregressive decoder us small local receptive field latent code learn long range dependency second parametrize prior distribution latent code autoregressive also report state binarized mnist dynamical statically binarization omniglot caltech silhouette review bit back interpretation nice contribution community novel interpretation help better understand sometimes like highlight improved fine grained control kind information get included learned representation useful application instance image retrieval learned representation could used retrieve object similar shape matter texture however propose complementary class improvement lossy code explicit information placement section learning prior autoregressive flow section however never actually showed without prior pixelcnn decoder performs impact latent code prior used also clear windowaround represents subset,6
373.json,propose transfer learning variant neural based model applied bunch tagging task field multi tasking huge approach proposed seem novel term machine learning part general architecture shared amount shared layer dependent task interest novelty lie type architecture used particular setup tagging task experimental show approach seems work well much labeled data available figure table show limited improvement full scale figure debatable though seems fixed architecture size varying amount labeled data likely tuning architecture size better overall read well novelty seems limited experimental section seems disappointing,4
373.json,response well answered question thanks evaluation changed proposes hierarchical framework transfer learning sequence tagging expected help target task source task sharing many level representation possible general framework various neural model extensive solid experiment performance competitive state multiple benchmark datasets framework clear except detail training procedure need added experimental show task pair framework help resource target task improvement increase level representation shared firstly suggest term source target precisely defined current framework pair sort interchangeable either source target task especially used task sampling difference resourced thus could thought multi tasking task imbalanced resource question framework simultaneously help task pair learning generalizable representation different domain application language mostly likely help resourced come sacrifice high resourced side secondly show resourced task improved selected task pair also interesting helpful know often could happen task randomly paired chosen resource pool high resource pool often could framework help resourced moreover choice lie intuitively many level representation could shared possible implicitly assumes share help although tend believe interesting empirical comparison example could perhaps select cross domain pair pair mentioned author answer review question general think solid exploration could done direction tend accept,6
324.json,idea pruning matter great good thinking taking next level studying pruning across different layer extra point clarity description good picture even extra point actually specifying space layer mapping mathbb symbol thumb experiment well done encouraging course experiment even nicer ever case question issue proposed pruning criterion proposed pruning filter level opinion curious weight criterion compare approach compare pruning criterion better pruning random overall liked,6
324.json,proposes simple idea prune weight filter convnets order reduce flop memory consumption proposed method experimented resnets cifar imagenet pro creates structured sparsity automatically improves performance without changing underlying convolution implementation simple implement con evaluation pruning impact transfer learning generally positive work main idea almost trivial aware paper propose exactly idea show good experimental therefore inclined accept major downside evaluate impact filter pruning transfer learning example much interest task cifar even imagenet instead main interest academia industry value learned representation transferring task might expect filter pruning kind pruning harm transfer learning possible main task performance transfer learning strongly hurt missed opportunity explore direction title say vggbn model,6
332.json,hand fairly standard us deep metric learning siamese architecture connection human perception involving persistence quite interesting expert human vision comparison general induced hierarchical grouping particular seem like something interest people community experimental suite disappointed synthetic could used minimally viable real dataset aloi,6
627.json,problem understanding motivation claimed captured latent representation text image training translate better without image test time demonstrate convincingly image help mention setup strange image test time speculative comment observed gain come image model qualitative analysis convince model learned latent representation guessing gain le overfitting participation image training dataset small experiment sure fair compare model vnmt given following description section vnmt fine tuned model fine tuned vnmt explanation besides problem presentation many symbol used unnecessary example used source target section symbol used consistent manner making sometimes hard follow example section reference obtained understand mean better way present sure correct section computed sigma sigma used something like minus sign make look like ablation test similarly symbol thing explanation figure missing symbol appendix derivation,2
559.json,extension matching network vinyals nip instead using example support test method represents class mean learned embeddings training procedure experimental setting similar original matching network completely sure advantage original matching network seems dealing shot case method identical since example seen class mean embedding embedding dealing shot case original matching network compute weighted average example cost experimental reported prototypical net slightly better matching network think simple straightforward novel extension fully convinced advantage,4
735.json,considers alternate formulation kernel rank constraint incorporated regularization term objective writing clear focus keep shifting estimating causal factor nonlinear dimensionality reduction kernel posed inverse problem problem reformulation kernel us somewhat standard trick clear advantage proposed approach existing method theoretical analysis overall approach empirical comparison existing state sure mean causal factor reference abstract problem formulation page without definition discussion kpca sure interested step outlined page finding image outline disadvantage existing kpca approach first dimensional manifold assumption holding exactly received lot attention machine learning literature common assume data lie near dimensional manifold rather dimensional manifold second disadvantage somewhat unclear finding data point image corresponding projection input space standard step kpca page never define mathcal time mathcal time mathcal time clearly cannot cartesian product assume notation somehow implies tuples page section mathcal mathcal set mean mathcal mathcal page mathcal never defined experiment none standard algorithm matrix completion optspace considered experiment comparison alternate existing approach rigid structure motion proof main result theorem using holder inequality stated term involves fourth power weight equal using orthonormal constraint useful give detail argument go point,2
365.json,method proposes compress weight matrix deep network using density diversity penalty together computing trick sorting weight make computation affordable strategy tying weight density diversity penalty consists added cost corresponding norm weight density norm pairwise difference layer regularly frequent value weight matrix zero encourage sparsity weight collapse value diversity penalty tied together updated using averaged gradient training process alternate training density diversity penalty untied weight training without penalty tied weight experiment datasets mnist vision timit speech show method achieves good compression rate without loss performance presented clearly present interesting idea seems state compression approach open many avenue research strategy weight tying great interest outside compression domain learn regularity data result table confusing unfortunately minor issue english mistake network consist convolutional layer table confusing compared baseline method seems perform worse table overall table overall table overall le sparse diverse baseline suggest worse compression rate inconsistent text say similar better assume sparsity value inverted fact report number modal value fraction total,8
365.json,work introduces number technique compress fully connected neural network maintaining similar performance including density diversity penalty associated training algorithm core technique explicitly penalize overall magnitude weight well diversity weight approach sparse weight matrix comprised relatively unique value despite introducing efficient mean computing gradient respect diversity penalty still find necessary apply penalty probability mini batch approach achieves impressive compression fully connected layer relatively little loss accuracy wonder cost sort weight even mini batch might make method intractable larger network perhaps sparsity could help remove cost think biggest fault number different thing going approach well explored independently sparse initialization weight tying probabilistic application density diversity penalty setting mode alternating schedule weight tied standard training diversity penalty training provide enough discussion relative importance part furthermore quantitative metric shown compression rate function sparsity diversity cannot compared really like component algorithm affect diversity sparsity overall compression quick verification section claim density diversity penalty applied fixed probability batch implies structured phase alternating application density diversity weight tied standard cross entropy scheme applying density diversity penalty probabilistically density diversity phase preliminary rating think interesting lack sufficient empirical evaluation many component result algorithm appearance collection trick result good performance without fully explaining effective minor note please resize equation within margin resizebox columnwidth blah work well latex,5
467.json,parallel work idea using auto encoder provide extra information discriminator approach seems promising reported result feature learning part bigan still space improve compare standard supervised convnet,6
575.json,propose combine objective downstream loss really nice natural idea however execution presentation leave desired current version clear overall objective asked review question answer fully clarify objective final layer objective including constraint interpolation objective saying layer objective cosine distance squared cosine distance really mean minimizing distance matched pair view course work without intervening layer could minimize setting projection single point better comparison contrastive loss like hermann blunsom mentioned reviewer question aim minimize distance matched pair separate mismatched one mismatched one uniformly drawn picked cleverer discriminative layer objective tailored downstream task could make sense loose terminology refer correlation cross correlation vector correlation normally applies scalar need define mean cross correlation typically refers time series taking matrix finally sure approach fully differentiable regular perhaps worth revisiting term well also small note relationship cosine distance correlation related view dimension vector sample single random variable case cosine distance mean normalized vector correlation corresponding random variable viewing dimension vector random variable fear claim cosine distance correlation herring couple typo prosed proposed allong along,2
426.json,discus aligning word vector across language embeddings learned independently monolingual setting reasonable scenario strategy could come helpful feel address interesting problem mostly well executed somewhat lack evaluation nice stronger downstream task attempted inverted softmax idea nice minor issue ought addressed published version mention haghighi learning bilingual lexicon monolingual corpus strike piece prior work regarding learning bilingual alignment link work ought discussed likewise hermann blunsom multilingual distributed representation without word alignment probably correct cite learning multilingual word embeddings multilingual aligned data nicer experiment performed language pair rather romance language argumentation around orthogonality requirement feel related idea using mahalanobis distance covar matrix learn mapping might worth including discussion better suggestion alternative using term translation performance discussing word alignment across language translation implies something complex mind mikolov citation abstract messed,6
308.json,strong submission regarding important recently introduced method neural network generative adversarial network analyze theoretically convergence gans discus stability gans important best knowledge first theoretical paper gans contrary submission field actually provides deep theoretical insight architecture stability issue regarding gans extremely important since first proposed version gans architecture unstable work well practice theorem novel introduces mathematical technique interesting technical question regarding proof theorem pretty minor,9
685.json,proposes extension neural network language model better handle large vocabulary main idea obtain word embeddings combining character level embeddings convolutional network compare word embeddings character embeddings well combined character word embeddings quite obvious embeddings used input tricky output layer propose handle problem allows speed training impact inference testing full softmax output layer must calculated normalized costly clear network used testing open vocabulary since used reranking unnormalized probability requested word could obtained output however reranking best list feature different sentence compared wonder whether work well without proper normalization addition provide perplexity table figure need normalization clear performed mention output vocabulary doubt softmax calculated value please explain evaluated reranking best list system iwslt task abstract mention gain bleu agree claim vanilla word based well known achieves already gain bleu therefore proposed brings additional improvement bleu statistically significant conjecture similar variation could obtained training several model different initialization unfortunately model character representation output work well already several work form character level representation input could please discus computational complexity training inference minor comment figure caption figure misleading format citation unusual subword unit botha blunsom subword unit botha blunsom,2
685.json,submission interesting approach character based language modeling pursued retains word level representation context optionally also output however approach cited submission well jozefowicz jozefowicz already beyond submission applying approach using rnns lstms also jozefowicz provide comparative discussion different approach character level modeling missing least discussing existing work remaining novelty approach application machine translation although remains somewhat unclear inhowfar reranking best list handle problem translation related part problem elaborated said claim submission seems somewhat exaggerated like statement making notion vocabulary obsolete whereas express doubt concerning interpretation perplexity explicit output vocabulary example modeling especially frequent word form still expected contribute shown arxiv claim objective requires finite vocabulary statement correct unit considered limited full word form however using subwords even individual character implicitly larger even infinite vocabulary covered likelihood criterion even though require different proposed corresponding statement qualified respect character embeddings used output clarified description explicit enough view concerning configuration desirable better idea arrived specific configuration parameterization described might want mention came similar conclusion performance using character embeddings output discus suggestion possible improvement given therein way calculate interpret perplexity unknown word shaik iwslt table size full training vocabulary provided minor comment bottom three different input layer three different input layer plural font within figure small first item note denote parameter estimation parameter estimation parameter estimation first paragraph factored factored second paragraph best list best list best list best list last sentence despite adaptive gradient verb article missing,3
390.json,introduces approach reinforcement learning control wherein rather training single controller perform task metacontroller access base level controller number accessory expert utilized metacontroller decide many time call controller expert expert invoke iteration controller special addition provided current state given summary history previous call previous expert sequence control expert advice embedded fixed size vector lstm method tested body control task shown benefit multiple iteration pondering even simple expert metacontroller deliver accuracy computational cost benefit fixed iteration control general well written reasonably easy follow note topic metareasoning studied extent differentiable fully trainable component within system appears stage difficult evaluate impact kind approach overall architecture intriguing probably merit publication whether scale domain remains subject future work experimental validation interesting well carried remains limited scope moreover given complex architecture discussion training difficulty convergence issue specific comment question suggestion figure meaning graphical language explained instance arrow different thickness line style mean different thing figure caption better explain content figure example colour different line refer also dot error bar given explained bottom part make understanding figure difficult figure shaded area represents confidence interval regression line addition helpful give standard error regression slope verify excludes zero slope significant well fraction explained variance figure fraction sample using expert appear decrease monotonically increasing cost expert bottom left part right plot shaded box lot variance fraction experiment experiment supplementary material helpful thank detail,6
543.json,present javascript framework including webcl component training deploying deep neural network show possible reach competitive speed technology even higher speed compiled application viennacl gpus remaining little factor three slower compiled high performance software nvidia gpus offer compelling possibility easily deployable training application setting deep learning main point criticism different batch size used even technical limit javascript library fair smaller batch size framework well gpus probably favor presented framework include information graph especially stated question include node value possible application server many performance client setting dedicated high performance server quite likely even good value compare sake consistency please include subfigure firefox chrome node apart point well written understandable conclusive,6
406.json,explores ensemble optimisation context policy gradient training ensemble training hanging fruit many year space finally touch interesting subject well written accessible particular question posed section well posed interesting said weak point obviously particular choice domain parameter eagerly look forward journal version experiment repeated sort source domain target domain parameter combination,7
514.json,pro part address industrially important topic namely make deep network work properly point cloud many potential application invariant permutation point within cloud well rigid transformation cloud depends application propose formalism dealing composition different kind invariance con explanation generalization really hard follow stronger le broad went depth permutation invariance case easy come network structure permutation invariant seems author tried network family different point cloud size couple option number parameter averaging dropout dropout unless space completely systematically explored much reason practitioner proposed structure random structure cook also permutation invariant using layer shared point instead three invariant layer seems simpler general also permutation invariant clear valuable author definition minimally invariant sufficiently large composition invariant layer universal approximator permutation invariant function concerned proposed invariant layer might strongly variant spatial transformation well vulnerable large outlier particular term subtracts corner cloud bounding operator inside first layer cloud go learned affine transform pixelwise nonlinearity seems like could saturate whole network reviewing confidence chance formalism first part valuable realize fully understood,5
514.json,discus way enforce invariance neural network using weight sharing formalize feature function invariant collection relation main invariance studied invariant function used anomaly detection setting point cloud classification problem invariance high level important issue course since want spend parameter spurious ordering relationship potentially quite wasteful like formalization invariance presented however weakness feel prevent strong submission first exposition abstract could really running concrete example starting beginning second invariance main type invariance studied defined author formalization invariance never explicitly related might think invariance permutation input output dimension explicitly defining invariance relating structural invariance formulation better explain thing never made clear example figure data structure like discussion compositionality structure question resulting compositional structure still valid structure ignored kind compositionality important neural network specifically relating proposed notion invariance function composition seems important condition composition invariant function remain invariant clear layer invariance network make entire network invariant example look anomaly detection network example clear final predictor invariant sense regarding experiment baseline presented anomaly detection baseline presented point cloud classification problem proposed best addressed know enough dataset whether exactly fair comparison also never really made clear invariance desirable property point cloud classification setting suggestion network us fully connected layer us data augmentation enforce invariance also classical kernel random thing example case left right symmetry parameter shared within relation vague undefined convolution called convolution appendix convolutional relationship symmetric function theory,4
514.json,review informed guess unfortunately cannot ass lack understanding spent several hour trying read possible follow partially limitation also think overly abstract level presentation clearly written bourbaki book clearly written prefer leave accept reject decision reviewer better understanding even made serious mistake able tell proposal positive apparently clearly written empirical evaluation quite promising effort needed order address broader audience could potentially interested topic therefore like provide feedback level presentation main source problem ground abstract formalism concrete example example show revelation rather explaining connect previous concept example could unlock people understanding convolution inner product operation connect setting described know convolution tied space time understood equivariant operation shifting signal shift output explained pair used order build relation structure define invariance relate setting going set relation function operator shift invariant operator convolution involves many step hand holding needed convolution associated relation relation referring input given coordinate contribution output offset offset case backward arrow center node node arrow across node cardinal cartesian convolution signal processing term clearly standard term talking separable filter square symbol figure horizontal vertical graph standing relationship realize question seem trivial left homework reader think part publishing part homework reader becomes easy idea clearly target general case spending time explain particular case instance general case good space propose explain simplest possible example convolving signal filter convolution filter parameter show function well spatial invariance equivariance convolution reflected,6
451.json,incremental result several related mentioned already published claim technical assumption previous paper propose significantly weaker also quite technical main theoretical result theorem convincing furthermore badly written theoretical intuition given experimental section weak place formatting wrong,1
451.json,study energy landscape loss function neural network generally clearly written nicely provides intuition main contribution show level set loss becomes connected network overparameterized also quantifies degree disconnectedness possible term increase loss must allow find connected path seem might implication likelihood escaping local minimum stochastic gradient descent also present simple algorithm finding geodesic path network loss decreasing along path using show loss seems become nonconvex loss smaller also quite interesting work significant limitation surprising given difficulty fully analyzing network loss function however quite clear limitation especially include analyzing deep network analyzing oracle loss empirical loss also appreciated little practical discussion bound theorem hard tell whether bound tight enough practically relevant,6
781.json,address problem learning compact binary data representation hard time understanding setting writing making easier example find simple explanation problem familiar line research read response provided reviewer question read still fully understand setting thus really evaluate contribution work related work section exist instead analysis literature somehow scattered across derivation provided statement often miss reference one fourth paragraph section make conclude still requires significant work published,2
781.json,proposes kind expert sparse subset reliable expert chosen instead usual logarithmic opinion pool find unclear tried find proper definition joint could extract text proposed like algorithm also follow directly definition point definition even exists word objective function iterates proposed algorithm guaranteed improve train data also note product unifac model hinton try something similar subset expert activated generate input,2
502.json,propose evaluation metric dialogue system show higher correlation human annotation agree based metric like bleu simple capture enough semantic information metric proposed seems compliciated explain hand could also equation retrieval based dialogue system suggested basically train dialogue evaluate another high level question trust question also relevant last item detail comment detail comment justify captured evaluated metric term bleu know actually capture gram overlap guess hard captured true also difficult answer question like data dependence problem build incrementally shown equation metric us context reference compute score possible show score function using reference guarantee metric information source bleu rouge another question equation possible design metric nonlinear function since tell comparison bleu rouge metric figure much like comparison exponential scale linear scale found reason section convincing together based reason like correlation average score reasonable show without averaging table look like metric favor short response true metric basically opposite bleu since bleu panelize short sentence hand human annotator also tends give short respones high score since long sentence higher chance contain irrelevant word eliminate length factor annotation otherwise surprise correlation,3
555.json,topic interesting convincing specifically experiment part weak study include datasets familiar community well one often addressed deep learning comparison approach comprehensive,3
607.json,proposes attention based approach video description approach us three lstms attention mechanism sequentially predict word sequence frame lstm encoder frame first attention approach predicts spatial attention frame computes weighted average second lstm predicts attention hidden state encoder lstm third lstm temporally parallel second lstm generates sentence word time strength work relevant interesting problem using layer attention proposed knowledge used video description exact architecture thus novel work claim much without sufficient attribution blow experiment evaluated datasets msvd charade showing performance level related work msvd improvement charade weakness claim contribution novelty seem hold main contribution hierarchical attention memory clear presented significantly different presented attends spatial image location attend frame attends encoded video representation slight difference might lstm generate us additional lstm decoding state section propose memorize previous attention however consist last hidden state furthermore access attention alpha also discussed comment others remains unclear discussion comment claim attention function current time step also function previous attention network state true dependency true also lstm however access previous network state consist last hidden state well least formula figure suggests claim multi layer attention however remains unclear multi layer come state section feature tend discard level information useful modeling motion video ballas suggests approach follows attack problem however cannot motion attention frame available predicting next frame also clear capture anything level operates rather high level conv feature related work difference made clear paper cited section conceptual limitation independent attention mechanism spatial temporal spatial within frame independent sentence generation thus cannot attend different aspect frame different word make sense sentence jump trampoline focus saying trampoline saying trampoline however spatial attention fixed difficult also encoder explicitly look different aspect frame encoding might likely stuck always predict spatial attention frame might always attend move around never scene contradicts exactly receiving previous word input suggests softmax case emphasize text unusual common ground truth previous word training suggests hardmax highest predicted previous word encoded vector test time clarity helpful notation used different notation required helpful could contain detail additional figure corresponding part added space problem well known equation lstm softmax likelihood loss could omitted inlined evaluation claim proposed architecture outperforms previously proposed method lead state msvd dataset clearly wrong even given feature representation table achieve higher meteor strong claim also expect outperforms previous independent feature used also wrong achieve higher performance compared metric charade dataset claim also bold hardly method evaluated dataset least ablation reported table also reported charade dataset make dataset stronger claim missing qualitative attention show qualitative attention attention mechanism understand anything sensible happening diverse spatial temporal attention peaky rather uniform performance improvement significant ablation improvement meteor blue performance drop cider missing human evaluation disagree human evaluation feasible evaluation subset test data difficult even provide code typically happy share predicted sentence sufficient even better human evaluation explicitly mention share sentence seems clearly wrong ablation sentence available several comment raised reviewer others incorporated revised version still clear explanation given including spice evaluation making fix seems trivial hyperparameters inconsistent hyperparemters inconsistent ablation analysis frame sampled performance comparison frame selected validation performance ablation frame minor discussion point equation happens lstm formula provided handle input concatenated section state section proposed architecture alone learn representation video temporal structure video sequence also representation effectively visual space language space however seems true also many approach venugopalan iccv summary make strong claim approach approach lack novelty convincing related work ablation furthermore improved clarity visualization attention benefit,3
715.json,proposes pruning method reduce computation deep neural network particular whole feature map kernel connection removed much decrease classification accuracy however also following problem method somehow trivial since pruning mask mainly chosen simple random sampling novelty scalability limited experiment mainly focused classification rate ideal complexity improving computation efficiency include practical time consumption common reducing number operation lead reduced computational time highly parallel platform important improve computational efficiency large scale model imagenet classification network small model mnist cifar network however large scale network missing logical validity proposed method feature pruning train reduced size network trained scratch without transfer knowledge pretrained large network possible accuracy simply indicate hyper parameter optimal original network experimental necessary clarify necessity feature pruning note agree smaller network generalizable larger network comment response thanks replying comment still believe proposed method trivial nice show implementation compared existing toolbox torch caffe tensorflow implementation convolution efficient enough experiment cifar helpful better cifar really large scale speed critical imagenet place datasets example large scale datasets author reply question validity proposed method question critical,3
715.json,summary many different pruning technique reduce memory footprint model technique different granularity layer map kernel intra kernel pruning ratio sparsity representation work proposes method choose best pruning mask many trial tested cifar svhn mnist pro proposes method choose pruning mask trial analysis different pruning method con question proposed strategy selects best pruned network random pruning trial approach enables select pruning mask shot simpler multi step technique best pruning mask shot random pruning trial answered missing test approach bigger like alexnet googlenet resnet extended since reducing size embedded system final goal showing much memory space saved proposed technique compared approach like good misc typo figure caption featuer corrected,5
345.json,proposes empirical bayesian approach learn parameter neural network prior mixture prior weight lead clustering effect weight posterior distribution approximated delta peak clustering effect exploited parameter quantisation compression network parameter show lead compression rate predictive accuracy comparable related approach earlier work based three stage process pruning small magnitude weight clustering remaining one updating cluster centre optimise performance current work provides principled approach multi stage structure single iterative optimisation process first experiment described section show empirical bayes approach without hyper prior already lead pronounced clustering effect setting many weight zero particular compression rate obtained lenet section text refers figure suppose figure section describes experiment hyper prior used parameter distribution well hyper parameter learning rate optimised using spearmint snoek figure show performance different point hyper parameter space evaluated trained network give accuracy compressionrate point graph text claim best line seems little opportunistic interpretation given limited data moreover useful small discussion whether linear relationship expected currently experiment lack interpretation section describes obtained model compare recent comparable obtained term compression rate accuracy state current algorithm slow useful larger model briefly report obtained compare related work useful explain slows training respect standard training without weight clustering approach proposed algorithm scale term relevant quantity data contribution mostly experimental leveraging fairly standard idea empirical bayesian learning introduce weight clustering effect training said interesting result relatively straightforward approach lead state network compression technique could improved clearly describing algorithm used training scale large network datasets another point deserve discussion hyper parameter search performed using test data assume compared method dealt search hyper parameter determine accuracy compression tradeoff ideally think method evaluated across different point trade,6
345.json,revives classic idea involving regularization purpose compression modern model resource constrained device compression midst lot people rediscovering idea area nice explicitly draw upon classic approach early obtain competitive standard benchmark much study instance simple idea applied effectively important problem written illuminating manner appropriate reference classic approach addition filter visualization enhances contribution,6
596.json,proposed implicit resonet knowledge base completion proposed performs inference implicitly search controller shared memory proposed approach demonstrates promising benchmark dataset pro proposed approach demonstrates strong performance dataset idea using shared memory knowledge base completion interesting proposed approach general applied various task con qualitative analysis hard proposed approach work knowledge base completion task introduction section improved specifically motivate shared memory introduction different existing method using unshared memory knowledge base completion similarly function search controller unclear introduction section unclear search mean content knowledge base completion concept shared memory search controller make sense reading section,5
596.json,summary proposes knowledge base completion highlight adopting implicit shared memory make assumption structure completely learned training modeling multi step search process decide terminate experimental seem pretty good also perform analysis shortest path synthetic task demonstrate better standard seqseq well written easy follow major comment actually like idea also impressed work well main concern present little analysis work whether sensitive hyper parameter besides reporting final hyper parameter believe size shared memory using experiment think number fixed task least depend scale could verify experiment even possible make memory structure dynamic size setting stochastic search process also highlight could demonstrate much really help think necessary compare following remove termination gate number inference step well also show performance varies step appreciate attempt shortest path synthetic task however think much better demonstrate real setting still perform shortest path analysis using freebase entity relation minor comment afraid output gate illustrated figure confusing output depending search process terminated,5
353.json,combine hierarchical variational autoencoder pixelcnns distribution natural image report good although state likelihood natural image briefly start explore information encoded latent representation hierarchical believe combining pixelcnn already suggested pixelcnn important interesting contribution encoding high level variation different latent stage interesting seems terribly surprising since size image region latent variable also corresponding scale showing pixelcnn improves latent representation regard interesting task much stronger result also claim combining pixelcnn reduces number computationally expensive autoregressive layer remains unclear much efficient whole pixelcnn comparable likelihood general find clarity presentation wanting example agree reviewer exact structure remains unclear difficult reproduce,5
754.json,us pointer network sparse window identifier improve code suggestion dynamically typed language code suggestion seems area attention pointer truly show advantage capturing long term dependency sparse pointer method seem provide better attention similar window size specifically comparing window size attention sparse pointer method show sparse pointer winning fairly definitively across board given major advantage pointer method able large window size well thanks supervision pointer provides unfortunate though understandable potential memory issue larger window size different batch size sparse pointer attention model unfortunate given complicates otherwise straight comparison model construction filtering python corpus sound promising still inaccessible listed todo given code suggestion seems interesting area future long term dependency work promising avenue future task exploration overall dataset likely interesting contribution even though potential issue,5
493.json,proposes bound misclassification error bound lead training classifier adaptive loss function algorithm operates successive step parameter trained minimizing loss weighted probability observed class given parameter previous step bound improves standard likelihood outlier underfitting prevents learning algorithm properly optimize true classification error experiment performed confirm therotical intuition motivation show different case algorithm lead improved classification error underfitting occurs using standard loss case bound lead improvement loss sufficient dataset also discus relationship proposed idea reinforcement learning well classifier uncertain label easy read well written overall second read found difficult fully understand problem somewhat mixed together considering binary classification simplicity optimization classification error randomized classifier predicts probability theta optimization deterministic classifier predicts sign theta robust outlier underfitting reason confused standard approach supervised classification mentioned abstract deterministic classifier test time loss constant upper bound classification error deterministic classifier however bound discussed concern randomized classifier question experiment kind classifier used randomized sentence first page suggest assuming class chosen according standard deterministic classifier theta case either deal learning randomized classifier case compare performance deterministic counterpart people practice make sense soon accept optimization criterion good surrogate case think write made clearer case algorithm minimize upper bound classification error case done correspond usually done binary classification comment section allowing uncertainty decision improved adding reference bartlett wegkamp classification reject option using hinge loss sayedi trading mistake know prediction seems sign missing theta theta lambda section idea presented interesting original give relatively score willing increase score clarification made final comment think clear enough current form even though still improvement justification extent error randomized classifier good surrogate error true classifier smoothed version loss acceptable explanation standard classification setup le clear section dealing additional uncertain label increase score,5
755.json,study optimization issue linear resnet show mathematically shortcut zero initialization hessian condition number independent depth skimmed proof checked carefully result nice observation training deep linear network think fully resolved linear nonlinear issue question though revision added using relu unit seems added position network typically done resnet moreover relu differentiable zero point satisfy condition theorem differentiable activation like sigmoid tanh equation appendix seems nonlinear activation condition number depends derivative sigma prime therefore tanh derivative zero condition number linear tanh activation probably enough explain difference performance optimization linear nonlinear network situation evolve learning point success resnet convnets general computer vision believe type nonlinearity pooling result generalizes pooling well minor last paragraph approximation error typically mean powerful class better training error necessarily better test error mean zero initialization small random perturbation exactly zero initialization large random perturbation,4
755.json,resnet architecture shortcut shown empirical success several domain therefore studying optimization architecture valuable attempt address property network shortcut experiment interesting however main issue current linear linear think studying linear network valuable careful extend network linear activation without enough evidence especially true hessian hessian linear network large condition number iclr submission singularity hessian deep learning even case optimization challenging therefore agree claim linear network moreover plot mnist enough claim linear network behave similar linear network hessian zero initial point explanation interested hessain zero initial point acceptable zero initial point interesting particular point cannot tell hessian optimization,3
305.json,nice demonstrates trained image compression decompression system achieves better rate quality trade offs established image compression algorithm like jpeg addition showing efficacy learning application contribution introduction differentiable version rate function show used effective training different rate distortion trade offs expect impact beyond compression application task might benefit differentiable approximation similar function provided thoughtful response review question still argue minimize distortion fixed range quantization sufficiently complex network learn automatically produce code within fixed range highest possible entropy meet upper bound second argument convincing force specific form compressor output used match effective compression current system require complex network able carry computation currently done separate variable rate encoder used store,7
647.json,proposes rim variational inference procedure author claim novelty lie separation inference procedure making inference approach effectiveness shown image restoration experiment unrolling inference author raise interesting perspective towards free configuration inference separable learnt jointly however quite agree argument regarding although defined inference problem necessarily separate step required fact either defined prior explicit prior evaluation step shown believe implementation follows procedure proposed could explained whole inference procedure eventually becomes learnable neural network energy implicitly defined learning parameter moreover block architecture linearity tanh restrict flexibility implicitly form inherent family variational energy inference algorithm also similar based fact similar feeling novelty somewhat limited also discussion added term architecture nonlinearity chosen,4
651.json,proposed regularizer seems particular combination existing method though implied connection nonlinearities stochastic regularizers intriguing opinion empirical performance exceed performance achieved similar method large enough margin arrive meaningful conclusion,3
651.json,method proposed essential train neural network without traditional nonlinearity using multiplicative gating gaussian evaluated preactivation motivated relaxation probit bernoulli stochastic gate experiment performed work somewhat novel interesting little said preferable similar parameterizations sigmoidal softsign stronger empirical interrogation work exploration nearby conceptual space cifar look okay today standard mnist quite neural net better decade relu baseline timit frame classification also interesting without evaluating word error rate within speech pipeline minor point idea forth network without additional nonlinearities comparable linear function rather misleading expectation nonlinear function input varying input example multiplying adding constant linearly reflected expected output network sense nonlinear relu network least locally linear plot difficult read grayscale,4
485.json,present analysis ability deep network relu function represent particular type dimensional manifold specifically focus call monotonic chain linear segment essentially set intersecting tangent plane present construction efficiently model manifold deep present basic error analysis resulting construction presented novel best knowledge hardly surprising given already know representational power deep network given study selects deep network architecture data structure compatible particular three main concern respect presented last decade quite work learning data representation set local tangent plane example spring mind local tangent space analysis zhang manifold charting brand alignment local model verbeek roweis vlassis none work referred related work even though seems highly relevant analysis presented instance interesting technique compare deep network trained produce embedding figure provide insight inductive bias deep introduces learn better representation parametric technique better inductive bias learn worse representation loss optimized convex difficult analysis generalizes complex data local linearity assumption data manifold vacuous given sparsity data high dimensional space generalizes deep network architecture pure relu network instance modern network variant batch normalization already appears break presented analysis error bound presented section appears vacuous practical setting upper bound error exponential total curvature quantity quite large practical setting underlined analysis swiss roll dataset state bound case loose fact bound already loose arguably simple manifold make error analysis tell little representational power deep net encourage address issue revision issue harder address essential addressed line work pioneered impact understanding deep learning minor comment prior work refer fully supervised siamese network approach approach differ taken approach unsupervised noted first study unsupervised representation learner parametrized deep network important example deep autoencoders hinton salakhutdinov work denoising autoencoders bengio group parametric maaten loss experiment using difference ground truth distance distance computed network seems network produce infinitely large distance loss minus infinity difference squared,5
485.json,summary discus data special type dimensional structure monotonic chain efficiently represented term neural network hidden layer pro interesting easy follow view capability neural network highlighting dimensionality reduction aspect pointing possible direction investigation con present construction illustrating certain structure captured network address learning problem although present experiment structure emerge le comment interesting study ramification presented observation case deep network also study extent proposed picture describes totality function representable network minor comment figure could referenced first text color coded color code thank thinking revising point first question note isometry manifold page mention orthogonal projection realized network page divided segment segment maybe best word page mean relative error baseline number mean,6
606.json,present framework formulate data structure learnable interesting novel approach could generalize well interesting datastructures algorithm current state revision strong weakness remaining analysis related work experimental evidence reviewer detailed related work already especially deepmind affiliated presented interesting highly related neural touring machine following work course hard make direct comparison experimental section complexity implementation least important mention compare work conceptually experimental section show mostly qualitative fully conclusively treat topic suggestion improvement highly interesting learn accuracy stack queue structure increasing number element store queue stack used arbitrary situation push operation occuring even though trained solely consecutive push consecutive pop enhanced setting diverge point encoded element mnist even though binary space element element hence encoded efficiently parsing cnns quite well learning performance expected strongly degrade learn stack number case optimal parser loss le encoding argue direction experiment needed increasing number stack queue element experimenting mnist parsing front actual stack queue network could help strengthening falsifying claim claim mental representation little support throughout indication correspondence mental model could found allow hold claim otherwise remove focus aspect maybe mention mental model motivation,3
411.json,work build stoke schkufza superoptimization engine program binary work starting existing program proposing modification according proposal distribution proposal accepted according metropolis criterion acceptance criterion take account correctness program performance program thus mcmc process likely converge correct program high performance typically proposal distribution fixed contribution work learn proposal distribution function feature program word opcodes program experiment compare baseline uniform proposal distribution baseline learns weight proposal distribution without conditioning feature program evaluation show proposed method slightly better performance compared baseline significance work iclr seems quite progress learning representation straightforward application neural network reinforce another task differentiable component task superoptimization significant interest iclr reader attendee conference like aaai seem better work proposed method seemingly novel typical mcmc based synthesis method lacking learning component however make work compelling consider demonstrating proposed method synthesis task even generally task mcmc used learnt proposal distribution beneficial superoptimization alone small improvement baseline compelling enough also clear significant representation learning going since feature used represent program neural network cannot possibly learn anything correlation presence opcodes good move cannot possibly understand program semantics interesting contribution used tree lstm attempt learn semantics program quite naive method learning make favorable candidate acceptance,5
692.json,proposes enhance attention mechanism sentiment classification using global context computed lstm proposed model outperform many existing model literature sentiment analysis datasets idea using lstm compute global context attention actually novel proposed several time literature luong shen especially luong already proposed combine global context local context attention regarding experiment course nice work well without need trick like dropout trained word embeddings however even better work well using trick show model using trick compare literature luong effective approach attention based neural machine translation emnlp,2
368.json,summary describes estimate likelihood currently popular decoder based generative model using annealed importance sampling validates method using bidirectional monte carlo example mnist compare performance gans vaes review although seems like fairly straight forward application correct missed important trick make work much appreciate educational value empirical contribution lead clarity debate around density estimation performance gans enable people space permitting might good idea expand description component mentioned basic description algorithm given explain well algorithm work initially confused widely different number figure first glance expectation figure comparing gmmn labeling bottom leading word caption description perhaps mention caption continuous mnist us discrete mnist gmmn probably gmmn using reconstruction evaluation model necessary sufficient condition good depending likelihood posterior sample might density prior example great could point discus limitation test minor perhaps reference mackay density network mackay decoder based generative model section write prior drastically different true posterior especially high dimension think flow could improved especially people le familiar importance sampling think relevance posterior importance sampling clear point section claim often meaningful estimate space underflow problem meaningful seems like wrong word perhaps revise practical estimate underflow problem meaningful estimate connection compression surprise entropy,6
368.json,describes method evaluate generative model gmmn much needed community still eyeball generated image judge quality however technical increment nip measuring reliability mcmc inference bidirectional monte carlo small nonexistent please correct wrong grosse relative contribution application method generative model section seem make mistake write think mean also value normalized true value anyways think typo equation could precise page page said procedure initialized instead however unclear value picked perhaps confused term overfitting bottom overfit relative another test accuracy higher even though train test accuracy also higher think perhaps last sentence page underfits le gmmn experimental interesting expose fact gans gmmns seem much lover test accuracy despite fact sample look great,5
387.json,proposes modification parametric texture synthesis gatys take account long range correlation texture gram matrix spatially shifted feature vector synthesis loss synthesised texture visually superior original gatys method particular texture structured long range correlation brick well written method intuition clearly exposed perform quite wide range synthesis experiment different texture concern true method including gatys variability sample clearly global minimum proposed objective original image issue partially circumvented performing inpainting experiment synthesised path need stay coherent border additional insight problem plus work simple nice modification gatys worth publishing constitute major breakthrough,6
503.json,proposes gated muiltimodal unit building block connectionist model capable handling multiple modality figure bimodal case return weighted activation gain gating unit anything special keep multi modal case weighted well equation section look like multi modal case also rationale using tanh nonlinearity relu somehow experimentally optimised choice find interesting discussion possibility handling missing data case modality unavailable test time possible current back fewer modality synthetic example suggest fact possible number perhaps could added table synthetic experiment compare fully connected really similar complexity least hidden unit modality followed logistic regression least term capability drawing decision boundary comparable think broader discussion shall written related work associated mixture expert model fact similar conceptually well multiplicative model also gating unit lstm principle play similar role multiple modality spliced input overall interesting associated released dataset minor comment typo layer section layer apology unacceptably late review multiplicative lstm sequence modelling krause murray renals,6
503.json,proposed gated multimodal unit information fusion learns decide modality influence activation unit using multiplicative gate collected large genre dataset imdb showed get good performance proposed approach seems quite interesting audience expect used general scenario beyond movie genre prediction quite straightforward test algorithm application done biggest shortcoming opinion another concern lie evaluate performance information fusion abstract claim improves macro score performance single modality model respect visual textual information respectively however improvement modal complementary fusion always higher fact much better baseline proposed long list technique fusion difficult conduct impressive comparison real dataset think nice work movie dataset also expect technique including fine tuning dropout distillation help nice author could compare technique also hope could talk detail connection mixture expert model based nonlinear gated function method suffer local minimum optimization small datasets like depth discussion similarity difference gain attention encourage author open source code datasets,5
780.json,analyzes ring based allreduce approach multi data parallel training deep comment name linear pipeline somewhat confusing reader technique usually referred ring based approach allreduce literature author standard name make connection easier cost analysis ring based allreduce already provided existing literature applied analysis case multi deep training concluded scaling invariant number gpus ring based allreduce approach already supported nvidia nccl library although claim implementation come earlier nccl implementation overlap communication computation already applied technique system tensorflow schedule proposed exploit overlap partially backprop reduce note dependency pattern exploited forward layer depend update parameter layer last iteration done dependency scheduler since analysis allreduce nice include detailed analysis tree shape reduction ring based approach approach discussion approach missing current summary discussed existing allreduce technique data parallel multi training deep cost analysis based existing personally find claimed result surprising follows existing analysis allreduce analysis might help reader view baseline analysis allreduce could also improved comment,4
515.json,proposes tensor train decomposition represent full polynomial linear form reduce computation complexity inference training stochastic gradient riemann manifold proposed solve based formulation empirical experiment validate proposed method proposed approach interesting novel like vote acceptance suggestion include computational complexity iteration,6
779.json,conduct comprehensive series experiment vocabulary selection strategy reduce computational cost neural machine translation range technique investigated ranging simple method word occurences relatively complex svms experiment solid comprehensive useful practical term good best vocabulary selection method effective achieving high proportion coverage full vocabulary however feel experiment section vocabulary selection training rather limited scope liked experiment major criticism little novelty technique mostly standard method rather simple particular seems much additional material beyond work although work solid lack originality let minor comment word occurence measure smoothing used make measure robust count,4
779.json,compare several strategy guessing short list vocabulary target language neural machine translation primary finding word alignment dictionary work better variety technique take significant impact need make case might want vocabulary rather character word unit like think likely many good reason could argued synthesize morphology deal transliteration suggest particular model experiment course think useful minor contribution show word alignment good getting short list strongly make case abandon work direction minor comment addition approach modeling vocabulary discriminative word lexicon mauser neural version also worth mentioning useful know coverage rate actual full vocabulary rather full vocabulary since presumably technique could used work much larger vocabulary reducing vocabulary size training technique taking union vocabulary mini batch seems like rather strange objective vocabulary single sentence used probabilistic semantics translation still preserved since vocab vocab deterministic whereas objective longer sensible probability mini batch vocabulary case thus difficult implement seems like least sensible comparison make,3
407.json,well known soft mixture expert adapted applied specific type transfer learning problem reinforcement learning namely transfer action policy value function similar task although treated experimental setup reminiscent hierarchical work aspect consider length regrettably possible implication work architecture even learning algorithm choice could simply stated term objective target task rather hand engineered experimenter clearly interesting direction future work illuminates pro diligently explains network architecture fit various widely used reinforcement learning setup facilitate continuation work experiment good proof concept beyond even work provides convincing clue collection deep network trained entirely different task generalize better related task used together rather conventional transfer learning fine tuning con well recount related work section library fixed policy long formally proposed reuse learning similar task indeed well understood hierarchical literature beneficial reuse library fixed fernandez veloso jointly learned policy apply entire state space option pricop well understood build library convincingly shed light direction tell transfer task picked effectively illustrate potential proposed architecture tackle negative transfer compositional reuse well known challenging situation outlined previous work parisotto rusu since main contribution empirical nature curious shown figure look plotted wall clock time since relatively data efficiency limitation achieving perfect play pong mnih illuminating consider task final performance plausibly limited data availability also interesting presented achieved reduced amount computation reduced representation size compared learning scratch especially useful source task actual policy trained target task finally perhaps underwhelming take quarter data required learning pong scratch figure perfect pong policy already expert library simply evaluating expert episode using average score weighted majority vote action choice probably achieve final performance smaller fraction data,6
309.json,improving feature learning deep reinforcement learning augmenting main policy optimization problem term corresponding domain independent auxiliary task task control learning policy attempt maximally modify state space pixel immediate reward prediction value function replay except latter auxiliary task used help shape feature sharing lstm feature extraction network experiment show benefit approach atari labyrinth problem particular much better data efficiency well written idea sound pretty convincing clear acceptance high level thing none major concern believe something extra computational cost optimizing auxiliary task much lose term training speed costly component possible please make clearer abstract intro agent learning different policy task read abstract agent also maximises many pseudo reward function simultaneously reinforcement learning first understanding learned single policy optimize reward together realized mistake reaching feature control idea validated empirically preliminary experiment convincing seems help slightly initially like idea worried fact task changing learning since extracted feature modified might stability convergence issue play since mentioned performance agent still steadily improving keep going least best one auxiliary task weight parameter lambda hyperparameters optimize experiment validate using good choice please mention fact auxiliary task trained true learning since trained policy step empirical reward discussed openreview comment minor stuff policy gradient algorithm adjust policy maximise expected reward actually loss minimized lambdac within figure seem referenced text also figure referenced feature discovered manner shared shared text around refers loss term defined explicitly please explain clip mean dueling network legend figure liked ablated version atari particular pattern individual contribution labyrinth observed legend figure mentioned labyrinth clear text figure right show actually left plot figure also later shown figure figure right figure show learning curve hyperparameter setting three labyrinth navigation level think referring left middle plot figure level text might also need fixing left side show average performance curve agent three method right half show missing comma something method appendix detail included supplementary material value lambdapc guess edit know question already answered comment need answer,7
309.json,work proposes train agent also perform auxiliary task positing help model learn stronger feature propose pseudo control task control change pixel intensity control activation latent feature also propose supervised regression task predict immediate reward following sequence event latter learned offline skewed sampling experience replay buffer order balance seeing reward chance agent perform significantly well discrete action continuous space task reach baseline performance le iteration work contrast traditional passive unsupervised based learning instead forcing learn potentially useless representation input learn possibly impossible partial observability task modelling objective learning control local internal feature environment complement learning optimal control policy approach novel proposes interesting alternative unsupervised learning take advantage possibility control agent environment proposed task explained rather high level convenient understand intuition think lower level detail might useful example explicitly mentioned reaching appendix otherwise work clear easily understandable reader familiar deep methodology sound hand hand distribution best hyperparameters might different unreal also measuring ensures presuming best hyperparameters unreal within explored interval method best hyperparameters found think weakness rather considering number thing crucially needed future work little experimental analysis effect auxiliary task appart strong effect performance vein pixel feature control seems impact labyrinth beat anything else except unreal think worth looking either isolation depth measuring performance task,7
535.json,present video captioning soft hard attention using network encoder decoder experiment presented youtubetext idea image captioning soft hard attention video captioning soft attention already demonstrated previous work main contribution specific architecture attention different layer work well presented experiment clearly show benefit attention multiple layer however light previous work captioning contribution resulting insight incremental conference iclr experiment analysis main contribution strengthen recommend resubmission suitable venue,3
427.json,present theoretically well motivated visualizing part input feature responsible output decision insight feature maximally change output simultaneously unpredictable feature important one previous work focused finding feature maximally change output without accounting predictability feature build upon idea presented work robnik ikonja kononenko indicate proposed visualization mechanism based modeling conditional distribution identifies salient region compared mechanism based modeling marginal distribution like presented visualization single image across multiple network multiple class show proposed method indeed pick class discriminative feature provided link visualization random sample image comment encourage include appendix concern zeiler proposed visualization method greying small square region image similar computing visualization using marginal distribution compute marginal visualization using sample however limit infinite sample image region gray conditional distribution computed using normal distribution provides regularization therefore estimating conditional marginal distribution using sample justified like comparison grey image patch akin zeiler used visualization approach based conditional distribution,5
427.json,propose visualize area image provide mostly influence certain response mostly apply elegant convincing improvement basic method robnik sikonja konononko dnns thus improving analysis making usable image dnns provide thorough analysis method show convincing example however handpicked nice maybe least figure showing analysis random pick imagenet thing like method compare method mention introduction like gradient based one deconvolution based one clearly written necessary detail given nice read alltogether problem understanding dnns function draw conclusion discussed author method provides clear contribution lead progress field like figure showing alexnet googlenet differ collect evidence think several potential application method therefore consider high significance update great adopting suggestion therefore improve rating,8
562.json,proposes extension framework known whereby multiple generator discriminator trained parallel generator discriminator pairing shuffled according periodic schedule pro proposed approach simple easy replicate con confusing read suggestive conclusively show performance main argument lead improved convergence improved coverage mode coverage visualization suggestive still enough evidence conclude fact improving coverage convergence difficult ass effect basis learning curve proposed metric circular performance depends collection baseline compared estimating likelihood seems promising evaluate using inception score perhaps systematic determine effect grid search hyperparameters train equal number gans gans setting histogram final inception score likelihood estimate trained model help show whether tended produce better model overall approach seems promising many open question regarding current form section remark seems like section proposed metric described adequate detail,3
718.json,multiagent system proposed generalization neural network proposed system used le restrictive network structure efficiently computing necessary computation graph unfortunately find proposed system different framework artificial neural network although today neural network structure designed matrix matrix multiplication limited architecture word proposed multiagent system framed artificial neural network complicated layer connectivity structure considering neuron layer computation efficiency argued among different sparsely connected denoising autoencoder multiagent system framework baseline comparison fully connected neural network employ matrix matrix multiplication,2
466.json,investigates identity parametrization also known shortcut output layer form instead shown perform well practice resnet discussion experiment interesting comment section studying linear network interesting however clear could translate insight linear network example proved every critical point global minimum think helpful discussion relationship linear linear network section construction interesting expressive power residual network within constant factor general feedforward network need different proof given finite sample expressivity feedforward network appreciate clarify section like experiment choice random projection layer brilliant however since combined choice convolutional residual network hard reader separate affect therefore suggest reporting number convolutional residual network learned layer also resnet random projection layer minor comment agree batch normalization reduced identity transformation know bringing abstract without proper discussion good idea page assumption,5
489.json,analyzes various unsupervised sentence embedding approach mean auxiliary prediction task examining well classifier predict word order word content sentence length ass much type information captured different embedding model main focus comparison encoder decoder permutation invariant cbow also analysis skip thought vector since trained different corpus hard compare several interesting perhaps counter intuitive emerge analysis nice examining part explaining however found discussion word order experiment rather unsatisfying seems appropriate question something like well compared theoretical upper bound deduced natural language statistic investigated angle section preferred effect natural language statistic discussed front rather presented explanation urprising observation similar reaction word order experiment interesting opinion fascinating lstm encoder seem rely natural language ordering statistic seems like term parameter expressivity also think strange word content accuracy begin drop high dimensional embeddings suppose could investigated handicapping decoder overall nice investigating aspect information content stored various type sentence embeddings recommend acceptance,7
489.json,present experiment investigating kind information captured common unsupervised approach sentence representation learning trivial somewhat surprising example show possible reconstruct word order word representation show lstm sentence autoencoders encode interpretable feature even randomly permuted nonsense sentence effective unsupervised sentence representation learning important largely unsolved problem kind work seems like straightforwardly helpful towards addition experimental paradigm presented likely broadly applicable range representation learning system seem somewhat strange major technical concern think informative recommend acceptance minor flag massive drop cbow performance figure explained seem implausible enough warrant serious investigation absolutely certain appear different codebase different random seed implementing fortunately point largely orthogonal major writing comment agree word order cbow surprising think slightly misleading cbow predictive word order represent word order possible probabilistically reconstruct word order information encode saying lstm auto encoders effective encoding word order word content really make sense quantity comparable,7
523.json,introduce adaptive softmax approximation tailored faster performance gpus idea sensible class based hierarchical softmax cluster hierarchy distributed resulting matrix multiplication optimally sized computation based empirical test indicate system indeed work well term presentation found clear unclear element fortunately underlying concept logic seem quite clear unfortunately various point writing various minor typo mentioned anonreviewer addition spot notation describing recurrent network section mention surely different used previous paragraph regular feedforward think belonged equation matrix strange also section intuition cluster case good idea include helpful concept underlying complexity analysis straightforward could made clearer adding additional figure figure along well placed additional sentence unpacking logic argument easier follow step example combined analysis previous page made sense term arriving complexity putting head distribution root tree perhaps appendix might appropriate place explanation,6
671.json,present general framework defining wide variety recurrent neural network architecture including seqseq model tree structured model attention family dynamically connected architecture framework defines general purpose recurrent unit called tbru take transition system defining constraining input output input function defines mapping input fixed width vector representation recurrence function defines input recurrent step function current state cell computes output input fixed recurrent many example instantiation framework provided including sequential tagging rnns google mcparseface parser encoder decoder network tree lstms le familiar example demonstrate power framework interesting contribution work ease used incorporate dynamic recurrent connection definition transition system particular explores application dynamic connection syntactic dependency parsing standalone task multitasking parsing extractive summarization using compositional phrase representation feature parser summarization previous work used discrete parse feature particularly simple elegant framework experimental demonstrate multitasking lead accurate summarization model using framework incorporate structure existing parsing model also lead increased accuracy efficiency loss compared attention raison etre particular example perhaps described even thoroughly explicitly made clear possible soon possible important contribution get lost description presentation framework emphasizing attention seqseq represented framework distracting make seem le novel anonreviewer clearly missed point first pas idea across emphasize benefit representation love detailed analysis representation importance achieving experimental think also helpful emphasize difference stack lstm example overall think present valuable contribution though exposition could improved analysis experimental expanded,6
630.json,summary proposes read attention based representation document copy mechanism summarization task read sentence input document twice creates hierarchical representation instead bidirectional decoding us representation document obtained read mechanism point word source document abstractive summarization show improvement dataset provide analysis different configuration contribution main contribution read attention mechanism read sentence twice obtains better representation document writing text need work several typo explanation architecture really clear part feel somewhat bloated pro proposed simple extension proposed summarization better baseline con improvement large justification strong enough need better writeup several part text using clear precise language need better reorganization part text somewhat informal application oriented question training speed compared regular lstm criticism similar approach read mechanism proposed already explored context algorithmic learning consider application summarization task significant contribution justification behind read mechanism proposed weak really clear additional gating alphai needed read stage also suggest pointer mechanism unknown rare word adopted read attention mechanism however clear real gain coming whether read mechanism pointing application focused contribution term point view weak possible read mechanism task summarization order whether improvement writing need work general well written minor comment correction recommend fixing page better single value scalar gating page single value lack ability variance among dimension scalar gating capture page initial zero vector initialized zero vector beginning sequence example part refer part refer table better naming model table needed location table zaremba wojciech ilya sutskever reinforcement learning neural turing machine arxiv preprint arxiv gulcehre caglar pointing unknown word arxiv preprint arxiv,4
630.json,work explores neural model sentence summarisation using read attention copy mechanism grant ability direct copying word representation source sentence experiment demonstrate achieved better dataset overall well written confusing point claim lack evidence experimental incomplete detailed comment read attention work better vanilla attention happen read sentence multiple time compared staked lstm number parameter ablation experiment section need reading sentence gigaword dataset source compression dataset need multiple input sentence compare single sent input sent input copy mechanism multiple word appeared source sentence copied according equation copy vector decoder however kind issue hard copy mechanism besides comparison hard copy mechanism vector copy mechanism experiment section vocabulary size part main track evidence showing special property vector copy mechanism trivial experiment dataset compare date model gigaword dataset compare rush quite weak baseline model irresponsible claim achieved state performance context summarization typo table,4
519.json,show hidden state lstm normalised order preserve mean variance method gradient behaviour analysed experimental seem indicate method compare well similar approach point writing sloppy part review exhaustive list experimental show marginal improvement statistical significance impossible ass completely author fault partially rely published others weight normalisation seems viable alternative performance runtime similar implementation complexity weight norm however arguably much lower effort could clear current state practitioner well researcher effort judge whether proposed method really worth replicate section nice applaud analysis list typo maintain maintain requisits requisite lstm lstm gradient equivalent equation gradient cannot equivalent equation beacause page wrong,3
372.json,proposes memory module large scale life long shot learning module general enough apply module several neural network architecture show improvement performance using nearest neighbor memory access completely recently explored chandar nearest neighbor based memory shot learning also explored provides experimental evidence approach applied variety architecture addressed review question response willing release source code reproduce least omniglot experiment synthetic task experiment reference charles blundell benigno uria alexander pritzel avraham ruderman joel leibo jack daan wierstra demis hassabis free episodic control corr,5
667.json,address problem data sparsity healthcare domain leveraging hierarchy medical concept organized ontology focus sequential prediction given patient medical record sequence medical code might occur rarely instead simply assigning medical code independent embedding feeding proposed approach assigns node medical ontology basic embedding composes final embedding medical code taking learned weighted average attention mechanism medical code ancestor ontology notably well written approach quite intuitive following comment patient visit taken medical code found visit average learned weighted average bias number code visit basic embeddings fine tuned well find hurt performance explanation looking figure seem close figure clear figure missing also wondering significant difference nice comment finally think interesting application applying well established deep learning technique deal important issue arises applying deep learning model domain scarce data resource however like comment offer insight iclr community think iclr good avenue work,5
536.json,approximation capability neural network studied approximating different class function goal provide analog approximation theorem class noise stable function class function noise stable output significantly depend individual input seems interesting class therefore find problem definition interesting well written easy follow proof argument major comment presentation understand argument noise stability measure true dimensionality data based dependence function different dimension therefore possible restate prove analog approximation theorem based true dimensionality data also unclear stability based bound tighter dimension based bound grow exponentially find discussion interesting unfortunately present result bound depend dimension constant grows exponentially entirely right picture epsilon stability could depend dimension believe problem epsilon grows dimension contribution even though connection interesting contribution significant enough presented direct application previous work lemma restating known believe discussion need added make complete work,4
473.json,work offer theoretical justification reusing input word embedding output projection layer proposing additional loss designed minimize distance predictive distribution estimate true data distribution nice setup since effectively smooth label given input however construction estimate true data distribution seems engineered provide weight tying justification obvious projection matrix rename example could obtained wordvec embeddings trained large dataset could learned additional parameter case learned matrix seems result independent matrix output projection layer usually done experimental good provide support approximate derivation done section particularly distance plot figure minor comment third line abstract second line section space space tilde frac tilde,5
473.json,provides theoretical framework tying parameter input word embeddings output word representation softmax experiment show significant improvement idea sharing tying weight input output word embeddings noted others thread main negative side proposed justification appears though certainly interesting concerned given dataset kind literature glad tried least dataset think nice find include accepted considered using character word unit context,6
473.json,give theoretical motivation tieing word embedding output projection matrix argument us augmented loss function spread output probability mass among word close word embedding main drawback framework augmented loss function trainable parameter used regularization expected give gain large enough datasets augmented loss heavily engineered produce desired result parameter tying clear happens relax adding parameter estimating different nevertheless argument interesting clearly written simulated indeed validate argument seem promising minor comment section clarify conditioned example entire history enumerated,7
561.json,introduces hierarchical clustering method using learned feature build tree life assumption feature similarity indicates distance tree tried three different way construct tree approximation central point minimum spanning tree multidimensional scaling based method work best nice application using deep feature however lean toward rejecting following reason experiment conducted small scale experiment include fish specie canine specie vehicle class quantitative visualizing generated tree versus wordnet tree moreover assumption using wordnet quite valid wordnet designed biology purpose might reflect true evolutionary relationship specie limited technical novelty part pipeline standard pretrained feature extraction previous method construct hierarchical clustering think technical contribution limited,3
561.json,like creative application computer vision biology least good narrative confident biologist actually care tree life built method really biology either methodology evaluation boil hierarchical clustering visual category ground truth assumed wordnet hierarchy biological ground truth inheritance relationship specie even possible define probably specie interbreed definitely vehicle actual biological inheritance tree human task worried visual relationship inheritance relationship graph right structure tree tree needlessly lossy imposes weird relationship imagenet photo rabbit tree distance maximally distant rabbit device level hierarchy real rabbit animal branch image really semantically unrelated possible visual world hierarchy biological world reasonably defined could define task trying recover biological inheritance tree visual input although know tough situation like convergent evolution still could evaluate well various visual feature recover hierarchical relationship biological organism quite even still feel like solution search problem say type exercise help understand deep feature sure sure much reveals guess fair question particular feature produce meaningful class class distance clear biological tree life wordnet hierarchy right ground truth argue finally mention human baseline place really seeing experiment show proposed method using deep representation competitive human being building tree life based visual similarity specie later reconstructed quality good human being could reconstruct based visual similarity extent experiment qualitative result declaration good human could,3
424.json,first discus general framework improving optimization complicated function using series approximation series approximation well behaved compared original function optimization principle sped connected particular formulation neural network behave simpler network high noise level regain full capacity training proceeds noise lower idea motivation interesting sound mentioned review question wondering relationship shaping method agree differs shaping typically work modifying problem implementation architecture shaped nevertheless central idea case solve series optimization problem increasing difficulty therefore strongly suggest including discussion difference shaping curriculum learning also sure different shaping present approach presentation method neural network lack clarity presentation improving presentation make much easier digest particular understood point referenced please explain step clearly connect step define clearly defining several concern experimental evaluation discussion method work solving much challenging network training problem thin deep network specific concern mlps trained parity pentomino deep experiment training thin network systematically increasing depth better test method network depth well known pose optimization challenge instead stated without reference learning mapping sequence character word embeddings difficult problem case gain primarily regularization effect method compared weight noise regularization method also suggest comparing highway network since thematic similarity possible automatically anneal behavior simple complex net training considering typically initialized bias towards copying behavior cifar experiment mollified also residual connection either case mollified actually train slower residual stochastic depth network inconsistent overall idea development promising need work clear accept,5
577.json,proposes method attempt understand happening within neural network using linear classifier probe inserted various level network think idea nice overall allows network designer better understand representational power layer network time work feel rushed particular fact provide real network used competition make le strong since researcher want created competitive network architecture enough evidence work decides whether ideally encourage consider continuing line research show information given linear classifier construct better network architecture unfortunately think enough novelty justify accepting work conference,4
432.json,nicely written unifies value based policy based regularized policy gradient method pointing connection value function policy established theoretical insightful likely useful field much beyond specific algorithm proposed said exploit theory produce unified version learning policy gradient prof work better state algorithm atari suite empirical section well explained term optimization done minor comment related stationary distribution used policy subtlety using discounted discounted distribution crucial tabular case need addressed long function approximation case said major problem current version overall definitely worthy acceptance likely influence broad swath open door theoretical well algorithm development,8
432.json,nice exploring connection value based method policy gradient formalizing relation softmax like policy induced value regularized form presentation although seems flow first part think could cast extension generalization dueling network intuitive exposition algorithm finding small concern general case derivation section expectation function theta dependency seems ignored although update derivation policy sampling policy expectation close enough usually okay except particular case trust region method generally true thus might solving different problem actually care solving comparison dueling architecture could added closest method nice game improvement overall strong good theoretical insight,6
672.json,introduces joint multimodal variational autoencoder directed graphical modeling multimodal data latent variable rather straightforward extension standard data modality generated shared latent representation independently order deal missing input modality directional inference modality introduces modality specific encoder trained minimize divergence latent variable distribution joint modality specific recognition network demonstrates effectiveness mnist celeba datasets term test likelihood conditional image generation editing proposed method rather straightforward extension therefore inherent probabilistic inference method example missing data modality able infer joint representation well filling missing modality iterative sampling introduced rezende given marginal improvement convinced contribution modality specific encoders section addition inference method introduced generating figure look somewhat unprincipled wondering conditional image generation following principled approach iterative sampling experimental joint image attribute generation also missing,4
672.json,proposed method modeling multimodal datasets inference network every combination missing present modality method evaluated modeling mnist celeba datasets mnist hardly multimodal dataset propose label separate modality get modeled variational autoencoder reviewer find choice perplexing even modality never actually missing applicability suggested method questionable addition difference likelihood different model tiny likely noise experiment report likelihood model trained maximize likelihood clear conclusion drawn comparison,2
737.json,squeezenet came read interest series completely reasonable engineering suggestion save parameter memory cnns object recognition imagenet suggestion make sense provide excellent compression versus alexnet look like combined nice definitely worth publishing since arxiv came people worked extend already evidence impact deserves permanent published home negative side architecture tested imagenet unclear whether idea transfer task audio text recognition many architecture tweaking paper real mathematical theoretical support idea sensible empirically work whole think deserves appear iclr mainline work deep learning architecture,6
625.json,description present reinforcement learning architecture based natural language input meta controller chooses subtasks communicates subtask controller choose primitive action based communicated subtask goal scale reinforcement learning agent large scale task subtask controller embeds subtask definition argument vector multi layer perceptron including analogy making regularization subtask vector combined input layer output given observation subtask mlps compute action probability policy exponential falloff output compute termination probability sigmoid output meta controller take list sentence instruction embeds sequence subtask argument necessarily mapping context vector computed observation previous sentence embedding previous subtask completion state subtask argument computed context vector mechanism involving instruction retrieval memory pointer hard soft decision whether update subtask training involves policy distillation actor critic training subtask controller actor critic training meta controller keeping subtask controller frozen system tested grid world agent move interacts pick transforms various item enemy type compared flat controller using subtask controller subtask control mere concatenation subtask embedding input without analogy making regularization evaluation proposed architecture seems reasonable although clear specific combining subtask embeddings subtask controller right feel grid world really represents large scale task particular size grid small disappointing since main motivation work moreover method compared state alternative especially problematic test established benchmark really possible based shown performance context work,3
625.json,seen instantiating famous founder john mccarthy learning take advice studied depth later researcher jack mostow card game heart idea agent given high level instruction solve problem must distill level policy quite related human learn complex task many domain driving driving instructor provide advice keep certain distance front fairly complex neural deep learning controller architecture used although detail system somewhat confusing term many detail presented simpler approach might easier follow least initially experiment unfortunately rather simplistic maze worthwhile approach scaled complex task sort usually seen deep paper day atari physic simulator nice overall idea somewhat confusing description solution inadequate experiment le satisfactory domain grid world,4
449.json,looking comment section agree large degree author standpoint many issue discussed point comment opinion good summary contribution think contribution groundbreaking believe significant enough merit acceptance reason commenting looked several comment section iclr seeing general trend review strong focus performance review tend short judge paper large degree whether percentage point better worse reported baseline comment experimental evaluation convincing improvement svhn effect drop path seems vanish data augmentation believe paper judged scientific contribution point especially paper state focus scientific contribution amazing performance believe trend focus excessively performance problematic number reason deep learning community focused heavily datasets mnist imagenet cifar cifar svhn mean time large chunk deep learning literature battling sota title hence expecting attain title high arbitrary standard sota imagenet improves year outperforms underperform performance standard idea scientific merit declined drastically value year really true even draw fair comparison standard datasets point trick neural network includes drop ensembling various form data augmentation various form normalization initialization various linearity various learning rate schedule various form pooling label smoothing gradient clipping gazillion way fraction percentage point performance every single unique combination trick even though trick unrelated hence truly fair comparison compare every reference exact trick combination presenting reference used take exorbitant amount time worse many paper even report trick used code reverse engineer mention slight difference introduced using tensorflow torch caffe light request reviewer baseline improvement clearly demonstrated making isolated change seems unrealistic community make excessive fine tuning model mandatory publication requiring model beat sota force author fine tune nauseum lead arm race publication spend ever time fine tuning model lead training test also waste time researcher could better spent exploring idea give much power research science always certain background rate published either number outright fake experimental protocol invalid someone used test validation someone exorbitant number random rerun published best single result worse likely hold sota title given time good result requiring publication beat sota give much power punishes reporting many strong baseline careful report many recent table thorough criticized beating baseline feeling selective baseline report beat received higher score written depth review another conference used opinion weak baseline ended getting high reviewer mark think coincidence argument apply though think lesser degree judging model excessively many parameter runtime however agree reviewer information model compare term metric enhance like discussion final version general think benefit appendix detail training procedure also agree reviewer layer deepest improving test error table ultra deep hence putting ultra deep title seems exaggerated recommend scaling back language however think ultra deep layer necessary veit showed network appear ultra deep might ultra deep practice training layer function test time without residual connection seems enough achievement summary think make scientific contribution point independent performance competitive performance enough publication instead requiring sota believe achieves mark,6
776.json,proposes iteratively refining translation hypothesis several benefit including enabling translation condition left context also right context potentially enabling rapid accurate decoding motivation given often translator text generator generally process refinement generating output important idea currently playing much role neural model welcome contribution however think important first step feel lack depth analysis suggests quite ready final publication version example many possible connection prior work part could better contextualize work specific substantively section could interpreted globally normalized undirected translation trained using pseudo likelihood objective analysis squarely back context traditional discriminative translation model used undirected feature decoding algorithm look like standard greedy hill climbing algorithm albeit extra heuristic selecting variable update also nothing unfamiliar second criticism limitation well discussed example proposed editing procedure cannot obviously remove insert word translation think reasonable assumption made sake tractability unfortunate since missing extra word function word common problem baseline model used second standard objection absolute positional model relative positional model seem particularly crucial bring work especially since might make design decision justifiable overall initial step interesting direction need thorough analysis demonstrate value thorough analysis also likely suggest important variant example global translation really goal post editing fix output complex operation ideal related work think could done work context come currently going part idea iterative refinement proposed problem complex output space example draw gregor conditional adversarial network model used refine image proposed recently isola several stochastic hill climbing approach proposed work parsing zhang random initial guess greedy hill climbing using series local refinement structured prediction cascade wei taskar mention general coarse fine modeling strategy finally arun gibbs sampler refine initial guess decoding complex explicit error rather novel context correction point although proposed architecture different discriminative word lexicon model mauser neural version similar spirit also number paper automatic post editing including shared task standard test set baseline also datasets could actually used train post editing human generated data minimally using technique described could useful foil model presented target sentence also embedded distributional space lookup table think distributional space unclear maybe target sentence represented term distributed word representation lookup table something like distributional suggests representation derived word distributed corpus whereas learning representation task modeling distribution except indirectly section section computes distribution target word type absolute position output sentence given target language context source language context introduced used refine existing hypothesis immediately clear training data least section gold standard translation training could interpreted variety way becomes clearer reading later le clear reading beginning first time fixed sized window representing target word context also seems make something like assumption since lexical feature alignment positional feature determine attention clarified since make assumption transparent also suggest possible refinement including representation component allow like response learned although leaving might behave like relative positional absolute positional probably attractive finally discussion fixed window used represent target sentence worth including since global context apparently used represent source sentence relationship training objective pseudo likelihood besag might worth mentioning since believe objective certain global suggests alternative decoding algorithm certainly different analysis proposed decoding objective section condition true context position true target current target guess source completely understand rationale since test time variable available replacement yref seems hard justify,4
776.json,work proposes iteratively improve sentence generated another system case phrase based system neural take source sentence window gold word around current target word predicts current target word testing gold word replaced generated word interesting area research convinced proposed approach experimental evidence lacking current framework impossible anything rudimentary word replacement cannot change went fridge even though hungry although hungry went fridge fact word edited average support specific comment interesting improvement baseline neural system seems strange least look window word mean making decision change word know generated outside window relatedly idea changing individual word based local word level score seems counterintuitive given full generated sentence want global score scoring sentence level could also make room greedy search strategy could potentially facilitate richer edits approach compare simply rank best output instead editing consider learning encoder decoder take generates yref decoding attend minor comment iteratively improving generated text also explored,3
371.json,summary proposes neural physic engine network architecture simulates object interaction decides explicitly represent object rather video frame incorporates knowledge physic almost exclusively training data tested domain bouncing ball proposed architecture process object scene time pair object embedded common space effect object represented embeddings summed combined focus object state predict focus object change velocity alternative baseline presented either forego pairwise embedding single object embedding encode focus object neighbor sequence lstm state outperforms baseline dramatically showing importance architecture choice learning object based simulation tested multiple way ability predict object trajectory long time span measured generalization different number object measured generalization slightly altered environment difference shaped wall measured finally also trained predict object mass using interaction object also outperforms baseline comment clarifying question input blue figure concatenation summed embeddings state vector object input blue module combination vector section begin first physic change across inertial frame suffices separately predict future state object conditioned past state object neighborhood similar fragkiadaki think argument forego visual representation used previous work favor object representation clear contrast visual representation addressed approach novel though le taking consideration concurrent work battaglia nip titled interaction network learning object relation physic work offer different network architecture experiment well great presentation object based representation learning predict physical behavior shared overall evaluation pleasure read provided many experiment offered clear interesting conclusion offer novel approach though le compared concurrent work battaglia represents significant step forward current investigation intuitive physic,8
408.json,pro interesting training criterion con missing proper technique based baseline comment dataset quite small curve detection measurement probably helpful besides detailed analysis necessary precision word seen training compared detection performance vocabulary word interesting show scatter plot embedding orthographic distance,4
408.json,proposes approach learning word vector representation character sequence acoustic span jointly clearly written approach experiment seem reasonable term execution motivation task feel synthetic requires acoustic span word already segmented continuous speech major assumption evaluation task feel synthetic overall particular evaluating character based comparison seems also phoneme based comparison discussion character edit distance relative acoustic span similarity seems natural also include phoneme string edit distance discussion experiment especially true word similarity test rather looking levenshtein edit distance character evaluate edit distance phone string relative acoustic embedding distance beyond evaluation task interesting compared character embeddings phone string embeddings believe last function could remain identical swapping character phone symbol finally topic discussion experiment look homophone obvious network learn handle vocabulary size training data amount make really problem although many pair constructed pair easy distinction experiment conclusion stronger larger vocabulary word segment data subsampling pair perhaps biased towards difficult similar pair seems approach unable address task keyword spotting longer spoken utterance case please discussion solving problem word embeddings given existing word segmentation motivating example using approach retrieve word seems flawed recognizer must used segment word beforehand,5
490.json,work basically combined pointer network applied language modelling smart point aim language modelling longer context memory seen word especially rare word useful predicting rest sentence hence combination pointer network standard language balance copying seen word predicting unseen word generally combined pointer network applied sentence compression vector representation source sequence used compute gate instead introduces sentinel vector carry mixture suitable case language modelling interested variation sentinel mixture implementation though current version achieved good addition wikitext language modelling dataset interesting probably standard dataset evaluating continuously updated language benchmark dataset overall well written recommend accepted,7
306.json,work present lstm based meta learning framework learn optimization algorithm another learning algorithm globally well written presentation main material clear crux drawing parallel robbins monroe update rule lstm update rule exploit satisfy main desideratum shot learning quick acquisition knowledge slower extraction general transferable knowledge intriguing several trick used andrychowicz parameter sharing normalization novel design choice specific implementation batch normalization well motivated experiment convincing strong concern question following redundant loss gradient parameter input meta learner ablative study make sure simpler combination enough great architectural component network learned similar fashion number neuron type unit opinion related work section mainly focused meta learning shallow meta learning rather topic similar approach tried solve problem even using lstms samy bengio thesis genetic programming search learning rule neural network bengio bengio cloutier convince schmidhuber done something make sure find update related work section overall like believe discussed material relevant wide audience iclr,8
582.json,problem utilizing available information across modality product learn meaningful joint embedding interesting certainly seems like promising direction improving recommender system especially cold start scenario unaware approach combining many modality proposed effective solution could indeed significant however many aspect proposed architecture seem optimal major benefit neural network based system entire system trained jointly proposed approach stick together largely trained module different modality justifiable little training data available train jointly product pair however seem case amazon dataset although worked dataset perhaps missing something either discussed consider lack jointly fine tuned major shortcoming proposed approach discussion pairwise residual unit confusing well motivated residual formulation understand correctly applies relu layer concatenation modality specific embeddings giving similarity product added similarity obtained concatenation directly additional fully connected layer mix modality specific embeddings form final embedding perhaps lower dimensionality least presented baseline pairwise residual unit claimed contribution find provided explanation convincing residual approach reduce parameter count minor choice textcnn text embedding vector seems fine although wonder lstm based approach perform however detail surrounding used obscured response question mention run concatenation first word title product description especially description seems insufficiently long contain information care could given motivating choice made finally familiar state dataset comparison accurately reflect seems competing technique presented none challenging cold start scenario minor detail second paragraph page reference say cite julian,2
701.json,proposes method augmenting trained network task additional inference path specific additional task replacement standard fine tuning approach pro method simple clearly explained standard fine tuning used widely improvement analysis general interest experiment performed multiple domain vision con additional module incur rather large cost resulting parameter roughly computation original network stiched network cost addressed text make method significantly le practical real world performance often important given large additional cost core idea sufficiently validated order verify improved performance actually coming unique aspect proposed technique rather simply fact higher capacity network used additional baseline needed allowing original network weight learned target task well additional module outperforming baseline validation verify freezing original weight provides interesting form regularization network training full module stitched network scratch source task fine tuning target task outperforming baseline verify weight never see source dataset useful method evaluated imagenet away common domain trained network used fine tuned task never seen network trained cifar deployed anywhere hard know whether method practically useful computer vision application based cifar often improved performance cifar translate imagenet context theoretical contribution small datasets acceptable network fine tuning enough practical spectrum claiming improvement necessitate imagenet evaluation overall think proposed idea interesting potentially promising current form sufficiently evaluated convince performance boost simply come larger network lack imagenet evaluation call question real world application edit indeed missed fact stanford car transfer learning imagenet thanks correction however experiment case showing late fusion ensembling conventional approach compared stitched network idea real novelty furthermore case particularly weak showing ensemble resnet outperforms alone completely expected given resnet alone stronger base network resnet resnet stronger result still surprising demonstrating stitched network idea imagenet comparing corresponding resnet finetuning could enough push current version experiment sufficiently validate stitched network idea opinion,3
717.json,make three main methodological contribution definition neural feature pixel average image highly activation neuron ranking neuron based color selectivity ranking neuron based class selectivity main weakness none methodological contribution significant singularly significant result arises application method however main strength assortment moderately sized interesting conclusion basic behavior neural net example indexing class selectivity neuron found highly class selective neuron like digital clock conv cardoon conv ladybug conv much fully connected layer know previously reported color selective neuron found even higher layer color selectivity conv main color axis emerge black white blue yellow orange cyan cyan magenta curiously observation correlate evidence human visual system shapley hawken great observation overall recommend accepted although difficult predict time fair chance smaller conclusion turn important hindsight year hence small comment cite learning generate chair wrong first combined resulting confusing cite exactly color selectivity index computing opponent color space well defined previously familiar intuitively seems selecting unit respond constant color highest color selectivity unit color finally last unit lowest color selectivity almost edge pattern white black instead blue orange considered drastically different probably clearly described sake argument imagine mushroom sensitive neuron conv fire highly mushroom color anything else dataset contains capped mushroom color selectivity index neuron high high somewhat misleading unit actually color selective dataset happens mushroom subtle point worth considering probably discussing,6
740.json,update looked arxiv version much longer appears rigorous indeed insightful however reviewing submission overall assessment change minor incremental contribution want compress conference submission type recommend choosing message want convey focus iclr submission focus parmac algorithm focus properly remove move appendix extension theoretical remark extra page explaining algorithm additionally make sure clearly explain relation arxiv particular submission compressed version original review submission proposes parmac based method auxiliary coordinate formulating distributed variant idea related work part convex method recommend citing general communication efficient framework cocoa reddi believe work related practical objective number paper cited le relevant section explaining quite clearly written find part particularly useful section much le clearly written trouble following notation particularly speedup part different symbol introduced different place perhaps quick summary paragraph notation introduction helpful paragraph write reader knew data anything distributed mentioned specified later clear meant submodel perhaps precise example pointing back useful understand written independent set submodels traverse machine circular fashion understand initialized identically importantly understand single output algorithm averaging seem make sense since addressed suppose wrong leaving guess actually meant fact able understand actually happening major issue like later paragraph extension speedup convergence topology understand whether novel contribution refer work detail novel explanation sufficient particularly speedup part contains undefined quantity find novel provide enough explanation understand anything compared version compressed size referring work statement recover original convergence guarantee seems strong trivial show author point work look topology part claiming something true without explaining true seems strange statement section seem also vague unjustified unexplained experimental section seems suggest method interesting binary autoencoders conclude anything model parmac also compared alternative method focusing scaling property conclusion contains statement strong misleading based particular analysed parallel speedup convergence seems ungrounded claim convergence property remain essentially unaltered parmac unsupported regardless meaning essentially unchanged summary method seems relevant particular class binary autoencoders clarity presentation insufficient able recreate algorithm used experiment contains number questionable claim,3
740.json,proposes extension method subproblems trained distributed cluster arranged circular configuration basic idea decouple optimization parameter output piece auxiliary coordinate optimization alternate updating coordinate given parameter optimizing parameter given output circular configuration update independent massively parallelized greatly benefit concrete example problem decompose instance applied effectively deep convolutional network recurrent model practical perspective much impact beyond showing particular decoupling scheme work better others also seem idea worth comparing least circular parameter server configuration decoupled problem parallel parallel also benefit extremely easy implement toolbox work better practically useful also hard understand exactly passed around round round trade offs deep feed forward network assuming problem every hidden unit seems like step different bit walk around cluster taking step coordinate stored machine mean passing around parameter vector hidden unit synchronization step gather parameter submodel requiring traversal circular structure machine update coordinate based complete slice data mean feed forward network producing intermediate activation layer data point something comparable parallel could following mini batch size machine parmac compared running mini batch parallel completing step roughly equivalent synchronized type implementation step distribute worker gradient back update really helpful compare practice hard understand intuitively proposed method theoretically better parallel except issue smooth function optimization decoupling also fundamentally change problem since back propagation directly anymore seems like conflate thing well necessarily going work type architecture,5
605.json,method overall seems interesting structural approach variational autoencoders however seems lack motivation well application area sufficient prove effectiveness attractiveness using structural information context find intuitive using flat sequence representation especially clear structure data however experimental seem fail convincing regard issue lack variety application general experiment seem limited regard considering speaks natural language application interesting latent representation learned task much impact success task compared various baseline opinion potentially strong idea however need stronger possibly wider variety application proof concept,2
384.json,summary present deep neural network task machine comprehension squad dataset proposed based previous work match lstm pointer match lstm produce attention word given question word given passage sequentially matching word passage word question pointer used generate answer either generating word answer predicting starting ending token answer provided passage experimental show variant proposed outperform baseline presented squad also show analysis obtained variation performance across answer length question type strength novel task machine comprehension rather using hand crafted feature significant performance boost baseline presented squad insightful analysis performance better answer short question difficult answer weakness question suggestion show quantitatively much modelling attention match lstm answer pointer layer help insightful could compare performance without attention match lstm without attention answer pointer layer good could provide insight huge performance boundary sequence answer pointer layer like variation performance proposed question require different type reasoning table squad provide insight strength weakness proposed type reasoning required could please explain activation resulting equation repeated across dimension learn different activation dimension wonder used ensemble last table shown improves performance could please discus compare table detail review summary present reasonable task machine comprehension squad dataset outperforms baseline significantly however good analysis ablation study insight included regarding much attention help boundary better sequence performance change reasoning required becomes difficult,6
453.json,proposed proximal quasi newton method learn binary main contribution combine conditioning binarization proximal framework interesting proximal newton method interpret different binarization scheme give interpretation existing approach however theoretical analysis convincing useful formulated optimization problem essentially mixed integer programming even though treat integer part constraint address proximal operator constraint still discrete guarantee proximal newton algorithm could converge practically useful condition practice hard verify assumption beta theorem relation could hard hold loss surface could extremely complicated,6
392.json,work proposes approach image compression using auto encoders impressive besting state field pro clear possible replicate inclined compared work field promising need emphasize think emphasized fact well technology surprising better state image compression definitely better neural network approach compression though con training procedure seems clunky requires multiple training stage freezing weight motivation behind figure strange clear trying illustrate confuse reader talk effect jpeg discus neural network architecture quantization,7
686.json,proposed complex compression reconstruction method additional parameter reducing memory footprint deep network show complex proposal better simple hashed proposal question also counting extra parameter reconstruction network memory comparison otherwise experiment unfair since hashing reconstruction cost dominate feed forward back propagation update imperative compare method running time hashed quite simple created additional bottleneck please also show impact running time small improvement loss computational cost acceptable convinced method lightweight allowed complicated compression reconstruction shelf method cost huge,4
405.json,update going response author revision increased review score reason thank reviewer investigating difference work scheduled sampling unsupervised learning using lstm providing insight least show empirically pred scheme better high dimensional video long term prediction good briefly discus final revision either appendix main text revised contains comprehensive presented result discussion quite useful research community high dimensional video prediction involves large scale experiment computationally expensive summary present architecture action conditional future prediction proposed architecture combine action recurrent connection lstm core performs better previous state architecture also explores compare different architecture frame dependent independent mode observation prediction dependent architecture experimental result show proposed architecture fully prediction dependent training scheme achieves state performance several complex visual domain also shown proposed prediction architecture used improve exploration environment novelty novelty proposed architecture strong difference work action combined lstm action combined lstm jumpy prediction already introduced srivastava deep learning area experiment experiment well designed thorough specifically evaluates different training scheme compare different architecture using several rich domain atari world besides proposed method achieves state many domain present application based exploration clarity well written easy follow overall although proposed architecture much novel achieves promising atari game environment addition systematic evaluation different architecture presented useful community reference nitish srivastava elman mansimov ruslan salakhutdinov unsupervised learning lstms icml,6
540.json,study knowledge transfer problem small capacity network bigger follow work netnet iclr netmorph icml comment study macroscopic problem morphing process composed multiple atomic operation atomic operation proposed netnet netmorph study general modularized process principally thus asks novel question solution composing multiple atomic transformation seems quite reasonable related work section better change network morphism knowledge transfer subsection title work known knowledge transfer help connect existing work author show experiment variant resnet experiment show initializing resnet give better error rate one trained scratch unclear source study knowledge transfer problem small capacity network bigger follow work netnet iclr netmorph icml comment study macroscopic problem morphing process composed multiple atomic operation atomic operation proposed netnet netmorph study general modularized process principally thus asks novel question solution composing multiple atomic transformation seems quite reasonable related work section better change network morphism knowledge transfer subsection title work known knowledge transfer help connect existing work author show experiment variant resnet experiment show initializing resnet give better error rate one trained scratch unclear source major advantage type knowledge transfer netnet netmorph speedup training exploration seems experiment demonstrate advantage possibly lose initialization batchnorm major drawback method proposed author principle quite complicated transformation transform entire resnet single conv layer experiment consists simple module transformation covered atomic operation interesting complicated transformation even effective summary study novel problem knowledge transfer macroscopic level method could interest iclr community experiment improved comment make convincing practically useful strongly encourage,5
540.json,present work neural network architecture morphing reported imagenet larger resnet network architecture xception densenet maybe also reported small datasets network offer confidence usability production system biggest issue computational time effort technique mentioned detail always want able quantify extra effort understanding using technique especially minor,6
540.json,present interesting incremental approach exploring convolutional network hierarchy incremental manner baseline network reached good recognition performance experiment presented cifar imagenet benchmark morphing various resnet model better performing model somewhat computation although baseline le strong presented literature claim significant error reduction imagenet cifar main idea rewrite convolution multiple convolution expanding number filter quite unexpected approach yield improvement baseline however basic tenet network morphing experimental evidence given fundamental question raised quality morphed network compare topology trained scratch incremental training time morphing relate network trained scratch extra computational cost morphed network come quality baseline resnet model behind reported literature github github resnet supposed recall reported evidence first three point necessary evaluate validity claim written reasonably well understood quite well missing evidence weaker baseline make look somewhat le convincing inclined revise score experimental evidence given main message point,4
628.json,submodular product network applied scene understanding spns shown great success deep linear model since work poon propose extension initial spns submodular introducing submodular pairwise potential propose inference algorithm evaluated stanford background dataset compared multiple baseline pro formulation spns inference algorithm con discus sspn structure learned generative process chooses symbol operation level evaluation lacking showed approach baseline leaving every approach evaluation could also done regular image segmentation hierarchical segmentation idea great however need work published also recommend include detail approach present full extended experiment full learning approach,3
487.json,original comment look good baseline proposed quite instance table misclassification rate result floating point well obtain topology near like significant compression level state good baseline hidden layer cifar experiment understand sparsely connected single precision floating point worse sparsely connected binaryconnect better binary float think experiment using technique easily applied float binary gaussian noise regularization therefore point view comparison float binary fair critic also original paper binary ternary precision fact convolutional network floating standard precision lower error rate baseline reply still convince still think technique applied challenging scenario,5
468.json,main contribution show uniform quantization work well variable length huffman coding improves fixed length quantization proposing hessian weighted mean opposed standardly used vanilla mean hessian weighting well motivated also explained efficient approximation free using adam optimizer quite neat opposed vanilla mean main benefit approach apart improved performance tuning layer compression rate required achieved free conclude like really novel seem paper done nice know work well quite neat also work well easy follow good complaint long minor note still understand part storing additional bit binary codeword layer indication layer layer quantization problem array quantized weight value layer store quantized weight layer layer layer codebook overhead joint quantization storing codebook layer insignificant understand additional part anyway really important think affect might want additionally clarify point maybe missing something obvious likely people well,6
653.json,theoretically justified faster convergence term average gradient norm attained processing fixed number sample using small mini batch asgd smaller number learner indicates inherent inefficiency speed obtained parallelizing gradient descent method taking advantage hardware look good overall make connection algorithm design hardware property main concern lemma look incorrect factor please clarify check subsequent theorem,5
653.json,show larger mini batch used serial setting number sample needed processed convergence guarantee larger similar behavior discussed using multiple learner asynchronous behavior known convex optimization better mini batch algorithm accelerated gradient method nip convergence form sqrt using sample lead sqrt time improvement extends similar result nonconvex case underlying mathematics mainly ghadimi however behavior also known indeed summarized deep learning textbook chapter hence novelty limited theoretical suggest best mini batch however using mini batch often beneficial practice discussed deep learning textbook using mini batch allows using larger learning rate note assumes learning rate mini batch size moreover multicore architecture allows parallel execution mini batch almost free hence practical significance also limited comment equation since number sample processed mini batch size number mini batch processed mentioned first paragraph section different mini batch size considered differ however used figure convergence speed main interest show training objective instead,3
595.json,present small trick improve quality variational autoencoders optimizing elbo initializing prediction network instead using directly idea using jacobian vector replace simple embeddings interpreting variational autoencoders idea jacobian natural replacement embeddings interesting seem cleanly generalize notion embeddings linear model interesting comparison work seeking provide context specific embeddings either clustering smarter technique like neelakantan efficient parametric estimation multiple embeddings word vector space chen unified word sense representation disambiguation evidence provided experimental section hard convinced jacobian generated embeddings substantially better context sensitive prior work similarly idea optimizing elbo interesting fully explored unclear example tradeoff complexity network step optimizing elbo term compute versus accuracy overall idea good like little fleshed,4
700.json,pointed limitation existing deep architecture particular hard optimize small size datasets proposed stack marginal fisher analysis build deep model proposed method tested several small size datasets compared several feature learning method also applied existing technique deep learning backprop denoising dropout improve performance contribution limited long proposed fail theoretically empirically justify stacking mfa include deep architecture requires backprop multiple layer comparison address instead method compared learned layer layer randomly initialized deep perform poorly datasets also clear came particular architecture hyper parameter used different datasets writing need significantly improved detail omitted example dropout applied,3
645.json,propose method extends linear view representation learning method linear multiview technique combine information multiple source linear representation learning technique general method well described seems lead benefit different experiment phonetic transcription hashtag recommendation even method mostly extension classical tool scheme learns deep network view essentially combination different source information seems effective studied datasets interesting discus following issue complexity proposed method representation learning part alternative solution combine different network view could make proposed solution novel experimental setting especially synthetic experiment detailed possible datasets made available encourage reproducibility related work complete unfortunately especially perspective numerous multiview multi modal multi layer algorithm proposed literature different application like image retrieval classification bibliographic data example like kumar dong ping chen bronstein many others proposed work direction last year need compare work obviously complete description related could help appreciating better true benefit dgcca,5
645.json,proposes deep extension generalized main contribution deriving gradient update gcca objective disagree claim first multiview representation learning technique combine flexibility nonlinear representation learning statistical power incorporating information many independent resource view proposes multiview representation learning method linear capable handling view much relevant proposing objective function proposed maximizes correlation view minimizes self cross reconstruction error intuitively similar nonlinear version multiple view comparing method crucial prove usefulness dgcca incomplete without comparison also change strong claim related work section minimal significant advance view linear representation learning worth mentioning reference janarthanan rajendran mitesh khapra sarath chandar balaraman ravindran bridge correlational neural network multilingual multimodal representation learning naacl,4
665.json,summary present large scale machine reading comprehension dataset called marco different existing datasets question real user query context passage real document free form answer generated human instead span context also includes analysis dataset performance model dataset strength question dataset real query user instead human writing question given context context passage extracted real document used search engine find answer given query answer generated human instead span context large scale dataset million query current release includes query weakness found distribution actual question user intelligent agent different conceived crowdsourcing text statement backed study clearly present additional information today model learn marco existing datasets talk challenge involved obtaining good performance dataset human performance compared model presented section train test split subset marco every query multiple answer subset dssm mentioned table include experiment section prove marco better dataset table performance memory network already close best passage mean enough room improvement seems written hurry partial analysis evaluation various mistake text preliminary evaluation proposed dataset marco unique existing datasets good representative task encountered search engine think useful dataset community benefit given huge potential dataset lack analysis evaluation needed present dataset worth think benefit comprehensive analysis dataset,5
665.json,dataset brings unique value existing reading comprehension challenge unlike others marco derived query log thus represents real question people rather solicited question might rather artificial practical setting potential downside using query log however people adapt language question search engine user question know current search engine reasonably answer thus people limit complexity question language think could addressed concern selective query log sampling simple question easily answered keyword matching without sophisticated reading comprehension sampling complex question require least paraphrasing ideally synthesis information taken sentence great several effort construct large scale reading comprehension challenge main concern whether majority question answered relatively easy text matching without intelligent reading reasoning also read like running time deadline appreciate analytic quantitative comparison existing datasets insight degree challenge required handle marco example could collect statistic exact match exists text snippet paraphrasing required otherwise relevant answer directly available text snippet requires synthesizing information taken sentence requires external knowledge author response mention unlikely formal complete analysis helpful,5
389.json,pro presenting based alternative wavenet generating audio sample time rnns natural candidate task interesting alternative furthermore claim make significant improvement quality produce sample another novelty quantitative likelihood based measure ass addition human comparison used wavenet work con lacking equation detail remedied camera ready version lacking detailed explanation modeling choice clear used bottom layer instead another clear linear projection used sampling instead feeding state sample powerful type transformation admit wavenet implementation probably good original make comparison questionable despite con given modeling detail provided think valuable contribution,7
736.json,proposes simple domain adaptation technique batch normalization performed separately domain pro method simple easy understand apply experiment demonstrate method compare favorably existing method standard domain adaptation task analysis section show small number target domain sample needed adaptation network con little novelty method arguably simple called method rather straightforward intuitive approach using network batch normalization domain adaptation alternative using statistic source domain target domain example le natural guess alternative done inception table analysis section superfluous except sanity check divergence distribution distribution shifted scaled section clear point made overall much novelty hard argue simplicity thing method clearly competitive outperforming prior work standard benchmark domain adaptation tradition started frustratingly easy domain adaptation accepted section removed rewritten clarity final version,5
736.json,update thank comment still think method need experimental evaluation restricted setting trained imagenet also important show effectiveness scenario need train everything scratch love fair comparison state method datasets bousmalis ganin lempitsky opinion crucial point allow increase rating proposes simple trick turning batch normalization domain adaptation technique show batch mean variance normally computed part procedure sufficient discriminate domain observation lead idea adaptation target domain performed replacing population statistic computed source dataset corresponding statistic target dataset overall think suitable workshop track rather main conference track main concern following although main idea simple feel like composed make main contribution le obvious idea could described abstract avoided review question using much stronger base account bulk reported improvement order prove effectiveness trick need conduct fair comparison competing method highly desirable conduct comparison also case trained scratch opposed reusing imagenet trained network,3
464.json,proposes reinforcement learning learn compose word sentence parse tree helpful downstream task shift reduce framework employed used learn policy action shift reduce experiment four datasets sick imdb snli show proposed approach outperformed approach using predefined tree structure left right right left well written good point firstly idea using learn parse tree using downstream task interesting novel employing shift reduce framework smart choice action minimal shift reduce secondly shown somewhat confirms need parse tree indeed interesting current debate whether syntax helpful following comment seems aware recent work using learn structure composition andreas different composition function lstm classical recursive neural different inductive bias wondering tree structure found independent composition function choice rnns theory equivalent turing machine wondering restricting expressiveness reducing dimension help focus discovering helpful tree structure andreas learning compose neural network question answering naacl,7
608.json,summary explore advantage disadvantage using activation function first demonstrate even simple task using activation result complex optimize loss function compare network trained different activation mnist dataset discover periodicity activation necessary learning task well different algorithmic task periodicity function helpful pro closed form derivation loss surface interesting clarity tone advantage disadvantage educational con seems like preliminary investigation potential benefit evidence support contrary needed conclude anything significant mnist seem indicate truncated good interesting tanh maybe us saturated part seem relatively interchangeable algorithmic task hard conclude something concrete,3
433.json,present clever training generative allows exact inference sampling likelihood evaluation main idea make jacobian come using change variable formula data latents triangular make determinant easy calculate hence learning possible nicely present core idea achieve choosing special routings latents data part transformation identity part complex function input deep example resulting jacobian tractable structure routing cascaded achieve even complex transformation experimental side trained several datasets quite convincing sample quality quantitive measure happy useful type task resulting latent representation help classification inference image restoration summary nicely written quite good interesting happy recommend acceptance,7
433.json,proposes generative us real valued volume preserving transformation order achieve efficient exact inference sampling data point change variable technique obtain distribution data simple prior distribution latent variable carefully designing bijective function used change variable technique obtain jacobian triangular allows efficient computation generative model tractable inference efficient sampling active research area definitely contributes field achieving state behind change fact proposed method innovative worth exploring try bridge auto model variational autoencoders generative adversarial network clearly mention difference similarity type generative model actively researched compared autoregressive model proposed approach offer fast sampling compared generative adversarial network real offer tractable likelihood evaluation compared variational autoencoders inference exact compared deep boltzmann machine learning proposed method tractable clear real goal bridge existing popular generative model present interesting experiment showing capability proposed technique making code available online certainly contribute field intention releasing code typo section also apply batch normalization,7
649.json,analyzes dependency tree standard window context word vector learning good goal believe fall short thorough analysis subject matter analyze glove like objective function often work better algorithm used compare absolute term published vector model fails gain particularly interesting insight modify people work fails push state make available resource people,3
649.json,evaluates different context type affect quality word embeddings plethora benchmark ambivalent hand continues important line work decoupling various parameter embedding algorithm time focusing context hand sure understand conclusion experiment appear significant consistent advantage context type benchmark sensitive enough detect difference exist accepted rather elaborate version try answer fundamental question,5
472.json,work proposes geometry data considered known priori order consistent sparse coding namely data sample similar neighbour sparse code similar term support general idea unique interesting admits adjacency matrix known priori novelty mostly lie definition regularisation term norm technique mostly regularisation based idea develop srsc algorithm analysed detail shown perform better competitor based sparse coding regularisation scheme term clustering performance inspired lista propose approximate solution srsc problem called deep srsc act sort fast encoder idea interesting seems quite efficient experiment usps data even framework seems strongly inspired lista scheme however better motivated limitation srsc presented clearly overall well written pretty complete extremely original main idea though actual algorithm implementation seem effective,6
615.json,seems time complexity miss information experimental suggest outperform adadelta suppose adam given time complexity axis showing time suggest much time slower memory size lowest minimum test loss suggests main driven force momentum second order information rather useless,3
584.json,propose transfer learning approach applied number task task appear order term complexity easy syntactic task somewhat harder semantic task novelty propose transfer learning plugging model corresponding task respect known hierarchy term complexity task respect overall architecture look like cascaded architecture transfer learning existing literature area first google found,5
642.json,explores performance area energy accuracy tradeoff encountered designing custom number representation deep learning inference common image based benchmark googlenet used demonstrate fewer bit custom floating point representation lead improvement runtime performance energy efficiency small loss accuracy question custom floating point number representation take account support normal number custom floating point unit clocked frequency baseline floating point unit different frequency used impact overall system design term feeding data floating point unit memory comment recommend using ieee half precision floating point sign exponent mantissa baseline comparison point well known community float overkill inference major vendor already include support ieee half precision float opinion claim switching custom floating point lead saving energy misleading might true floating point unit might consume le energy smaller width operand however large fraction total energy spent data movement memory result reducing floating point unit energy consumption certain factor translate reduction total energy reader familiar nuance example typical member community mislead claim similar note comment explicitly mention claimed speedup floating point unit translate overall workload speedup although speedup compute unit roughly quadratic width bandwidth requirement scale linearly width result possible custom floating point unit starved memory bandwidth case claim speedup energy saving need revisited also comment complexity overhead introduced data access designing various system bus data path number representation byte aligned moving custom number representation example improve performance energy efficiency floating point unit gain partially eroded additional overhead supporting byte aligned memory access,4
592.json,replaces gaussian prior often used group sparse prior modify approximate posterior function also generates group sparse sample development novel form generative inference process vaes active important area research believe specific choice prior proposed well motivated however believe several conceptual claim incorrect experimental unconvincing suspect compare likelihood bit competing algorithm nats detailed comment table likelihood reported competing technique nats reported likelihood cvae using sample higher likelihood true data sample also higher likelihood achieved fitting mean mixture data done note evaluation generative model nearly impossible outperform mean mixture parzen estimation make extremely skeptical evae however assume evae likelihood actually bit multiply convert nats corresponds totally believable likelihood note parzen window implementation report likelihood bit experiment comparing likelihood bit competing likelihood nats also label unit bit nats table really really good report compare variational lower bound likelihood alternatively concerned bound loose exact measure likelihood even parzen window correct parzen estimate likelihood extremely poor posse drawback likelihood evaluation approximate many additional drawback well mnist sample quality appear visually competitive also appears image probability activation pixel rather actual sample sample accurate either make sure describe shown figure experiment datasets still concerned issue raised question briefly comment response minibatches constructed random subset training example also balanced epitome assignment nice make feel better epitome used think response address cvae trade data reconstruction factorial approximate posterior factorial construction nothing cvae make le factorial cvae zero contribution term particular word unit deactivated example training deactivated term zero unit true standard variance mean term example training zero training example loss trained lower bound likelihood though term look like reconstruction error naively imagine overfits correspond data sample becoming likely generative parzen concern strange train binary treat probability activation sample continuous space evaluate sample believe true training lower bound likelihood immediately provides another method quantitative evaluation additionally could technique compute exact likelihood believe parzen window evaluation better measure quality even term sample generation likelihood,3
592.json,proposes elegant solution important problem vaes namely regularizes killing latent dimension people used annealing term free bit hack around issue better solution needed offered solution introduce sparsity latent representation every input latent distribution activated across dataset many latents still learned understand need topology latent representation place prior arbitrary subset latents seems increase representational power without compromising solution problem trying solve number way latents combine longer exponentially large seems pity first paragraph mystery effect sample utilization capacity lead overfitting experiment modest sufficient interesting idea resolve fundamental issue vaes thus deserves place conference,7
480.json,present method joint motion prediction activity classification sequence different application motion fruit fly online handwriting recognition method us classical encoder decoder pipeline skip connection allowing direct communication encoder decoder respective level abstraction motion discretized predicted using classification trained classification loss combined loss motion prediction goal leverage latter loss semi supervised setting part data contain action label idea leveraging prediction train feature representation discrimination however present couple interesting idea partially inspired work area biggest concern experimental evaluation experimental section contains large amount figure visualize learned qualitative however quantitative evaluation rarer application compare classification performance another method previously published first author application performance gain motion prediction figure look small compared baseline sure significant recognition handwriting application part evaluated figure difficult understand interpret term besnet used without introduction figure seems tell multiple different story suggest splitting least different figure,5
603.json,pro using neural network domain con clear guaranteed network generates syntactically correct code question comment ntnt accuracy computed maximizing multiplied posterior probability classification combination decision possible ntnt considered using obvious included beginning model since selected size lexicon thus limited possible prediction question likely optimal value alpha also previous comment estimating using section second paragraph compare number comparable,4
551.json,summary present differentiable histogram filter state estimation tracking proposed histogram filter particular bayesian filter represents discretized state using belief prediction step parameterized locally linear translation invariant motion measurement represented multi layered neural network whole system learned supervised unsupervised objective experiment carried synthetic robot localization task major claim problem specific structure bayesian filter state estimation improve pure deep learning approach data efficiency generalization ability nice argument importance prior knowledge deep learning approach specific task histogram filter derived state estimation unsupervised learning possible seems hidden assumption deep learning natural choice recursive state estimation rest built upon assumption including lstm baseline however assumption true bayesian filter first established approach classic problem important justify deep learning even necessary solving task presented request pure bayesian filter baseline experiment derived histogram filter seems particularly designed discretized state space clear well generalized continuous state space using notation interestingly observation discrete binary well eventually make possible derive closed form measurement update setup might constrained generalizing continuous observation trivial task even mention using image observation like haarnoja design choice overall narrow scope applicability,3
443.json,proposes ssnt allow noisy channel conditional generation still allows incremental generation also propose approximate search strategy decoding extensive empirical evaluation pro generally well written ssnt quite interesting application well motivated furthermore empirical evaluation well done obtain good con might concerned whether additional training decoding complexity warranted instance might plausibly obtain benefit proposed approach reranking full output standard seqseq score combining worth noting naacl something similar conversation modeling time able rerank search helpful might nice experiment addressing comment given main thrust provide might slightly clearer section presented perspective modeling instead switching back original initially seems strange suggest noisy channel addressing explaining away problem since explicit uncalibrated term however since seqseq model appear naturally target side language modeling incorporating explicit term seems quite clever,6
785.json,introduces neural network architecture training procedure predicting speed vehicle several second future based video vehicle state input architecture allows several rnns compete make best prediction best prediction receiving back propagation training time step preliminary experimental show scheme yield reduced prediction error clear best performing chosen time point test time integrated prediction obtained prediction minimum error output layer mean prediction cannot made already know value predicted seems possible larger generic might able generate accurate prediction understand correctly competitive architecture many parameter baseline improved performance competitive scheme larger large amount additional work required sustain claim scheme successfully extracting driver intention interesting scheme suitably extended automatically infer intention stop stop sign slowing stopping front pas simply changing lane adding label dataset enable comparison clearly generally intention driver seems related goal pursuing moment fair amount work inverse reinforcement learning examines problem context driving style well,1
785.json,proposes neural network architecture state prediction driving based competitive learning competitive learning creates several duplicate baseline neural architecture training update architecture minimum loss experiment compare competitive learning approach single baseline architecture driving benchmark task understandable could benefit copy editing competitive learning approach seems rather adhoc feel quite incomplete without significant discussion comparison ensembling much recent work shown duplicating ensembling neural architecture produce gain clear competitive learning better ensembling seems le theoretically sound huge confound experiment competitive learning architecture many free parameter baseline architecture think comparing ensembling number architecture duplicated perhaps comparing single baseline larger hidden layer make total number free parameter comparable critical validating proposed approach graphical driving process depicted figure seems nonsensical observed variable known given dependency shown best poor notation driving action decided time affect vehicle state time depends also according figure driving decision depend observed vehicle state also seems invalid paragraph break abstract figure caption include brief explanation variable shown,1
394.json,summary proposes variant dropout applicable rnns state unit randomly retained opposed zero provides noise give regularization effect also prevents loss information time fact making easier send gradient back flow right identity connection without attenuation experiment show work quite well still worse variational dropout penn tree bank language modeling task given simplicity idea likely become widely useful strength simple idea work well detailed experiment help understand effect zoneout probability validate applicability different task domain weakness beat variational dropout maybe better hyper parameter tuning help quality experimental design writeup high quality clarity clear well written experimental detail seem adequate originality proposed idea novel significance interest anyone working rnns large group people minor suggestion mention zoneout thing working noise ability pas gradient back without decay might help tease apart contribution factor example fixed mask unrolled network different time step instead resampling every training case tell much help come identity connection alone,7
681.json,work proposes improvement scattering network linearity allows fourier domain computation compact supported fourier domain representation computing additional variance feature technical contribution seem worthwhile since result better speed improve accuracy unfortunately poorly described evaluated writing clear evaluation broad recommended acceptance since idea merit biggest fault presentation many sentence overly long full unnecessary obfuscating language last paragraph section though unfortunately permeates whole likewise equation made unnecessarily complicated example need line many index psil operator element wise hyperparameter dependency index necessary well repetition iteration reasoning applied equation argument cardinality really help prove variance informative fact could easily write cardinality concatenated quantity cardinality another argument machine learning theory better strive make argument le hyperbolic better substantiated claim finding invariant input abstract fundamental structure last paragraph section really backed math guarantee singling example semantically relevant representation learning procedure machine learning give least guarantee feature building seems heuristic take away main idea part need better researched,4
475.json,proposes novel variational encoder architecture contains discrete variable contains undirected discrete component capture distribution disconnected manifold directed hierarchical continuous component model actual manifold induced discrete variable essence cluster data time learns continuous manifold representation cluster training procedure model also presented quite involved experiment illustrate state performance public datasets including mnist omniglot caltech overall interesting could useful variety application domain approach complex somewhat mathematically involved exactly clear compare relates formulation particularly contain discrete latent variable continuous output prime example graham taylor geoffrey hinton factored conditional restricted boltzmann machine modeling motion style proc international conference machine learning icml discussion certainly added,7
567.json,proposes multiview learning approach finding dependent subspace optimized maximizing cross view similarity neighborhood data sample motivation come information retrieval task position work alternative based multiview learning note however based technique different purpose rather broadly applicable setting considered main point sure mean time complexity appear simply report computational cost evaluating objective equation sense many iteration bfgs method since going difficult given nature optimization problem appreciate sense hard easy practice optimize objective varies various problem dimension argue scalability first concern understandable going make remark computational cost better clarified reported cost small part overall approach rather time complexity since position approach alternative remark even though nonconvex optimization problem solved exactly computational cost linear data size quadratic dimensionality even naive implementation method proposed seem tractable least immediately empirical synthetic data confusing first data generation procedure quite convoluted sure need process coordinate separately different group permute combine simple benchmark take different linear transformation shared representation independent noise suffice confirm proposed method something reasonable also baffled recover true subspace arguably level additive noise impact recoverability however proposed method nearly exact noise level perhaps severe also clear using regularization without regularization funny manner need clarified,3
567.json,present multi view learning algorithm project input different view linearly neighborhood relationship transition probability agree across view good motivation study multi view learning information retrieval perspective concern time complexity algorithm current form high last paragraph page might reason conducted experiment small datasets using linear projection proposed method nice property require projection dimension across view like directly model neighborhood relationship based approach still directly optimizing typical retrieval ranking based criterion hand contrastive loss hermann blunsom multilingual distributed representation without word alignment iclr certainly relevant information retrieval approach shall discussed compared major concern experiment mentioned previous comment limited case linear mapping desirable nonlinear mapping dimension reduction argued linear projection provide better interpretability found empirical justification moreover could achieve interpretability visualizing projection variation input reflected along certain dimension commonly done nonlinear dimension reduction method agree general approach generalizes nonlinear projection easily fact conducted experiment nonlinear projection comparison nonlinear variant multi view learning algorithm limit significance current,3
422.json,mainly well written application explains sgvb applied state space model main idea cast state space deterministic temporal transformation innovation variable acting latent variable prior innovation variable function time approximate inference performed innovation variable rather state solution fairly specific problem discus prior beta depend past interesting application nonetheless idea could explained compactly clearly dive specific fairly quickly seems missed opportunity compliment amount detail appendix experiment example show promise section notation typically betat though variant possible probably better clarify betat given bayesian treatment merely optimized section last paragraph contribution forcing latent space transition seems rather trivial achieve interpretation implies factorization recognition factorization implied anywhere could principle beta,5
658.json,try solve problem interpretable representation focus product network argue spns powerful linear model able learn part combination however representation havent fully exploited generating embeddings pro idea interesting interpretable model representation important topic generating embeddings interpret spns novel idea experiment interesting could extended con author contribution fully clear multiple claim need support example spns indeed interpretable since bottom propagation information visible input could visualized every stage parse could also visualized done amer todorovic another example proposition claim mpns perfect encoder decoder since node always value however uniformally distributed node equal value case address edge case good comparison could generative adversarial network gans generative stochastic network gsns variational autoencoders since state generative model rather comparing rbms nade suggest take sometime evaluate approach suggested method make sure clarify contribution eliminate claiming statement agree comment raised anon reviewer,5
658.json,propose evaluate using generate embeddings input output variable using decode output embeddings output variable advantage predicting label embeddings decouple dependency predicted space show experimentally using based embeddings better produced fairly dense hard read discussion main contribution propose scheme learning using decode output spns embed propose decode partial data perform analysis scheme lead perfect encoding decoding many many experiment comparing various way using proposed method make prediction multi label classification datasets main concern follows point using generative model representation learning experiment main task discriminative predict multiple discriminative baseline regularized logistic regression structure output nice discriminative structured prediction method belief propagation many experiment suggest encoder decoder scheme working better alternative please give detail relative computation complexity method thing still trouble understanding method work better made alternative learning better distribution better separating correlation output individual node larger representation think experiment overkill anything presented detract already many number graph presented easy understand hundred number figure claim correct clear enough said comment please refer shortcut make dense fewer word cost readability think convinced method work love table show condition baseline average result across datasets method average result reasonable best competitor method please show exact match hamming loss demonstrate independent linear prediction structured prediction still plenty number make much easier reader verify claim everything else appendix something like input predicted output decoder hamming exact match baseline given predict decode presentation like make sense really hard time consuming reviewer verify laid currently,5
571.json,extends boosting task learning generative model data strong learner obtained geometric average weak learner normalized normalized rbms generative model genbgm classifier trained discriminate strong learner iteration true data distribution discbgm latter method closely related noise contrastive estimation gans approach benefit strong theoretical guarantee strict condition boosting iteration guaranteed improve likelihood downside method appears lack normalization constant resulting strong learner heuristic weight weak learner seems matter practice discriminative approach suffers expensive training procedure round boosting first requires generating training worth sample previous strong learner sample obtained mcmc experimental section clearly weak point method evaluated synthetic dataset single real world dataset mnist generation feature extraction mechanism classification synthetic experiment clearest showcasing method mnist baseline model much weak convincing modestly sized obtain nats within hour single clearly achievable goal furthermore despite argument contrary firmly believe mixing base learner academic exercise burden implementing different model training algorithm section fails answer fundamental question better train large maximizing elbow train iteration boosting using vaes size baseline experimental detail also lacking especially respect sampling procedure used draw sample also benefit likelihood estimate obtained regard novelty prior work also missing reference self supervised boosting welling cursory read seems strong similarity genbgm approach ought discussed overall fence idea boosting generative model intriguing seems well motivated potential impact reason given theoretical contribution willing overlook issue highlighted hope address time rebuttal,5
571.json,propose approach combine multiple weak generative model stronger using principle boosting approach simple elegant basically creates unnormalized product expert individual expert trained greedily optimize overall joint unfortunately approach joint undesirable property unknown normalisation constant joint therefore intractable likelihood test make drawing exact sample joint intractable problem unfortunately fixed using different base learner direct result product expert formulation boosting experiment dimensional data illustrate proposed procedure work principle boosting formulation produce better individual weak learner better bagging experiment mnist le convincing without undisputable measure like likelihood hard draw conclusion sample figure visually look weak compared even simple model like nade think could improved significantly adding quantitative analysis investigating effect combining undirected undirected autoregressive nade model measuring improvement number base learner require method estimate partition function estimating proxy,4
434.json,extends batch normalization successfully rnns batch normalization previously failed done poorly experiment datasets tackled show definitively improvement batch norm lstms provide standard lstms also cover variety example including character level text word level question answering task pixel level mnist pmnist supplied training curve also quite clearly show potential improvement training time important metric consideration experiment pmnist also solidly show advantage batch norm recurrent setting establishing long term dependency additionally also appreciated gradient flow insight specifically impact unit variance tanh derivative showing batch normalization additionally task figure hugely useful overall find useful additional contribution usage batch normalization necessary information successfully employing recurrent setting,7
526.json,interesting derive bound show satisfies regret bound along empirical evidence cifar cross entropy loss auto encoder loss least empirically comparing observed training loss taylor loss better particular optimizer performs training loss statement validation observed test statement smaller difference also shown regret loss satisfied different scale network layer neuron whole network taylor approximation used investigate activation configuration network used connect difficulty optimizing kink loss surface along empirical study exploration activation surface adam rmsprop optimizers exploration better resulting training loss impact weaker performance could related fixed learning rate anneal learning rate improve performance translate exploration tightening actual loss taylor loss might useful cross validation empirical study like something generalization resulting network reason subscript jacobian change,6
361.json,using bayesian neural network learning curve training algorithm application hyper parameter optimization learning curve terminate run early save time build existing work used parametric learning curve parameter learning curve form last layer bayesian neural network seems like totally sensible idea think main strength address actual need based personal experience high demand working system early termination hyperparameter optimization like know wish asked review question whether plan release code sincerely hope think code significant part contribution since nature practical conceptual experiment seem thorough underwhelming le interested part whether learning curve actually modeled well interested impact hyperparameter optimization hoping speedup result using method left feeling unsure speedup really instead objective function iteration interested inverse plot number iteration needed fixed objective function value since really interested much time save ideally also mention real time sometimes hyperparameter optimization method slow unusable finally figure feel missing histogram termination time different run provide intuition figure tell fraction run terminated early early right sense except least run clearly terminated early since neccessary proposed method outperform method overall think merit acceptance solid effort interesting problem progress fairly incremental live,6
418.json,introduces technique stabilizing training generative adversrial network unrolling inner discriminator optimization loss function several step optimizing generator respect final state optimization process experimental evidence actually help compelling example show problem technique help substantially lstm mnist generator example show procedure help stabilizing training unusual architecture generator image generation experiment definitive convincing future work interesting whether method smaller memory requirement could devised based similar principle strongly recommend accept,8
789.json,propose sample vaes markov chain us confusing notation oversells novelty ignoring relevant previous qualitative difference regular sampling gibbs chain convincing judging figure great workshop perhaps notation discussion related work produce convincing perhaps simply upscaled figure comment rezende original already discus markov chain ignored notation nonstandard confusing page unclear mean approximated also clear meant page called learned distribution general also learned distribution true impossible draw sample sample dataset draw explained whether analysis applies continuous observed space also discrete observed space figure convincing,2
789.json,argues standard ancestral sampling stochastic autoencoders variational autoencoder adversarial autoencoder imposes overly restrictive constraint encoder distribution must marginally match latent variable prior propose alternative markov chain monte carlo approach avoids need specify simple parametric form prior clearly written critically notation either deeply flawed simple misunderstanding respect manipulation probability distribution example seem suggest distribution parametrized true must either trivially simple energy based indication speaking energy based another example possible confusion statement ratio distribution believe supposed ratio marginals overall seems like confusion represent standard notation used vaes represent decoder distribution represent encoder distribution seems using term seem like single consistent interpretation empirical consist entirely qualitative sample reconstruction single dataset celeba sample also quality sota model interpolation shown figure seems look like interpolation pixel space proposed,2
623.json,work present empirical observation support statement hessian loss function deep learning degenerate statement refer understanding least three interpretation hessian loss function deep learning degenerate point parameter space network weight matrix hessian loss function deep learning degenerate critical point hessian loss function deep learning degenerate local minimum global minimum none interpretation solidly supported observation provided comment follows state much information actual hessian look like wonder hessian investigated actual approximate please clarify provide reference computing actual hessian clear whether optimization done batch gradient descent algorithm batch back propagation algorithm stochastic algorithm training done stochastic algorithm hard conclude neural network trained local minimum done full batch algorithm accumulating point local minimum global minimum since negative likelihood function used training essentially joint learning approach newton weight matrix negative likelihood vector certainly whole loss function convex parameter least square error function used make difference claiming degeneracy hessian finally statement still negative eigenvalue even small magnitude puzzling potential reason training algorithm converge accumulating point local minimum saddle point training algorithm converge converged calculation actual hessian might inaccurate,3
336.json,apology late submission review thank author response earlier question submission proposes improved implementation pixelcnn generative improvement small considered specific technical detail dropout skip connection others slightly substantial different likelihood multiscale analysis submission demonstrates state likelihood cifar summary main contribution autoregressive type model pixelcnn example nice class model likelihood evaluated closed form main differentiator type model conditional likelihood pixel conditioned causal neighbourhood modelled line work theis mcgsm theis spatial lstm conditional distribution modelled continuous density real number approach limitation know observed data pixel intensity quantized discrete integer representation discrete distribution could give better likelihood furthermore continuous distribution tail assign probability mass outside valid range pixel intensity hurt likelihood recent work oord colleague conditional likelihood modelled arbitrary discrete distribution possible value pixel intensity suffer limitation continuous likelihood also seems wasteful data efficient propose something middle keeping discretized nature conditional likelihood restricting discrete distribution one whose modeled linear combination sigmoids approach make sense appear revolutionary significant second somewhat significant modification downsampling multiscale modelling opposed dilated convolution main motivation saving computation time keeping multiscale flexibility also introduce shortcut connection compensate potential loss information perform downsampling feel modification particularly revolutionary multiscale image analysis autoregressive generative model done example theis several paper overall felt submission fall short presenting substantially idea read like documentation particular implementation existing idea,5
336.json,review proposes five modification improve pixelcnn generative tractable likelihood empirically showed impact proposed modification using series ablation experiment also reported state result cifar improving generative model especially image active research area definitely contributes pro motivate modification well proposed also used ablation experiment show important discretized mixture logistic distribution conditional distribution pixel instead softmax allows lower output dimension better suited learning ordinal relationship pixel value also mentioned speeded training time le computation well convergence optimization shown make interesting remark dependency color channel pixel likely relatively simple require deep network allows simplified architecture separate feature map group depending whether pixel current location con clear predictive distribution green channel blue look like precisely mean mixture component linearly depending value pixel liked equation minor comment written sequence layer text section say block resnet layer remaining layer first green square blue square white rectangle represents reason mixture indicator shared across three channel,6
766.json,summary proposed edgeboxes fast rcnn batch normalization pedestrian detection review summary cover enough datasets reported improve state writing poor overall work lack novelty clear reject pro show using batch normalization improve con inria include caltech kitti reported fair improving state overall idea limited interest considering work like zhang cvpr fast pedestrian detection zhang eccv faster pedestrian detection issue text quality limited takeaway quality clarity fair poor english originality significance acceptance future conference work need polish improving best known inra caltech kitti ideally present additional insight minor comment text lack polish influent influence maken made usually important important achieve excellent achieve better please consider asking help native speaker future submission also sense sentence computational citation parenthesis citation incorrect family name wrong position joseph lawrence zitnick rodrigo benenson,2
459.json,proposed tensor factorization approach learn cross task structure better generalization presentation clean clear experimental justification convincing mentioned including discussion effect size performance useful final version also work field related question build dmtrl task trained architecture important pretraining random initialization also work data unbalanced namely class example affect,6
662.json,extends trainable memory addressing scheme also investigates continuous differentiable well discrete differentiable addressing mechanism pro extension trainable addressing experiment discrete addressing experiment babi task con memnn performance code available could experiment real world task,6
399.json,strength elegant expanding capacity enabling training large model necessary exploiting large datasets computationally feasible manner effective batch size training drastically increased also interesting experimental effect increasing number moes expected weakness many different way increasing capacity enable exploitation large datasets nice discus alternative term computational efficiency factor,5
376.json,contribution large scale experiment used measure capacity trainability different architecture capacity experiment suggest across architecture rnns store three bit information parameter ungated rnns highest parameter capacity architecture able store approximately floating point number hidden unit trainability experiment show ungated architecture irnn much harder train gated architecture lstm ugrnn also proposes novel architecture ugrnn experiment suggest ugrnn similar parameter capacity ungated much easier train deep layer model easier train existing architecture clarity well written easy follow novelty first knowledge empirically measure number bit information stored learnable parameter idea measuring network capacity finding dataset size hyperparameters maximizes mutual information particularly novel experimental setup proposed ugrnn similar identical minimal gated unit proposed zhou minimal gated unit recurrent neural network international journal automation computing significance mixed feeling significance found experiment interesting feel reveal anything particularly surprising unexpected recurrent network hard experimental change either think rnns future work hand valuable intuitive rnns confirmed rigorous experiment especially since researcher computational resource perform large scale experiment capacity experiment parameter capacity unit capacity essentially force network random data application rnns however expect work random data instead applied machine translation language modeling image captioning number real world task hope rnns learn data anything random clear architecture ability random data beneficial modeling real world data indeed experiment section show architecture vary capacity random data text experiment section show architecture significantly vary capacity real world data think experimental sufficient prove significance proposed ugrnn architecture interesting ugrnn achieve comparable bit parameter ungated deep rnns easily trainable architecture experiment real world task language modeling text show architecture significantly better lstm summary wish experiment revealed surprising insight rnns though certainly value experimentally verifying intuitive proposed ugrnn architecture show promising synthetic task wish showed convincing performance real world task overall think good outweighs idea value community pro first knowledge explicitly measure bit parameter rnns store experimentally confirms several intuitive idea rnns rnns architecture store number hidden unit input different architecture compared parameter count hidden unit count careful hyperparameter tuning architecture perform text language modeling gated architecture easier train gated rnns con experiment reveal anything particularly surprising unexpected ugrnn architecture feel well motivated utility ugrnn architecture well established,6
634.json,present exciting layerwise origin target synthesis method generating large number diverse adversarials well understanding robustness various layer methodology used visualize amount perturbation necessary producing change higher level feature approach match feature another unrelated image interesting go beyond producing adversarials classification also generate adversarials face recognition model result matched instance database presented approach definitely sound interesting original analysis presented relatively shallow touch obvious question much experimental quantitative evidence efficacy method compared approach produce adversarials visualization exciting hard draw meaningful conclusion definitely improve present interesting conclusion based idea,5
634.json,present relatively novel visualize feature hidden unit neural network generate adversarial example idea gradient descent pixel space given hidden unit layer either done choosing pair image using difference activation unit thing gradient descent activation unit given image general method seems intriguing comment clear statement beginning actually true positive negative sign change change class mathematically case moreover contradictory evidence mnist face support intuition pas score given intuition citation think worth explaining actually sentence pas score seems complete correlation infty visual estimation good adversarial example sure take home message number general lot cannot produce high quality adversarial example lower layer seems false mnist liked work include quantitative extract adversarial example different layer training train network compare test addition visualization present main drawback addition basically comparison method hard judge merit work vacuum edit rebuttal thanks addressing experimental validation concern think make interesting revising score accordingly,5
771.json,proposes method estimating context sensitivity paraphrase us inform word embedding learning main idea presented convincingly seem plausible main weakness shortcoming experimental evaluation exploration evaluation convincingly determine whether significant improvement simpler method particularly require paraphrase database likewise section convince obvious formulation stronger choice explained convincingly better alternative explored balance lean towards rejecting encouraging submit revised improved version near point future detailed minor point grammatically mostly correct benefit revision help native english speaker current form long section difficult understand unconventional sentence structure table need better descriptive label somewhat inconclusive particularly analogy task table surprising cbow better semantic aspect task embeddings specifically tailored good enriched cbow included analogy task related work section several paper mentioned learn embeddings combination lexica corpus repeatedly said first work kind enough work feel little misleading,4
771.json,try leverage external lexicon knowledge base improve corpus based word representation determining fuzzy potential paraphrase appropriate particular context think lost translation grammatical storytelling style made really difficult concentrate even unintelligible time important criterion conference communicate idea clearly unfortunately feel meet standard addition evaluation rather lacking many way evaluate word representation google analogy dataset many issue example linzen repeval well drozd coling finally work provide qualitative result motivation method work better fail learned word representation lexicon corpus based method general,2
321.json,interesting work hierarchical control similar work heess experiment strong manage complete benchmark previous work could analysis experiment weaker side like reviewer find term intrinsic motivation somewhat inappropriate mostly current meaning training robot locomotion rewarding speed rewarding grasping manipulating geared towards task later accomplish training task heess identical similar mutual information regularization elegant work generally well seem help complex maze note interpretation analysis result factorization sagent srest clearly detailed duan specify sagent replicability srest clearly specified well miss interesting provide analysis switching behavior agent generally analysis policy failure mode effect switching time performance welcome,6
622.json,extend spin glass analysis choromanska net yield novel dynamic ensemble net connection batch normalization analysis loss surface net well written many insightful explanation although technical contribution extend spin glass analysis one choromanska updated version could eliminate unrealistic assumption analysis provides novel dynamic ensemble connection batch normalization give insightful structure net essential show dynamic behaviour regime without batch normalization untangle normalization effect ensemble feature hence claim steady increase norm weight maintain feature setting figure restrictive empirically support claim least cifar without batch normalization showing effect norm increase support claim theorem strengthen work provides initial rigorous framework analyze better inherent structure current state architecture variant stimulate potentially significant towards careful understanding current state model rather always attempting improve performance net applying intuitive incremental heuristic important progress solid understanding,6
527.json,brief summary explores extension multiplicative rnns lstm type model resulting proposal similar show experimental character level language modeling task general think well written explanation quite clear criticism term contribution weak motivation make sense however similar work done already extension mainly stand application encouraging hand still behind state without using dynamic evaluation standard choice modification standard algorithm parameter rmsprop multiplying output gate nonlinearity experimental limited character level language modeling overview review pro simple modification seems reasonably well practice well written con lack good enough experimental enough contribution almost trivial extension existing algorithm standard modification existing algorithm zhang zhang bengio salakhutdinov multiplicative integration recurrent neural network inadvances neural information processing system sutskever marten hinton generating text recurrent neural network inproceedings international conference machine learning icml,3
527.json,pro clearly written mlstm seems useful according interesting experiment data con number parameter comparison different model missing mlstm behind model task,5
570.json,summary present approach neural answer construction task answering factoid question particular love advice question main feature proposed following incorporates bias semantics behind question word embeddings addition optimizing closeness question answer also optimizes optimum combination sentence predicted answer proposed evaluated using dataset japanese online service shown outperform baseline relatively absolutely also experiment baseline model ablation proposed strength motivation behind proposed approach need understand ambiguous word depending context need generate answer rather selecting answer held site reasonable novelty involves following incorporating bias semantics behind question word embeddings using paragraphvec like modified take input word question question title token question category token modelling optimum combination sentence conclusion supplement sentence predicted answer designing abstract scenario answer inspired automated service composition framework extracting important topic conclusion sentence emphasizing supplemental sentence using attention mechanism attention mechanism similar proposed method shown outperform current best method relatively absolutely seems significant improvement present ablation study provide insight much different component incorporating bias word embeddings incorporating attention conclusion supplement helping towards performance improvement weakness suggestion question abstract pattern determined determine answer love advice question generally constitute sympathy conclusion supplement conclusion encouragement much improvement performance using abstract pattern compared case using patter candidate answer picked union corpus rather picking respective corpus corpus sympathy conclusion seems abstract pattern specific type question abstract pattern love advice different business advice thus seems like abstract pattern need hand coded different type hence cannot generalize across different type present explicit analysis much combinational optimization sentence help comparison performance without combinational optimization keeping rest architecture could also plot accuracy function combinational optimization score provide insight significant combinational optimization score towards overall accuracy say current system designed factoid cannot generalize question outside stored site claim contribution order ground claim show experimentally well proposed method generalized domain question although question asked human expert human evaluation evaluation datasets analyze different question compared question present evaluation datasets human evaluation output proposed lstm judged judged human expert human expert judged output system human expert judged output system set output judged human expert rating expert combined every question wonder human evaluation human worker expert compare output proposed lstm output like hear asking advice evaluation biased whether sentence good whether combination good looking qualitative example table personally like output lstm proposed seem provide direct answer question first example output lstm say wait feel excited whereas output proposed say better concentrate confess love seems indirect question asked given question ground truth answer different task answer selection answer construction mention attentive lstm evaluated current best answer selection method section accuracy lower lstm table explain pointing issue question long compared answer hence attention noisy issue exist dataset used say proposed method achieves gain current best conclusion section refer lstm current best method however description attentive lstm section mention attention lstm current best method could please clarify discrepancy minor correction remove space abstract review summary problem factoid dealt interesting useful problem solve motivation presented behind proposed approach reasonable experiment show proposed outperforms baseline however abstract pattern determine answer seems like hand designing hence seems like abstract pattern need designed every type factoid question hence proposed approach generalizable type also need analysis provide insight contribution different component,3
659.json,proposes sequence transduction first us traditional statistical alignment method provide alignment encoder decoder type provides experiment number morphological generation datasets show improvement model although much smaller improvement soft attention task found well written thorough experiment analysis concern work particularly different previous approach thus focused contribution limited application type shorter input suggest approach sufficient shorter sequence compare approach chorowski jailty summary found well executed well written novelty scope small said feel work make good short,4
659.json,proposes approach sequence transduction case monotonic alignment input output plausible assumed alignment provided part training data chinese restaurant process used actual experiment idea make sense although applicability limited domain monotonic alignment available discussed review period strongly overlapping related work probabilistic model hard alignment sequence transduction recurrent neural network graf also attempt external alignment model neural transducer jaitly said think approach sufficiently novel also concern regarding evaluation think fair compare proposed depends external alignment vanilla soft attention learns alignment scratch control experiment soft attention could trained match external alignment pretraining could reduce overfitting small dataset proposed approach brings improvement larger dataset especially sigmorphon improvement obtained certain class language main issue lack novelty comparison trained external alignment without,3
589.json,proposes combine graph convolution rnns solve problem input graph idea graph convolutional layer used extract feature matrix multiplication replaced graph convolution operation applied language modelling yielding lower perplexity penn treebank compared lstm outperformed lstm moving mnist model idea actually trivial line current trend combining different architecture instance idea replacing matrix multiplication graph convolution small extension regarding experiment section skeptical experiment carried reason instead using given development tune model blindly used available configuration different pro good experimental con idea quite trivial experiment carried improperly,3
474.json,summary present advanced self learning extract compositional rule bach chorale extends previous work rule hierarchy conceptual informational dimension adaptive memory selection assumes feature follow dirichlet distribution sonority column midi number act word language unigram statistic used learn fundamental rule music theory gram higher order help characterize part writing sonority clustered together based feature function iteration partition induced feature recognized rule sufficiently significant result sample syllabus different difficulty stride satisfactory gap generated term set learned rule quality strength exploration hierarchy dimension make learning process cognitive interpretable also demonstrate effective memory selection speed learning flaw discussed might limit learning interpretation capacity proposed failing capture long distance dependence music reply question mentioned experimented sure related included besides elaborated interpretation survey seeking opinion student music department might make evaluation system performance persuasive clarity pro clearly delivers improved automatic theorist system learns represents music concept well thoroughly interprets compare learned rule music theory proper analogy example help reader perceive idea easily con although detailed definition found previous rover paper great described optimization clearly figure related part conceptual hierarchy filter equation prime symbol appear subscript originality representation music concept rule still open area investigate topic novel illustrates alternative besides interpretable feature learning method autoencoders significance good corresponding interpretation learned rule music theory mentioned student music could involved self learning loop interact interesting hope advantage combined practice music theory teaching learning,7
618.json,sincerely apologize late review first part strong emphasis technical part could benefit high level argument method aim achieve limitation overcome misunderstood contribution case please correct main novel part suggestion learn group parameterizations instead fixing instead applying common spatial filter brabandere applied steerable frame first contribution suggests general frame base better suited represent sensory input data commonly used pixel basis experiment cifar indicate true general considering basis hyper parameter expensive search conducted find gauss frame give better assume suggest gauss frame always better least weak evidence single network presented maybe first contribution stated pixel network representation corrected larger number parameter someone interested using runtime consideration strongly suggest improve figure us several time different notation depiction mix box single symbol illustrative figure took time decipher figure flow summary sufficiently clear technical many place readability improved introduction frame beginning lack motivation rather unclear someone concept work fall general category method impose knowledge filter transformation network architecture always side algorithmic technical part several way practical side possible approach problem wondering learned certainly inspired based content integrate build work lacking insight transformational parameter relevant problem spatial transformer network weaker technical elegance side provided exactly insight feature transformation learned algorithm missing table learn among four choice work empirically better destroyed hermite frame resnet able recover construct network architecture superset performance could avoided algorithm clear similar dynamic filter network unfortunately convinced usefulness particular formulation expect stronger insight transformation comparison standard technique clear delineation advised,3
403.json,introduces novel memory mechanism ntms based differentiable group allows place memory element point manifold still allowing training backpropagation general version memory possibly allows training efficient addressing scheme pro novel interesting idea memory access nicely written con need manually specify group better network could learn best accessing memory clear really work better standard compared simplified version clear useful practice comparison real task,5
546.json,attempt extract analytical equation governing physical system observation important task able capture succinct interpretable rule physical system follows great importance however simple naive tool scale complex task offering insight advance field contribution first four page submission summarised sentence learn weight small network cosine sinusoid input element product activation function weight sparse learnt network weight fixed structure presented learnt equation research us tool literature seen abbreviation page long time build modern technique advanced since encourage review modern literature continue working important task,2
792.json,introduced regularization scheme soft target produced mixing true hard label current prediction similar method proposed section hinton distilling knowledge neural network pro comprehensive analysis label similarity con weak baseline sure found best hyper parameter experiment trained layer fully connected mnist hidden unit without regularizer achieved using adam initialization reported architecture failed bring novel idea similar hinton probably enough iclr,3
511.json,familiarity dealt parabolic diffusion past detail transforming problem supervised loss escape therefore indicated review taken educated guess imagine many reader iclr face similar problem accepted least prepare appendix provides introduction high level comment seems another disadvantage approach network must trained domain including domain size system function boundary condition correct wonder worth trouble existing tool already solve shed light unifying approach require minimal change generalize across sensitive network result domain different size seems single size tested error increase domain size general approach type diffusion,4
442.json,update raised score think argument adversarial example compelling think convincingly prof method act decent regularizer convinced competitive regularizer example believe sufficient evidence give better regularizer dropout normalization also think much harder tune method discussed rebuttal reply summary understand correctly proposes take bottleneck term variational autoencoders pull latent variable towards noise prior like apply supervised learning context reconstruction term replaced usual supervised cross entropy objective argument effective regularizer increase robustness adversarial attack pro presentation quite good easy follow idea reasonable relationship previous work well described robustness adversarial example experiment seems convincing though expert area compare external quantitative baseline robustness adversarial example help since sure method compare regularizers term combatting adversarial example example us high dropout rate confer comparable robustness adversarial example perhaps expense accuracy con mnist accuracy seem strong unless missing something maxout icml listed many permutation invariant mnist error rate error rate listed necessarily prove method competitive regularizer also suspect tuning method make work well harder regularizers like dropout many distinct architectural choice method particularly many hidden layer come example output could directly follow could several layer output tell say simple logistic regression weight matrix followed softmax obvious choice made work best empirically wonder happen trained discovered adversarial example also using method learn higher variance presented adversarial example,6
442.json,summary deep variational information bottleneck explores optimization neural network variational approximation information bottleneck tishby example mnist show used regularization improve robustness adversarial attack review potentially useful important application regularization adversarial robustness privacy mentioned combining recent advance deep learning make widely applicable excellent idea given theoretical contribution fairly straight forward application well known idea liked stronger experimental section since proposed approach allows scale better demonstration larger problem mnist also clear whether proposed approach still work well regularize interesting network many layer dropout included quantitative comparison robustness adversarial example figure number sample chosen error bar figure page claim posterior covariance becomes larger beta decrease increase really case hard judge based figure since figure differently scaled might worth comparing variational fair autoencoders louizos also learn representation minimizing information shared aspect input well written easy follow,5
317.json,present amortised estimation method problem learning neural network learns project affine subspace solution consistent method method enables finding propoer solution using variety method gans noise assisted density assisted optimisation nicely demonstrated several datasets like though feel writing quite presentation made clearer hard follow time considering subject matter quite complicated making clearer help also love analysis resulting network kind feature learn,6
747.json,propose measure feature importance specifically pixel contribute network classification image simple albeit particularly effective heuristic measuring feature importance measure gradient predicted class pixel input image assigns score pixel rank much output prediction change given pixel change build propose measure feature importance computing gradient output scaled version input image alpha alpha scalar summing across value alpha obtain feature importance score scaling simply linear scaling pixel value alpha black image alpha original image call scaled image counterfactuals seems like quite unnecessarily grandiose name literally scaled image show number visualization indicate proposed feature importance score reasonable looking gradient respect original image also show quantitative evidence pixel highlighted proposed measure likely fall object rather spurious part image particular figure method also applied type network quantitative evidence quite limited spent qualitative goal understanding deep network importance clear whether really help elucidate much main interesting observation scaling image small alpha creating faint image place importance pixel object related correct class prediction beyond build deeper insight gained propose hand wavy explanation using small alpha faint image force network focus object argument convincing interesting probe deeper easy ultimately clear proposed scheme feature importance ranking useful first still quite noisy truly help understand deep particular image performing single gradient descent step image collection scaled version image hardly begin probe internal working network moreover admit scheme make assumption pixel independent clearly false considering present simple idea long main page reference appendix general writing long winded overly verbose detracted substantially also define unnecessary terminology gradient coutnerfactuals sound quite fancy related idea explored writing encourage tighten writing figure readable page length clearly spell idea explored early,2
481.json,main contribution applying adversarial training imagenet larger dataset previously considered comparing different adversarial training approach focusing importantly transferability different method also uncover explain label leaking effect important contribution clear well written good assessing comparing adversarial training method understanding relation another wide range empirical shown help elucidate adversarial training procedure make important contribution towards understand adversarial training believe iclr appropriate venue work,6
710.json,applies challenging bioacoustics segmentation problem including humpback whole sound bird sound segmentation although technique novel application data driven method bioacoustics segmentation quite challenging yield scientific finding valuable contribution bioacoustics field concern fair comparison simple method including better provide comparison especially pointed computational cost issue simple method solve issue,4
655.json,work proposes basic probability assignment improve deep transfer learning particular weighting scheme inspired dempster shaffer exploiting confusion matrix source task introduced also suggest learning convolutional filter separately break convexity main problem writing many typo presentation clear example training weak classifier constructed remains unclear despite author previous answer explanation training validation set compute also convinced convexity problem author provide ablation study validate necessity separately learning filter last question cifar three channel mnist handled pairing datasets second experiment overall believe proposed idea reweighing interesting work globally improved clarified suggest reject,2
593.json,proposed variant semi supervised lead unified objective supervised unsupervised variant give software implementation model flexibility specifying variable supervised development introduces extra term compared original semi supervised formulation proposed kingma experiment seems term much formulation performance difference proposed method kingma significant figure therefore benefit formulation likely software engineering flexibility convenience flexibility convenience nice better demonstrate situation proposed method applied previous method trivial title written make expect currently expecting example structured hidden variable posterior page really structured interpretation generative title main contribution variant semi supervised quite aside plug estimation discrete variable work function continuous function however continuous example take form another form approach using expectation replace work therefore plug estimation limitation,5
593.json,introduces variant semi supervised variational auto encoder framework present introducing structure observed variable inside recognition network find presentation inference auxiliary variable could avoided actually make presentation unnecessarily complicated specifically expression auxiliary variable helpful devising unified implementation modeling wise without auxiliary variable recover minimal extension part generating space actually observed observed variable mean posterior need also condition incorporate information convey done actually different kingma surprised experiment show large deviation method given similarity model useful could give possible explanation superiority method compared kingma wondering experimental setup kingma bottom mention cnns feature extraction clear kingma related note wondering comparison jampani particular also using rate supervision fair comparison experiment section interesting demonstrates useful property approach discussion supervision rate review answer helpful giving insight successful training protocol semi supervised learning overall interesting title introduction made expect something title expected method interpreting general deep generative model instead described approach semi supervised variant naturally including labelled example disentangles latent space general property semi supervised probabilistic unique approach described moreover intro expected general approximation scheme variational posterior similar ranganath trully allows flexible distribution however case given contribution defining slight variant semi supervised perhaps importantly formulating amendable easier automation term software methodologically much contribution current literature mention plan extend framework probabilistic programming setting seems indeed promising useful extension minor note three kingma paper cited main text kingma causing confusion suggest using kingma,4
439.json,present technique combine deep learning style input output training search technique match input program provided output order magnitude speedup augmented baseline presented summary proposed search source code implementation based rather small domain specific language compelling also expected degree quality well written clarity derivation intuition could explained detail main story well described originality suggested idea speed search based technique using neural net perfectly plausible significance experimental setup restricted smaller scale illustrated improvement clearly apparent detail employed test program seems rather small addition ensure test program semantically disjoint training program could provide additional detail small size test disjoint property enforced length program rather small point time detailed ablation regarding runtime seems useful search based procedure probably still computationally expensive part hence neural provides additional prior information rather tackling real task,5
706.json,proposes piecewise constant parameterisation neural variational model could explore multi modality latent variable develop powerful neural model experiment neural variational document model variational hierarchical recurrent encoder decoder model show introduction piecewise constant distribution help achieve better perplexity modelling document seemly better performance modelling dialogue idea piecewise constant prior latent variable interesting well written even page long design experiment fails demonstrate claim detailed comment follows author explains limitation vaes standard gaussian prior last paragraph last paragraph hence multimodal prior help vaes overcome issue optimisation however lack evidence showing multimodality prior help break bottleneck last paragraph author claimed decoder parameter matrix directly affected latent variable connects decoder combination piecewise constant gaussian latent variable matter discovered experiment show,3
643.json,interesting relates finding neurscience biology method sparse coding adaptive able automatically generate even delete code data coming nonstationary distribution point make algorithm could discussed give solid view contribution technique novel spirit code added needed removed dont much relate organization data behavior method building shown first natural image le structured difficult later perform curriculum learning happens data simply change structure apparent movement simple complex flower bird fish leaf tree make sense improvement training data structure going something artificial simpler complex le structured domain interesting idea useful interesting insight sure ready publication,4
375.json,good interesting probabilistic motivation weighted word model hopefully soon added comparison wang manning make stronger though sufficiently large datasets still work better second last paragraph introduction describe problem large cooccurrence count already fixed glove embeddings weighting function minor comment capturing similarity typo line intro recently wieting learned citet instead parenthesized citation,6
621.json,proposed coconet neural autoregressive convolution music composition task also proposed blocked gibbs sampling instead ancestral sampling original nade generate better piece music experimental showed coconet better baseline human evaluation task amazon mechanical turk illustrated generate compelling music general think good using nade based convolution operation music generation task using blocked gibbs sampling contains kind novelty however novelty incremental since blocked gibbs sampling nade already proposed using nade based music modeling also proposed boulanger lewandowski,5
564.json,think backbone interesting could lead something potentially quite useful like idea connecting signal processing recurrent network using tool setting however work nugget interesting observation feel together better think writeup everything improved urge strive think idea connect past interesting nice experiment understand better connection help,5
657.json,describes approximate fasttext approach memory footprint reduced several order magnitude preserving classification accuracy original fasttext approach based linear classifier word embeddings type method extremely fast train test size quite large focus approximating original approach lossy compression technique namely embeddings classifier matrix compressed product quantization aggressive dictionary pruning carried experiment various datasets either small large number class conducted tune parameter demonstrate effectiveness approach negligible loss classification accuracy important reduction term size memory footprint achieved order fold compared original size well written overall goal clearly defined well carried well experiment different option compressing data evaluated compared also interesting nevertheless propose novel idea text classification focus adapting existing lossy compression technique necessarily problem specifically introduces straightforward variant unnormalized vector dictionary pruning cast covering problem hard greedy approach shown yield excellent nonetheless hashing trick bloom filter simply borrowed previous paper technique quite generic could well used work minor problem made clear full size computed exactly proportion full size matrix dictionary rest account hard follow size bottleneck also seems depend target application small large number test class nice provide formula calculate total size function parameter dictionary number class part lack clarity instance greedy approach prune dictionary exposed le line page though straightforward likewise clear binary search used hashing trick introduce overhead hundred overall look like solid work potentially limited impact research wise,5
728.json,start pointing need method perform state temporal representation learning allow gaining insight learned perhaps order allow human operator intervene necessary important goal practical point view great research direction reason like encourage pursue however convinced current incarnation work right answer part issue related presentation part require rethinking order interpretability fairly specific way performing abstraction example skill always start single skill initiation state likewise state seems unnecessarily restrictive clear restriction needed convenience similarly clustering basis forming higher level state specific kind clustering used clear done clustering opposed method ensuring temporal coherence particular form employed also seems restrictive reference supplementary material choice explained could find posted version either explain clearly specific choice necessary even better think still keeping interpretability presentation point view benefit formal definition amdp samdp well formal description algorithm employed constructing representation bellman equation model update rule algorithm learning intuition given math precisely stated overhead constructing samdp computation time space clarified well experiment well carried nice gridworld experiment visualization easy perform understand well atari game gridworld still place despite reviewer might positive proposed approach many moving part rely specific choice significance general ease unclear point perhaps complete supplementary helped respect small comment line contain typo notation wrong sign equation,3
682.json,based previous work stepped sigmoid unit relu hidden unit discriminatively trained supervised model leaky relu proposed generative learning interesting unlike traditional first defining energy function deriving conditional distribution propose form conditional first derive energy function however general formulation novel generalized exponential family glms earlier focus specifying conditionals joint marginal becomes complicated hard compute experiment nice binary visbles leaky relu hiddens demonstrate superiority leaky relu hidden unit addition binary mnist modeling compare correct annealing distribution longer gaussian perhaps faast experiment compare agains baseline trained using fast interesting combine hidden function easiness annealed sampling however baseline comparison stepped sigmoid unit nair hinton model like spike slab rbms others missing without comparison hard tell whether leaky relu rbms better even continuous visible domain,4
682.json,proposed variant nonlinearity leaky relu contrast sigmoid function nonlinearity gradually annealing leakiness coefficient corresponding gaussian gaussian sample higher mixing rate idea annealing leakiness show estimate partition function accurately main comment proposed account real valued data however primarily used binary data real valued gaussian well recognized real valued data demonstrate superiority author also include comparison binary data also enough compare datasets newly proposed claim marginal distribution visible variable truncated gaussian incorrect truncated normal value variable constrained within region requiring variable region,4
378.json,proposes novel exploration strategy promotes exploration appreciated reward region proposed importance sampling based approach simple modification reinforce experiment several algorithmic task show proposed performing better reinforce learning show promising automated algorithm discovery using reinforcement learning however clear main motivation main motivation better exploration policy gradient method benchmarked algorithm standard reinforcement learning task huge body literature improving reinforce considered simple version reinforce standard task urex better main motivation improving performance algorithm learning task baseline still weak make clear main motivation also action space small beginning raise concern entropy regularization might scale larger action space comparison ment urex large action space problem give insight whether urex affected large action space rebuttal missed action sequence argument pointed small action space issue question regarding weak baseline several trick used literature tackle high variance issue reinforce example mnih gregor increased rating still encourage improve baseline,6
689.json,proposes generative mixture basic local structure dependency local structure tensor tensor decomposition result earlier expressive power cnns along hierarchical tucker provide inference mechanism however conditioned existence decomposition discus applicable method general case subspace decomposition exists efficient approximation error answer question deep learning theoretical analysis needed claim subjective need emphasize clarify claim mention restriction hence theoretical perspective flaw claim justified completely claim cannot justified current tensor literature also mentioned discussion therefore corrected claim made clarification approach restricted clear subclass tensor ignore theoretical aspect consider empirical perspective experiment appear enough accept mnist cifar simple baseline extensive experiment required also experiment missing data covering real case synthetic also lack extension beyond image since repeatedly mention approach go beyond image since theory part complete experiment essential acceptance,3
328.json,fit model spike train retinal ganglion cell driven natural image think title thus include word activity otherwise actually formally incorrect anyhow proposes specifically recurrent network time series prediction compare seems previous approach generalized linear overall stated paradigm predict spike well look learn nature general sound plausible though convinced learned figure show predict spike better nice shown complicated produce better fit data though course still variation real data initial outline better predictive help better understand neural processing retina tell learned specialist retina know several layer recurrencies retina surprised better seems complicated recurrent model lstm improve performance according statement however comparison level also difficult complex model need data hence actually expect layer even detailed retina could eventually improve prediction even also puzzled neuron network share parameter weight show simplified model capture spike train characteristic free parameter eventually outperform correspondingly training data,3
684.json,proposes based reinforcement learning approach focusing predicting future reward given current state future action achieved residual recurrent neural network output expected reward increase various time step future demonstrate usefulness approach experiment conducted atari game simple playing strategy consists evaluating random sequence move picking highest expected reward enough chance dying interestingly game tested exhibit better performance agent trained multitask setting learning game simultaneously hinting transfer learning occurring submission easy enough read reward prediction architecture look like original sound idea however several point believe prevent work reaching iclr detailed first issue discrepancy algorithm proposed section actual implementation section experiment section output supposed expected accumulated reward future time step single scalar experiment instead number probability dying another probability higher score without dying might work better also mean idea presented main body actually evaluated guess work well otherwise implement differently addition experimental quite limited game hand picked easy enough comparison technique friend realize main focus exhibiting state since policy used simple heuristic show prediction used drive decision said think experiment tried demonstrate obtain better reinforcement learning algorithm actually reinforcement learning done since supervised algorithm used manually defined hardcoded policy another question could addressed experiment good prediction classification error dying probability future reward compared simpler baseline finally previous work section limited focusing particular saying little topic based think like instance action conditional video prediction using deep network atari game obvious must cite minor comment notation unusual denoting state rather action potentially confusing reason stray away standard notation using tensor concatenation great choice either since usually indicates product residual nothing reward defined control performed time instead seems control performed time recurrent confusion mean median used observation since layer normalization inequality observation observation proof take much space simple result first probability dying come nowhere since know output approach able learn good strategy mean good strategy please mean fully connected nice also architecture differs classical architecture mnih please clarify answer openreview comment table say iteration iteration confusing figure show degradation pong demon attack seems worse actually learned random play able play least time better clear come demon attack plot figure show potential problem mentioned earlier mentioned,3
734.json,update read reply thread opinion changed propose deep vcca deep version probabilistic using likelihood parameterized nonlinear function neural net variational inference applied inference network reparameterization gradient additional variant termed vcca private also introduced includes local latent variable data point view connection multi view auto encoder also shown since development black variational inference variational auto encoders methodology specific paper like arguably interesting straightforward extension probabilistic neural parameterized likelihood inference mechanically black approach using reparameterization gradient inference network approach also us mean field approximation quite given many recent development expressive approximation rezende mohamed tran connection multi view auto encoders first insightful difference variational inference well known insight abstract argue distinction additional sampling ultimately matter regularizer even noisy sample variance normal variational approximation collapse zero thus become point mass approximation equivalent optimizing point estimate mvae objective suspect know degree remark unclear said think strong merit application experiment strong comparing alternative multi view approach number interesting data set private variable simple demonstrate successfully disentangle view latent representation shared view preferable compare method using probabilistic inference full bayes linear also number approximation taken almost standard necessary mean field family inference network separate much approximate inference strongly recommend using mcmc amortized variational inference least experiment rezende mohamed variational inference normalizing flow presented international conference machine learning tran ranganath blei variational gaussian process presented international conference learning representation,4
775.json,straightforward easy read clear since parameterisations outputting torque seems like much difference known function convert representation another least torque possible reason proportional control little better tracking cost function position result reference pose cost locomotion cost result task spin guess work interesting likely generalise scenario sense rather limited video nice,5
775.json,study deep reinforcement learning paradigm controlling high dimensional character experiment compare effect different control parameterizations torque muscle activation control target joint position target joint velocity performance reinforcement learning optimized control policy evaluated different planer gate cycle trajectory illustrated abstract parameterizations fact better result robust higher quality policy significance originality explored parameterizations relatively standard humanoid control real novelty systematic evaluation various parameterizations think type study important insightful however finding specific problem specific tested architecture clear finding transferable network control problem domain iclr community limited breadth perhaps broader appeal robotics graphic community clarity well written pretty easy understand someone background constrained multi body simulation control experiment experimental validation lacking somewhat opinion given fundamentally experimental liked analysis sensitivity various parameter analysis variance performance policy optimized multiple time,5
325.json,train generative transforms noise sample gradual denoising process similar generative based diffusion unlike diffusion approach us small number denoising step thus computationally efficient rather consisting reverse trajectory conditional chain approximate posterior jump run direction generative allows inference chain behave like perturbation around generative pull towards data also seems somewhat related ladder network tractable variational bound likelihood liked idea found visual sample quality given short chain impressive inpainting particularly nice since shot inpainting possible generative modeling framework much convincing likelihood comparison depend parzen likelihood detailed comment follow theta theta theta theta using like infer inference order generative chain reminds slightly ladder network learned paragraph break learned learn inverse learn reverse experiment experiment sensitive infusion rate appears provide accurate model think showed direct comparison sohl dickstein neat,7
598.json,numerous work learning waveform training letter based network speech recognition however work combining purely convnet done interesting large scale corpus librispeech used though baseline hybrid system provided reader unclear system close state table contribution sequence training criterion variant blank symbol dropped viewed sequence training learning acoustic frame labeling speech recognition recurrent neural network however instead generating denominator lattice using frame level trained first directly compute sequence level loss considering competing hypothesis normalizer therefore trained perspective closely related povey sequence training hmms another reviewer pointed reference discussion provided approach expensive frame level training ctc however table implementation much faster system sampling rate said step size also baidu system also tried increasing step size people found work equally better significantly computational cost,5
598.json,submission proposes letter level decoder variation approach call blank symbol dropped replaced letter repetition symbol explicit normalization dropped description letter level though novel well variant interesting approach evaluated librispeech task claim approach competitive compare modelling variant comparison letter level approach available word level missing compared obtained panayotov performance obtained seems comparable word level model worse word level hybrid model though panayotov also appled speaker adaptation done suggest comparison panyotov addition mentioning baidu librispeech comparable much larger amount training data allow reader quantitative idea pointed text baidu implementation aimed larger vocabulary therefore comparison table seem helpful work without discussing implementation using quite huge analysis window nearly even though also window mrasta feature comment arrive large window advantage observe interesting submission well written though detail experience using normalized transition score beam pruning desirable table better readable unit number shown shown within table caption prior partial publication work nip workshop clearly mentioned referenced mean transition scalar repeat comment already given review period minor comment sentence train properly train properly paragraph boostrap bootstrap error avoided performing automatic spell check bayse bayes definition logadd wrong comment applies also nip workshop line possible sequence letter possible sequence letter plural first line threholding thresholding spell check figure mention corpus used,6
448.json,present mathematical analysis information propagated deep feed forward neural network novel analysis addressing problem vanishing exploding gradient backward pas backpropagation dropout algorithm clear well written analysis thorough experimental showing agreement nice,7
673.json,find compelling basic idea seems fast neighbor searcher memory augmented make memory lookup scalable however precisely point number standardized neighbor searcher understand choose benchmark standard moreover test problem clear need vector based fast hashing text also find repeated distinction mips distracting library input modified switch problem indeed convert mc problem,3
366.json,proposes neural variational inference method topic model show nice trick approximate dirichlet prior using softmax basis gaussian trained maximize variational lower bound also study better alleviate component collapsing issue problematic continuous latent variable follow gaussian distribution look promising experimental protocol sound fine minor comment please citation neural variational inference typo approximation dirichlet prior distribution approximation dirichlet prior distribution table written dmfvi trained failed deliver result wait report number table perplexity collapsed gibbs nvdm lower proposed model prodlda generates coherent topic intuition training speed convergence differs using different learning rate momentum scheduling approach shown figure also interesting analysis latent variable component collapsing although indirectly show learning rate momentum scheduling trick remove issue overall clearly proposes main idea explain good experimental support original claim explains well challenge demonstrate solution minh neural variational inference learning belief network icml rezende stochastic backpropagation approximate inference deep generative model icml,6
366.json,interesting framework topic model main idea train recognition inference phase called amortized inference much faster normal inference inference must iteratively every document comment find notation theta alpha awkward alpha generative seems agnostic document length meaning latent variable generate probability word space however recognition happy radically change probability document length change input change seems undesirable maybe normalize input recognition network prodlda might well equivalent exponential family variant thereof,5
537.json,address problem decoding barcode like marker depicted image main insight train generated data produced trained using unlabeled image leverage undergoes learnt image transformation blur lighting background parameter image transformation trained confuses discriminator trained using image generated compared hand crafted feature training real image proposed method performs baseline decoding barcode marker proposed architecture could potentially interesting however champion evaluation could improved critical missing baseline comparison generic without hard judge benefit structured also worth seeing result combine generated real image final task couple reference relevant work object detection using rendered view shape xingchao peng baochen karim kate saenko learning deep object detector model iccv deep exemplar detection adapting real rendered view francisco massa bryan russell mathieu aubry cvpr problem domain decoding barcode marker bee limited great applied another problem domain object detection model shown reference direct comparison prior work could performed found writing somewhat vague throughout instance first reading introduction clear exactly contribution minor comment really render image look like image perhaps spatially warped homography page chapter section table loss used dcnn last four image look like strange artifact explain,4
496.json,proposes novel variant recurrent network able learn hierarchy information sequential data character word approach require boundary information segment sequence meaningful group like chung organized layer capturing information form different level abstraction lowest level activate upper decide update based controller state cell called feature discrete variable allowing potentially fast inference time however make challenging learn leading straight estimator hinton experiment section thorough obtain competitive performance several challenging task qualitative show also capture natural boundary overall present strong novel promising experimental minor note remark complaint writing related work introduction principle learning deep neural network well human brain please provide evidence human brain part claim modelling temporal data recent resurgence recurrent neural network remarkable advance believe missing mikolov reference spite fact hierarchical multiscale structure naturally exist many temporal data missing reference related work recent clockwork koutnk extends hierarchicalrnn hihi bengio extends narx hihi bengio model focus online prediction problem prediction need made believe missing reference particular socher work older recursive network norm gradient clipped threshold pascanu first work using gradient clipping believe introduced mikolov missing reference recurrent neural network based language mikolov learning long term dependency narx recurrent neural network sequence labelling structured domain hierarchical recurrent neural network fernandez learning sequential task incrementally adding higher order ring,7
316.json,discus guarantee privacy training data proposed approach multiple model trained disjoint datasets used teacher train student predict output chosen noisy voting among teacher theoretical nice also intuitive since teacher result provided noisy voting student duplicate teacher behavior however probabilistic bound quite number empirical parameter make difficult decide whether security guaranteed experiment mnist svhn good however claim proposed approach mostly useful sensitive data like medical history nice conduct experiment application,6
419.json,present topicrnn combination augments traditional latent topic switching variable includes excludes additive effect latent topic generating word experiment task performed language modeling sentiment analysis imbd show topicrnn outperforms vanilla achieves sota result imdb question comment table feature feature like lstm included even though lower perplexity topicrnn think still useful much adding latent topic close lstm generated text table meaningful supposed highlight generated text topic trading imdb scalable proposed method large vocabulary size accuracy imdb extracted feature used directly perform classification instead passed neural network hidden state think fairer comparison method presented baseline,6
462.json,proposes idea help defending adversarial example training complementary classifier detect show adversarial example fact easily detected moreover detector generalizes well similar weaker adversarial example idea simple trivial final scheme proposed idea help building defensive system actually provides potential direction based novelty suggest acceptance main concern completeness effective method reported defend dynamic adversary could difficult rather seem much effort investigate part difficult defend dynamic adversary important interesting question following conclusion investigation essentially help improve understanding adversarial example said novelty still significant minor comment need improve clarity important detail skipped example provide detail dynamic adversary dynamic adversary training method,6
383.json,look solid idea natural seem promising well mostly concerned computational cost method day gpus relatively tiny datasets quite prohibitive application ever encounter think main question approach scale larger image also applied exotic possibly tiny datasets experiment caltech instance curious approach suitable data regime area know right away suitable architecture look like cifar mnist svhn everyone know well reasonable initialization look like show proof discover competitive architecture something like caltech recommend publication minor resnets mentioned table,5
356.json,set tackle program synthesis problem given input output pair discover program generated propose bipartite component generative tree structured program component input output pair encoder conditioning consider applying many variant basic flashfill experiment explore practical dataset achieve fine number range model considered carefulness exposition basic experimental setup make valuable important area research question think strengthen think worth accepting question comment dataset good choice simple easy understand effect rule based strategy computing well formed input string clarify backtracking search assume trying generate latent function general describing accuracy increase sample size could summarize simply reporting probability latent function perhaps worth reporting sure missed something,6
356.json,proposes able infer program input output example pair focusing restricted domain specific language capture fairly wide variety string transformation similar used flash fill excel approach successive extension program tree conditioned embedding input output pair extension probability computed function leaf production rule embeddings main contribution called recursive reverse recursive neural computes globally aware embedding leaf something look like belief propagation tree training operation differentiable many strong point contrast related work deep learning community imagine used actual application near future idea good motivate quite well moreover explored many variant understand work well finally exposition clear even long made pleasure read weakness still super accurate perhaps trained small program asked infer program much longer unclear simply train longer program also seems number pair fixed pair might able additional pair based experiment pair hurt overall however certainly like accepted iclr miscellaneous comment many expansion probability expression might better write softmax comment adding bidirectional lstm process global leaf representation calculating score detail given done claim using hyperbolic tangent activation function important interested discussion something like relu good unclear batching done setting since program different tree topology discussion appreciated related good detail optimization algorithm adagrad adam learning rate schedule weight initialized moment particularly reproducible figure unsolved benchmark great program size harder example approach fail benchmark require long program reason missing related work piech learning program embeddings trained recursive neural network matched abstract syntax tree program submitted online course predict program output synthesize program,7
478.json,pro representation nice property derived compared mathematical baseline background simple algorithm obtain representation con sound like applied math analysis nature representation could done instance understanding nature layer least first,7
400.json,proposes variant recurrent neural network orthogonal temporal dimension used decoder generate tree structure including topology encoder decoder setting architecture well motivated several application addition presented need generate tree structure given unstructured data weakness limitation experiment ifttt dataset seems interesting appropriate application also synthetic dataset however interesting natural language application syntactic tree structure still consider experiment sufficient first step showcase novel architecture strength experiment different design decision building topology predictor component architecture decide terminate opposed making single arbitrary choice future application architecture seems interesting direction future work suggest acceptance conference contribution,5
400.json,response well answered question thanks evaluation changed proposes neural generating tree structure output scratch separate recurrence depth sibling separate topology label generation outperforms previous method benchmark ifttt dataset compared previous tree decoding method avoids manually annotating subtrees special token thus good alternative problem solid experiment synthetic dataset outperforms alternative method real world ifttt dataset couple interesting believe worth investigation firstly synthetic dataset precision drop rapidly number node vector representation sequential encoder fails provide sufficient information long sequence tree decoder good tree decoder tolerant long sequence input large tree structure believe important understand better developed example fault encoder maybe attention layer added preserve information input sequence moreover besides showing precision change number node tree might interesting investigate go number depth number width symmetricity moreover greedy search used decoding might interesting help beam search tree decoding ifttt dataset listing statistic dataset might helpful better understanding difficulty task deep tree large vocabulary language program side well written except minor typo mentioned review question general believe solid explored direction tend accept,6
482.json,well written organized presented enjoyed reading commend attention narrative explanation present methodology architecture instead addressed important application predicting medication patient using given record billing code dataset impressive useful frankly interesting typical datasets machine learning said investigation deep thought empirical application despite focus application encouraged cutting edge choice kera adadelta architecture point criticism numerical view brief anecdotal essentially negative result tsne place interpretable leaf table recognize dataset offer vast amount empirical evidence analysis might expect major algorithmic theoretical advance clear think disqualifying deeply concerning simply found underwhelming constructive recommend removing replacing meaningful analysis performance found mostly uninformative negative result think stated sentence rather large figure jargon used expertise required familiar typical iclr reader another reviewer suggested perhaps iclr right venue work certainly reviewer point medical healthcare venue suitable want cast vote keeping community benefit thoughtful depth application instead think addressed tightening point jargon making easy evaluate iclr reader stand researcher without medical experience take table faith rather getting apply well trained quantitative overall nice,6
498.json,summary explains dropout latent variable dropout variable depending unit dropped observed accordingly marginalised maximum likelihood tractable standard dropout corresponds simple monte carlo approximation introduces theoretical framework analysing discrepancy called inference training ensemble latent variable testing usually expectation activation many model becomes activation averaged weight framework introduces several notion expectation linearity allow study transition function generally layer small inference theorem give bound inference finally regularisation term introduced account minimisation inference learning experiment performed mnist cifar cifar show method potential perform better standard dropout level monte carlo dropout standard method compute real dropout output consistently training assumption ensemble course quite expensive computationally study give interesting theoretical dropout latent variable standard dropout monte carlo approximation probably widely applicable study dropout framework study inference interesting although maybe somewhat le widely applicable proposed convincing although tested simple datasets gain relatively small increased computational cost training hyper parameter introduced line typo expecatation,7
498.json,put forward entirely also sufficiently understood interpretation dropout regularization derive useful theorem estimate bound quantity interest analyzing dropout regularized network perspective furthermore introduce explicit regularization term well understood impact quantity experimental section convincingly show proposed regularization indeed expected effect perspective dropout therefore useful meaningful proposed regularization also seems positive impact model performance demonstrate small scale benchmark problem therefore belief approach large impact practitioner train model general perspective well aligned recently proposed idea dropout bayesian approximation insight theorem might enable future work direction,7
749.json,formulates number rule designing convolutional neural network architecture image processing computer vision problem essentially read like review modern architecture also proposes architectural idea inspired rule experimentally evaluated cifar cifar seem achieve relatively poor performance datasets table merit unclear sure collection rule extracted prior work warrant publication research idea summarise observation cnns choice computer vision task year summary could useful newcomer however seems boil common sense rest might suited introduction training cnns course blog post also seems skewed towards recent work fairly incremental attention given flurry resnet variant state universal convolutional neural network activation downsampled number channel increased input final layer wrong already discussed previously question design pattern think answer given nature design pattern apply time excuse making sweeping claim probably removed feel normalization put layer input sample equal footing allows backprop train effectively section paragraph vague language many possible interpretation probably clarified also seems start sentence feel seem like kind thing opinion claim corroborated experiment measurement several instance issue across connection taylor series proposed taylor series network seems tenuous think name appropriate resulting function even polynomial term represent different function particularly interesting object nonlinear function overall read like collection thought idea well delineated experimental unconvincing,2
749.json,grouped recent work convolutional neural network design specifically respect image classification identify core design principle guiding field large principle produce along associated reference include number useful correct observation asset anyone unfamiliar field explore number architecture cifar cifar guided principle collected quality reference subject grouped well valuable young researcher clearly explored many architectural change part experiment publicly available code base always nice overall writing seems jump around motivation behind design principle feel lost confusion example design pattern increase symmetry argues architectural symmetry sign beauty quality presented core design principle without justification similarly design pattern train includes training method network trained harder problem necessary improve generalization performance inference presented middle paragraph supporting reference explanation experimental portion feel scattered many different approach presented based subset design principle general approach either minor modification existing network different fractalnet pooling strategy novel architecture perform well exception fractal fractal network achieves slightly improved accuracy also introduces many network parameter increased capacity original fractalnet preliminary rating useful perhaps noble task collect distill research many source find pattern perhaps gap state field however pattern presented seem well developed include principle poorly explained furthermore innovative architecture motivated design principle either fall short achieve slightly better accuracy introducing many parameter fractal fractal network addressing topic higher level design trend appreciate additional rigorous experimentation around principle rather novel architecture presented,2
773.json,applies biclustering overcome drawback fabia proposed method performs best among biclustering method however first concern methodological point view novelty proposed method seems small replied question another reviewer gave reply convincing actually difficult follow instance figure bicluster matrix constructed outer product hidden unit could find definition furthermore could know estimated method therefore understand method performs biclustering totally sure suitable publication prons empirical performance good con novelty proposed method description unclear,4
745.json,propose parallel mechanism stochastic gradient descent method case gradient computed linear operation including least square linear regression polynomial regression problem motivation recover effect compared sequential using proposed sound combiner make combiner efficient also randomized projection matrix dimension reduction experiment show proposed method better speedup previous method like hogwild allreduce feel might fundamental misunderstanding combiner matrixm generate quite large expensive compute sequential algorithm maintains update weight vector thus requires space time number feature contrast matrix consequently space time complexity parallel practice mean need processor constant speedup infeasible proposition particularly datasets thousand million feature think need space complexity updating dimensional vector note rank matrix form complexity space reduced compute equivalently defined form product number rank matrix complexity space complexity context much smaller seriously doubt author assumption experiment strategy based incorrect assumption space complexity speedup unclear unclear computation parallel sequential algorithm bring speedup computed efficient suggest make following change make clear theoretically solid provide computational complexity step proposed algorithm convergence rate analysis convergence analysis enough like dimension reduction affect complexity,3
778.json,address reduce test time computational load dnns another factorization approach proposed show good comparison method comprehensive provides good insight,5
739.json,present algorithm effective calculation polynomial feature sparse matrix idea proper mapping matrix polynomial version order derive effective expansion algorithm analyse time complexity convincing experiment overall algorithm definitely interesting quite simple nice many possible application however superficial term experiment application proposed scheme importantly main scope iclr obvious work probably submitted better target,2
739.json,beyond expertise cannot give solid review comment regarding technique better educated guess however seems topic relevant focus iclr also quality writing requires improvement especially literature review experiment analysis,2
597.json,applies pointer network architecture wherein attention mechanism fashioned point element input sequence allowing decoder output said element order solve simple combinatorial optimization problem well known travelling salesman problem network trained reinforcement learning using actor critic method actor trained using reinforce method critic used estimate reward baseline within reinforce objective well written easy understand reinforcement learning attention framework learn structure space combinatorial problem variable size tackled appears novel importantly provides interesting research avenue revisiting classical neural based solution combinatorial optimization problem using recently developed sequence sequence approach think merit consideration conference comment important reservation take exception conclusion pointer network approach handle general type combinatorial optimization problem crux combinatorial problem practical application lie complex constraint define feasible solution simple generalization involve time window multiple salesman problem longer simple exclude possible solution enumeration solution striking previously visited instance fact many problem finding single feasible solution might general challenge relevant include discussion whether neural combinatorial optimization approach could scale important class problem understanding approach presented mostly suitable assignment problem simple constraint structure operation research literature replete large number benchmark problem become standard compare solver quality instance tsplib contains large number instance,5
597.json,proposes reinforcement learning solving combinatorial optimization problem pointer network interesting enables generalization arbitrary input size proposed method also fintunes test example active search achieve better performance proposed method theoretically interesting show combined solve combinatorial optimization problem achieve comparable performance traditional heuristic based algorithm however lack complexity comparison baseline make impossible tell whether proposed method practical value matter complicated fact proposed method run baseline hard even come meaningful unit complexity money spent hardware electricity instance viable option performance comparison taken grain salt traditional heuristic based algorithm often give better performance allowed computation controlled across algorithm,5
554.json,combine drqn eligibility trace also experiment adam optimizer optimizing network direction worth exploring experiment demonstrate benefit using eligibility trace adam atari game method novel thus primary contribution applying eligibility trace adam drqn experimental evaluation well written easy understand experiment provide quantitative detailed qualitative intuition method perform however atari game difficult tell well method perform generally showing several game domain significantly improve showing error bar multiple random seed also improve,3
333.json,provides interesting analysis condition enable generation natural looking texture quite surprising analysis quite thorough think evaluation method require work reviewer mentioned could interesting line work moving forward take much current think accepted,7
756.json,present method embedding data instance dimensional space preserve form similarity although present notion basically every trained embedding auto encoders wordvec representing item dimensional space inherently encodes similarity even looking specific case word context embeddings method novel either method almost identical similarity function presented simple word embedding lexical substitution melamud novelty claim must accurate position respect existing work addition think evaluation could done better plenty benchmark word embeddings context example,2
486.json,develops simple reasonable algorithm graph node prediction classification formulation intuitive lead simple based training easily leverage existing speedup experiment thorough compare many reasonable baseline large real benchmark datasets although quite aware literature method similar alternative link node prediction problem still think approach quite simple reasonably supported good evaluation,6
691.json,introduces reinforcement learning environment called retro learning environment interface open source libretro offer access various emulator associated game similar atari arcade learning environment generic first supported platform snes game console game added later argue snes game pose challenge atari complex graphic game mechanic several variant evaluated experiment also proposed compare learning algorihms letting compete multiplayer game like idea going toward complex game found atari environment console game easily added sound promising openai universe deepmind came though sure really need another right especially since using rom emulated game technically illegal look like cause much trouble atari might start raising eyebrow community move advanced recent game especially nintendo still make money besides introduction environment good benchmark five game value also mention contribution benchmarking technique allowing algorithm compete rather playing game seems exaggerated idea pitting core many competition decade hardly something finding reinforcement learning algorithm tend specialize opponent also particular surprising overall believe feel brings enough table major conference mean however environment find spot somewhat crowded space game playing framework small comment lot typo many mention said infinite mario still serf benchmark platform however know shutdown nintendo happy requires emulator computer version console game file upon initialization rather file emulator provided different requires emulator stella also provided dddqn result super mario clear figure display zero using reward shaping reference seems incomplete,3
556.json,appreciate work think clear enough moreover local minimia time show except figure solution found necessarily local minimum talk fact slice convex problem look like one show well known first order method fail deal certain convex conditioned problem even dimensional noiseless case place solution fail make progress necessarily local minimum sentence like given suggest study superficial interesting empirical observation often observe incremental improvement optimization method decrease rapidly even convex problem,3
612.json,summary make contribution next step prediction input output space affine transforms adjacent frame evaluation method quality generated data assessed measuring reduction performance another classifier tested generated data show according metric proposed work better baseline model including recent work mathieu us adversarial training strength attempt solve major problem unsupervised learning video evaluating show using transform space prevent blurring problem large extent main aim show generated data reduces performance much le extent baseline validates assumption video approximated quite time step sequence affine transforms starting initial frame weakness proposed metric make sense truly care performance particular classifier given task significantly narrow scope applicability metric arguably important reason unsupervised learning come representation widely applicable across variety task proposed metric help evaluate generative model designed achieve objective possible generative model compared interact idiosyncrasy chosen classifier unintended way therefore hard draw strong conclusion relative merit generative model experiment ameliorate several different classifier dual stream network state method show ranking different generative model consistent across choice classifier adding experiment help increase certainty conclusion drawn using input frame sampled seems like little context really expect extrapolate kind motion seen idea working space affine transforms much appealing shown really generated trivial motion pattern currently motion pattern seem almost linear extrapolation predicts motion access content get access previous motion seems might disadvantage motion predictor cannot cue like object boundary decide motion field collide probably easier argue occlusion content space quality clarity clearly written easy follow assumption clearly specified validated experimental detail seem adequate originality idea generating video predicting motion used previously several recent paper also idea however exact implementation proposed evaluation protocol novel significance proposed evaluation method interesting alternative especially extended include multiple classifier representative different state approach given hard evaluate generative model video could help start effort standardize benchmark minor comment suggestion caption table column show accuracy test taking different number input frame input input refers input classifier output next step prediction however next sentence approach map time patch time stride take frame input input refers input next step prediction might good idea rephrase sentence make distinction clear order better understand space affine transform parameter might help include histogram parameter help glance typical range parameter expect outlier order compare transforms instead could consider close identity metric performance classifier ground truth data upper bound performance generative strictly true possible though highly unlikely generative might make data look cleaner sharper highlight aspect could improve performance classifier even compared ground truth especially true generative access classifier could make classifier fire highlight discriminative feature generated output overall proposes future prediction affine transform space reduce blurriness make video look relatively realistic least classifier however improved showing predict trivial motion flow experiment strengthened adding classifier besides,5
632.json,summary propose scoring function knowledge base embedding scoring function called transgaussian novel take generalization well known transe scoring function proposed function tested task knowledge base completion question answering overall judgment think proposed work interesting idea worth explore presentation experimental section problem regarding presentation understand attention intended standardly literature plus hardly anything share memory network neural turing machine parallel make convincing regarding experimental section fair comparison test standard benchmark reporting state model finally lack discussion insight behavior proposed detailed comment section calculate context loose order relation make sense,3
697.json,dear response clarified confusion still following question response said first contribution different formulation divide word embedding learning step step look rank riemannian optimization step factorizes matrix claiming outperforms previous approach directly optimizes since result factor provide intuition justification proposed method work better though parameterized differently first step method previous method optimizing rank matrix admittedly riemannian optimization avoids rotational degree freedom invertible matrix mentioning certain point source gain learning curve objective help riemannian optimization indeed effective another detail could easily find following said disadvantage approach factor directly reflect similarity multiply factor optimizers factorize product using method section downstream task sure cause much difference performance overall think always interesting apply advanced optimization technique machine learning problem current stronger machine learning perspective thorough comparison discussion mentioned provided hand expertise leave reviewer decide significance experimental,4
